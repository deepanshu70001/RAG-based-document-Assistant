{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64662f11",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "first step of rag pipeline\n",
    "\n",
    "pdf >> parshing >> document >> chunking >> Embedding >> Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ea2e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Document Data Structure\n",
    "#using langchain (core comp : page content(str),metadata(dict (source file name , page , time )) )\n",
    "\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27d8a419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'pages': 1, 'Data': '2026-01-01'}, page_content='hi , this is example for rag pipeline ')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#meta data help in content filtering\n",
    "doc=Document(\n",
    "    page_content=\"hi , this is example for rag pipeline \",\n",
    "    metadata={\n",
    "        \"source\":\"example.txt\",\n",
    "        \"pages\":1,\n",
    "        \"Data\":\"2026-01-01\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f724dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##creating simple text file\n",
    "import os\n",
    "os.makedirs(\"../Data/TextFiles\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d84fb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/TextFiles/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "\"../data/TextFiles/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"✅ Sample text files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5c0ca69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#reading text file to make document\n",
    "#textloader\n",
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5d8f299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/TextFiles/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "loader=TextLoader(\"../data/TextFiles/python_intro.txt\",encoding=\"utf-8\")\n",
    "document=loader.load() # load() convert file Data into document\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbf3af3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## directory loader\n",
    "#reading all files present at a given place\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "dir_loader=DirectoryLoader( #specify config of loader\n",
    "    \"../Data/TextFiles\", #file dir\n",
    "    glob=\"**/*.txt\", #pattern to match\n",
    "    show_progress=True,\n",
    "    loader_kwargs={'encoding':\"utf-8\"},\n",
    "    loader_cls=TextLoader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6da8e6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 2005.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '..\\\\Data\\\\TextFiles\\\\machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    '), Document(metadata={'source': '..\\\\Data\\\\TextFiles\\\\python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dirdoc=dir_loader.load()\n",
    "print(dirdoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb97b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:19<00:00,  9.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 0, 'page_label': 'Cover'}, page_content='Andreas C. Müller & Sarah Guido\\nIntroduction to \\nMachine \\nLearning  \\nwith P y t h o n   \\nA GUIDE FOR DATA SCIENTISTS'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 1, 'page_label': 'BackCover'}, page_content=''),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 2, 'page_label': 'i'}, page_content='Andreas C. Müller and Sarah Guido\\nIntroduction to Machine Learning\\nwith Python\\nA Guide for Data Scientists\\nBoston Farnham Sebastopol TokyoBeijing Boston Farnham Sebastopol TokyoBeijing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 3, 'page_label': 'ii'}, page_content='978-1-449-36941-5\\n[LSI]\\nIntroduction to Machine Learning with Python\\nby Andreas C. Müller and Sarah Guido\\nCopyright © 2017 Sarah Guido, Andreas Müller. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles ( http://safaribooksonline.com). For more information, contact our corporate/\\ninstitutional sales department: 800-998-9938 or corporate@oreilly.com.\\nEditor: Dawn Schanafelt\\nProduction Editor: Kristen Brown\\nCopyeditor: Rachel Head\\nProofreader: Jasmine Kwityn\\nIndexer: Judy McConville\\nInterior Designer: David Futato\\nCover Designer: Karen Montgomery\\nIllustrator: Rebecca Demarest\\nOctober 2016:  First Edition\\nRevision History for the First Edition\\n2016-09-22: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781449369415 for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Introduction to Machine Learning with\\nPython, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk. If any code samples or other technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 4, 'page_label': 'iii'}, page_content='Table of Contents\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  vii\\n1. Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\\nWhy Machine Learning?                                                                                                   1\\nProblems Machine Learning Can Solve                                                                      2\\nKnowing Y our Task and Knowing Y our Data                                                            4\\nWhy Python?                                                                                                                      5\\nscikit-learn                                                                                                                          5\\nInstalling scikit-learn                                                                                                     6\\nEssential Libraries and Tools                                                                                            7\\nJupyter Notebook                                                                                                           7\\nNumPy                                                                                                                             7\\nSciPy                                                                                                                                 8\\nmatplotlib                                                                                                                        9\\npandas                                                                                                                            10\\nmglearn                                                                                                                          11\\nPython 2 Versus Python 3                                                                                               12\\nVersions Used in this Book                                                                                             12\\nA First Application: Classifying Iris Species                                                                13\\nMeet the Data                                                                                                                14\\nMeasuring Success: Training and Testing Data                                                        17\\nFirst Things First: Look at Y our Data                                                                        19\\nBuilding Y our First Model: k-Nearest Neighbors                                                    20\\nMaking Predictions                                                                                                      22\\nEvaluating the Model                                                                                                   22\\nSummary and Outlook                                                                                                   23\\niii'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 5, 'page_label': 'iv'}, page_content='2. Supervised Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  25\\nClassification and Regression                                                                                         25\\nGeneralization, Overfitting, and Underfitting                                                             26\\nRelation of Model Complexity to Dataset Size                                                         29\\nSupervised Machine Learning Algorithms                                                                  29\\nSome Sample Datasets                                                                                                 30\\nk-Nearest Neighbors                                                                                                    35\\nLinear Models                                                                                                               45\\nNaive Bayes Classifiers                                                                                                 68\\nDecision Trees                                                                                                               70\\nEnsembles of Decision Trees                                                                                      83\\nKernelized Support Vector Machines                                                                        92\\nNeural Networks (Deep Learning)                                                                          104\\nUncertainty Estimates from Classifiers                                                                      119\\nThe Decision Function                                                                                              120\\nPredicting Probabilities                                                                                             122\\nUncertainty in Multiclass Classification                                                                 124\\nSummary and Outlook                                                                                                 127\\n3. Unsupervised Learning and Preprocessing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  131\\nTypes of Unsupervised Learning                                                                                 131\\nChallenges in Unsupervised Learning                                                                        132\\nPreprocessing and Scaling                                                                                            132\\nDifferent Kinds of Preprocessing                                                                             133\\nApplying Data Transformations                                                                               134\\nScaling Training and Test Data the Same Way                                                       136\\nThe Effect of Preprocessing on Supervised Learning                                           138\\nDimensionality Reduction, Feature Extraction, and Manifold Learning              140\\nPrincipal Component Analysis (PCA)                                                                    140\\nNon-Negative Matrix Factorization (NMF)                                                           156\\nManifold Learning with t-SNE                                                                                 163\\nClustering                                                                                                                        168\\nk-Means Clustering                                                                                                    168\\nAgglomerative Clustering                                                                                         182\\nDBSCAN                                                                                                                     187\\nComparing and Evaluating Clustering Algorithms                                              191\\nSummary of Clustering Methods                                                                             207\\nSummary and Outlook                                                                                                 208\\n4. Representing Data and Engineering Features. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  211\\nCategorical Variables                                                                                                     212\\nOne-Hot-Encoding (Dummy Variables)                                                                213\\niv | Table of Contents'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 6, 'page_label': 'v'}, page_content='Numbers Can Encode Categoricals                                                                         218\\nBinning, Discretization, Linear Models, and Trees                                                   220\\nInteractions and Polynomials                                                                                      224\\nUnivariate Nonlinear Transformations                                                                      232\\nAutomatic Feature Selection                                                                                        236\\nUnivariate Statistics                                                                                                    236\\nModel-Based Feature Selection                                                                                238\\nIterative Feature Selection                                                                                         240\\nUtilizing Expert Knowledge                                                                                         242\\nSummary and Outlook                                                                                                 250\\n5. Model Evaluation and Improvement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  251\\nCross-Validation                                                                                                            252\\nCross-Validation in scikit-learn                                                                               253\\nBenefits of Cross-Validation                                                                                     254\\nStratified k-Fold Cross-Validation and Other Strategies                                      254\\nGrid Search                                                                                                                     260\\nSimple Grid Search                                                                                                    261\\nThe Danger of Overfitting the Parameters and the Validation Set                     261\\nGrid Search with Cross-Validation                                                                          263\\nEvaluation Metrics and Scoring                                                                                   275\\nKeep the End Goal in Mind                                                                                      275\\nMetrics for Binary Classification                                                                             276\\nMetrics for Multiclass Classification                                                                       296\\nRegression Metrics                                                                                                     299\\nUsing Evaluation Metrics in Model Selection                                                        300\\nSummary and Outlook                                                                                                 302\\n6. Algorithm Chains and Pipelines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  305\\nParameter Selection with Preprocessing                                                                    306\\nBuilding Pipelines                                                                                                          308\\nUsing Pipelines in Grid Searches                                                                                 309\\nThe General Pipeline Interface                                                                                    312\\nConvenient Pipeline Creation with make_pipeline                                              313\\nAccessing Step Attributes                                                                                          314\\nAccessing Attributes in a Grid-Searched Pipeline                                                 315\\nGrid-Searching Preprocessing Steps and Model Parameters                                  317\\nGrid-Searching Which Model To Use                                                                        319\\nSummary and Outlook                                                                                                 320\\n7. Working with Text Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  323\\nTypes of Data Represented as Strings                                                                         323\\nTable of Contents | v'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 7, 'page_label': 'vi'}, page_content='Example Application: Sentiment Analysis of Movie Reviews                                 325\\nRepresenting Text Data as a Bag of Words                                                                 327\\nApplying Bag-of-Words to a Toy Dataset                                                               329\\nBag-of-Words for Movie Reviews                                                                            330\\nStopwords                                                                                                                       334\\nRescaling the Data with tf–idf                                                                                      336\\nInvestigating Model Coefficients                                                                                 338\\nBag-of-Words with More Than One Word (n-Grams)                                            339\\nAdvanced Tokenization, Stemming, and Lemmatization                                        344\\nTopic Modeling and Document Clustering                                                               347\\nLatent Dirichlet Allocation                                                                                       348\\nSummary and Outlook                                                                                                 355\\n8. Wrapping Up. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  357\\nApproaching a Machine Learning Problem                                                               357\\nHumans in the Loop                                                                                                  358\\nFrom Prototype to Production                                                                                    359\\nTesting Production Systems                                                                                         359\\nBuilding Y our Own Estimator                                                                                     360\\nWhere to Go from Here                                                                                                361\\nTheory                                                                                                                          361\\nOther Machine Learning Frameworks and Packages                                           362\\nRanking, Recommender Systems, and Other Kinds of Learning                       363\\nProbabilistic Modeling, Inference, and Probabilistic Programming                  363\\nNeural Networks                                                                                                        364\\nScaling to Larger Datasets                                                                                         364\\nHoning Y our Skills                                                                                                     365\\nConclusion                                                                                                                      366\\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  367\\nvi | Table of Contents'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 8, 'page_label': 'vii'}, page_content='Preface\\nMachine learning is an integral part of many commercial applications and research\\nprojects today, in areas ranging from medical diagnosis and treatment to finding your\\nfriends on social networks. Many people think that machine learning can only be\\napplied by large companies with extensive research teams. In this book, we want to\\nshow you how easy it can be to build machine learning solutions yourself, and how to\\nbest go about it. With the knowledge in this book, you can build your own system for\\nfinding out how people feel on Twitter, or making predictions about global warming.\\nThe applications of machine learning are endless and, with the amount of data avail‐\\nable today, mostly limited by your imagination.\\nWho Should Read This Book\\nThis book is for current and aspiring machine learning practitioners looking to\\nimplement solutions to real-world machine learning problems. This is an introduc‐\\ntory book requiring no previous knowledge of machine learning or artificial intelli‐\\ngence (AI). We focus on using Python and the scikit-learn library, and work\\nthrough all the steps to create a successful machine learning application. The meth‐\\nods we introduce will be helpful for scientists and researchers, as well as data scien‐\\ntists working on commercial applications. Y ou will get the most out of the book if you\\nare somewhat familiar with Python and the NumPy and matplotlib libraries.\\nWe made a conscious effort not to focus too much on the math, but rather on the\\npractical aspects of using machine learning algorithms. As mathematics (probability\\ntheory, in particular) is the foundation upon which machine learning is built, we\\nwon’t go into the analysis of the algorithms in great detail. If you are interested in the\\nmathematics of machine learning algorithms, we recommend the book The Elements\\nof Statistical Learning  (Springer) by Trevor Hastie, Robert Tibshirani, and Jerome\\nFriedman, which is available for free at the authors’ website . We will also not describe\\nhow to write machine learning algorithms from scratch, and will instead focus on\\nvii'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 9, 'page_label': 'viii'}, page_content='how to use the large array of models already implemented in scikit-learn and other\\nlibraries.\\nWhy We Wrote This Book\\nThere are many books on machine learning and AI. However, all of them are meant\\nfor graduate students or PhD students in computer science, and they’re full of\\nadvanced mathematics. This is in stark contrast with how machine learning is being\\nused, as a commodity tool in research and commercial applications. Today, applying\\nmachine learning does not require a PhD. However, there are few resources out there\\nthat fully cover all the important aspects of implementing machine learning in prac‐\\ntice, without requiring you to take advanced math courses. We hope this book will\\nhelp people who want to apply machine learning without reading up on years’ worth\\nof calculus, linear algebra, and probability theory.\\nNavigating This Book\\nThis book is organized roughly as follows:\\n• Chapter 1  introduces the fundamental concepts of machine learning and its\\napplications, and describes the setup we will be using throughout the book.\\n• Chapters 2 and 3 describe the actual machine learning algorithms that are most\\nwidely used in practice, and discuss their advantages and shortcomings.\\n• Chapter 4 discusses the importance of how we represent data that is processed by\\nmachine learning, and what aspects of the data to pay attention to.\\n• Chapter 5 covers advanced methods for model evaluation and parameter tuning,\\nwith a particular focus on cross-validation and grid search.\\n• Chapter 6 explains the concept of pipelines for chaining models and encapsulat‐\\ning your workflow.\\n• Chapter 7 shows how to apply the methods described in earlier chapters to text\\ndata, and introduces some text-specific processing techniques.\\n• Chapter 8 offers a high-level overview, and includes references to more advanced\\ntopics.\\nWhile Chapters 2 and 3 provide the actual algorithms, understanding all of these\\nalgorithms might not be necessary for a beginner. If you need to build a machine\\nlearning system ASAP , we suggest starting with Chapter 1 and the opening sections of\\nChapter 2, which introduce all the core concepts. Y ou can then skip to “Summary and\\nOutlook” on page 127 in Chapter 2, which includes a list of all the supervised models\\nthat we cover. Choose the model that best fits your needs and flip back to read the\\nviii | Preface'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 10, 'page_label': 'ix'}, page_content='section devoted to it for details. Then you can use the techniques in Chapter 5 to eval‐\\nuate and tune your model.\\nOnline Resources\\nWhile studying this book, definitely refer to the scikit-learn website for more in-\\ndepth documentation of the classes and functions, and many examples. There is also\\na video course created by Andreas Müller, “ Advanced Machine Learning with scikit-\\nlearn, ” that supplements this book. Y ou can find it at http://bit.ly/\\nadvanced_machine_learning_scikit-learn.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program ele‐\\nments such as variable or function names, databases, data types, environment\\nvariables, statements, and keywords. Also used for commands and module and\\npackage names.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by values deter‐\\nmined by context.\\nThis element signifies a tip or suggestion.\\nThis element signifies a general note.\\nPreface | ix'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 11, 'page_label': 'x'}, page_content='This icon indicates a warning or caution.\\nUsing Code Examples\\nSupplemental material (code examples, IPython notebooks, etc.) is available for\\ndownload at https://github.com/amueller/introduction_to_ml_with_python.\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “An Introduction to Machine Learning\\nwith Python by Andreas C. Müller and Sarah Guido (O’Reilly). Copyright 2017 Sarah\\nGuido and Andreas Müller, 978-1-449-36941-5. ”\\nIf you feel your use of code examples falls outside fair use or the permission given\\nabove, feel free to contact us at permissions@oreilly.com.\\nSafari® Books Online\\nSafari Books Online is an on-demand digital library that deliv‐\\ners expert content in both book and video form from the\\nworld’s leading authors in technology and business.\\nTechnology professionals, software developers, web designers, and business and crea‐\\ntive professionals use Safari Books Online as their primary resource for research,\\nproblem solving, learning, and certification training.\\nSafari Books Online offers a range of plans and pricing  for enterprise, government,\\neducation, and individuals.\\nMembers have access to thousands of books, training videos, and prepublication\\nmanuscripts in one fully searchable database from publishers like O’Reilly Media,\\nPrentice Hall Professional, Addison-Wesley Professional, Microsoft Press, Sams, Que,\\nx | Preface'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 12, 'page_label': 'xi'}, page_content='Peachpit Press, Focal Press, Cisco Press, John Wiley & Sons, Syngress, Morgan Kauf‐\\nmann, IBM Redbooks, Packt, Adobe Press, FT Press, Apress, Manning, New Riders,\\nMcGraw-Hill, Jones & Bartlett, Course Technology, and hundreds more. For more\\ninformation about Safari Books Online, please visit us online.\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. Y ou can access this page at http://bit.ly/intro-machine-learning-python.\\nTo comment or ask technical questions about this book, send email to bookques‐\\ntions@oreilly.com.\\nFor more information about our books, courses, conferences, and news, see our web‐\\nsite at http://www.oreilly.com.\\nFind us on Facebook: http://facebook.com/oreilly\\nFollow us on Twitter: http://twitter.com/oreillymedia\\nWatch us on Y ouTube: http://www.youtube.com/oreillymedia\\nAcknowledgments\\nFrom Andreas\\nWithout the help and support of a large group of people, this book would never have\\nexisted.\\nI would like to thank the editors, Meghan Blanchette, Brian MacDonald, and in par‐\\nticular Dawn Schanafelt, for helping Sarah and me make this book a reality.\\nI want to thank my reviewers, Thomas Caswell, Olivier Grisel, Stefan van der Walt,\\nand John Myles White, who took the time to read the early versions of this book and\\nprovided me with invaluable feedback—in addition to being some of the corner‐\\nstones of the scientific open source ecosystem.\\nPreface | xi'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 13, 'page_label': 'xii'}, page_content='I am forever thankful for the welcoming open source scientific Python community,\\nespecially the contributors to scikit-learn. Without the support and help from this\\ncommunity, in particular from Gael Varoquaux, Alex Gramfort, and Olivier Grisel, I\\nwould never have become a core contributor to scikit-learn or learned to under‐\\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\\nutors who donate their time to improve and maintain this package.\\nI’m also thankful for the discussions with many of my colleagues and peers that hel‐\\nped me understand the challenges of machine learning and gave me ideas for struc‐\\nturing a textbook. Among the people I talk to about machine learning, I specifically\\nwant to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe,\\nHugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\\nand Dan Cervone.\\nMy thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader\\nof an early version of this book, and helped me shape it in many ways.\\nOn the personal side, I want to thank my parents, Harald and Margot, and my sister,\\nMiriam, for their continuing support and encouragement. I also want to thank the\\nmany people in my life whose love and friendship gave me the energy and support to\\nundertake such a challenging task.\\nFrom Sarah\\nI would like to thank Meg Blanchette, without whose help and guidance this project\\nwould not have even existed. Thanks to Celia La and Brian Carlson for reading in the\\nearly days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\\nto DTS, for your everlasting and endless support.\\nxii | Preface'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 14, 'page_label': '1'}, page_content='CHAPTER 1\\nIntroduction\\nMachine learning is about extracting knowledge from data. It is a research field at the\\nintersection of statistics, artificial intelligence, and computer science and is also\\nknown as predictive analytics or statistical learning. The application of machine\\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\\nmatic recommendations of which movies to watch, to what food to order or which\\nproducts to buy, to personalized online radio and recognizing your friends in your\\nphotos, many modern websites and devices have machine learning algorithms at their\\ncore. When you look at a complex website like Facebook, Amazon, or Netflix, it is\\nvery likely that every part of the site contains multiple machine learning models.\\nOutside of commercial applications, machine learning has had a tremendous influ‐\\nence on the way data-driven research is done today. The tools introduced in this book\\nhave been applied to diverse scientific problems such as understanding stars, finding\\ndistant planets, discovering new particles, analyzing DNA sequences, and providing\\npersonalized cancer treatments.\\nY our application doesn’t need to be as large-scale or world-changing as these exam‐\\nples in order to benefit from machine learning, though. In this chapter, we will\\nexplain why machine learning has become so popular and discuss what kinds of\\nproblems can be solved using machine learning. Then, we will show you how to build\\nyour first machine learning model, introducing important concepts along the way.\\nWhy Machine Learning?\\nIn the early days of “intelligent” applications, many systems used handcoded rules of\\n“if ” and “else” decisions to process data or adjust to user input. Think of a spam filter\\nwhose job is to move the appropriate incoming email messages to a spam folder. Y ou\\ncould make up a blacklist of words that would result in an email being marked as\\n1'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 15, 'page_label': '2'}, page_content='spam. This would be an example of using an expert-designed rule system to design an\\n“intelligent” application. Manually crafting decision rules is feasible for some applica‐\\ntions, particularly those in which humans have a good understanding of the process\\nto model. However, using handcoded rules to make decisions has two major disad‐\\nvantages:\\n• The logic required to make a decision is specific to a single domain and task.\\nChanging the task even slightly might require a rewrite of the whole system.\\n• Designing rules requires a deep understanding of how a decision should be made\\nby a human expert.\\nOne example of where this handcoded approach will fail is in detecting faces in\\nimages. Today, every smartphone can detect a face in an image. However, face detec‐\\ntion was an unsolved problem until as recently as 2001. The main problem is that the\\nway in which pixels (which make up an image in a computer) are “perceived” by the\\ncomputer is very different from how humans perceive a face. This difference in repre‐\\nsentation makes it basically impossible for a human to come up with a good set of\\nrules to describe what constitutes a face in a digital image.\\nUsing machine learning, however, simply presenting a program with a large collec‐\\ntion of images of faces is enough for an algorithm to determine what characteristics\\nare needed to identify a face.\\nProblems Machine Learning Can Solve\\nThe most successful kinds of machine learning algorithms are those that automate\\ndecision-making processes by generalizing from known examples. In this setting,\\nwhich is known as supervised learning, the user provides the algorithm with pairs of\\ninputs and desired outputs, and the algorithm finds a way to produce the desired out‐\\nput given an input. In particular, the algorithm is able to create an output for an input\\nit has never seen before without any help from a human. Going back to our example\\nof spam classification, using machine learning, the user provides the algorithm with a\\nlarge number of emails (which are the input), together with information about\\nwhether any of these emails are spam (which is the desired output). Given a new\\nemail, the algorithm will then produce a prediction as to whether the new email is\\nspam.\\nMachine learning algorithms that learn from input/output pairs are called supervised\\nlearning algorithms because a “teacher” provides supervision to the algorithms in the\\nform of the desired outputs for each example that they learn from. While creating a\\ndataset of inputs and outputs is often a laborious manual process, supervised learning\\nalgorithms are well understood and their performance is easy to measure. If your\\napplication can be formulated as a supervised learning problem, and you are able to\\n2 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 16, 'page_label': '3'}, page_content='create a dataset that includes the desired outcome, machine learning will likely be\\nable to solve your problem.\\nExamples of supervised machine learning tasks include:\\nIdentifying the zip code from handwritten digits on an envelope\\nHere the input is a scan of the handwriting, and the desired output is the actual\\ndigits in the zip code. To create a dataset for building a machine learning model,\\nyou need to collect many envelopes. Then you can read the zip codes yourself\\nand store the digits as your desired outcomes.\\nDetermining whether a tumor is benign based on a medical image\\nHere the input is the image, and the output is whether the tumor is benign. To\\ncreate a dataset for building a model, you need a database of medical images. Y ou\\nalso need an expert opinion, so a doctor needs to look at all of the images and\\ndecide which tumors are benign and which are not. It might even be necessary to\\ndo additional diagnosis beyond the content of the image to determine whether\\nthe tumor in the image is cancerous or not.\\nDetecting fraudulent activity in credit card transactions\\nHere the input is a record of the credit card transaction, and the output is\\nwhether it is likely to be fraudulent or not. Assuming that you are the entity dis‐\\ntributing the credit cards, collecting a dataset means storing all transactions and\\nrecording if a user reports any transaction as fraudulent.\\nAn interesting thing to note about these examples is that although the inputs and out‐\\nputs look fairly straightforward, the data collection process for these three tasks is\\nvastly different. While reading envelopes is laborious, it is easy and cheap. Obtaining\\nmedical imaging and diagnoses, on the other hand, requires not only expensive\\nmachinery but also rare and expensive expert knowledge, not to mention the ethical\\nconcerns and privacy issues. In the example of detecting credit card fraud, data col‐\\nlection is much simpler. Y our customers will provide you with the desired output, as\\nthey will report fraud. All you have to do to obtain the input/output pairs of fraudu‐\\nlent and nonfraudulent activity is wait.\\nUnsupervised algorithms are the other type of algorithm that we will cover in this\\nbook. In unsupervised learning, only the input data is known, and no known output\\ndata is given to the algorithm. While there are many successful applications of these\\nmethods, they are usually harder to understand and evaluate.\\nExamples of unsupervised learning include:\\nIdentifying topics in a set of blog posts\\nIf you have a large collection of text data, you might want to summarize it and\\nfind prevalent themes in it. Y ou might not know beforehand what these topics\\nare, or how many topics there might be. Therefore, there are no known outputs.\\nWhy Machine Learning? | 3'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 17, 'page_label': '4'}, page_content='Segmenting customers into groups with similar preferences\\nGiven a set of customer records, you might want to identify which customers are\\nsimilar, and whether there are groups of customers with similar preferences. For\\na shopping site, these might be “parents, ” “bookworms, ” or “gamers. ” Because you\\ndon’t know in advance what these groups might be, or even how many there are,\\nyou have no known outputs.\\nDetecting abnormal access patterns to a website\\nTo identify abuse or bugs, it is often helpful to find access patterns that are differ‐\\nent from the norm. Each abnormal pattern might be very different, and you\\nmight not have any recorded instances of abnormal behavior. Because in this\\nexample you only observe traffic, and you don’t know what constitutes normal\\nand abnormal behavior, this is an unsupervised problem.\\nFor both supervised and unsupervised learning tasks, it is important to have a repre‐\\nsentation of your input data that a computer can understand. Often it is helpful to\\nthink of your data as a table. Each data point that you want to reason about (each\\nemail, each customer, each transaction) is a row, and each property that describes that\\ndata point (say, the age of a customer or the amount or location of a transaction) is a\\ncolumn. Y ou might describe users by their age, their gender, when they created an\\naccount, and how often they have bought from your online shop. Y ou might describe\\nthe image of a tumor by the grayscale values of each pixel, or maybe by using the size,\\nshape, and color of the tumor.\\nEach entity or row here is known as a sample (or data point) in machine learning,\\nwhile the columns—the properties that describe these entities—are called features.\\nLater in this book we will go into more detail on the topic of building a good repre‐\\nsentation of your data, which is called feature extraction or feature engineering. Y ou\\nshould keep in mind, however, that no machine learning algorithm will be able to\\nmake a prediction on data for which it has no information. For example, if the only\\nfeature that you have for a patient is their last name, no algorithm will be able to pre‐\\ndict their gender. This information is simply not contained in your data. If you add\\nanother feature that contains the patient’s first name, you will have much better luck,\\nas it is often possible to tell the gender by a person’s first name.\\nKnowing Your Task and Knowing Your Data\\nQuite possibly the most important part in the machine learning process is under‐\\nstanding the data you are working with and how it relates to the task you want to\\nsolve. It will not be effective to randomly choose an algorithm and throw your data at\\nit. It is necessary to understand what is going on in your dataset before you begin\\nbuilding a model. Each algorithm is different in terms of what kind of data and what\\nproblem setting it works best for. While you are building a machine learning solution,\\nyou should answer, or at least keep in mind, the following questions:\\n4 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 18, 'page_label': '5'}, page_content='• What question(s) am I trying to answer? Do I think the data collected can answer\\nthat question?\\n• What is the best way to phrase my question(s) as a machine learning problem?\\n• Have I collected enough data to represent the problem I want to solve?\\n• What features of the data did I extract, and will these enable the right\\npredictions?\\n• How will I measure success in my application?\\n• How will the machine learning solution interact with other parts of my research\\nor business product?\\nIn a larger context, the algorithms and methods in machine learning are only one\\npart of a greater process to solve a particular problem, and it is good to keep the big\\npicture in mind at all times. Many people spend a lot of time building complex\\nmachine learning solutions, only to find out they don’t solve the right problem.\\nWhen going deep into the technical aspects of machine learning (as we will in this\\nbook), it is easy to lose sight of the ultimate goals. While we will not discuss the ques‐\\ntions listed here in detail, we still encourage you to keep in mind all the assumptions\\nthat you might be making, explicitly or implicitly, when you start building machine\\nlearning models.\\nWhy Python?\\nPython has become the lingua franca for many data science applications. It combines\\nthe power of general-purpose programming languages with the ease of use of\\ndomain-specific scripting languages like MATLAB or R. Python has libraries for data\\nloading, visualization, statistics, natural language processing, image processing, and\\nmore. This vast toolbox provides data scientists with a large array of general- and\\nspecial-purpose functionality. One of the main advantages of using Python is the abil‐\\nity to interact directly with the code, using a terminal or other tools like the Jupyter\\nNotebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\\nmentally iterative processes, in which the data drives the analysis. It is essential for\\nthese processes to have tools that allow quick iteration and easy interaction.\\nAs a general-purpose programming language, Python also allows for the creation of\\ncomplex graphical user interfaces (GUIs) and web services, and for integration into\\nexisting systems.\\nscikit-learn\\nscikit-learn is an open source project, meaning that it is free to use and distribute,\\nand anyone can easily obtain the source code to see what is going on behind the\\nWhy Python? | 5'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 19, 'page_label': '6'}, page_content='scenes. The scikit-learn project is constantly being developed and improved, and it\\nhas a very active user community. It contains a number of state-of-the-art machine\\nlearning algorithms, as well as comprehensive documentation about each algorithm.\\nscikit-learn is a very popular tool, and the most prominent Python library for\\nmachine learning. It is widely used in industry and academia, and a wealth of tutori‐\\nals and code snippets are available online. scikit-learn works well with a number of\\nother scientific Python tools, which we will discuss later in this chapter.\\nWhile reading this, we recommend that you also browse the scikit-learn user guide \\nand API documentation for additional details on and many more options for each\\nalgorithm. The online documentation is very thorough, and this book will provide\\nyou with all the prerequisites in machine learning to understand it in detail.\\nInstalling scikit-learn\\nscikit-learn depends on two other Python packages, NumPy and SciPy. For plot‐\\nting and interactive development, you should also install matplotlib, IPython, and\\nthe Jupyter Notebook. We recommend using one of the following prepackaged\\nPython distributions, which will provide the necessary packages:\\nAnaconda\\nA Python distribution made for large-scale data processing, predictive analytics,\\nand scientific computing. Anaconda comes with NumPy, SciPy, matplotlib,\\npandas, IPython, Jupyter Notebook, and scikit-learn. Available on Mac OS,\\nWindows, and Linux, it is a very convenient solution and is the one we suggest\\nfor people without an existing installation of the scientific Python packages. Ana‐\\nconda now also includes the commercial Intel MKL library for free. Using MKL\\n(which is done automatically when Anaconda is installed) can give significant\\nspeed improvements for many algorithms in scikit-learn.\\nEnthought Canopy\\nAnother Python distribution for scientific computing. This comes with NumPy,\\nSciPy, matplotlib, pandas, and IPython, but the free version does not come with\\nscikit-learn. If you are part of an academic, degree-granting institution, you\\ncan request an academic license and get free access to the paid subscription ver‐\\nsion of Enthought Canopy. Enthought Canopy is available for Python 2.7.x, and\\nworks on Mac OS, Windows, and Linux.\\nPython(x,y)\\nA free Python distribution for scientific computing, specifically for Windows.\\nPython(x,y) comes with NumPy, SciPy, matplotlib, pandas, IPython, and\\nscikit-learn.\\n6 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 20, 'page_label': '7'}, page_content='1 If you are unfamiliar with NumPy or matplotlib, we recommend reading the first chapter of the SciPy Lec‐\\nture Notes.\\nIf you already have a Python installation set up, you can use pip to install all of these\\npackages:\\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\\nEssential Libraries and Tools\\nUnderstanding what scikit-learn is and how to use it is important, but there are a\\nfew other libraries that will enhance your experience. scikit-learn is built on top of\\nthe NumPy and SciPy scientific Python libraries. In addition to NumPy and SciPy, we\\nwill be using pandas and matplotlib. We will also introduce the Jupyter Notebook,\\nwhich is a browser-based interactive programming environment. Briefly, here is what\\nyou should know about these tools in order to get the most out of scikit-learn.1\\nJupyter Notebook\\nThe Jupyter Notebook is an interactive environment for running code in the browser.\\nIt is a great tool for exploratory data analysis and is widely used by data scientists.\\nWhile the Jupyter Notebook supports many programming languages, we only need\\nthe Python support. The Jupyter Notebook makes it easy to incorporate code, text,\\nand images, and all of this book was in fact written as a Jupyter Notebook. All of the\\ncode examples we include can be downloaded from GitHub.\\nNumPy\\nNumPy is one of the fundamental packages for scientific computing in Python. It\\ncontains functionality for multidimensional arrays, high-level mathematical func‐\\ntions such as linear algebra operations and the Fourier transform, and pseudorandom\\nnumber generators.\\nIn scikit-learn, the NumPy array is the fundamental data structure. scikit-learn\\ntakes in data in the form of NumPy arrays. Any data you’re using will have to be con‐\\nverted to a NumPy array. The core functionality of NumPy is the ndarray class, a\\nmultidimensional ( n-dimensional) array. All elements of the array must be of the\\nsame type. A NumPy array looks like this:\\nIn[2]:\\nimport numpy as np\\nx = np.array([[1, 2, 3], [4, 5, 6]])\\nprint(\"x:\\\\n{}\".format(x))\\nEssential Libraries and Tools | 7'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 21, 'page_label': '8'}, page_content='Out[2]:\\nx:\\n[[1 2 3]\\n [4 5 6]]\\nWe will be using NumPy a lot in this book, and we will refer to objects of the NumPy\\nndarray class as “NumPy arrays” or just “arrays. ”\\nSciPy\\nSciPy is a collection of functions for scientific computing in Python. It provides,\\namong other functionality, advanced linear algebra routines, mathematical function\\noptimization, signal processing, special mathematical functions, and statistical distri‐\\nbutions. scikit-learn draws from SciPy’s collection of functions for implementing\\nits algorithms. The most important part of SciPy for us is scipy.sparse: this provides\\nsparse matrices, which are another representation that is used for data in scikit-\\nlearn. Sparse matrices are used whenever we want to store a 2D array that contains\\nmostly zeros:\\nIn[3]:\\nfrom scipy import sparse\\n# Create a 2D NumPy array with a diagonal of ones, and zeros everywhere else\\neye = np.eye(4)\\nprint(\"NumPy array:\\\\n{}\".format(eye))\\nOut[3]:\\nNumPy array:\\n[[ 1.  0.  0.  0.]\\n [ 0.  1.  0.  0.]\\n [ 0.  0.  1.  0.]\\n [ 0.  0.  0.  1.]]\\nIn[4]:\\n# Convert the NumPy array to a SciPy sparse matrix in CSR format\\n# Only the nonzero entries are stored\\nsparse_matrix = sparse.csr_matrix(eye)\\nprint(\"\\\\nSciPy sparse CSR matrix:\\\\n{}\".format(sparse_matrix))\\nOut[4]:\\nSciPy sparse CSR matrix:\\n  (0, 0)    1.0\\n  (1, 1)    1.0\\n  (2, 2)    1.0\\n  (3, 3)    1.0\\n8 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 22, 'page_label': '9'}, page_content='Usually it is not possible to create dense representations of sparse data (as they would\\nnot fit into memory), so we need to create sparse representations directly. Here is a\\nway to create the same sparse matrix as before, using the COO format:\\nIn[5]:\\ndata = np.ones(4)\\nrow_indices = np.arange(4)\\ncol_indices = np.arange(4)\\neye_coo = sparse.coo_matrix((data, (row_indices, col_indices)))\\nprint(\"COO representation:\\\\n{}\".format(eye_coo))\\nOut[5]:\\nCOO representation:\\n  (0, 0)    1.0\\n  (1, 1)    1.0\\n  (2, 2)    1.0\\n  (3, 3)    1.0\\nMore details on SciPy sparse matrices can be found in the SciPy Lecture Notes.\\nmatplotlib\\nmatplotlib is the primary scientific plotting library in Python. It provides functions\\nfor making publication-quality visualizations such as line charts, histograms, scatter\\nplots, and so on. Visualizing your data and different aspects of your analysis can give\\nyou important insights, and we will be using matplotlib for all our visualizations.\\nWhen working inside the Jupyter Notebook, you can show figures directly in the\\nbrowser by using the %matplotlib notebook and %matplotlib inline commands.\\nWe recommend using %matplotlib notebook , which provides an interactive envi‐\\nronment (though we are using %matplotlib inline  to produce this book). For\\nexample, this code produces the plot in Figure 1-1:\\nIn[6]:\\n%matplotlib inline\\nimport matplotlib.pyplot as plt\\n# Generate a sequence of numbers from -10 to 10 with 100 steps in between\\nx = np.linspace(-10, 10, 100)\\n# Create a second array using sine\\ny = np.sin(x)\\n# The plot function makes a line chart of one array against another\\nplt.plot(x, y, marker=\"x\")\\nEssential Libraries and Tools | 9'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 23, 'page_label': '10'}, page_content='Figure 1-1. Simple line plot of the sine function using matplotlib\\npandas\\npandas is a Python library for data wrangling and analysis. It is built around a data\\nstructure called the DataFrame that is modeled after the R DataFrame. Simply put, a\\npandas DataFrame is a table, similar to an Excel spreadsheet. pandas provides a great\\nrange of methods to modify and operate on this table; in particular, it allows SQL-like\\nqueries and joins of tables. In contrast to NumPy, which requires that all entries in an\\narray be of the same type, pandas allows each column to have a separate type (for\\nexample, integers, dates, floating-point numbers, and strings). Another valuable tool\\nprovided by pandas is its ability to ingest from a great variety of file formats and data‐\\nbases, like SQL, Excel files, and comma-separated values (CSV) files. Going into\\ndetail about the functionality of pandas is out of the scope of this book. However,\\nPython for Data Analysis  by Wes McKinney (O’Reilly, 2012) provides a great guide.\\nHere is a small example of creating a DataFrame using a dictionary:\\nIn[7]:\\nimport pandas as pd\\n# create a simple dataset of people\\ndata = {\\'Name\\': [\"John\", \"Anna\", \"Peter\", \"Linda\"],\\n        \\'Location\\' : [\"New York\", \"Paris\", \"Berlin\", \"London\"],\\n        \\'Age\\' : [24, 13, 53, 33]\\n       }\\ndata_pandas = pd.DataFrame(data)\\n# IPython.display allows \"pretty printing\" of dataframes\\n# in the Jupyter notebook\\ndisplay(data_pandas)\\n10 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 24, 'page_label': '11'}, page_content='This produces the following output:\\nAge Location Name\\n0 24 New York John\\n1 13 Paris Anna\\n2 53 Berlin Peter\\n3 33 London Linda\\nThere are several possible ways to query this table. For example:\\nIn[8]:\\n# Select all rows that have an age column greater than 30\\ndisplay(data_pandas[data_pandas.Age > 30])\\nThis produces the following result:\\nAge Location Name\\n2 53 Berlin Peter\\n3 33 London Linda\\nmglearn\\nThis book comes with accompanying code, which you can find on GitHub. The\\naccompanying code includes not only all the examples shown in this book, but also\\nthe mglearn library. This is a library of utility functions we wrote for this book, so\\nthat we don’t clutter up our code listings with details of plotting and data loading. If\\nyou’re interested, you can look up all the functions in the repository, but the details of\\nthe mglearn module are not really important to the material in this book. If you see a\\ncall to mglearn in the code, it is usually a way to make a pretty picture quickly, or to\\nget our hands on some interesting data.\\nThroughout the book we make ample use of NumPy, matplotlib\\nand pandas. All the code will assume the following imports:\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport mglearn\\nWe also assume that you will run the code in a Jupyter Notebook\\nwith the %matplotlib notebook  or %matplotlib inline  magic\\nenabled to show plots. If you are not using the notebook or these\\nmagic commands, you will have to call plt.show to actually show\\nany of the figures.\\nEssential Libraries and Tools | 11'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 25, 'page_label': '12'}, page_content='2 The six package can be very handy for that.\\nPython 2 Versus Python 3\\nThere are two major versions of Python that are widely used at the moment: Python 2\\n(more precisely, 2.7) and Python 3 (with the latest release being 3.5 at the time of\\nwriting). This sometimes leads to some confusion. Python 2 is no longer actively\\ndeveloped, but because Python 3 contains major changes, Python 2 code usually does\\nnot run on Python 3. If you are new to Python, or are starting a new project from\\nscratch, we highly recommend using the latest version of Python 3 without changes.\\nIf you have a large codebase that you rely on that is written for Python 2, you are\\nexcused from upgrading for now. However, you should try to migrate to Python 3 as\\nsoon as possible. When writing any new code, it is for the most part quite easy to\\nwrite code that runs under Python 2 and Python 3.2 If you don’t have to interface with\\nlegacy software, you should definitely use Python 3. All the code in this book is writ‐\\nten in a way that works for both versions. However, the exact output might differ\\nslightly under Python 2.\\nVersions Used in this Book\\nWe are using the following versions of the previously mentioned libraries in this\\nbook:\\nIn[9]:\\nimport sys\\nprint(\"Python version: {}\".format(sys.version))\\nimport pandas as pd\\nprint(\"pandas version: {}\".format(pd.__version__))\\nimport matplotlib\\nprint(\"matplotlib version: {}\".format(matplotlib.__version__))\\nimport numpy as np\\nprint(\"NumPy version: {}\".format(np.__version__))\\nimport scipy as sp\\nprint(\"SciPy version: {}\".format(sp.__version__))\\nimport IPython\\nprint(\"IPython version: {}\".format(IPython.__version__))\\nimport sklearn\\nprint(\"scikit-learn version: {}\".format(sklearn.__version__))\\n12 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 26, 'page_label': '13'}, page_content='Out[9]:\\nPython version: 3.5.2 |Anaconda 4.1.1 (64-bit)| (default, Jul  2 2016, 17:53:06)\\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\\npandas version: 0.18.1\\nmatplotlib version: 1.5.1\\nNumPy version: 1.11.1\\nSciPy version: 0.17.1\\nIPython version: 5.1.0\\nscikit-learn version: 0.18\\nWhile it is not important to match these versions exactly, you should have a version\\nof scikit-learn that is as least as recent as the one we used.\\nNow that we have everything set up, let’s dive into our first application of machine\\nlearning.\\nThis book assumes that you have version 0.18 or later of scikit-\\nlearn. The model_selection module was added in 0.18, and if you\\nuse an earlier version of scikit-learn, you will need to adjust the\\nimports from this module.\\nA First Application: Classifying Iris Species\\nIn this section, we will go through a simple machine learning application and create\\nour first model. In the process, we will introduce some core concepts and terms.\\nLet’s assume that a hobby botanist is interested in distinguishing the species of some\\niris flowers that she has found. She has collected some measurements associated with\\neach iris: the length and width of the petals and the length and width of the sepals, all\\nmeasured in centimeters (see Figure 1-2).\\nShe also has the measurements of some irises that have been previously identified by\\nan expert botanist as belonging to the species setosa, versicolor, or virginica. For these\\nmeasurements, she can be certain of which species each iris belongs to. Let’s assume\\nthat these are the only species our hobby botanist will encounter in the wild.\\nOur goal is to build a machine learning model that can learn from the measurements\\nof these irises whose species is known, so that we can predict the species for a new\\niris.\\nA First Application: Classifying Iris Species | 13'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 27, 'page_label': '14'}, page_content='Figure 1-2. Parts of the iris flower\\nBecause we have measurements for which we know the correct species of iris, this is a\\nsupervised learning problem. In this problem, we want to predict one of several\\noptions (the species of iris). This is an example of a classification problem. The possi‐\\nble outputs (different species of irises) are called classes. Every iris in the dataset\\nbelongs to one of three classes, so this problem is a three-class classification problem.\\nThe desired output for a single data point (an iris) is the species of this flower. For a\\nparticular data point, the species it belongs to is called its label.\\nMeet the Data\\nThe data we will use for this example is the Iris dataset, a classical dataset in machine\\nlearning and statistics. It is included in scikit-learn in the datasets module. We\\ncan load it by calling the load_iris function:\\nIn[10]:\\nfrom sklearn.datasets import load_iris\\niris_dataset = load_iris()\\nThe iris object that is returned by load_iris is a Bunch object, which is very similar\\nto a dictionary. It contains keys and values:\\n14 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 28, 'page_label': '15'}, page_content='In[11]:\\nprint(\"Keys of iris_dataset: \\\\n{}\".format(iris_dataset.keys()))\\nOut[11]:\\nKeys of iris_dataset:\\ndict_keys([\\'target_names\\', \\'feature_names\\', \\'DESCR\\', \\'data\\', \\'target\\'])\\nThe value of the key DESCR is a short description of the dataset. We show the begin‐\\nning of the description here (feel free to look up the rest yourself):\\nIn[12]:\\nprint(iris_dataset[\\'DESCR\\'][:193] + \"\\\\n...\")\\nOut[12]:\\nIris Plants Database\\n====================\\nNotes\\n----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive att\\n...\\n----\\nThe value of the key target_names is an array of strings, containing the species of\\nflower that we want to predict:\\nIn[13]:\\nprint(\"Target names: {}\".format(iris_dataset[\\'target_names\\']))\\nOut[13]:\\nTarget names: [\\'setosa\\' \\'versicolor\\' \\'virginica\\']\\nThe value of feature_names is a list of strings, giving the description of each feature:\\nIn[14]:\\nprint(\"Feature names: \\\\n{}\".format(iris_dataset[\\'feature_names\\']))\\nOut[14]:\\nFeature names:\\n[\\'sepal length (cm)\\', \\'sepal width (cm)\\', \\'petal length (cm)\\',\\n \\'petal width (cm)\\']\\nThe data itself is contained in the target and data fields. data contains the numeric\\nmeasurements of sepal length, sepal width, petal length, and petal width in a NumPy\\narray:\\nA First Application: Classifying Iris Species | 15'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 29, 'page_label': '16'}, page_content='In[15]:\\nprint(\"Type of data: {}\".format(type(iris_dataset[\\'data\\'])))\\nOut[15]:\\nType of data: <class \\'numpy.ndarray\\'>\\nThe rows in the data array correspond to flowers, while the columns represent the\\nfour measurements that were taken for each flower:\\nIn[16]:\\nprint(\"Shape of data: {}\".format(iris_dataset[\\'data\\'].shape))\\nOut[16]:\\nShape of data: (150, 4)\\nWe see that the array contains measurements for 150 different flowers. Remember\\nthat the individual items are called samples in machine learning, and their properties\\nare called features. The shape of the data array is the number of samples multiplied by\\nthe number of features. This is a convention in scikit-learn, and your data will\\nalways be assumed to be in this shape. Here are the feature values for the first five\\nsamples:\\nIn[17]:\\nprint(\"First five columns of data:\\\\n{}\".format(iris_dataset[\\'data\\'][:5]))\\nOut[17]:\\nFirst five columns of data:\\n[[ 5.1  3.5  1.4  0.2]\\n [ 4.9  3.   1.4  0.2]\\n [ 4.7  3.2  1.3  0.2]\\n [ 4.6  3.1  1.5  0.2]\\n [ 5.   3.6  1.4  0.2]]\\nFrom this data, we can see that all of the first five flowers have a petal width of 0.2 cm\\nand that the first flower has the longest sepal, at 5.1 cm.\\nThe target array contains the species of each of the flowers that were measured, also\\nas a NumPy array:\\nIn[18]:\\nprint(\"Type of target: {}\".format(type(iris_dataset[\\'target\\'])))\\nOut[18]:\\nType of target: <class \\'numpy.ndarray\\'>\\ntarget is a one-dimensional array, with one entry per flower:\\n16 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 30, 'page_label': '17'}, page_content='In[19]:\\nprint(\"Shape of target: {}\".format(iris_dataset[\\'target\\'].shape))\\nOut[19]:\\nShape of target: (150,)\\nThe species are encoded as integers from 0 to 2:\\nIn[20]:\\nprint(\"Target:\\\\n{}\".format(iris_dataset[\\'target\\']))\\nOut[20]:\\nTarget:\\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\\n 2 2]\\nThe meanings of the numbers are given by the iris[\\'target_names\\'] array:\\n0 means setosa, 1 means versicolor, and 2 means virginica.\\nMeasuring Success: Training and Testing Data\\nWe want to build a machine learning model from this data that can predict the spe‐\\ncies of iris for a new set of measurements. But before we can apply our model to new\\nmeasurements, we need to know whether it actually works—that is, whether we\\nshould trust its predictions.\\nUnfortunately, we cannot use the data we used to build the model to evaluate it. This\\nis because our model can always simply remember the whole training set, and will\\ntherefore always predict the correct label for any point in the training set. This\\n“remembering” does not indicate to us whether our model will generalize well (in\\nother words, whether it will also perform well on new data).\\nTo assess the model’s performance, we show it new data (data that it hasn’t seen\\nbefore) for which we have labels. This is usually done by splitting the labeled data we\\nhave collected (here, our 150 flower measurements) into two parts. One part of the\\ndata is used to build our machine learning model, and is called the training data or\\ntraining set. The rest of the data will be used to assess how well the model works; this\\nis called the test data, test set, or hold-out set.\\nscikit-learn contains a function that shuffles the dataset and splits it for you: the\\ntrain_test_split function. This function extracts 75% of the rows in the data as the\\ntraining set, together with the corresponding labels for this data. The remaining 25%\\nof the data, together with the remaining labels, is declared as the test set. Deciding\\nA First Application: Classifying Iris Species | 17'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 31, 'page_label': '18'}, page_content='how much data you want to put into the training and the test set respectively is some‐\\nwhat arbitrary, but using a test set containing 25% of the data is a good rule of thumb.\\nIn scikit-learn, data is usually denoted with a capital X, while labels are denoted by\\na lowercase y. This is inspired by the standard formulation f(x)=y in mathematics,\\nwhere x is the input to a function and y is the output. Following more conventions\\nfrom mathematics, we use a capital X because the data is a two-dimensional array (a\\nmatrix) and a lowercase y because the target is a one-dimensional array (a vector).\\nLet’s call train_test_split on our data and assign the outputs using this nomencla‐\\nture:\\nIn[21]:\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(\\n    iris_dataset[\\'data\\'], iris_dataset[\\'target\\'], random_state=0)\\nBefore making the split, the train_test_split function shuffles the dataset using a\\npseudorandom number generator. If we just took the last 25% of the data as a test set,\\nall the data points would have the label 2, as the data points are sorted by the label\\n(see the output for iris[\\'target\\'] shown earlier). Using a test set containing only\\none of the three classes would not tell us much about how well our model generalizes,\\nso we shuffle our data to make sure the test data contains data from all classes.\\nTo make sure that we will get the same output if we run the same function several\\ntimes, we provide the pseudorandom number generator with a fixed seed using the\\nrandom_state parameter. This will make the outcome deterministic, so this line will\\nalways have the same outcome. We will always fix the random_state in this way when\\nusing randomized procedures in this book.\\nThe output of the train_test_split function is X_train, X_test, y_train, and\\ny_test, which are all NumPy arrays. X_train contains 75% of the rows of the dataset,\\nand X_test contains the remaining 25%:\\nIn[22]:\\nprint(\"X_train shape: {}\".format(X_train.shape))\\nprint(\"y_train shape: {}\".format(y_train.shape))\\nOut[22]:\\nX_train shape: (112, 4)\\ny_train shape: (112,)\\n18 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 32, 'page_label': '19'}, page_content='In[23]:\\nprint(\"X_test shape: {}\".format(X_test.shape))\\nprint(\"y_test shape: {}\".format(y_test.shape))\\nOut[23]:\\nX_test shape: (38, 4)\\ny_test shape: (38,)\\nFirst Things First: Look at Your Data\\nBefore building a machine learning model it is often a good idea to inspect the data,\\nto see if the task is easily solvable without machine learning, or if the desired infor‐\\nmation might not be contained in the data.\\nAdditionally, inspecting your data is a good way to find abnormalities and peculiari‐\\nties. Maybe some of your irises were measured using inches and not centimeters, for\\nexample. In the real world, inconsistencies in the data and unexpected measurements\\nare very common.\\nOne of the best ways to inspect data is to visualize it. One way to do this is by using a\\nscatter plot. A scatter plot of the data puts one feature along the x-axis and another\\nalong the y-axis, and draws a dot for each data point. Unfortunately, computer\\nscreens have only two dimensions, which allows us to plot only two (or maybe three)\\nfeatures at a time. It is difficult to plot datasets with more than three features this way.\\nOne way around this problem is to do a pair plot, which looks at all possible pairs of\\nfeatures. If you have a small number of features, such as the four we have here, this is\\nquite reasonable. Y ou should keep in mind, however, that a pair plot does not show\\nthe interaction of all of features at once, so some interesting aspects of the data may\\nnot be revealed when visualizing it this way.\\nFigure 1-3 is a pair plot of the features in the training set. The data points are colored\\naccording to the species the iris belongs to. To create the plot, we first convert the\\nNumPy array into a pandas DataFrame. pandas has a function to create pair plots\\ncalled scatter_matrix. The diagonal of this matrix is filled with histograms of each\\nfeature:\\nIn[24]:\\n# create dataframe from data in X_train\\n# label the columns using the strings in iris_dataset.feature_names\\niris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)\\n# create a scatter matrix from the dataframe, color by y_train\\ngrr = pd.scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15), marker=\\'o\\',\\n                        hist_kwds={\\'bins\\': 20}, s=60, alpha=.8, cmap=mglearn.cm3)\\nA First Application: Classifying Iris Species | 19'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 33, 'page_label': '20'}, page_content='Figure 1-3. Pair plot of the Iris dataset, colored by class label\\nFrom the plots, we can see that the three classes seem to be relatively well separated\\nusing the sepal and petal measurements. This means that a machine learning model\\nwill likely be able to learn to separate them.\\nBuilding Your First Model: k-Nearest Neighbors\\nNow we can start building the actual machine learning model. There are many classi‐\\nfication algorithms in scikit-learn that we could use. Here we will use a k-nearest\\nneighbors classifier, which is easy to understand. Building this model only consists of\\nstoring the training set. To make a prediction for a new data point, the algorithm\\nfinds the point in the training set that is closest to the new point. Then it assigns the\\nlabel of this training point to the new data point.\\n20 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 34, 'page_label': '21'}, page_content=\"The k in k-nearest neighbors signifies that instead of using only the closest neighbor\\nto the new data point, we can consider any fixed number k of neighbors in the train‐\\ning (for example, the closest three or five neighbors). Then, we can make a prediction\\nusing the majority class among these neighbors. We will go into more detail about\\nthis in Chapter 2; for now, we’ll use only a single neighbor.\\nAll machine learning models in scikit-learn are implemented in their own classes,\\nwhich are called Estimator classes. The k-nearest neighbors classification algorithm\\nis implemented in the KNeighborsClassifier class in the neighbors module. Before\\nwe can use the model, we need to instantiate the class into an object. This is when we\\nwill set any parameters of the model. The most important parameter of KNeighbor\\nsClassifier is the number of neighbors, which we will set to 1:\\nIn[25]:\\nfrom sklearn.neighbors import KNeighborsClassifier\\nknn = KNeighborsClassifier(n_neighbors=1)\\nThe knn object encapsulates the algorithm that will be used to build the model from\\nthe training data, as well the algorithm to make predictions on new data points. It will\\nalso hold the information that the algorithm has extracted from the training data. In\\nthe case of KNeighborsClassifier, it will just store the training set.\\nTo build the model on the training set, we call the fit method of the knn object,\\nwhich takes as arguments the NumPy array X_train containing the training data and\\nthe NumPy array y_train of the corresponding training labels:\\nIn[26]:\\nknn.fit(X_train, y_train)\\nOut[26]:\\nKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\\n           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\\n           weights='uniform')\\nThe fit method returns the knn object itself (and modifies it in place), so we get a\\nstring representation of our classifier. The representation shows us which parameters\\nwere used in creating the model. Nearly all of them are the default values, but you can\\nalso find n_neighbors=1, which is the parameter that we passed. Most models in\\nscikit-learn have many parameters, but the majority of them are either speed opti‐\\nmizations or for very special use cases. Y ou don’t have to worry about the other\\nparameters shown in this representation. Printing a scikit-learn model can yield\\nvery long strings, but don’t be intimidated by these. We will cover all the important\\nparameters in Chapter 2. In the remainder of this book, we will not show the output\\nof fit because it doesn’t contain any new information.\\nA First Application: Classifying Iris Species | 21\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 35, 'page_label': '22'}, page_content='Making Predictions\\nWe can now make predictions using this model on new data for which we might not\\nknow the correct labels. Imagine we found an iris in the wild with a sepal length of\\n5 cm, a sepal width of 2.9 cm, a petal length of 1 cm, and a petal width of 0.2 cm.\\nWhat species of iris would this be? We can put this data into a NumPy array, again by\\ncalculating the shape—that is, the number of samples (1) multiplied by the number of\\nfeatures (4):\\nIn[27]:\\nX_new = np.array([[5, 2.9, 1, 0.2]])\\nprint(\"X_new.shape: {}\".format(X_new.shape))\\nOut[27]:\\nX_new.shape: (1, 4)\\nNote that we made the measurements of this single flower into a row in a two-\\ndimensional NumPy array, as scikit-learn always expects two-dimensional arrays\\nfor the data.\\nTo make a prediction, we call the predict method of the knn object:\\nIn[28]:\\nprediction = knn.predict(X_new)\\nprint(\"Prediction: {}\".format(prediction))\\nprint(\"Predicted target name: {}\".format(\\n       iris_dataset[\\'target_names\\'][prediction]))\\nOut[28]:\\nPrediction: [0]\\nPredicted target name: [\\'setosa\\']\\nOur model predicts that this new iris belongs to the class 0, meaning its species is\\nsetosa. But how do we know whether we can trust our model? We don’t know the cor‐\\nrect species of this sample, which is the whole point of building the model!\\nEvaluating the Model\\nThis is where the test set that we created earlier comes in. This data was not used to\\nbuild the model, but we do know what the correct species is for each iris in the test\\nset.\\nTherefore, we can make a prediction for each iris in the test data and compare it\\nagainst its label (the known species). We can measure how well the model works by\\ncomputing the accuracy, which is the fraction of flowers for which the right species\\nwas predicted:\\n22 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 36, 'page_label': '23'}, page_content='In[29]:\\ny_pred = knn.predict(X_test)\\nprint(\"Test set predictions:\\\\n {}\".format(y_pred))\\nOut[29]:\\nTest set predictions:\\n [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0 2]\\nIn[30]:\\nprint(\"Test set score: {:.2f}\".format(np.mean(y_pred == y_test)))\\nOut[30]:\\nTest set score: 0.97\\nWe can also use the score method of the knn object, which will compute the test set\\naccuracy for us:\\nIn[31]:\\nprint(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))\\nOut[31]:\\nTest set score: 0.97\\nFor this model, the test set accuracy is about 0.97, which means we made the right\\nprediction for 97% of the irises in the test set. Under some mathematical assump‐\\ntions, this means that we can expect our model to be correct 97% of the time for new\\nirises. For our hobby botanist application, this high level of accuracy means that our\\nmodel may be trustworthy enough to use. In later chapters we will discuss how we\\ncan improve performance, and what caveats there are in tuning a model.\\nSummary and Outlook\\nLet’s summarize what we learned in this chapter. We started with a brief introduction\\nto machine learning and its applications, then discussed the distinction between\\nsupervised and unsupervised learning and gave an overview of the tools we’ll be\\nusing in this book. Then, we formulated the task of predicting which species of iris a\\nparticular flower belongs to by using physical measurements of the flower. We used a\\ndataset of measurements that was annotated by an expert with the correct species to\\nbuild our model, making this a supervised learning task. There were three possible\\nspecies, setosa, versicolor, or virginica, which made the task a three-class classification\\nproblem. The possible species are called classes in the classification problem, and the\\nspecies of a single iris is called its label.\\nThe Iris dataset consists of two NumPy arrays: one containing the data, which is\\nreferred to as X in scikit-learn, and one containing the correct or desired outputs,\\nSummary and Outlook | 23'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 37, 'page_label': '24'}, page_content='which is called y. The array X is a two-dimensional array of features, with one row per\\ndata point and one column per feature. The array y is a one-dimensional array, which\\nhere contains one class label, an integer ranging from 0 to 2, for each of the samples.\\nWe split our dataset into a training set, to build our model, and a test set, to evaluate\\nhow well our model will generalize to new, previously unseen data.\\nWe chose the k-nearest neighbors classification algorithm, which makes predictions\\nfor a new data point by considering its closest neighbor(s) in the training set. This is\\nimplemented in the KNeighborsClassifier class, which contains the algorithm that\\nbuilds the model as well as the algorithm that makes a prediction using the model.\\nWe instantiated the class, setting parameters. Then we built the model by calling the\\nfit method, passing the training data ( X_train) and training outputs ( y_train) as\\nparameters. We evaluated the model using the score method, which computes the\\naccuracy of the model. We applied the score method to the test set data and the test\\nset labels and found that our model is about 97% accurate, meaning it is correct 97%\\nof the time on the test set.\\nThis gave us the confidence to apply the model to new data (in our example, new\\nflower measurements) and trust that the model will be correct about 97% of the time.\\nHere is a summary of the code needed for the whole training and evaluation\\nprocedure:\\nIn[32]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    iris_dataset[\\'data\\'], iris_dataset[\\'target\\'], random_state=0)\\nknn = KNeighborsClassifier(n_neighbors=1)\\nknn.fit(X_train, y_train)\\nprint(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))\\nOut[32]:\\nTest set score: 0.97\\nThis snippet contains the core code for applying any machine learning algorithm\\nusing scikit-learn. The fit, predict, and score methods are the common inter‐\\nface to supervised models in scikit-learn, and with the concepts introduced in this\\nchapter, you can apply these models to many machine learning tasks. In the next\\nchapter, we will go into more depth about the different kinds of supervised models in\\nscikit-learn and how to apply them successfully.\\n24 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 38, 'page_label': '25'}, page_content='CHAPTER 2\\nSupervised Learning\\nAs we mentioned earlier, supervised machine learning is one of the most commonly\\nused and successful types of machine learning. In this chapter, we will describe super‐\\nvised learning in more detail and explain several popular supervised learning algo‐\\nrithms. We already saw an application of supervised machine learning in Chapter 1:\\nclassifying iris flowers into several species using physical measurements of the\\nflowers.\\nRemember that supervised learning is used whenever we want to predict a certain\\noutcome from a given input, and we have examples of input/output pairs. We build a\\nmachine learning model from these input/output pairs, which comprise our training\\nset. Our goal is to make accurate predictions for new, never-before-seen data. Super‐\\nvised learning often requires human effort to build the training set, but afterward\\nautomates and often speeds up an otherwise laborious or infeasible task.\\nClassification  and Regression\\nThere are two major types of supervised machine learning problems, called classifica‐\\ntion and regression.\\nIn classification, the goal is to predict a class label, which is a choice from a predefined\\nlist of possibilities. In Chapter 1 we used the example of classifying irises into one of\\nthree possible species. Classification is sometimes separated into binary classification,\\nwhich is the special case of distinguishing between exactly two classes, and multiclass\\nclassification, which is classification between more than two classes. Y ou can think of\\nbinary classification as trying to answer a yes/no question. Classifying emails as\\neither spam or not spam is an example of a binary classification problem. In this\\nbinary classification task, the yes/no question being asked would be “Is this email\\nspam?”\\n25'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 39, 'page_label': '26'}, page_content='1 We ask linguists to excuse the simplified presentation of languages as distinct and fixed entities.\\nIn binary classification we often speak of one class being the posi‐\\ntive class and the other class being the negative class. Here, positive\\ndoesn’t represent having benefit or value, but rather what the object\\nof the study is. So, when looking for spam, “positive” could mean\\nthe spam class. Which of the two classes is called positive is often a\\nsubjective matter, and specific to the domain.\\nThe iris example, on the other hand, is an example of a multiclass classification prob‐\\nlem. Another example is predicting what language a website is in from the text on the\\nwebsite. The classes here would be a pre-defined list of possible languages.\\nFor regression tasks, the goal is to predict a continuous number, or a floating-point\\nnumber in programming terms (or real number in mathematical terms). Predicting a\\nperson’s annual income from their education, their age, and where they live is an\\nexample of a regression task. When predicting income, the predicted value is an\\namount, and can be any number in a given range. Another example of a regression\\ntask is predicting the yield of a corn farm given attributes such as previous yields,\\nweather, and number of employees working on the farm. The yield again can be an\\narbitrary number.\\nAn easy way to distinguish between classification and regression tasks is to ask\\nwhether there is some kind of continuity in the output. If there is continuity between\\npossible outcomes, then the problem is a regression problem. Think about predicting\\nannual income. There is a clear continuity in the output. Whether a person makes\\n$40,000 or $40,001 a year does not make a tangible difference, even though these are\\ndifferent amounts of money; if our algorithm predicts $39,999 or $40,001 when it\\nshould have predicted $40,000, we don’t mind that much.\\nBy contrast, for the task of recognizing the language of a website (which is a classifi‐\\ncation problem), there is no matter of degree. A website is in one language, or it is in\\nanother. There is no continuity between languages, and there is no language that is\\nbetween English and French.1\\nGeneralization, \\nOverfitting,  and Underfitting\\nIn supervised learning, we want to build a model on the training data and then be\\nable to make accurate predictions on new, unseen data that has the same characteris‐\\ntics as the training set that we used. If a model is able to make accurate predictions on\\nunseen data, we say it is able to generalize from the training set to the test set. We\\nwant to build a model that is able to generalize as accurately as possible.\\n26 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 40, 'page_label': '27'}, page_content='2 In the real world, this is actually a tricky problem. While we know that the other customers haven’t bought a\\nboat from us yet, they might have bought one from someone else, or they may still be saving and plan to buy\\none in the future.\\nUsually we build a model in such a way that it can make accurate predictions on the\\ntraining set. If the training and test sets have enough in common, we expect the\\nmodel to also be accurate on the test set. However, there are some cases where this\\ncan go wrong. For example, if we allow ourselves to build very complex models, we\\ncan always be as accurate as we like on the training set.\\nLet’s take a look at a made-up example to illustrate this point. Say a novice data scien‐\\ntist wants to predict whether a customer will buy a boat, given records of previous\\nboat buyers and customers who we know are not interested in buying a boat. 2 The\\ngoal is to send out promotional emails to people who are likely to actually make a\\npurchase, but not bother those customers who won’t be interested.\\nSuppose we have the customer records shown in Table 2-1.\\nTable 2-1. Example data about customers\\nAge Number of \\ncars owned\\nOwns house Number of children Marital status Owns a dog Bought a boat\\n66 1 yes 2 widowed no yes\\n52 2 yes 3 married no yes\\n22 0 no 0 married yes no\\n25 1 no 1 single no no\\n44 0 no 2 divorced yes no\\n39 1 yes 2 married yes no\\n26 1 no 2 single no no\\n40 3 yes 1 married yes no\\n53 2 yes 2 divorced no yes\\n64 2 yes 3 divorced no no\\n58 2 yes 2 married yes yes\\n33 1 no 1 single no no\\nAfter looking at the data for a while, our novice data scientist comes up with the fol‐\\nlowing rule: “If the customer is older than 45, and has less than 3 children or is not\\ndivorced, then they want to buy a boat. ” When asked how well this rule of his does,\\nour data scientist answers, “It’ s 100 percent accurate!” And indeed, on the data that is\\nin the table, the rule is perfectly accurate. There are many possible rules we could\\ncome up with that would explain perfectly if someone in this dataset wants to buy a\\nboat. No age appears twice in the data, so we could say people who are 66, 52, 53, or\\nGeneralization, Overfitting,  and Underfitting  | 27'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 41, 'page_label': '28'}, page_content='3 And also provably, with the right math.\\n58 years old want to buy a boat, while all others don’t. While we can make up many\\nrules that work well on this data, remember that we are not interested in making pre‐\\ndictions for this dataset; we already know the answers for these customers. We want\\nto know if new customers are likely to buy a boat. We therefore want to find a rule that\\nwill work well for new customers, and achieving 100 percent accuracy on the training\\nset does not help us there. We might not expect that the rule our data scientist came\\nup with will work very well on new customers. It seems too complex, and it is sup‐\\nported by very little data. For example, the “or is not divorced” part of the rule hinges\\non a single customer.\\nThe only measure of whether an algorithm will perform well on new data is the eval‐\\nuation on the test set. However, intuitively 3 we expect simple models to generalize\\nbetter to new data. If the rule was “People older than 50 want to buy a boat, ” and this\\nwould explain the behavior of all the customers, we would trust it more than the rule\\ninvolving children and marital status in addition to age. Therefore, we always want to\\nfind the simplest model. Building a model that is too complex for the amount of\\ninformation we have, as our novice data scientist did, is called overfitting. Overfitting\\noccurs when you fit a model too closely to the particularities of the training set and\\nobtain a model that works well on the training set but is not able to generalize to new\\ndata. On the other hand, if your model is too simple—say, “Everybody who owns a\\nhouse buys a boat”—then you might not be able to capture all the aspects of and vari‐\\nability in the data, and your model will do badly even on the training set. Choosing\\ntoo simple a model is called underfitting.\\nThe more complex we allow our model to be, the better we will be able to predict on\\nthe training data. However, if our model becomes too complex, we start focusing too\\nmuch on each individual data point in our training set, and the model will not gener‐\\nalize well to new data.\\nThere is a sweet spot in between that will yield the best generalization performance.\\nThis is the model we want to find.\\nThe trade-off between overfitting and underfitting is illustrated in Figure 2-1.\\n28 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 42, 'page_label': '29'}, page_content='Figure 2-1. Trade-off of model complexity against training and test accuracy\\nRelation of Model Complexity to Dataset Size\\nIt’s important to note that model complexity is intimately tied to the variation of\\ninputs contained in your training dataset: the larger variety of data points your data‐\\nset contains, the more complex a model you can use without overfitting. Usually, col‐\\nlecting more data points will yield more variety, so larger datasets allow building\\nmore complex models. However, simply duplicating the same data points or collect‐\\ning very similar data will not help.\\nGoing back to the boat selling example, if we saw 10,000 more rows of customer data,\\nand all of them complied with the rule “If the customer is older than 45, and has less\\nthan 3 children or is not divorced, then they want to buy a boat, ” we would be much\\nmore likely to believe this to be a good rule than when it was developed using only\\nthe 12 rows in Table 2-1.\\nHaving more data and building appropriately more complex models can often work\\nwonders for supervised learning tasks. In this book, we will focus on working with\\ndatasets of fixed sizes. In the real world, you often have the ability to decide how\\nmuch data to collect, which might be more beneficial than tweaking and tuning your\\nmodel. Never underestimate the power of more data.\\nSupervised Machine Learning Algorithms\\nWe will now review the most popular machine learning algorithms and explain how\\nthey learn from data and how they make predictions. We will also discuss how the\\nconcept of model complexity plays out for each of these models, and provide an over‐\\nSupervised Machine Learning Algorithms | 29'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 43, 'page_label': '30'}, page_content='4 Discussing all of them is beyond the scope of the book, and we refer you to the scikit-learn documentation\\nfor more details.\\nview of how each algorithm builds a model. We will examine the strengths and weak‐\\nnesses of each algorithm, and what kind of data they can best be applied to. We will\\nalso explain the meaning of the most important parameters and options. 4 Many algo‐\\nrithms have a classification and a regression variant, and we will describe both.\\nIt is not necessary to read through the descriptions of each algorithm in detail, but\\nunderstanding the models will give you a better feeling for the different ways\\nmachine learning algorithms can work. This chapter can also be used as a reference\\nguide, and you can come back to it when you are unsure about the workings of any of\\nthe algorithms.\\nSome Sample Datasets\\nWe will use several datasets to illustrate the different algorithms. Some of the datasets\\nwill be small and synthetic (meaning made-up), designed to highlight particular\\naspects of the algorithms. Other datasets will be large, real-world examples.\\nAn example of a synthetic two-class classification dataset is the forge dataset, which\\nhas two features. The following code creates a scatter plot ( Figure 2-2) visualizing all\\nof the data points in this dataset. The plot has the first feature on the x-axis and the\\nsecond feature on the y-axis. As is always the case in scatter plots, each data point is\\nrepresented as one dot. The color and shape of the dot indicates its class:\\nIn[2]:\\n# generate dataset\\nX, y = mglearn.datasets.make_forge()\\n# plot dataset\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\nplt.legend([\"Class 0\", \"Class 1\"], loc=4)\\nplt.xlabel(\"First feature\")\\nplt.ylabel(\"Second feature\")\\nprint(\"X.shape: {}\".format(X.shape))\\nOut[2]:\\nX.shape: (26, 2)\\n30 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 44, 'page_label': '31'}, page_content='Figure 2-2. Scatter plot of the forge dataset\\nAs you can see from X.shape, this dataset consists of 26 data points, with 2 features.\\nTo illustrate regression algorithms, we will use the synthetic wave dataset. The wave\\ndataset has a single input feature and a continuous target variable (or response) that\\nwe want to model. The plot created here ( Figure 2-3) shows the single feature on the\\nx-axis and the regression target (the output) on the y-axis:\\nIn[3]:\\nX, y = mglearn.datasets.make_wave(n_samples=40)\\nplt.plot(X, y, \\'o\\')\\nplt.ylim(-3, 3)\\nplt.xlabel(\"Feature\")\\nplt.ylabel(\"Target\")\\nSupervised Machine Learning Algorithms | 31'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 45, 'page_label': '32'}, page_content='Figure 2-3. Plot of the wave dataset, with the x-axis showing the feature and the y-axis\\nshowing the regression target\\nWe are using these very simple, low-dimensional datasets because we can easily visu‐\\nalize them—a printed page has two dimensions, so data with more than two features\\nis hard to show. Any intuition derived from datasets with few features (also called\\nlow-dimensional datasets) might not hold in datasets with many features ( high-\\ndimensional datasets). As long as you keep that in mind, inspecting algorithms on\\nlow-dimensional datasets can be very instructive.\\nWe will complement these small synthetic datasets with two real-world datasets that\\nare included in scikit-learn. One is the Wisconsin Breast Cancer dataset ( cancer,\\nfor short), which records clinical measurements of breast cancer tumors. Each tumor\\nis labeled as “benign” (for harmless tumors) or “malignant” (for cancerous tumors),\\nand the task is to learn to predict whether a tumor is malignant based on the meas‐\\nurements of the tissue.\\nThe data can be loaded using the load_breast_cancer function from scikit-learn:\\nIn[4]:\\nfrom sklearn.datasets import load_breast_cancer\\ncancer = load_breast_cancer()\\nprint(\"cancer.keys(): \\\\n{}\".format(cancer.keys()))\\n32 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 46, 'page_label': '33'}, page_content='Out[4]:\\ncancer.keys():\\ndict_keys([\\'feature_names\\', \\'data\\', \\'DESCR\\', \\'target\\', \\'target_names\\'])\\nDatasets that are included in scikit-learn are usually stored as\\nBunch objects, which contain some information about the dataset\\nas well as the actual data. All you need to know about Bunch objects\\nis that they behave like dictionaries, with the added benefit that you\\ncan access values using a dot (as in bunch.key instead of\\nbunch[\\'key\\']).\\nThe dataset consists of 569 data points, with 30 features each:\\nIn[5]:\\nprint(\"Shape of cancer data: {}\".format(cancer.data.shape))\\nOut[5]:\\nShape of cancer data: (569, 30)\\nOf these 569 data points, 212 are labeled as malignant and 357 as benign:\\nIn[6]:\\nprint(\"Sample counts per class:\\\\n{}\".format(\\n      {n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))}))\\nOut[6]:\\nSample counts per class:\\n{\\'benign\\': 357, \\'malignant\\': 212}\\nTo get a description of the semantic meaning of each feature, we can have a look at\\nthe feature_names attribute:\\nIn[7]:\\nprint(\"Feature names:\\\\n{}\".format(cancer.feature_names))\\nOut[7]:\\nFeature names:\\n[\\'mean radius\\' \\'mean texture\\' \\'mean perimeter\\' \\'mean area\\'\\n \\'mean smoothness\\' \\'mean compactness\\' \\'mean concavity\\'\\n \\'mean concave points\\' \\'mean symmetry\\' \\'mean fractal dimension\\'\\n \\'radius error\\' \\'texture error\\' \\'perimeter error\\' \\'area error\\'\\n \\'smoothness error\\' \\'compactness error\\' \\'concavity error\\'\\n \\'concave points error\\' \\'symmetry error\\' \\'fractal dimension error\\'\\n \\'worst radius\\' \\'worst texture\\' \\'worst perimeter\\' \\'worst area\\'\\n \\'worst smoothness\\' \\'worst compactness\\' \\'worst concavity\\'\\n \\'worst concave points\\' \\'worst symmetry\\' \\'worst fractal dimension\\']\\nSupervised Machine Learning Algorithms | 33'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 47, 'page_label': '34'}, page_content='5 This is called the binomial coefficient, which is the number of combinations of k elements that can be selected\\nfrom a set of n elements. Often this is written as nk and spoken as “n choose k”—in this case, “13 choose 2. ”\\nY ou can find out more about the data by reading cancer.DESCR if you are interested.\\nWe will also be using a real-world regression dataset, the Boston Housing dataset.\\nThe task associated with this dataset is to predict the median value of homes in sev‐\\neral Boston neighborhoods in the 1970s, using information such as crime rate, prox‐\\nimity to the Charles River, highway accessibility, and so on. The dataset contains 506\\ndata points, described by 13 features:\\nIn[8]:\\nfrom sklearn.datasets import load_boston\\nboston = load_boston()\\nprint(\"Data shape: {}\".format(boston.data.shape))\\nOut[8]:\\nData shape: (506, 13)\\nAgain, you can get more information about the dataset by reading the DESCR attribute\\nof boston. For our purposes here, we will actually expand this dataset by not only\\nconsidering these 13 measurements as input features, but also looking at all products\\n(also called interactions) between features. In other words, we will not only consider\\ncrime rate and highway accessibility as features, but also the product of crime rate\\nand highway accessibility. Including derived feature like these is called feature engi‐\\nneering, which we will discuss in more detail in Chapter 4. This derived dataset can be\\nloaded using the load_extended_boston function:\\nIn[9]:\\nX, y = mglearn.datasets.load_extended_boston()\\nprint(\"X.shape: {}\".format(X.shape))\\nOut[9]:\\nX.shape: (506, 104)\\nThe resulting 104 features are the 13 original features together with the 91 possible\\ncombinations of two features within those 13.5\\nWe will use these datasets to explain and illustrate the properties of the different\\nmachine learning algorithms. But for now, let’s get to the algorithms themselves.\\nFirst, we will revisit the k-nearest neighbors (k-NN) algorithm that we saw in the pre‐\\nvious chapter.\\n34 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 48, 'page_label': '35'}, page_content='k-Nearest Neighbors\\nThe k-NN algorithm is arguably the simplest machine learning algorithm. Building\\nthe model consists only of storing the training dataset. To make a prediction for a\\nnew data point, the algorithm finds the closest data points in the training dataset—its\\n“nearest neighbors. ”\\nk-Neighbors classification\\nIn its simplest version, the k-NN algorithm only considers exactly one nearest neigh‐\\nbor, which is the closest training data point to the point we want to make a prediction\\nfor. The prediction is then simply the known output for this training point. Figure 2-4\\nillustrates this for the case of classification on the forge dataset:\\nIn[10]:\\nmglearn.plots.plot_knn_classification(n_neighbors=1)\\nFigure 2-4. Predictions made by the one-nearest-neighbor model on the forge dataset\\nHere, we added three new data points, shown as stars. For each of them, we marked\\nthe closest point in the training set. The prediction of the one-nearest-neighbor algo‐\\nrithm is the label of that point (shown by the color of the cross).\\nSupervised Machine Learning Algorithms | 35'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 49, 'page_label': '36'}, page_content='Instead of considering only the closest neighbor, we can also consider an arbitrary\\nnumber, k, of neighbors. This is where the name of the k-nearest neighbors algorithm\\ncomes from. When considering more than one neighbor, we use voting to assign a\\nlabel. This means that for each test point, we count how many neighbors belong to\\nclass 0 and how many neighbors belong to class 1. We then assign the class that is\\nmore frequent: in other words, the majority class among the k-nearest neighbors. The\\nfollowing example (Figure 2-5) uses the three closest neighbors:\\nIn[11]:\\nmglearn.plots.plot_knn_classification(n_neighbors=3)\\nFigure 2-5. Predictions made by the three-nearest-neighbors model on the forge dataset\\nAgain, the prediction is shown as the color of the cross. Y ou can see that the predic‐\\ntion for the new data point at the top left is not the same as the prediction when we\\nused only one neighbor.\\nWhile this illustration is for a binary classification problem, this method can be\\napplied to datasets with any number of classes. For more classes, we count how many\\nneighbors belong to each class and again predict the most common class.\\nNow let’s look at how we can apply the k-nearest neighbors algorithm using scikit-\\nlearn. First, we split our data into a training and a test set so we can evaluate general‐\\nization performance, as discussed in Chapter 1:\\n36 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 50, 'page_label': '37'}, page_content='In[12]:\\nfrom sklearn.model_selection import train_test_split\\nX, y = mglearn.datasets.make_forge()\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\nNext, we import and instantiate the class. This is when we can set parameters, like the\\nnumber of neighbors to use. Here, we set it to 3:\\nIn[13]:\\nfrom sklearn.neighbors import KNeighborsClassifier\\nclf = KNeighborsClassifier(n_neighbors=3)\\nNow, we fit the classifier using the training set. For KNeighborsClassifier this\\nmeans storing the dataset, so we can compute neighbors during prediction:\\nIn[14]:\\nclf.fit(X_train, y_train)\\nTo make predictions on the test data, we call the predict method. For each data point\\nin the test set, this computes its nearest neighbors in the training set and finds the\\nmost common class among these:\\nIn[15]:\\nprint(\"Test set predictions: {}\".format(clf.predict(X_test)))\\nOut[15]:\\nTest set predictions: [1 0 1 0 1 0 0]\\nTo evaluate how well our model generalizes, we can call the score method with the\\ntest data together with the test labels:\\nIn[16]:\\nprint(\"Test set accuracy: {:.2f}\".format(clf.score(X_test, y_test)))\\nOut[16]:\\nTest set accuracy: 0.86\\nWe see that our model is about 86% accurate, meaning the model predicted the class\\ncorrectly for 86% of the samples in the test dataset.\\nAnalyzing KNeighborsClassifier\\nFor two-dimensional datasets, we can also illustrate the prediction for all possible test\\npoints in the xy-plane. We color the plane according to the class that would be\\nassigned to a point in this region. This lets us view the decision boundary, which is the\\ndivide between where the algorithm assigns class 0 versus where it assigns class 1.\\nSupervised Machine Learning Algorithms | 37'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 51, 'page_label': '38'}, page_content='The following code produces the visualizations of the decision boundaries for one,\\nthree, and nine neighbors shown in Figure 2-6:\\nIn[17]:\\nfig, axes = plt.subplots(1, 3, figsize=(10, 3))\\nfor n_neighbors, ax in zip([1, 3, 9], axes):\\n    # the fit method returns the object self, so we can instantiate\\n    # and fit in one line\\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X, y)\\n    mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=.4)\\n    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\\n    ax.set_title(\"{} neighbor(s)\".format(n_neighbors))\\n    ax.set_xlabel(\"feature 0\")\\n    ax.set_ylabel(\"feature 1\")\\naxes[0].legend(loc=3)\\nFigure 2-6. Decision boundaries created by the nearest neighbors model for different val‐\\nues of n_neighbors\\nAs you can see on the left in the figure, using a single neighbor results in a decision\\nboundary that follows the training data closely. Considering more and more neigh‐\\nbors leads to a smoother decision boundary. A smoother boundary corresponds to a\\nsimpler model. In other words, using few neighbors corresponds to high model com‐\\nplexity (as shown on the right side of Figure 2-1), and using many neighbors corre‐\\nsponds to low model complexity (as shown on the left side of Figure 2-1 ). If you\\nconsider the extreme case where the number of neighbors is the number of all data\\npoints in the training set, each test point would have exactly the same neighbors (all\\ntraining points) and all predictions would be the same: the class that is most frequent\\nin the training set.\\nLet’s investigate whether we can confirm the connection between model complexity\\nand generalization that we discussed earlier. We will do this on the real-world Breast\\nCancer dataset. We begin by splitting the dataset into a training and a test set. Then\\n38 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 52, 'page_label': '39'}, page_content='we evaluate training and test set performance with different numbers of neighbors.\\nThe results are shown in Figure 2-7:\\nIn[18]:\\nfrom sklearn.datasets import load_breast_cancer\\ncancer = load_breast_cancer()\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, stratify=cancer.target, random_state=66)\\ntraining_accuracy = []\\ntest_accuracy = []\\n# try n_neighbors from 1 to 10\\nneighbors_settings = range(1, 11)\\nfor n_neighbors in neighbors_settings:\\n    # build the model\\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\\n    clf.fit(X_train, y_train)\\n    # record training set accuracy\\n    training_accuracy.append(clf.score(X_train, y_train))\\n    # record generalization accuracy\\n    test_accuracy.append(clf.score(X_test, y_test))\\nplt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\\nplt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\\nplt.ylabel(\"Accuracy\")\\nplt.xlabel(\"n_neighbors\")\\nplt.legend()\\nThe plot shows the training and test set accuracy on the y-axis against the setting of\\nn_neighbors on the x-axis. While real-world plots are rarely very smooth, we can still\\nrecognize some of the characteristics of overfitting and underfitting (note that\\nbecause considering fewer neighbors corresponds to a more complex model, the plot\\nis horizontally flipped relative to the illustration in Figure 2-1). Considering a single\\nnearest neighbor, the prediction on the training set is perfect. But when more neigh‐\\nbors are considered, the model becomes simpler and the training accuracy drops. The\\ntest set accuracy for using a single neighbor is lower than when using more neigh‐\\nbors, indicating that using the single nearest neighbor leads to a model that is too\\ncomplex. On the other hand, when considering 10 neighbors, the model is too simple\\nand performance is even worse. The best performance is somewhere in the middle,\\nusing around six neighbors. Still, it is good to keep the scale of the plot in mind. The\\nworst performance is around 88% accuracy, which might still be acceptable.\\nSupervised Machine Learning Algorithms | 39'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 53, 'page_label': '40'}, page_content='Figure 2-7. Comparison of training and test accuracy as a function of n_neighbors\\nk-neighbors regression\\nThere is also a regression variant of the k-nearest neighbors algorithm. Again, let’s\\nstart by using the single nearest neighbor, this time using the wave dataset. We’ve\\nadded three test data points as green stars on the x-axis. The prediction using a single\\nneighbor is just the target value of the nearest neighbor. These are shown as blue stars\\nin Figure 2-8:\\nIn[19]:\\nmglearn.plots.plot_knn_regression(n_neighbors=1)\\n40 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 54, 'page_label': '41'}, page_content='Figure 2-8. Predictions made by one-nearest-neighbor regression on the wave dataset\\nAgain, we can use more than the single closest neighbor for regression. When using\\nmultiple nearest neighbors, the prediction is the average, or mean, of the relevant\\nneighbors (Figure 2-9):\\nIn[20]:\\nmglearn.plots.plot_knn_regression(n_neighbors=3)\\nSupervised Machine Learning Algorithms | 41'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 55, 'page_label': '42'}, page_content='Figure 2-9. Predictions made by three-nearest-neighbors regression on the wave dataset\\nThe k-nearest neighbors algorithm for regression is implemented in the KNeighbors\\nRegressor class in scikit-learn. It’s used similarly to KNeighborsClassifier:\\nIn[21]:\\nfrom sklearn.neighbors import KNeighborsRegressor\\nX, y = mglearn.datasets.make_wave(n_samples=40)\\n# split the wave dataset into a training and a test set\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n# instantiate the model and set the number of neighbors to consider to 3\\nreg = KNeighborsRegressor(n_neighbors=3)\\n# fit the model using the training data and training targets\\nreg.fit(X_train, y_train)\\nNow we can make predictions on the test set:\\nIn[22]:\\nprint(\"Test set predictions:\\\\n{}\".format(reg.predict(X_test)))\\n42 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 56, 'page_label': '43'}, page_content='Out[22]:\\nTest set predictions:\\n[-0.054  0.357  1.137 -1.894 -1.139 -1.631  0.357  0.912 -0.447 -1.139]\\nWe can also evaluate the model using the score method, which for regressors returns\\nthe R2 score. The R2 score, also known as the coefficient of determination, is a meas‐\\nure of goodness of a prediction for a regression model, and yields a score between 0\\nand 1. A value of 1 corresponds to a perfect prediction, and a value of 0 corresponds\\nto a constant model that just predicts the mean of the training set responses, y_train:\\nIn[23]:\\nprint(\"Test set R^2: {:.2f}\".format(reg.score(X_test, y_test)))\\nOut[23]:\\nTest set R^2: 0.83\\nHere, the score is 0.83, which indicates a relatively good model fit.\\nAnalyzing KNeighborsRegressor\\nFor our one-dimensional dataset, we can see what the predictions look like for all\\npossible feature values (Figure 2-10). To do this, we create a test dataset consisting of\\nmany points on the line:\\nIn[24]:\\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\\n# create 1,000 data points, evenly spaced between -3 and 3\\nline = np.linspace(-3, 3, 1000).reshape(-1, 1)\\nfor n_neighbors, ax in zip([1, 3, 9], axes):\\n    # make predictions using 1, 3, or 9 neighbors\\n    reg = KNeighborsRegressor(n_neighbors=n_neighbors)\\n    reg.fit(X_train, y_train)\\n    ax.plot(line, reg.predict(line))\\n    ax.plot(X_train, y_train, \\'^\\', c=mglearn.cm2(0), markersize=8)\\n    ax.plot(X_test, y_test, \\'v\\', c=mglearn.cm2(1), markersize=8)\\n    ax.set_title(\\n        \"{} neighbor(s)\\\\n train score: {:.2f} test score: {:.2f}\".format(\\n            n_neighbors, reg.score(X_train, y_train),\\n            reg.score(X_test, y_test)))\\n    ax.set_xlabel(\"Feature\")\\n    ax.set_ylabel(\"Target\")\\naxes[0].legend([\"Model predictions\", \"Training data/target\",\\n                \"Test data/target\"], loc=\"best\")Supervised Machine Learning Algorithms | 43'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 57, 'page_label': '44'}, page_content='Figure 2-10. Comparing predictions made by nearest neighbors regression for different\\nvalues of n_neighbors\\nAs we can see from the plot, using only a single neighbor, each point in the training\\nset has an obvious influence on the predictions, and the predicted values go through\\nall of the data points. This leads to a very unsteady prediction. Considering more\\nneighbors leads to smoother predictions, but these do not fit the training data as well.\\nStrengths, weaknesses, and parameters\\nIn principle, there are two important parameters to the KNeighbors classifier: the\\nnumber of neighbors and how you measure distance between data points. In practice,\\nusing a small number of neighbors like three or five often works well, but you should\\ncertainly adjust this parameter. Choosing the right distance measure is somewhat\\nbeyond the scope of this book. By default, Euclidean distance is used, which works\\nwell in many settings.\\nOne of the strengths of k-NN is that the model is very easy to understand, and often\\ngives reasonable performance without a lot of adjustments. Using this algorithm is a\\ngood baseline method to try before considering more advanced techniques. Building\\nthe nearest neighbors model is usually very fast, but when your training set is very\\nlarge (either in number of features or in number of samples) prediction can be slow.\\nWhen using the k-NN algorithm, it’s important to preprocess your data (see Chap‐\\nter 3 ). This approach often does not perform well on datasets with many features\\n(hundreds or more), and it does particularly badly with datasets where most features\\nare 0 most of the time (so-called sparse datasets).\\nSo, while the nearest k-neighbors algorithm is easy to understand, it is not often used\\nin practice, due to prediction being slow and its inability to handle many features.\\nThe method we discuss next has neither of these drawbacks.\\n44 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 58, 'page_label': '45'}, page_content='Linear Models\\nLinear models are a class of models that are widely used in practice and have been\\nstudied extensively in the last few decades, with roots going back over a hundred\\nyears. Linear models make a prediction using a linear function of the input features,\\nwhich we will explain shortly.\\nLinear models for regression\\nFor regression, the general prediction formula for a linear model looks as follows:\\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\\nHere, x[0] to x[p] denotes the features (in this example, the number of features is p)\\nof a single data point, w and b are parameters of the model that are learned, and ŷ is\\nthe prediction the model makes. For a dataset with a single feature, this is:\\nŷ = w[0] * x[0] + b\\nwhich you might remember from high school mathematics as the equation for a line.\\nHere, w[0] is the slope and b is the y-axis offset. For more features, w contains the\\nslopes along each feature axis. Alternatively, you can think of the predicted response\\nas being a weighted sum of the input features, with weights (which can be negative)\\ngiven by the entries of w.\\nTrying to learn the parameters w[0] and b on our one-dimensional wave dataset\\nmight lead to the following line (see Figure 2-11):\\nIn[25]:\\nmglearn.plots.plot_linear_regression_wave()\\nOut[25]:\\nw[0]: 0.393906  b: -0.031804\\nSupervised Machine Learning Algorithms | 45'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 59, 'page_label': '46'}, page_content='Figure 2-11. Predictions of a linear model on the wave dataset\\nWe added a coordinate cross into the plot to make it easier to understand the line.\\nLooking at w[0] we see that the slope should be around 0.4, which we can confirm\\nvisually in the plot. The intercept is where the prediction line should cross the y-axis:\\nthis is slightly below zero, which you can also confirm in the image.\\nLinear models for regression can be characterized as regression models for which the\\nprediction is a line for a single feature, a plane when using two features, or a hyper‐\\nplane in higher dimensions (that is, when using more features).\\nIf you compare the predictions made by the straight line with those made by the\\nKNeighborsRegressor in Figure 2-10, using a straight line to make predictions seems\\nvery restrictive. It looks like all the fine details of the data are lost. In a sense, this is\\ntrue. It is a strong (and somewhat unrealistic) assumption that our target y is a linear\\n46 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 60, 'page_label': '47'}, page_content='6 This is easy to see if you know some linear algebra.\\ncombination of the features. But looking at one-dimensional data gives a somewhat\\nskewed perspective. For datasets with many features, linear models can be very pow‐\\nerful. In particular, if you have more features than training data points, any target y\\ncan be perfectly modeled (on the training set) as a linear function.6\\nThere are many different linear models for regression. The difference between these\\nmodels lies in how the model parameters w and b are learned from the training data,\\nand how model complexity can be controlled. We will now take a look at the most\\npopular linear models for regression.\\nLinear regression (aka ordinary least squares)\\nLinear regression, or ordinary least squares (OLS), is the simplest and most classic lin‐\\near method for regression. Linear regression finds the parameters w and b that mini‐\\nmize the mean squared error between predictions and the true regression targets, y,\\non the training set. The mean squared error is the sum of the squared differences\\nbetween the predictions and the true values. Linear regression has no parameters,\\nwhich is a benefit, but it also has no way to control model complexity.\\nHere is the code that produces the model you can see in Figure 2-11:\\nIn[26]:\\nfrom sklearn.linear_model import LinearRegression\\nX, y = mglearn.datasets.make_wave(n_samples=60)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\\nlr = LinearRegression().fit(X_train, y_train)\\nThe “slope” parameters (w), also called weights or coefficients, are stored in the coef_\\nattribute, while the offset or intercept (b) is stored in the intercept_ attribute:\\nIn[27]:\\nprint(\"lr.coef_: {}\".format(lr.coef_))\\nprint(\"lr.intercept_: {}\".format(lr.intercept_))\\nOut[27]:\\nlr.coef_: [ 0.394]\\nlr.intercept_: -0.031804343026759746\\nSupervised Machine Learning Algorithms | 47'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 61, 'page_label': '48'}, page_content='Y ou might notice the strange-looking trailing underscore at the end\\nof coef_ and intercept_. scikit-learn always stores anything\\nthat is derived from the training data in attributes that end with a\\ntrailing underscore. That is to separate them from parameters that\\nare set by the user.\\nThe intercept_ attribute is always a single float number, while the coef_ attribute is\\na NumPy array with one entry per input feature. As we only have a single input fea‐\\nture in the wave dataset, lr.coef_ only has a single entry.\\nLet’s look at the training set and test set performance:\\nIn[28]:\\nprint(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))\\nOut[28]:\\nTraining set score: 0.67\\nTest set score: 0.66\\nAn R2 of around 0.66 is not very good, but we can see that the scores on the training\\nand test sets are very close together. This means we are likely underfitting, not over‐\\nfitting. For this one-dimensional dataset, there is little danger of overfitting, as the\\nmodel is very simple (or restricted). However, with higher-dimensional datasets\\n(meaning datasets with a large number of features), linear models become more pow‐\\nerful, and there is a higher chance of overfitting. Let’s take a look at how LinearRe\\ngression performs on a more complex dataset, like the Boston Housing dataset.\\nRemember that this dataset has 506 samples and 105 derived features. First, we load\\nthe dataset and split it into a training and a test set. Then we build the linear regres‐\\nsion model as before:\\nIn[29]:\\nX, y = mglearn.datasets.load_extended_boston()\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\nlr = LinearRegression().fit(X_train, y_train)\\nWhen comparing training set and test set scores, we find that we predict very accu‐\\nrately on the training set, but the R2 on the test set is much worse:\\nIn[30]:\\nprint(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))\\n48 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 62, 'page_label': '49'}, page_content='7 Mathematically, Ridge penalizes the L2 norm of the coefficients, or the Euclidean length of w.\\nOut[30]:\\nTraining set score: 0.95\\nTest set score: 0.61\\nThis discrepancy between performance on the training set and the test set is a clear\\nsign of overfitting, and therefore we should try to find a model that allows us to con‐\\ntrol complexity. One of the most commonly used alternatives to standard linear\\nregression is ridge regression, which we will look into next.\\nRidge regression\\nRidge regression is also a linear model for regression, so the formula it uses to make\\npredictions is the same one used for ordinary least squares. In ridge regression,\\nthough, the coefficients (w) are chosen not only so that they predict well on the train‐\\ning data, but also to fit an additional constraint. We also want the magnitude of coef‐\\nficients to be as small as possible; in other words, all entries of w should be close to\\nzero. Intuitively, this means each feature should have as little effect on the outcome as\\npossible (which translates to having a small slope), while still predicting well. This\\nconstraint is an example of what is called regularization. Regularization means explic‐\\nitly restricting a model to avoid overfitting. The particular kind used by ridge regres‐\\nsion is known as L2 regularization.7\\nRidge regression is implemented in linear_model.Ridge. Let’s see how well it does\\non the extended Boston Housing dataset:\\nIn[31]:\\nfrom sklearn.linear_model import Ridge\\nridge = Ridge().fit(X_train, y_train)\\nprint(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))\\nOut[31]:\\nTraining set score: 0.89\\nTest set score: 0.75\\nAs you can see, the training set score of Ridge is lower than for LinearRegression,\\nwhile the test set score is higher. This is consistent with our expectation. With linear\\nregression, we were overfitting our data. Ridge is a more restricted model, so we are\\nless likely to overfit. A less complex model means worse performance on the training\\nset, but better generalization. As we are only interested in generalization perfor‐\\nmance, we should choose the Ridge model over the LinearRegression model.Supervised Machine Learning Algorithms | 49'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 63, 'page_label': '50'}, page_content='The Ridge model makes a trade-off between the simplicity of the model (near-zero\\ncoefficients) and its performance on the training set. How much importance the\\nmodel places on simplicity versus training set performance can be specified by the\\nuser, using the alpha parameter. In the previous example, we used the default param‐\\neter alpha=1.0. There is no reason why this will give us the best trade-off, though.\\nThe optimum setting of alpha depends on the particular dataset we are using.\\nIncreasing alpha forces coefficients to move more toward zero, which decreases\\ntraining set performance but might help generalization. For example:\\nIn[32]:\\nridge10 = Ridge(alpha=10).fit(X_train, y_train)\\nprint(\"Training set score: {:.2f}\".format(ridge10.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(ridge10.score(X_test, y_test)))\\nOut[32]:\\nTraining set score: 0.79\\nTest set score: 0.64\\nDecreasing alpha allows the coefficients to be less restricted, meaning we move right\\nin Figure 2-1. For very small values of alpha, coefficients are barely restricted at all,\\nand we end up with a model that resembles LinearRegression:\\nIn[33]:\\nridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\\nprint(\"Training set score: {:.2f}\".format(ridge01.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(ridge01.score(X_test, y_test)))\\nOut[33]:\\nTraining set score: 0.93\\nTest set score: 0.77\\nHere, alpha=0.1 seems to be working well. We could try decreasing alpha even more\\nto improve generalization. For now, notice how the parameter alpha corresponds to\\nthe model complexity as shown in Figure 2-1. We will discuss methods to properly\\nselect parameters in Chapter 5.\\nWe can also get a more qualitative insight into how the alpha parameter changes the\\nmodel by inspecting the coef_ attribute of models with different values of alpha. A\\nhigher alpha means a more restricted model, so we expect the entries of coef_ to\\nhave smaller magnitude for a high value of alpha than for a low value of alpha. This\\nis confirmed in the plot in Figure 2-12:\\n50 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 64, 'page_label': '51'}, page_content='In[34]:\\nplt.plot(ridge.coef_, \\'s\\', label=\"Ridge alpha=1\")\\nplt.plot(ridge10.coef_, \\'^\\', label=\"Ridge alpha=10\")\\nplt.plot(ridge01.coef_, \\'v\\', label=\"Ridge alpha=0.1\")\\nplt.plot(lr.coef_, \\'o\\', label=\"LinearRegression\")\\nplt.xlabel(\"Coefficient index\")\\nplt.ylabel(\"Coefficient magnitude\")\\nplt.hlines(0, 0, len(lr.coef_))\\nplt.ylim(-25, 25)\\nplt.legend()\\nFigure 2-12. Comparing coefficient magnitudes for ridge regression with different values\\nof alpha and linear regression\\nHere, the x-axis enumerates the entries of coef_: x=0 shows the coefficient associated\\nwith the first feature, x=1 the coefficient associated with the second feature, and so on\\nup to x=100. The y-axis shows the numeric values of the corresponding values of the\\ncoefficients. The main takeaway here is that for alpha=10, the coefficients are mostly\\nbetween around –3 and 3. The coefficients for the Ridge model with alpha=1 are\\nsomewhat larger. The dots corresponding to alpha=0.1 have larger magnitude still,\\nand many of the dots corresponding to linear regression without any regularization\\n(which would be alpha=0) are so large they are outside of the chart.\\nSupervised Machine Learning Algorithms | 51'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 65, 'page_label': '52'}, page_content='Another way to understand the influence of regularization is to fix a value of alpha\\nbut vary the amount of training data available. For Figure 2-13, we subsampled the\\nBoston Housing dataset and evaluated LinearRegression and Ridge(alpha=1) on\\nsubsets of increasing size (plots that show model performance as a function of dataset\\nsize are called learning curves):\\nIn[35]:\\nmglearn.plots.plot_ridge_n_samples()\\nFigure 2-13. Learning curves for ridge regression and linear regression on the Boston\\nHousing dataset\\nAs one would expect, the training score is higher than the test score for all dataset\\nsizes, for both ridge and linear regression. Because ridge is regularized, the training\\nscore of ridge is lower than the training score for linear regression across the board.\\nHowever, the test score for ridge is better, particularly for small subsets of the data.\\nFor less than 400 data points, linear regression is not able to learn anything. As more\\nand more data becomes available to the model, both models improve, and linear\\nregression catches up with ridge in the end. The lesson here is that with enough train‐\\ning data, regularization becomes less important, and given enough data, ridge and\\n52 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 66, 'page_label': '53'}, page_content='8 The lasso penalizes the L1 norm of the coefficient vector—or in other words, the sum of the absolute values of\\nthe coefficients.\\nlinear regression will have the same performance (the fact that this happens here\\nwhen using the full dataset is just by chance). Another interesting aspect of\\nFigure 2-13 is the decrease in training performance for linear regression. If more data\\nis added, it becomes harder for a model to overfit, or memorize the data.\\nLasso\\nAn alternative to Ridge for regularizing linear regression is Lasso. As with ridge\\nregression, using the lasso also restricts coefficients to be close to zero, but in a\\nslightly different way, called L1 regularization.8 The consequence of L1 regularization\\nis that when using the lasso, some coefficients are exactly zero. This means some fea‐\\ntures are entirely ignored by the model. This can be seen as a form of automatic fea‐\\nture selection. Having some coefficients be exactly zero often makes a model easier to\\ninterpret, and can reveal the most important features of your model.\\nLet’s apply the lasso to the extended Boston Housing dataset:\\nIn[36]:\\nfrom sklearn.linear_model import Lasso\\nlasso = Lasso().fit(X_train, y_train)\\nprint(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\\nprint(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))\\nOut[36]:\\nTraining set score: 0.29\\nTest set score: 0.21\\nNumber of features used: 4\\nAs you can see, Lasso does quite badly, both on the training and the test set. This\\nindicates that we are underfitting, and we find that it used only 4 of the 105 features.\\nSimilarly to Ridge, the Lasso also has a regularization parameter, alpha, that controls\\nhow strongly coefficients are pushed toward zero. In the previous example, we used\\nthe default of alpha=1.0. To reduce underfitting, let’s try decreasing alpha. When we\\ndo this, we also need to increase the default setting of max_iter (the maximum num‐\\nber of iterations to run):\\nSupervised Machine Learning Algorithms | 53'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 67, 'page_label': '54'}, page_content='In[37]:\\n# we increase the default setting of \"max_iter\",\\n# otherwise the model would warn us that we should increase max_iter.\\nlasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)\\nprint(\"Training set score: {:.2f}\".format(lasso001.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(lasso001.score(X_test, y_test)))\\nprint(\"Number of features used: {}\".format(np.sum(lasso001.coef_ != 0)))\\nOut[37]:\\nTraining set score: 0.90\\nTest set score: 0.77\\nNumber of features used: 33\\nA lower alpha allowed us to fit a more complex model, which worked better on the\\ntraining and test data. The performance is slightly better than using Ridge, and we are\\nusing only 33 of the 105 features. This makes this model potentially easier to under‐\\nstand.\\nIf we set alpha too low, however, we again remove the effect of regularization and end\\nup overfitting, with a result similar to LinearRegression:\\nIn[38]:\\nlasso00001 = Lasso(alpha=0.0001, max_iter=100000).fit(X_train, y_train)\\nprint(\"Training set score: {:.2f}\".format(lasso00001.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(lasso00001.score(X_test, y_test)))\\nprint(\"Number of features used: {}\".format(np.sum(lasso00001.coef_ != 0)))\\nOut[38]:\\nTraining set score: 0.95\\nTest set score: 0.64\\nNumber of features used: 94\\nAgain, we can plot the coefficients of the different models, similarly to Figure 2-12.\\nThe result is shown in Figure 2-14:\\nIn[39]:\\nplt.plot(lasso.coef_, \\'s\\', label=\"Lasso alpha=1\")\\nplt.plot(lasso001.coef_, \\'^\\', label=\"Lasso alpha=0.01\")\\nplt.plot(lasso00001.coef_, \\'v\\', label=\"Lasso alpha=0.0001\")\\nplt.plot(ridge01.coef_, \\'o\\', label=\"Ridge alpha=0.1\")\\nplt.legend(ncol=2, loc=(0, 1.05))\\nplt.ylim(-25, 25)\\nplt.xlabel(\"Coefficient index\")\\nplt.ylabel(\"Coefficient magnitude\")\\n54 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 68, 'page_label': '55'}, page_content='Figure 2-14. Comparing coefficient magnitudes for lasso regression with different values\\nof alpha and ridge regression\\nFor alpha=1, we not only see that most of the coefficients are zero (which we already\\nknew), but that the remaining coefficients are also small in magnitude. Decreasing\\nalpha to 0.01, we obtain the solution shown as the green dots, which causes most\\nfeatures to be exactly zero. Using alpha=0.00001, we get a model that is quite unregu‐\\nlarized, with most coefficients nonzero and of large magnitude. For comparison, the\\nbest Ridge solution is shown in teal. The Ridge model with alpha=0.1 has similar\\npredictive performance as the lasso model with alpha=0.01, but using Ridge, all coef‐\\nficients are nonzero.\\nIn practice, ridge regression is usually the first choice between these two models.\\nHowever, if you have a large amount of features and expect only a few of them to be\\nimportant, Lasso might be a better choice. Similarly, if you would like to have a\\nmodel that is easy to interpret, Lasso will provide a model that is easier to under‐\\nstand, as it will select only a subset of the input features. scikit-learn also provides\\nthe ElasticNet class, which combines the penalties of Lasso and Ridge. In practice,\\nthis combination works best, though at the price of having two parameters to adjust:\\none for the L1 regularization, and one for the L2 regularization.\\nSupervised Machine Learning Algorithms | 55'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 69, 'page_label': '56'}, page_content='Linear models for classification\\nLinear models are also extensively used for classification. Let’s look at binary classifi‐\\ncation first. In this case, a prediction is made using the following formula:\\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0\\nThe formula looks very similar to the one for linear regression, but instead of just\\nreturning the weighted sum of the features, we threshold the predicted value at zero.\\nIf the function is smaller than zero, we predict the class –1; if it is larger than zero, we\\npredict the class +1. This prediction rule is common to all linear models for classifica‐\\ntion. Again, there are many different ways to find the coefficients ( w) and the inter‐\\ncept (b).\\nFor linear models for regression, the output, ŷ, is a linear function of the features: a\\nline, plane, or hyperplane (in higher dimensions). For linear models for classification,\\nthe decision boundary is a linear function of the input. In other words, a (binary) lin‐\\near classifier is a classifier that separates two classes using a line, a plane, or a hyper‐\\nplane. We will see examples of that in this section.\\nThere are many algorithms for learning linear models. These algorithms all differ in\\nthe following two ways:\\n• The way in which they measure how well a particular combination of coefficients\\nand intercept fits the training data\\n• If and what kind of regularization they use\\nDifferent algorithms choose different ways to measure what “fitting the training set\\nwell” means. For technical mathematical reasons, it is not possible to adjust w and b\\nto minimize the number of misclassifications the algorithms produce, as one might\\nhope. For our purposes, and many applications, the different choices for item 1 in the\\npreceding list (called loss functions) are of little significance.\\nThe two most common linear classification algorithms are logistic regression, imple‐\\nmented in linear_model.LogisticRegression, and linear support vector machines\\n(linear SVMs), implemented in svm.LinearSVC (SVC stands for support vector classi‐\\nfier). Despite its name, LogisticRegression is a classification algorithm and not a\\nregression algorithm, and it should not be confused with LinearRegression.\\nWe can apply the LogisticRegression and LinearSVC models to the forge dataset,\\nand visualize the decision boundary as found by the linear models (Figure 2-15):\\n56 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 70, 'page_label': '57'}, page_content='In[40]:\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import LinearSVC\\nX, y = mglearn.datasets.make_forge()\\nfig, axes = plt.subplots(1, 2, figsize=(10, 3))\\nfor model, ax in zip([LinearSVC(), LogisticRegression()], axes):\\n    clf = model.fit(X, y)\\n    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,\\n                                    ax=ax, alpha=.7)\\n    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\\n    ax.set_title(\"{}\".format(clf.__class__.__name__))\\n    ax.set_xlabel(\"Feature 0\")\\n    ax.set_ylabel(\"Feature 1\")\\naxes[0].legend()\\nFigure 2-15. Decision boundaries of a linear SVM and logistic regression on the forge\\ndataset with the default parameters\\nIn this figure, we have the first feature of the forge dataset on the x-axis and the sec‐\\nond feature on the y-axis, as before. We display the decision boundaries found by\\nLinearSVC and LogisticRegression respectively as straight lines, separating the area\\nclassified as class 1 on the top from the area classified as class 0 on the bottom. In\\nother words, any new data point that lies above the black line will be classified as class\\n1 by the respective classifier, while any point that lies below the black line will be clas‐\\nsified as class 0.\\nThe two models come up with similar decision boundaries. Note that both misclas‐\\nsify two of the points. By default, both models apply an L2 regularization, in the same\\nway that Ridge does for regression.\\nFor LogisticRegression and LinearSVC the trade-off parameter that determines the\\nstrength of the regularization is called C, and higher values of C correspond to less\\nSupervised Machine Learning Algorithms | 57'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 71, 'page_label': '58'}, page_content='regularization. In other words, when you use a high value for the parameter C, Logis\\nticRegression and LinearSVC try to fit the training set as best as possible, while with\\nlow values of the parameter C, the models put more emphasis on finding a coefficient\\nvector (w) that is close to zero.\\nThere is another interesting aspect of how the parameter C acts. Using low values of C\\nwill cause the algorithms to try to adjust to the “majority” of data points, while using\\na higher value of C stresses the importance that each individual data point be classi‐\\nfied correctly. Here is an illustration using LinearSVC (Figure 2-16):\\nIn[41]:\\nmglearn.plots.plot_linear_svc_regularization()\\nFigure 2-16. Decision boundaries of a linear SVM on the forge dataset for different\\nvalues of C\\nOn the lefthand side, we have a very small C corresponding to a lot of regularization.\\nMost of the points in class 0 are at the top, and most of the points in class 1 are at the\\nbottom. The strongly regularized model chooses a relatively horizontal line, misclas‐\\nsifying two points. In the center plot, C is slightly higher, and the model focuses more\\non the two misclassified samples, tilting the decision boundary. Finally, on the right‐\\nhand side, the very high value of C in the model tilts the decision boundary a lot, now\\ncorrectly classifying all points in class 0. One of the points in class 1 is still misclassi‐\\nfied, as it is not possible to correctly classify all points in this dataset using a straight\\nline. The model illustrated on the righthand side tries hard to correctly classify all\\npoints, but might not capture the overall layout of the classes well. In other words,\\nthis model is likely overfitting.\\nSimilarly to the case of regression, linear models for classification might seem very\\nrestrictive in low-dimensional spaces, only allowing for decision boundaries that are\\nstraight lines or planes. Again, in high dimensions, linear models for classification\\n58 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 72, 'page_label': '59'}, page_content='become very powerful, and guarding against overfitting becomes increasingly impor‐\\ntant when considering more features.\\nLet’s analyze LinearLogistic in more detail on the Breast Cancer dataset:\\nIn[42]:\\nfrom sklearn.datasets import load_breast_cancer\\ncancer = load_breast_cancer()\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\\nlogreg = LogisticRegression().fit(X_train, y_train)\\nprint(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\\nprint(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\\nOut[42]:\\nTraining set score: 0.953\\nTest set score: 0.958\\nThe default value of C=1 provides quite good performance, with 95% accuracy on\\nboth the training and the test set. But as training and test set performance are very\\nclose, it is likely that we are underfitting. Let’s try to increase C to fit a more flexible\\nmodel:\\nIn[43]:\\nlogreg100 = LogisticRegression(C=100).fit(X_train, y_train)\\nprint(\"Training set score: {:.3f}\".format(logreg100.score(X_train, y_train)))\\nprint(\"Test set score: {:.3f}\".format(logreg100.score(X_test, y_test)))\\nOut[43]:\\nTraining set score: 0.972\\nTest set score: 0.965\\nUsing C=100 results in higher training set accuracy, and also a slightly increased test\\nset accuracy, confirming our intuition that a more complex model should perform\\nbetter.\\nWe can also investigate what happens if we use an even more regularized model than\\nthe default of C=1, by setting C=0.01:\\nIn[44]:\\nlogreg001 = LogisticRegression(C=0.01).fit(X_train, y_train)\\nprint(\"Training set score: {:.3f}\".format(logreg001.score(X_train, y_train)))\\nprint(\"Test set score: {:.3f}\".format(logreg001.score(X_test, y_test)))\\nOut[44]:\\nTraining set score: 0.934\\nTest set score: 0.930\\nSupervised Machine Learning Algorithms | 59'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 73, 'page_label': '60'}, page_content='As expected, when moving more to the left along the scale shown in Figure 2-1 from\\nan already underfit model, both training and test set accuracy decrease relative to the\\ndefault parameters.\\nFinally, let’s look at the coefficients learned by the models with the three different set‐\\ntings of the regularization parameter C (Figure 2-17):\\nIn[45]:\\nplt.plot(logreg.coef_.T, \\'o\\', label=\"C=1\")\\nplt.plot(logreg100.coef_.T, \\'^\\', label=\"C=100\")\\nplt.plot(logreg001.coef_.T, \\'v\\', label=\"C=0.001\")\\nplt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\\nplt.hlines(0, 0, cancer.data.shape[1])\\nplt.ylim(-5, 5)\\nplt.xlabel(\"Coefficient index\")\\nplt.ylabel(\"Coefficient magnitude\")\\nplt.legend()\\nAs LogisticRegression applies an L2 regularization by default,\\nthe result looks similar to that produced by Ridge in Figure 2-12.\\nStronger regularization pushes coefficients more and more toward\\nzero, though coefficients never become exactly zero. Inspecting the\\nplot more closely, we can also see an interesting effect in the third\\ncoefficient, for “mean perimeter. ” For C=100 and C=1, the coefficient\\nis negative, while for C=0.001, the coefficient is positive, with a\\nmagnitude that is even larger than for C=1. Interpreting a model\\nlike this, one might think the coefficient tells us which class a fea‐\\nture is associated with. For example, one might think that a high\\n“texture error” feature is related to a sample being “malignant. ”\\nHowever, the change of sign in the coefficient for “mean perimeter”\\nmeans that depending on which model we look at, a high “mean\\nperimeter” could be taken as being either indicative of “benign” or\\nindicative of “malignant. ” This illustrates that interpretations of\\ncoefficients of linear models should always be taken with a grain of\\nsalt.\\n60 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 74, 'page_label': '61'}, page_content='Figure 2-17. Coefficients learned by logistic regression on the Breast Cancer dataset for\\ndifferent values of C\\nSupervised Machine Learning Algorithms | 61'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 75, 'page_label': '62'}, page_content='If we desire a more interpretable model, using L1 regularization might help, as it lim‐\\nits the model to using only a few features. Here is the coefficient plot and classifica‐\\ntion accuracies for L1 regularization (Figure 2-18):\\nIn[46]:\\nfor C, marker in zip([0.001, 1, 100], [\\'o\\', \\'^\\', \\'v\\']):\\n    lr_l1 = LogisticRegression(C=C, penalty=\"l1\").fit(X_train, y_train)\\n    print(\"Training accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\\n          C, lr_l1.score(X_train, y_train)))\\n    print(\"Test accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\\n          C, lr_l1.score(X_test, y_test)))\\n    plt.plot(lr_l1.coef_.T, marker, label=\"C={:.3f}\".format(C))\\nplt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\\nplt.hlines(0, 0, cancer.data.shape[1])\\nplt.xlabel(\"Coefficient index\")\\nplt.ylabel(\"Coefficient magnitude\")\\nplt.ylim(-5, 5)\\nplt.legend(loc=3)\\nOut[46]:\\nTraining accuracy of l1 logreg with C=0.001: 0.91\\nTest accuracy of l1 logreg with C=0.001: 0.92\\nTraining accuracy of l1 logreg with C=1.000: 0.96\\nTest accuracy of l1 logreg with C=1.000: 0.96\\nTraining accuracy of l1 logreg with C=100.000: 0.99\\nTest accuracy of l1 logreg with C=100.000: 0.98\\nAs you can see, there are many parallels between linear models for binary classifica‐\\ntion and linear models for regression. As in regression, the main difference between\\nthe models is the penalty parameter, which influences the regularization and\\nwhether the model will use all available features or select only a subset.\\n62 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 76, 'page_label': '63'}, page_content='Figure 2-18. Coefficients learned by logistic regression with L1 penalty on the Breast\\nCancer dataset for different values of C\\nLinear models for multiclass classification\\nMany linear classification models are for binary classification only, and don’t extend\\nnaturally to the multiclass case (with the exception of logistic regression). A common\\ntechnique to extend a binary classification algorithm to a multiclass classification\\nalgorithm is the one-vs.-rest approach. In the one-vs.-rest approach, a binary model is\\nlearned for each class that tries to separate that class from all of the other classes,\\nresulting in as many binary models as there are classes. To make a prediction, all\\nbinary classifiers are run on a test point. The classifier that has the highest score on its\\nsingle class “wins, ” and this class label is returned as the prediction.\\nSupervised Machine Learning Algorithms | 63'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 77, 'page_label': '64'}, page_content='Having one binary classifier per class results in having one vector of coefficients ( w)\\nand one intercept (b) for each class. The class for which the result of the classification\\nconfidence formula given here is highest is the assigned class label:\\nw[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\\nThe mathematics behind multiclass logistic regression differ somewhat from the one-\\nvs.-rest approach, but they also result in one coefficient vector and intercept per class,\\nand the same method of making a prediction is applied.\\nLet’s apply the one-vs.-rest method to a simple three-class classification dataset. We\\nuse a two-dimensional dataset, where each class is given by data sampled from a\\nGaussian distribution (see Figure 2-19):\\nIn[47]:\\nfrom sklearn.datasets import make_blobs\\nX, y = make_blobs(random_state=42)\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nplt.legend([\"Class 0\", \"Class 1\", \"Class 2\"])\\nFigure 2-19. Two-dimensional toy dataset containing three classes\\n64 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 78, 'page_label': '65'}, page_content='Now, we train a LinearSVC classifier on the dataset:\\nIn[48]:\\nlinear_svm = LinearSVC().fit(X, y)\\nprint(\"Coefficient shape: \", linear_svm.coef_.shape)\\nprint(\"Intercept shape: \", linear_svm.intercept_.shape)\\nOut[48]:\\nCoefficient shape:  (3, 2)\\nIntercept shape:  (3,)\\nWe see that the shape of the coef_ is (3, 2), meaning that each row of coef_ con‐\\ntains the coefficient vector for one of the three classes and each column holds the\\ncoefficient value for a specific feature (there are two in this dataset). The intercept_\\nis now a one-dimensional array, storing the intercepts for each class.\\nLet’s visualize the lines given by the three binary classifiers ( Figure 2-20):\\nIn[49]:\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\nline = np.linspace(-15, 15)\\nfor coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\\n                                  [\\'b\\', \\'r\\', \\'g\\']):\\n    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\\nplt.ylim(-10, 15)\\nplt.xlim(-10, 8)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nplt.legend([\\'Class 0\\', \\'Class 1\\', \\'Class 2\\', \\'Line class 0\\', \\'Line class 1\\',\\n            \\'Line class 2\\'], loc=(1.01, 0.3))\\nY ou can see that all the points belonging to class 0 in the training data are above the\\nline corresponding to class 0, which means they are on the “class 0” side of this binary\\nclassifier. The points in class 0 are above the line corresponding to class 2, which\\nmeans they are classified as “rest” by the binary classifier for class 2. The points\\nbelonging to class 0 are to the left of the line corresponding to class 1, which means\\nthe binary classifier for class 1 also classifies them as “rest. ” Therefore, any point in\\nthis area will be classified as class 0 by the final classifier (the result of the classifica‐\\ntion confidence formula for classifier 0 is greater than zero, while it is smaller than\\nzero for the other two classes).\\nBut what about the triangle in the middle of the plot? All three binary classifiers clas‐\\nsify points there as “rest. ” Which class would a point there be assigned to? The answer\\nis the one with the highest value for the classification formula: the class of the closest\\nline.\\nSupervised Machine Learning Algorithms | 65'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 79, 'page_label': '66'}, page_content='Figure 2-20. Decision boundaries learned by the three one-vs.-rest classifiers\\nThe following example ( Figure 2-21) shows the predictions for all regions of the 2D\\nspace:\\nIn[50]:\\nmglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7)\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\nline = np.linspace(-15, 15)\\nfor coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\\n                                  [\\'b\\', \\'r\\', \\'g\\']):\\n    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\\nplt.legend([\\'Class 0\\', \\'Class 1\\', \\'Class 2\\', \\'Line class 0\\', \\'Line class 1\\',\\n            \\'Line class 2\\'], loc=(1.01, 0.3))\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\n66 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 80, 'page_label': '67'}, page_content=\"Figure 2-21. Multiclass decision boundaries derived from the three one-vs.-rest classifiers\\nStrengths, weaknesses, and parameters\\nThe main parameter of linear models is the regularization parameter, called alpha in\\nthe regression models and C in LinearSVC and LogisticRegression. Large values for\\nalpha or small values for C mean simple models. In particular for the regression mod‐\\nels, tuning these parameters is quite important. Usually C and alpha are searched for\\non a logarithmic scale. The other decision you have to make is whether you want to\\nuse L1 regularization or L2 regularization. If you assume that only a few of your fea‐\\ntures are actually important, you should use L1. Otherwise, you should default to L2.\\nL1 can also be useful if interpretability of the model is important. As L1 will use only\\na few features, it is easier to explain which features are important to the model, and\\nwhat the effects of these features are.\\nLinear models are very fast to train, and also fast to predict. They scale to very large\\ndatasets and work well with sparse data. If your data consists of hundreds of thou‐\\nsands or millions of samples, you might want to investigate using the solver='sag'\\noption in LogisticRegression and Ridge, which can be faster than the default on\\nlarge datasets. Other options are the SGDClassifier class and the SGDRegressor\\nclass, which implement even more scalable versions of the linear models described\\nhere.\\nAnother strength of linear models is that they make it relatively easy to understand\\nhow a prediction is made, using the formulas we saw earlier for regression and classi‐\\nfication. Unfortunately, it is often not entirely clear why coefficients are the way they\\nare. This is particularly true if your dataset has highly correlated features; in these\\ncases, the coefficients might be hard to interpret.\\nSupervised Machine Learning Algorithms | 67\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 81, 'page_label': '68'}, page_content='Linear models often perform well when the number of features is large compared to\\nthe number of samples. They are also often used on very large datasets, simply\\nbecause it’s not feasible to train other models. However, in lower-dimensional spaces,\\nother models might yield better generalization performance. We will look at some\\nexamples in which linear models fail in “Kernelized Support Vector Machines” on\\npage 92.\\nMethod Chaining\\nThe fit method of all scikit-learn models returns self. This allows you to write\\ncode like the following, which we’ve already used extensively in this chapter:\\nIn[51]:\\n# instantiate model and fit it in one line\\nlogreg = LogisticRegression().fit(X_train, y_train)\\nHere, we used the return value of fit (which is self) to assign the trained model to\\nthe variable logreg. This concatenation of method calls (here __init__ and then fit)\\nis known as method chaining. Another common application of method chaining in\\nscikit-learn is to fit and predict in one line:\\nIn[52]:\\nlogreg = LogisticRegression()\\ny_pred = logreg.fit(X_train, y_train).predict(X_test)\\nFinally, you can even do model instantiation, fitting, and predicting in one line:\\nIn[53]:\\ny_pred = LogisticRegression().fit(X_train, y_train).predict(X_test)\\nThis very short variant is not ideal, though. A lot is happening in a single line, which\\nmight make the code hard to read. Additionally, the fitted logistic regression model\\nisn’t stored in any variable, so we can’t inspect it or use it to predict on any other data.\\nNaive Bayes \\nClassifiers\\nNaive Bayes classifiers are a family of classifiers that are quite similar to the linear\\nmodels discussed in the previous section. However, they tend to be even faster in\\ntraining. The price paid for this efficiency is that naive Bayes models often provide\\ngeneralization performance that is slightly worse than that of linear classifiers like\\nLogisticRegression and LinearSVC.\\nThe reason that naive Bayes models are so efficient is that they learn parameters by\\nlooking at each feature individually and collect simple per-class statistics from each\\nfeature. There are three kinds of naive Bayes classifiers implemented in scikit-\\n68 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 82, 'page_label': '69'}, page_content='learn: GaussianNB, BernoulliNB, and MultinomialNB. GaussianNB can be applied to\\nany continuous data, while BernoulliNB assumes binary data and MultinomialNB\\nassumes count data (that is, that each feature represents an integer count of some‐\\nthing, like how often a word appears in a sentence). BernoulliNB and MultinomialNB\\nare mostly used in text data classification.\\nThe BernoulliNB classifier counts how often every feature of each class is not zero.\\nThis is most easily understood with an example:\\nIn[54]:\\nX = np.array([[0, 1, 0, 1],\\n              [1, 0, 1, 1],\\n              [0, 0, 0, 1],\\n              [1, 0, 1, 0]])\\ny = np.array([0, 1, 0, 1])\\nHere, we have four data points, with four binary features each. There are two classes,\\n0 and 1. For class 0 (the first and third data points), the first feature is zero two times\\nand nonzero zero times, the second feature is zero one time and nonzero one time,\\nand so on. These same counts are then calculated for the data points in the second\\nclass. Counting the nonzero entries per class in essence looks like this:\\nIn[55]:\\ncounts = {}\\nfor label in np.unique(y):\\n    # iterate over each class\\n    # count (sum) entries of 1 per feature\\n    counts[label] = X[y == label].sum(axis=0)\\nprint(\"Feature counts:\\\\n{}\".format(counts))\\nOut[55]:\\nFeature counts:\\n{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\\nThe other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐\\nferent in what kinds of statistics they compute. MultinomialNB takes into account the\\naverage value of each feature for each class, while GaussianNB stores the average value\\nas well as the standard deviation of each feature for each class.\\nTo make a prediction, a data point is compared to the statistics for each of the classes,\\nand the best matching class is predicted. Interestingly, for both MultinomialNB and\\nBernoulliNB, this leads to a prediction formula that is of the same form as in the lin‐\\near models (see “Linear models for classification” on page 56). Unfortunately, coef_\\nfor the naive Bayes models has a somewhat different meaning than in the linear mod‐\\nels, in that coef_ is not the same as w.\\nSupervised Machine Learning Algorithms | 69'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 83, 'page_label': '70'}, page_content='Strengths, weaknesses, and parameters\\nMultinomialNB and BernoulliNB have a single parameter, alpha, which controls\\nmodel complexity. The way alpha works is that the algorithm adds to the data alpha\\nmany virtual data points that have positive values for all the features. This results in a\\n“smoothing” of the statistics. A large alpha means more smoothing, resulting in less\\ncomplex models. The algorithm’s performance is relatively robust to the setting of\\nalpha, meaning that setting alpha is not critical for good performance. However,\\ntuning it usually improves accuracy somewhat.\\nGaussianNB is mostly used on very high-dimensional data, while the other two var‐\\niants of naive Bayes are widely used for sparse count data such as text. MultinomialNB\\nusually performs better than BinaryNB, particularly on datasets with a relatively large\\nnumber of nonzero features (i.e., large documents).\\nThe naive Bayes models share many of the strengths and weaknesses of the linear\\nmodels. They are very fast to train and to predict, and the training procedure is easy\\nto understand. The models work very well with high-dimensional sparse data and are\\nrelatively robust to the parameters. Naive Bayes models are great baseline models and\\nare often used on very large datasets, where training even a linear model might take\\ntoo long.\\nDecision Trees\\nDecision trees are widely used models for classification and regression tasks. Essen‐\\ntially, they learn a hierarchy of if/else questions, leading to a decision.\\nThese questions are similar to the questions you might ask in a game of 20 Questions.\\nImagine you want to distinguish between the following four animals: bears, hawks,\\npenguins, and dolphins. Y our goal is to get to the right answer by asking as few if/else\\nquestions as possible. Y ou might start off by asking whether the animal has feathers, a\\nquestion that narrows down your possible animals to just two. If the answer is “yes, ”\\nyou can ask another question that could help you distinguish between hawks and\\npenguins. For example, you could ask whether the animal can fly. If the animal\\ndoesn’t have feathers, your possible animal choices are dolphins and bears, and you\\nwill need to ask a question to distinguish between these two animals—for example,\\nasking whether the animal has fins.\\nThis series of questions can be expressed as a decision tree, as shown in Figure 2-22.\\nIn[56]:\\nmglearn.plots.plot_animal_tree()\\n70 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 84, 'page_label': '71'}, page_content='Figure 2-22. A decision tree to distinguish among several animals\\nIn this illustration, each node in the tree either represents a question or a terminal\\nnode (also called a leaf) that contains the answer. The edges connect the answers to a\\nquestion with the next question you would ask.\\nIn machine learning parlance, we built a model to distinguish between four classes of\\nanimals (hawks, penguins, dolphins, and bears) using the three features “has feath‐\\ners, ” “can fly, ” and “has fins. ” Instead of building these models by hand, we can learn\\nthem from data using supervised learning.\\nBuilding decision trees\\nLet’s go through the process of building a decision tree for the 2D classification data‐\\nset shown in Figure 2-23 . The dataset consists of two half-moon shapes, with each\\nclass consisting of 75 data points. We will refer to this dataset as two_moons.\\nLearning a decision tree means learning the sequence of if/else questions that gets us\\nto the true answer most quickly. In the machine learning setting, these questions are\\ncalled tests (not to be confused with the test set, which is the data we use to test to see\\nhow generalizable our model is). Usually data does not come in the form of binary\\nyes/no features as in the animal example, but is instead represented as continuous\\nfeatures such as in the 2D dataset shown in Figure 2-23. The tests that are used on\\ncontinuous data are of the form “Is feature i larger than value a?”\\nSupervised Machine Learning Algorithms | 71'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 85, 'page_label': '72'}, page_content='Figure 2-23. Two-moons dataset on which the decision tree will be built\\nTo build a tree, the algorithm searches over all possible tests and finds the one that is\\nmost informative about the target variable. Figure 2-24  shows the first test that is\\npicked. Splitting the dataset vertically at x[1]=0.0596 yields the most information; it\\nbest separates the points in class 1 from the points in class 2. The top node, also called\\nthe root, represents the whole dataset, consisting of 75 points belonging to class 0 and\\n75 points belonging to class 1. The split is done by testing whether x[1] <= 0.0596,\\nindicated by a black line. If the test is true, a point is assigned to the left node, which\\ncontains 2 points belonging to class 0 and 32 points belonging to class 1. Otherwise\\nthe point is assigned to the right node, which contains 48 points belonging to class 0\\nand 18 points belonging to class 1. These two nodes correspond to the top and bot‐\\ntom regions shown in Figure 2-24. Even though the first split did a good job of sepa‐\\nrating the two classes, the bottom region still contains points belonging to class 0, and\\nthe top region still contains points belonging to class 1. We can build a more accurate\\nmodel by repeating the process of looking for the best test in both regions.\\nFigure 2-25 shows that the most informative next split for the left and the right region\\nis based on x[0].\\n72 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 86, 'page_label': '73'}, page_content='Figure 2-24. Decision boundary of tree with depth 1 (left) and corresponding tree (right)\\nFigure 2-25. Decision boundary of tree with depth 2 (left) and corresponding decision\\ntree (right)\\nThis recursive process yields a binary tree of decisions, with each node containing a\\ntest. Alternatively, you can think of each test as splitting the part of the data that is\\ncurrently being considered along one axis. This yields a view of the algorithm as\\nbuilding a hierarchical partition. As each test concerns only a single feature, the\\nregions in the resulting partition always have axis-parallel boundaries.\\nThe recursive partitioning of the data is repeated until each region in the partition\\n(each leaf in the decision tree) only contains a single target value (a single class or a\\nsingle regression value). A leaf of the tree that contains data points that all share the\\nsame target value is called pure. The final partitioning for this dataset is shown in\\nFigure 2-26.\\nSupervised Machine Learning Algorithms | 73'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 87, 'page_label': '74'}, page_content='Figure 2-26. Decision boundary of tree with depth 9 (left) and part of the corresponding\\ntree (right); the full tree is quite large and hard to visualize\\nA prediction on a new data point is made by checking which region of the partition\\nof the feature space the point lies in, and then predicting the majority target (or the\\nsingle target in the case of pure leaves) in that region. The region can be found by\\ntraversing the tree from the root and going left or right, depending on whether the\\ntest is fulfilled or not.\\nIt is also possible to use trees for regression tasks, using exactly the same technique.\\nTo make a prediction, we traverse the tree based on the tests in each node and find\\nthe leaf the new data point falls into. The output for this data point is the mean target\\nof the training points in this leaf.\\nControlling complexity of decision trees\\nTypically, building a tree as described here and continuing until all leaves are pure\\nleads to models that are very complex and highly overfit to the training data. The\\npresence of pure leaves mean that a tree is 100% accurate on the training set; each\\ndata point in the training set is in a leaf that has the correct majority class. The over‐\\nfitting can be seen on the left of Figure 2-26. Y ou can see the regions determined to\\nbelong to class 1 in the middle of all the points belonging to class 0. On the other\\nhand, there is a small strip predicted as class 0 around the point belonging to class 0\\nto the very right. This is not how one would imagine the decision boundary to look,\\nand the decision boundary focuses a lot on single outlier points that are far away\\nfrom the other points in that class.\\nThere are two common strategies to prevent overfitting: stopping the creation of the\\ntree early (also called pre-pruning), or building the tree but then removing or collaps‐\\ning nodes that contain little information (also called post-pruning or just pruning).\\nPossible criteria for pre-pruning include limiting the maximum depth of the tree,\\nlimiting the maximum number of leaves, or requiring a minimum number of points\\nin a node to keep splitting it.\\n74 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 88, 'page_label': '75'}, page_content='Decision trees in scikit-learn are implemented in the DecisionTreeRegressor and\\nDecisionTreeClassifier classes. scikit-learn only implements pre-pruning, not\\npost-pruning.\\nLet’s look at the effect of pre-pruning in more detail on the Breast Cancer dataset. As\\nalways, we import the dataset and split it into a training and a test part. Then we build\\na model using the default setting of fully developing the tree (growing the tree until\\nall leaves are pure). We fix the random_state in the tree, which is used for tie-\\nbreaking internally:\\nIn[58]:\\nfrom sklearn.tree import DecisionTreeClassifier\\ncancer = load_breast_cancer()\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\\ntree = DecisionTreeClassifier(random_state=0)\\ntree.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\\nOut[58]:\\nAccuracy on training set: 1.000\\nAccuracy on test set: 0.937\\nAs expected, the accuracy on the training set is 100%—because the leaves are pure,\\nthe tree was grown deep enough that it could perfectly memorize all the labels on the\\ntraining data. The test set accuracy is slightly worse than for the linear models we\\nlooked at previously, which had around 95% accuracy.\\nIf we don’t restrict the depth of a decision tree, the tree can become arbitrarily deep\\nand complex. Unpruned trees are therefore prone to overfitting and not generalizing\\nwell to new data. Now let’s apply pre-pruning to the tree, which will stop developing\\nthe tree before we perfectly fit to the training data. One option is to stop building the\\ntree after a certain depth has been reached. Here we set max_depth=4, meaning only\\nfour consecutive questions can be asked (cf. Figures 2-24 and 2-26). Limiting the\\ndepth of the tree decreases overfitting. This leads to a lower accuracy on the training\\nset, but an improvement on the test set:\\nIn[59]:\\ntree = DecisionTreeClassifier(max_depth=4, random_state=0)\\ntree.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\\nSupervised Machine Learning Algorithms | 75'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 89, 'page_label': '76'}, page_content='Out[59]:\\nAccuracy on training set: 0.988\\nAccuracy on test set: 0.951\\nAnalyzing decision trees\\nWe can visualize the tree using the export_graphviz function from the tree module.\\nThis writes a file in the .dot file format, which is a text file format for storing graphs.\\nWe set an option to color the nodes to reflect the majority class in each node and pass\\nthe class and features names so the tree can be properly labeled:\\nIn[61]:\\nfrom sklearn.tree import export_graphviz\\nexport_graphviz(tree, out_file=\"tree.dot\", class_names=[\"malignant\", \"benign\"],\\n                feature_names=cancer.feature_names, impurity=False, filled=True)\\nWe can read this file and visualize it, as seen in Figure 2-27, using the graphviz mod‐\\nule (or you can use any program that can read .dot files):\\nIn[61]:\\nimport graphviz\\nwith open(\"tree.dot\") as f:\\n    dot_graph = f.read()\\ngraphviz.Source(dot_graph)\\nFigure 2-27. Visualization of the decision tree built on the Breast Cancer dataset\\n76 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 90, 'page_label': '77'}, page_content='The visualization of the tree provides a great in-depth view of how the algorithm\\nmakes predictions, and is a good example of a machine learning algorithm that is\\neasily explained to nonexperts. However, even with a tree of depth four, as seen here,\\nthe tree can become a bit overwhelming. Deeper trees (a depth of 10 is not uncom‐\\nmon) are even harder to grasp. One method of inspecting the tree that may be helpful\\nis to find out which path most of the data actually takes. The n_samples shown in\\neach node in Figure 2-27 gives the number of samples in that node, while value pro‐\\nvides the number of samples per class. Following the branches to the right, we see\\nthat worst radius <= 16.795  creates a node that contains only 8 benign but 134\\nmalignant samples. The rest of this side of the tree then uses some finer distinctions\\nto split off these 8 remaining benign samples. Of the 142 samples that went to the\\nright in the initial split, nearly all of them (132) end up in the leaf to the very right.\\nTaking a left at the root, for worst radius > 16.795  we end up with 25 malignant\\nand 259 benign samples. Nearly all of the benign samples end up in the second leaf\\nfrom the right, with most of the other leaves containing very few samples.\\nFeature importance in trees\\nInstead of looking at the whole tree, which can be taxing, there are some useful prop‐\\nerties that we can derive to summarize the workings of the tree. The most commonly\\nused summary is feature importance, which rates how important each feature is for\\nthe decision a tree makes. It is a number between 0 and 1 for each feature, where 0\\nmeans “not used at all” and 1 means “perfectly predicts the target. ” The feature\\nimportances always sum to 1:\\nIn[62]:\\nprint(\"Feature importances:\\\\n{}\".format(tree.feature_importances_))\\nOut[62]:\\nFeature importances:\\n[ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.01\\n  0.048  0.     0.     0.002  0.     0.     0.     0.     0.     0.727  0.046\\n  0.     0.     0.014  0.     0.018  0.122  0.012  0.   ]\\nWe can visualize the feature importances in a way that is similar to the way we visual‐\\nize the coefficients in the linear model (Figure 2-28):\\nIn[63]:\\ndef plot_feature_importances_cancer(model):\\n    n_features = cancer.data.shape[1]\\n    plt.barh(range(n_features), model.feature_importances_, align=\\'center\\')\\n    plt.yticks(np.arange(n_features), cancer.feature_names)\\n    plt.xlabel(\"Feature importance\")\\n    plt.ylabel(\"Feature\")\\nplot_feature_importances_cancer(tree)\\nSupervised Machine Learning Algorithms | 77'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 91, 'page_label': '78'}, page_content='Figure 2-28. Feature importances computed from a decision tree learned on the Breast\\nCancer dataset\\nHere we see that the feature used in the top split (“worst radius”) is by far the most\\nimportant feature. This confirms our observation in analyzing the tree that the first\\nlevel already separates the two classes fairly well.\\nHowever, if a feature has a low feature_importance, it doesn’t mean that this feature\\nis uninformative. It only means that the feature was not picked by the tree, likely\\nbecause another feature encodes the same information.\\nIn contrast to the coefficients in linear models, feature importances are always posi‐\\ntive, and don’t encode which class a feature is indicative of. The feature importances\\ntell us that “worst radius” is important, but not whether a high radius is indicative of a\\nsample being benign or malignant. In fact, there might not be such a simple relation‐\\nship between features and class, as you can see in the following example (Figures 2-29\\nand 2-30):\\nIn[64]:\\ntree = mglearn.plots.plot_tree_not_monotone()\\ndisplay(tree)\\nOut[64]:\\nFeature importances: [ 0.  1.]\\n78 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 92, 'page_label': '79'}, page_content='Figure 2-29. A two-dimensional dataset in which the feature on the y-axis has a nonmo‐\\nnotonous relationship with the class label, and the decision boundaries found by a deci‐\\nsion tree\\nFigure 2-30. Decision tree learned on the data shown in Figure 2-29\\nThe plot shows a dataset with two features and two classes. Here, all the information\\nis contained in X[1], and X[0] is not used at all. But the relation between X[1] and\\nSupervised Machine Learning Algorithms | 79'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 93, 'page_label': '80'}, page_content='the output class is not monotonous, meaning we cannot say “a high value of X[0]\\nmeans class 0, and a low value means class 1” (or vice versa).\\nWhile we focused our discussion here on decision trees for classification, all that was\\nsaid is similarly true for decision trees for regression, as implemented in Decision\\nTreeRegressor. The usage and analysis of regression trees is very similar to that of\\nclassification trees. There is one particular property of using tree-based models for\\nregression that we want to point out, though. The DecisionTreeRegressor (and all\\nother tree-based regression models) is not able to extrapolate, or make predictions\\noutside of the range of the training data.\\nLet’s look into this in more detail, using a dataset of historical computer memory\\n(RAM) prices. Figure 2-31 shows the dataset, with the date on the x-axis and the price\\nof one megabyte of RAM in that year on the y-axis:\\nIn[65]:\\nimport pandas as pd\\nram_prices = pd.read_csv(\"data/ram_price.csv\")\\nplt.semilogy(ram_prices.date, ram_prices.price)\\nplt.xlabel(\"Year\")\\nplt.ylabel(\"Price in $/Mbyte\")\\nFigure 2-31. Historical development of the price of RAM, plotted on a log scale\\n80 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 94, 'page_label': '81'}, page_content='Note the logarithmic scale of the y-axis. When plotting logarithmically, the relation\\nseems to be quite linear and so should be relatively easy to predict, apart from some\\nbumps.\\nWe will make a forecast for the years after 2000 using the historical data up to that\\npoint, with the date as our only feature. We will compare two simple models: a\\nDecisionTreeRegressor and LinearRegression. We rescale the prices using a loga‐\\nrithm, so that the relationship is relatively linear. This doesn’t make a difference for\\nthe DecisionTreeRegressor, but it makes a big difference for LinearRegression (we\\nwill discuss this in more depth in Chapter 4). After training the models and making\\npredictions, we apply the exponential map to undo the logarithm transform. We\\nmake predictions on the whole dataset for visualization purposes here, but for a\\nquantitative evaluation we would only consider the test dataset:\\nIn[66]:\\nfrom sklearn.tree import DecisionTreeRegressor\\n# use historical data to forecast prices after the year 2000\\ndata_train = ram_prices[ram_prices.date < 2000]\\ndata_test = ram_prices[ram_prices.date >= 2000]\\n# predict prices based on date\\nX_train = data_train.date[:, np.newaxis]\\n# we use a log-transform to get a simpler relationship of data to target\\ny_train = np.log(data_train.price)\\ntree = DecisionTreeRegressor().fit(X_train, y_train)\\nlinear_reg = LinearRegression().fit(X_train, y_train)\\n# predict on all data\\nX_all = ram_prices.date[:, np.newaxis]\\npred_tree = tree.predict(X_all)\\npred_lr = linear_reg.predict(X_all)\\n# undo log-transform\\nprice_tree = np.exp(pred_tree)\\nprice_lr = np.exp(pred_lr)\\nFigure 2-32, created here, compares the predictions of the decision tree and the linear\\nregression model with the ground truth:\\nIn[67]:\\nplt.semilogy(data_train.date, data_train.price, label=\"Training data\")\\nplt.semilogy(data_test.date, data_test.price, label=\"Test data\")\\nplt.semilogy(ram_prices.date, price_tree, label=\"Tree prediction\")\\nplt.semilogy(ram_prices.date, price_lr, label=\"Linear prediction\")\\nplt.legend()\\nSupervised Machine Learning Algorithms | 81'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 95, 'page_label': '82'}, page_content='9 It is actually possible to make very good forecasts with tree-based models (for example, when trying to predict\\nwhether a price will go up or down). The point of this example was not to show that trees are a bad model for\\ntime series, but to illustrate a particular property of how trees make predictions.\\nFigure 2-32. Comparison of predictions made by a linear model and predictions made\\nby a regression tree on the RAM price data\\nThe difference between the models is quite striking. The linear model approximates\\nthe data with a line, as we knew it would. This line provides quite a good forecast for\\nthe test data (the years after 2000), while glossing over some of the finer variations in\\nboth the training and the test data. The tree model, on the other hand, makes perfect\\npredictions on the training data; we did not restrict the complexity of the tree, so it\\nlearned the whole dataset by heart. However, once we leave the data range for which\\nthe model has data, the model simply keeps predicting the last known point. The tree\\nhas no ability to generate “new” responses, outside of what was seen in the training\\ndata. This shortcoming applies to all models based on trees.9\\nStrengths, weaknesses, and parameters\\nAs discussed earlier, the parameters that control model complexity in decision trees\\nare the pre-pruning parameters that stop the building of the tree before it is fully\\ndeveloped. Usually, picking one of the pre-pruning strategies—setting either\\n82 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 96, 'page_label': '83'}, page_content='max_depth, max_leaf_nodes, or min_samples_leaf—is sufficient to prevent overfit‐\\nting.\\nDecision trees have two advantages over many of the algorithms we’ve discussed so\\nfar: the resulting model can easily be visualized and understood by nonexperts (at\\nleast for smaller trees), and the algorithms are completely invariant to scaling of the\\ndata. As each feature is processed separately, and the possible splits of the data don’t\\ndepend on scaling, no preprocessing like normalization or standardization of features\\nis needed for decision tree algorithms. In particular, decision trees work well when\\nyou have features that are on completely different scales, or a mix of binary and con‐\\ntinuous features.\\nThe main downside of decision trees is that even with the use of pre-pruning, they\\ntend to overfit and provide poor generalization performance. Therefore, in most\\napplications, the ensemble methods we discuss next are usually used in place of a sin‐\\ngle decision tree.\\nEnsembles of Decision Trees\\nEnsembles are methods that combine multiple machine learning models to create\\nmore powerful models. There are many models in the machine learning literature\\nthat belong to this category, but there are two ensemble models that have proven to\\nbe effective on a wide range of datasets for classification and regression, both of\\nwhich use decision trees as their building blocks: random forests and gradient boos‐\\nted decision trees.\\nRandom forests\\nAs we just observed, a main drawback of decision trees is that they tend to overfit the\\ntraining data. Random forests are one way to address this problem. A random forest\\nis essentially a collection of decision trees, where each tree is slightly different from\\nthe others. The idea behind random forests is that each tree might do a relatively\\ngood job of predicting, but will likely overfit on part of the data. If we build many\\ntrees, all of which work well and overfit in different ways, we can reduce the amount\\nof overfitting by averaging their results. This reduction in overfitting, while retaining\\nthe predictive power of the trees, can be shown using rigorous mathematics.\\nTo implement this strategy, we need to build many decision trees. Each tree should do\\nan acceptable job of predicting the target, and should also be different from the other\\ntrees. Random forests get their name from injecting randomness into the tree build‐\\ning to ensure each tree is different. There are two ways in which the trees in a random\\nforest are randomized: by selecting the data points used to build a tree and by select‐\\ning the features in each split test. Let’s go into this process in more detail.\\nSupervised Machine Learning Algorithms | 83'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 97, 'page_label': '84'}, page_content=\"Building random forests.    To build a random forest model, you need to decide on the\\nnumber of trees to build (the n_estimators parameter of RandomForestRegressor or\\nRandomForestClassifier). Let’s say we want to build 10 trees. These trees will be\\nbuilt completely independently from each other, and the algorithm will make differ‐\\nent random choices for each tree to make sure the trees are distinct. To build a tree,\\nwe first take what is called a bootstrap sample of our data. That is, from our n_samples\\ndata points, we repeatedly draw an example randomly with replacement (meaning the\\nsame sample can be picked multiple times), n_samples times. This will create a data‐\\nset that is as big as the original dataset, but some data points will be missing from it\\n(approximately one third), and some will be repeated.\\nTo illustrate, let’s say we want to create a bootstrap sample of the list ['a', 'b',\\n'c', 'd']. A possible bootstrap sample would be ['b', 'd', 'd', 'c']. Another\\npossible sample would be ['d', 'a', 'd', 'a'].\\nNext, a decision tree is built based on this newly created dataset. However, the algo‐\\nrithm we described for the decision tree is slightly modified. Instead of looking for\\nthe best test for each node, in each node the algorithm randomly selects a subset of\\nthe features, and it looks for the best possible test involving one of these features. The\\nnumber of features that are selected is controlled by the max_features parameter.\\nThis selection of a subset of features is repeated separately in each node, so that each\\nnode in a tree can make a decision using a different subset of the features.\\nThe bootstrap sampling leads to each decision tree in the random forest being built\\non a slightly different dataset. Because of the selection of features in each node, each\\nsplit in each tree operates on a different subset of features. Together, these two mech‐\\nanisms ensure that all the trees in the random forest are different.\\nA critical parameter in this process is max_features. If we set max_features to n_fea\\ntures, that means that each split can look at all features in the dataset, and no ran‐\\ndomness will be injected in the feature selection (the randomness due to the\\nbootstrapping remains, though). If we set max_features to 1, that means that the\\nsplits have no choice at all on which feature to test, and can only search over different\\nthresholds for the feature that was selected randomly. Therefore, a high max_fea\\ntures means that the trees in the random forest will be quite similar, and they will be\\nable to fit the data easily, using the most distinctive features. A low max_features\\nmeans that the trees in the random forest will be quite different, and that each tree\\nmight need to be very deep in order to fit the data well.\\nTo make a prediction using the random forest, the algorithm first makes a prediction\\nfor every tree in the forest. For regression, we can average these results to get our final\\nprediction. For classification, a “soft voting” strategy is used. This means each algo‐\\nrithm makes a “soft” prediction, providing a probability for each possible output\\n84 | Chapter 2: Supervised Learning\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 98, 'page_label': '85'}, page_content='label. The probabilities predicted by all the trees are averaged, and the class with the\\nhighest probability is predicted.\\nAnalyzing random forests.    Let’s apply a random forest consisting of five trees to the\\ntwo_moons dataset we studied earlier:\\nIn[68]:\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.datasets import make_moons\\nX, y = make_moons(n_samples=100, noise=0.25, random_state=3)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\\n                                                    random_state=42)\\nforest = RandomForestClassifier(n_estimators=5, random_state=2)\\nforest.fit(X_train, y_train)\\nThe trees that are built as part of the random forest are stored in the estimator_\\nattribute. Let’s visualize the decision boundaries learned by each tree, together with\\ntheir aggregate prediction as made by the forest (Figure 2-33):\\nIn[69]:\\nfig, axes = plt.subplots(2, 3, figsize=(20, 10))\\nfor i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):\\n    ax.set_title(\"Tree {}\".format(i))\\n    mglearn.plots.plot_tree_partition(X_train, y_train, tree, ax=ax)\\nmglearn.plots.plot_2d_separator(forest, X_train, fill=True, ax=axes[-1, -1],\\n                                alpha=.4)\\naxes[-1, -1].set_title(\"Random Forest\")\\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\\nY ou can clearly see that the decision boundaries learned by the five trees are quite dif‐\\nferent. Each of them makes some mistakes, as some of the training points that are\\nplotted here were not actually included in the training sets of the trees, due to the\\nbootstrap sampling.\\nThe random forest overfits less than any of the trees individually, and provides a\\nmuch more intuitive decision boundary. In any real application, we would use many\\nmore trees (often hundreds or thousands), leading to even smoother boundaries.\\nSupervised Machine Learning Algorithms | 85'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 99, 'page_label': '86'}, page_content='Figure 2-33. Decision boundaries found by five randomized decision trees and the deci‐\\nsion boundary obtained by averaging their predicted probabilities\\nAs another example, let’s apply a random forest consisting of 100 trees on the Breast\\nCancer dataset:\\nIn[70]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, random_state=0)\\nforest = RandomForestClassifier(n_estimators=100, random_state=0)\\nforest.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))\\nOut[70]:\\nAccuracy on training set: 1.000\\nAccuracy on test set: 0.972\\nThe random forest gives us an accuracy of 97%, better than the linear models or a\\nsingle decision tree, without tuning any parameters. We could adjust the max_fea\\ntures setting, or apply pre-pruning as we did for the single decision tree. However,\\noften the default parameters of the random forest already work quite well.\\nSimilarly to the decision tree, the random forest provides feature importances, which\\nare computed by aggregating the feature importances over the trees in the forest. Typ‐\\nically, the feature importances provided by the random forest are more reliable than\\nthe ones provided by a single tree. Take a look at Figure 2-34.\\n86 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 100, 'page_label': '87'}, page_content='In[71]:\\nplot_feature_importances_cancer(forest)\\nFigure 2-34. Feature importances computed from a random forest that was fit to the\\nBreast Cancer dataset\\nAs you can see, the random forest gives nonzero importance to many more features\\nthan the single tree. Similarly to the single decision tree, the random forest also gives\\na lot of importance to the “worst radius” feature, but it actually chooses “worst perim‐\\neter” to be the most informative feature overall. The randomness in building the ran‐\\ndom forest forces the algorithm to consider many possible explanations, the result\\nbeing that the random forest captures a much broader picture of the data than a sin‐\\ngle tree.\\nStrengths, weaknesses, and parameters.    Random forests for regression and classifica‐\\ntion are currently among the most widely used machine learning methods. They are\\nvery powerful, often work well without heavy tuning of the parameters, and don’t\\nrequire scaling of the data.\\nEssentially, random forests share all of the benefits of decision trees, while making up\\nfor some of their deficiencies. One reason to still use decision trees is if you need a\\ncompact representation of the decision-making process. It is basically impossible to\\ninterpret tens or hundreds of trees in detail, and trees in random forests tend to be\\ndeeper than decision trees (because of the use of feature subsets). Therefore, if you\\nneed to summarize the prediction making in a visual way to nonexperts, a single\\ndecision tree might be a better choice. While building random forests on large data‐\\nsets might be somewhat time consuming, it can be parallelized across multiple CPU\\nSupervised Machine Learning Algorithms | 87'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 101, 'page_label': '88'}, page_content='cores within a computer easily. If you are using a multi-core processor (as nearly all\\nmodern computers do), you can use the n_jobs parameter to adjust the number of\\ncores to use. Using more CPU cores will result in linear speed-ups (using two cores,\\nthe training of the random forest will be twice as fast), but specifying n_jobs larger\\nthan the number of cores will not help. Y ou can set n_jobs=-1 to use all the cores in\\nyour computer.\\nY ou should keep in mind that random forests, by their nature, are random, and set‐\\nting different random states (or not setting the random_state at all) can drastically\\nchange the model that is built. The more trees there are in the forest, the more robust\\nit will be against the choice of random state. If you want to have reproducible results,\\nit is important to fix the random_state.\\nRandom forests don’t tend to perform well on very high dimensional, sparse data,\\nsuch as text data. For this kind of data, linear models might be more appropriate.\\nRandom forests usually work well even on very large datasets, and training can easily\\nbe parallelized over many CPU cores within a powerful computer. However, random\\nforests require more memory and are slower to train and to predict than linear mod‐\\nels. If time and memory are important in an application, it might make sense to use a\\nlinear model instead.\\nThe important parameters to adjust are n_estimators, max_features, and possibly\\npre-pruning options like max_depth. For n_estimators, larger is always better. Aver‐\\naging more trees will yield a more robust ensemble by reducing overfitting. However,\\nthere are diminishing returns, and more trees need more memory and more time to\\ntrain. A common rule of thumb is to build “as many as you have time/memory for. ”\\nAs described earlier, max_features determines how random each tree is, and a\\nsmaller max_features reduces overfitting. In general, it’s a good rule of thumb to use\\nthe default values: max_features=sqrt(n_features) for classification and max_fea\\ntures=log2(n_features) for regression. Adding max_features or max_leaf_nodes\\nmight sometimes improve performance. It can also drastically reduce space and time\\nrequirements for training and prediction.\\nGradient boosted regression trees (gradient boosting machines)\\nThe gradient boosted regression tree is another ensemble method that combines mul‐\\ntiple decision trees to create a more powerful model. Despite the “regression” in the\\nname, these models can be used for regression and classification. In contrast to the\\nrandom forest approach, gradient boosting works by building trees in a serial man‐\\nner, where each tree tries to correct the mistakes of the previous one. By default, there\\nis no randomization in gradient boosted regression trees; instead, strong pre-pruning\\nis used. Gradient boosted trees often use very shallow trees, of depth one to five,\\nwhich makes the model smaller in terms of memory and makes predictions faster.\\n88 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 102, 'page_label': '89'}, page_content='The main idea behind gradient boosting is to combine many simple models (in this\\ncontext known as weak learners), like shallow trees. Each tree can only provide good\\npredictions on part of the data, and so more and more trees are added to iteratively\\nimprove performance.\\nGradient boosted trees are frequently the winning entries in machine learning com‐\\npetitions, and are widely used in industry. They are generally a bit more sensitive to\\nparameter settings than random forests, but can provide better accuracy if the param‐\\neters are set correctly.\\nApart from the pre-pruning and the number of trees in the ensemble, another impor‐\\ntant parameter of gradient boosting is the learning_rate, which controls how\\nstrongly each tree tries to correct the mistakes of the previous trees. A higher learning\\nrate means each tree can make stronger corrections, allowing for more complex mod‐\\nels. Adding more trees to the ensemble, which can be accomplished by increasing\\nn_estimators, also increases the model complexity, as the model has more chances\\nto correct mistakes on the training set.\\nHere is an example of using GradientBoostingClassifier on the Breast Cancer\\ndataset. By default, 100 trees of maximum depth 3 and a learning rate of 0.1 are used:\\nIn[72]:\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, random_state=0)\\ngbrt = GradientBoostingClassifier(random_state=0)\\ngbrt.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\\nOut[72]:\\nAccuracy on training set: 1.000\\nAccuracy on test set: 0.958\\nAs the training set accuracy is 100%, we are likely to be overfitting. To reduce overfit‐\\nting, we could either apply stronger pre-pruning by limiting the maximum depth or\\nlower the learning rate:\\nSupervised Machine Learning Algorithms | 89'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 103, 'page_label': '90'}, page_content='In[73]:\\ngbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\\ngbrt.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\\nOut[73]:\\nAccuracy on training set: 0.991\\nAccuracy on test set: 0.972\\nIn[74]:\\ngbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)\\ngbrt.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\\nOut[74]:\\nAccuracy on training set: 0.988\\nAccuracy on test set: 0.965\\nBoth methods of decreasing the model complexity reduced the training set accuracy,\\nas expected. In this case, lowering the maximum depth of the trees provided a signifi‐\\ncant improvement of the model, while lowering the learning rate only increased the\\ngeneralization performance slightly.\\nAs for the other decision tree–based models, we can again visualize the feature\\nimportances to get more insight into our model (Figure 2-35). As we used 100 trees, it\\nis impractical to inspect them all, even if they are all of depth 1:\\nIn[75]:\\ngbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\\ngbrt.fit(X_train, y_train)\\nplot_feature_importances_cancer(gbrt)\\n90 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 104, 'page_label': '91'}, page_content='Figure 2-35. Feature importances computed from a gradient boosting classifier that was\\nfit to the Breast Cancer dataset\\nWe can see that the feature importances of the gradient boosted trees are somewhat\\nsimilar to the feature importances of the random forests, though the gradient boost‐\\ning completely ignored some of the features.\\nAs both gradient boosting and random forests perform well on similar kinds of data,\\na common approach is to first try random forests, which work quite robustly. If ran‐\\ndom forests work well but prediction time is at a premium, or it is important to\\nsqueeze out the last percentage of accuracy from the machine learning model, mov‐\\ning to gradient boosting often helps.\\nIf you want to apply gradient boosting to a large-scale problem, it might be worth\\nlooking into the xgboost package and its Python interface, which at the time of writ‐\\ning is faster (and sometimes easier to tune) than the scikit-learn implementation of\\ngradient boosting on many datasets.\\nStrengths, weaknesses, and parameters.    Gradient boosted decision trees are among the\\nmost powerful and widely used models for supervised learning. Their main drawback\\nis that they require careful tuning of the parameters and may take a long time to\\ntrain. Similarly to other tree-based models, the algorithm works well without scaling\\nand on a mixture of binary and continuous features. As with other tree-based models,\\nit also often does not work well on high-dimensional sparse data.\\nThe main parameters of gradient boosted tree models are the number of trees, n_esti\\nmators, and the learning_rate, which controls the degree to which each tree is\\nallowed to correct the mistakes of the previous trees. These two parameters are highly\\nSupervised Machine Learning Algorithms | 91'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 105, 'page_label': '92'}, page_content='interconnected, as a lower learning_rate means that more trees are needed to build\\na model of similar complexity. In contrast to random forests, where a higher n_esti\\nmators value is always better, increasing n_estimators in gradient boosting leads to a\\nmore complex model, which may lead to overfitting. A common practice is to fit\\nn_estimators depending on the time and memory budget, and then search over dif‐\\nferent learning_rates.\\nAnother important parameter is max_depth (or alternatively max_leaf_nodes), to\\nreduce the complexity of each tree. Usually max_depth is set very low for gradient\\nboosted models, often not deeper than five splits.\\nKernelized Support Vector Machines\\nThe next type of supervised model we will discuss is kernelized support vector\\nmachines. We explored the use of linear support vector machines for classification in\\n“Linear models for classification” on page 56. Kernelized support vector machines\\n(often just referred to as SVMs) are an extension that allows for more complex mod‐\\nels that are not defined simply by hyperplanes in the input space. While there are sup‐\\nport vector machines for classification and regression, we will restrict ourselves to the\\nclassification case, as implemented in SVC. Similar concepts apply to support vector\\nregression, as implemented in SVR.\\nThe math behind kernelized support vector machines is a bit involved, and is beyond\\nthe scope of this book. Y ou can find the details in Chapter 1 of Hastie, Tibshirani, and\\nFriedman’s The Elements of Statistical Learning. However, we will try to give you some\\nsense of the idea behind the method.\\nLinear models and nonlinear features\\nAs you saw in Figure 2-15 , linear models can be quite limiting in low-dimensional\\nspaces, as lines and hyperplanes have limited flexibility. One way to make a linear\\nmodel more flexible is by adding more features—for example, by adding interactions\\nor polynomials of the input features.\\nLet’s look at the synthetic dataset we used in “Feature importance in trees” on page 77\\n(see Figure 2-29):\\nIn[76]:\\nX, y = make_blobs(centers=4, random_state=8)\\ny = y % 2\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\n92 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 106, 'page_label': '93'}, page_content='10 We picked this particular feature to add for illustration purposes. The choice is not particularly important.\\nFigure 2-36. Two-class classification dataset in which classes are not linearly separable\\nA linear model for classification can only separate points using a line, and will not be\\nable to do a very good job on this dataset (see Figure 2-37):\\nIn[77]:\\nfrom sklearn.svm import LinearSVC\\nlinear_svm = LinearSVC().fit(X, y)\\nmglearn.plots.plot_2d_separator(linear_svm, X)\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nNow let’s expand the set of input features, say by also adding feature1 ** 2 , the\\nsquare of the second feature, as a new feature. Instead of representing each data point\\nas a two-dimensional point, (feature0, feature1), we now represent it as a three-\\ndimensional point, (feature0, feature1, feature1 ** 2) .10 This new representa‐\\ntion is illustrated in Figure 2-38 in a three-dimensional scatter plot:\\nSupervised Machine Learning Algorithms | 93'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 107, 'page_label': '94'}, page_content='Figure 2-37. Decision boundary found by a linear SVM\\nIn[78]:\\n# add the squared first feature\\nX_new = np.hstack([X, X[:, 1:] ** 2])\\nfrom mpl_toolkits.mplot3d import Axes3D, axes3d\\nfigure = plt.figure()\\n# visualize in 3D\\nax = Axes3D(figure, elev=-152, azim=-26)\\n# plot first all the points with y == 0, then all with y == 1\\nmask = y == 0\\nax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c=\\'b\\',\\n           cmap=mglearn.cm2, s=60)\\nax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c=\\'r\\', marker=\\'^\\',\\n           cmap=mglearn.cm2, s=60)\\nax.set_xlabel(\"feature0\")\\nax.set_ylabel(\"feature1\")\\nax.set_zlabel(\"feature1 ** 2\")\\n94 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 108, 'page_label': '95'}, page_content='Figure 2-38. Expansion of the dataset shown in Figure 2-37, created by adding a third\\nfeature derived from feature1\\nIn the new representation of the data, it is now indeed possible to separate the two\\nclasses using a linear model, a plane in three dimensions. We can confirm this by fit‐\\nting a linear model to the augmented data (see Figure 2-39):\\nIn[79]:\\nlinear_svm_3d = LinearSVC().fit(X_new, y)\\ncoef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_\\n# show linear decision boundary\\nfigure = plt.figure()\\nax = Axes3D(figure, elev=-152, azim=-26)\\nxx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)\\nyy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)\\nXX, YY = np.meshgrid(xx, yy)\\nZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]\\nax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)\\nax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c=\\'b\\',\\n           cmap=mglearn.cm2, s=60)\\nax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c=\\'r\\', marker=\\'^\\',\\n           cmap=mglearn.cm2, s=60)\\nax.set_xlabel(\"feature0\")\\nax.set_ylabel(\"feature1\")\\nax.set_zlabel(\"feature0 ** 2\")\\nSupervised Machine Learning Algorithms | 95'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 109, 'page_label': '96'}, page_content='Figure 2-39. Decision boundary found by a linear SVM on the expanded three-\\ndimensional dataset\\nAs a function of the original features, the linear SVM model is not actually linear any‐\\nmore. It is not a line, but more of an ellipse, as you can see from the plot created here\\n(Figure 2-40):\\nIn[80]:\\nZZ = YY ** 2\\ndec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])\\nplt.contourf(XX, YY, dec.reshape(XX.shape), levels=[dec.min(), 0, dec.max()],\\n             cmap=mglearn.cm2, alpha=0.5)\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\n96 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 110, 'page_label': '97'}, page_content='Figure 2-40. The decision boundary from Figure 2-39 as a function of the original two\\nfeatures\\nThe kernel trick\\nThe lesson here is that adding nonlinear features to the representation of our data can\\nmake linear models much more powerful. However, often we don’t know which fea‐\\ntures to add, and adding many features (like all possible interactions in a 100-\\ndimensional feature space) might make computation very expensive. Luckily, there is\\na clever mathematical trick that allows us to learn a classifier in a higher-dimensional\\nspace without actually computing the new, possibly very large representation. This is\\nknown as the kernel trick, and it works by directly computing the distance (more pre‐\\ncisely, the scalar products) of the data points for the expanded feature representation,\\nwithout ever actually computing the expansion.\\nThere are two ways to map your data into a higher-dimensional space that are com‐\\nmonly used with support vector machines: the polynomial kernel, which computes all\\npossible polynomials up to a certain degree of the original features (like feature1 **\\n2 * feature2 ** 5 ); and the radial basis function (RBF) kernel, also known as the\\nGaussian kernel. The Gaussian kernel is a bit harder to explain, as it corresponds to\\nan infinite-dimensional feature space. One way to explain the Gaussian kernel is that\\nSupervised Machine Learning Algorithms | 97'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 111, 'page_label': '98'}, page_content='11 This follows from the Taylor expansion of the exponential map.\\nit considers all possible polynomials of all degrees, but the importance of the features\\ndecreases for higher degrees.11\\nIn practice, the mathematical details behind the kernel SVM are not that important,\\nthough, and how an SVM with an RBF kernel makes a decision can be summarized\\nquite easily—we’ll do so in the next section.\\nUnderstanding SVMs\\nDuring training, the SVM learns how important each of the training data points is to\\nrepresent the decision boundary between the two classes. Typically only a subset of\\nthe training points matter for defining the decision boundary: the ones that lie on the\\nborder between the classes. These are called support vectors and give the support vec‐\\ntor machine its name.\\nTo make a prediction for a new point, the distance to each of the support vectors is\\nmeasured. A classification decision is made based on the distances to the support vec‐\\ntor, and the importance of the support vectors that was learned during training\\n(stored in the dual_coef_ attribute of SVC).\\nThe distance between data points is measured by the Gaussian kernel:\\nkrbf(x1, x2) = exp (ɣǁx1 - x2ǁ2)\\nHere, x1 and x2 are data points, ǁ x1 - x2 ǁ denotes Euclidean distance, and ɣ (gamma)\\nis a parameter that controls the width of the Gaussian kernel.\\nFigure 2-41  shows the result of training a support vector machine on a two-\\ndimensional two-class dataset. The decision boundary is shown in black, and the sup‐\\nport vectors are larger points with the wide outline. The following code creates this\\nplot by training an SVM on the forge dataset:\\nIn[81]:\\nfrom sklearn.svm import SVC\\nX, y = mglearn.tools.make_handcrafted_dataset()\\nsvm = SVC(kernel=\\'rbf\\', C=10, gamma=0.1).fit(X, y)\\nmglearn.plots.plot_2d_separator(svm, X, eps=.5)\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\n# plot support vectors\\nsv = svm.support_vectors_\\n# class labels of support vectors are given by the sign of the dual coefficients\\nsv_labels = svm.dual_coef_.ravel() > 0\\nmglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\n98 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 112, 'page_label': '99'}, page_content='Figure 2-41. Decision boundary and support vectors found by an SVM with RBF kernel\\nIn this case, the SVM yields a very smooth and nonlinear (not a straight line) bound‐\\nary. We adjusted two parameters here: the C parameter and the gamma parameter,\\nwhich we will now discuss in detail.\\nTuning SVM parameters\\nThe gamma parameter is the one shown in the formula given in the previous section,\\nwhich controls the width of the Gaussian kernel. It determines the scale of what it\\nmeans for points to be close together. The C parameter is a regularization parameter,\\nsimilar to that used in the linear models. It limits the importance of each point (or\\nmore precisely, their dual_coef_).\\nLet’s have a look at what happens when we vary these parameters (Figure 2-42):\\nIn[82]:\\nfig, axes = plt.subplots(3, 3, figsize=(15, 10))\\nfor ax, C in zip(axes, [-1, 0, 3]):\\n    for a, gamma in zip(ax, range(-1, 2)):\\n        mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)\\naxes[0, 0].legend([\"class 0\", \"class 1\", \"sv class 0\", \"sv class 1\"],\\n                  ncol=4, loc=(.9, 1.2))\\nSupervised Machine Learning Algorithms | 99'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 113, 'page_label': '100'}, page_content='Figure 2-42. Decision boundaries and support vectors for different settings of the param‐\\neters C and gamma\\nGoing from left to right, we increase the value of the parameter gamma from 0.1 to 10.\\nA small gamma means a large radius for the Gaussian kernel, which means that many\\npoints are considered close by. This is reflected in very smooth decision boundaries\\non the left, and boundaries that focus more on single points further to the right. A\\nlow value of gamma means that the decision boundary will vary slowly, which yields a\\nmodel of low complexity, while a high value of gamma yields a more complex model.\\nGoing from top to bottom, we increase the C parameter from 0.1 to 1000. As with the\\nlinear models, a small C means a very restricted model, where each data point can\\nonly have very limited influence. Y ou can see that at the top left the decision bound‐\\nary looks nearly linear, with the misclassified points barely having any influence on\\nthe line. Increasing C, as shown on the bottom right, allows these points to have a\\nstronger influence on the model and makes the decision boundary bend to correctly\\nclassify them.\\n100 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 114, 'page_label': '101'}, page_content='Let’s apply the RBF kernel SVM to the Breast Cancer dataset. By default, C=1 and\\ngamma=1/n_features:\\nIn[83]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, random_state=0)\\nsvc = SVC()\\nsvc.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.2f}\".format(svc.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.2f}\".format(svc.score(X_test, y_test)))\\nOut[83]:\\nAccuracy on training set: 1.00\\nAccuracy on test set: 0.63\\nThe model overfits quite substantially, with a perfect score on the training set and\\nonly 63% accuracy on the test set. While SVMs often perform quite well, they are\\nvery sensitive to the settings of the parameters and to the scaling of the data. In par‐\\nticular, they require all the features to vary on a similar scale. Let’s look at the mini‐\\nmum and maximum values for each feature, plotted in log-space (Figure 2-43):\\nIn[84]:\\nplt.plot(X_train.min(axis=0), \\'o\\', label=\"min\")\\nplt.plot(X_train.max(axis=0), \\'^\\', label=\"max\")\\nplt.legend(loc=4)\\nplt.xlabel(\"Feature index\")\\nplt.ylabel(\"Feature magnitude\")\\nplt.yscale(\"log\")\\nFrom this plot we can determine that features in the Breast Cancer dataset are of\\ncompletely different orders of magnitude. This can be somewhat of a problem for\\nother models (like linear models), but it has devastating effects for the kernel SVM.\\nLet’s examine some ways to deal with this issue.\\nSupervised Machine Learning Algorithms | 101'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 115, 'page_label': '102'}, page_content='Figure 2-43. Feature ranges for the Breast Cancer dataset (note that the y axis has a log‐\\narithmic scale)\\nPreprocessing data for SVMs\\nOne way to resolve this problem is by rescaling each feature so that they are all\\napproximately on the same scale. A common rescaling method for kernel SVMs is to\\nscale the data such that all features are between 0 and 1. We will see how to do this\\nusing the MinMaxScaler preprocessing method in Chapter 3, where we’ll give more\\ndetails. For now, let’s do this “by hand”:\\nIn[85]:\\n# compute the minimum value per feature on the training set\\nmin_on_training = X_train.min(axis=0)\\n# compute the range of each feature (max - min) on the training set\\nrange_on_training = (X_train - min_on_training).max(axis=0)\\n# subtract the min, and divide by range\\n# afterward, min=0 and max=1 for each feature\\nX_train_scaled = (X_train - min_on_training) / range_on_training\\nprint(\"Minimum for each feature\\\\n{}\".format(X_train_scaled.min(axis=0)))\\nprint(\"Maximum for each feature\\\\n {}\".format(X_train_scaled.max(axis=0)))\\n102 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 116, 'page_label': '103'}, page_content='Out[85]:\\nMinimum for each feature\\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\\nMaximum for each feature\\n [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\\n   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\\nIn[86]:\\n# use THE SAME transformation on the test set,\\n# using min and range of the training set (see Chapter 3 for details)\\nX_test_scaled = (X_test - min_on_training) / range_on_training\\nIn[87]:\\nsvc = SVC()\\nsvc.fit(X_train_scaled, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(\\n    svc.score(X_train_scaled, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))\\nOut[87]:\\nAccuracy on training set: 0.948\\nAccuracy on test set: 0.951\\nScaling the data made a huge difference! Now we are actually in an underfitting\\nregime, where training and test set performance are quite similar but less close to\\n100% accuracy. From here, we can try increasing either C or gamma to fit a more com‐\\nplex model. For example:\\nIn[88]:\\nsvc = SVC(C=1000)\\nsvc.fit(X_train_scaled, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(\\n    svc.score(X_train_scaled, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))\\nOut[88]:\\nAccuracy on training set: 0.988\\nAccuracy on test set: 0.972\\nHere, increasing C allows us to improve the model significantly, resulting in 97.2%\\naccuracy.\\nSupervised Machine Learning Algorithms | 103'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 117, 'page_label': '104'}, page_content='Strengths, weaknesses, and parameters\\nKernelized support vector machines are powerful models and perform well on a vari‐\\nety of datasets. SVMs allow for complex decision boundaries, even if the data has only\\na few features. They work well on low-dimensional and high-dimensional data (i.e.,\\nfew and many features), but don’t scale very well with the number of samples. Run‐\\nning an SVM on data with up to 10,000 samples might work well, but working with\\ndatasets of size 100,000 or more can become challenging in terms of runtime and\\nmemory usage.\\nAnother downside of SVMs is that they require careful preprocessing of the data and\\ntuning of the parameters. This is why, these days, most people instead use tree-based\\nmodels such as random forests or gradient boosting (which require little or no pre‐\\nprocessing) in many applications. Furthermore, SVM models are hard to inspect; it\\ncan be difficult to understand why a particular prediction was made, and it might be\\ntricky to explain the model to a nonexpert.\\nStill, it might be worth trying SVMs, particularly if all of your features represent\\nmeasurements in similar units (e.g., all are pixel intensities) and they are on similar\\nscales.\\nThe important parameters in kernel SVMs are the regularization parameter C, the\\nchoice of the kernel, and the kernel-specific parameters. Although we primarily\\nfocused on the RBF kernel, other choices are available in scikit-learn. The RBF\\nkernel has only one parameter, gamma, which is the inverse of the width of the Gaus‐\\nsian kernel. gamma and C both control the complexity of the model, with large values\\nin either resulting in a more complex model. Therefore, good settings for the two\\nparameters are usually strongly correlated, and C and gamma should be adjusted\\ntogether.\\nNeural Networks (Deep Learning)\\nA family of algorithms known as neural networks has recently seen a revival under\\nthe name “deep learning. ” While deep learning shows great promise in many machine\\nlearning applications, deep learning algorithms are often tailored very carefully to a\\nspecific use case. Here, we will only discuss some relatively simple methods, namely\\nmultilayer perceptrons for classification and regression, that can serve as a starting\\npoint for more involved deep learning methods. Multilayer perceptrons (MLPs) are\\nalso known as (vanilla) feed-forward neural networks, or sometimes just neural\\nnetworks.\\nThe neural network model\\nMLPs can be viewed as generalizations of linear models that perform multiple stages\\nof processing to come to a decision.\\n104 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 118, 'page_label': '105'}, page_content='Remember that the prediction by a linear regressor is given as:\\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\\nIn plain English, ŷ is a weighted sum of the input features x[0] to x[p], weighted by\\nthe learned coefficients w[0] to w[p]. We could visualize this graphically as shown in\\nFigure 2-44:\\nIn[89]:\\ndisplay(mglearn.plots.plot_logistic_regression_graph())\\nFigure 2-44. Visualization of logistic regression, where input features and predictions are\\nshown as nodes, and the coefficients are connections between the nodes\\nHere, each node on the left represents an input feature, the connecting lines represent\\nthe learned coefficients, and the node on the right represents the output, which is a\\nweighted sum of the inputs.\\nIn an MLP this process of computing weighted sums is repeated multiple times, first\\ncomputing hidden units that represent an intermediate processing step, which are\\nagain combined using weighted sums to yield the final result (Figure 2-45):\\nIn[90]:\\ndisplay(mglearn.plots.plot_single_hidden_layer_graph())\\nSupervised Machine Learning Algorithms | 105'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 119, 'page_label': '106'}, page_content='Figure 2-45. Illustration of a multilayer perceptron with a single hidden layer\\nThis model has a lot more coefficients (also called weights) to learn: there is one\\nbetween every input and every hidden unit (which make up the hidden layer), and\\none between every unit in the hidden layer and the output.\\nComputing a series of weighted sums is mathematically the same as computing just\\none weighted sum, so to make this model truly more powerful than a linear model,\\nwe need one extra trick. After computing a weighted sum for each hidden unit, a\\nnonlinear function is applied to the result—usually the rectifying nonlinearity (also\\nknown as rectified linear unit or relu) or the tangens hyperbolicus (tanh). The result of\\nthis function is then used in the weighted sum that computes the output, ŷ. The two\\nfunctions are visualized in Figure 2-46. The relu cuts off values below zero, while tanh\\nsaturates to –1 for low input values and +1 for high input values. Either nonlinear\\nfunction allows the neural network to learn much more complicated functions than a\\nlinear model could:\\nIn[91]:\\nline = np.linspace(-3, 3, 100)\\nplt.plot(line, np.tanh(line), label=\"tanh\")\\nplt.plot(line, np.maximum(line, 0), label=\"relu\")\\nplt.legend(loc=\"best\")\\nplt.xlabel(\"x\")\\nplt.ylabel(\"relu(x), tanh(x)\")\\n106 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 120, 'page_label': '107'}, page_content='Figure 2-46. The hyperbolic tangent activation function and the rectified linear activa‐\\ntion function\\nFor the small neural network pictured in Figure 2-45, the full formula for computing\\nŷ in the case of regression would be (when using a tanh nonlinearity):\\nh[0] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\\nh[1] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\\nh[2] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\\nŷ = v[0] * h[0] + v[1] * h[1] + v[2] * h[2]\\nHere, w are the weights between the input x and the hidden layer h, and v are the\\nweights between the hidden layer h and the output ŷ. The weights v and w are learned\\nfrom data, x are the input features, ŷ is the computed output, and h are intermediate\\ncomputations. An important parameter that needs to be set by the user is the number\\nof nodes in the hidden layer. This can be as small as 10 for very small or simple data‐\\nsets and as big as 10,000 for very complex data. It is also possible to add additional\\nhidden layers, as shown in Figure 2-47:\\nSupervised Machine Learning Algorithms | 107'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 121, 'page_label': '108'}, page_content='In[92]:\\nmglearn.plots.plot_two_hidden_layer_graph()\\nFigure 2-47. A multilayer perceptron with two hidden layers\\nHaving large neural networks made up of many of these layers of computation is\\nwhat inspired the term “deep learning. ”\\nTuning neural networks\\nLet’s look into the workings of the MLP by applying the MLPClassifier to the\\ntwo_moons dataset we used earlier in this chapter. The results are shown in\\nFigure 2-48:\\nIn[93]:\\nfrom sklearn.neural_network import MLPClassifier\\nfrom sklearn.datasets import make_moons\\nX, y = make_moons(n_samples=100, noise=0.25, random_state=3)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\\n                                                    random_state=42)\\nmlp = MLPClassifier(algorithm=\\'l-bfgs\\', random_state=0).fit(X_train, y_train)\\nmglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\n108 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 122, 'page_label': '109'}, page_content='Figure 2-48. Decision boundary learned by a neural network with 100 hidden units on\\nthe two_moons dataset\\nAs you can see, the neural network learned a very nonlinear but relatively smooth\\ndecision boundary. We used algorithm=\\'l-bfgs\\', which we will discuss later.\\nBy default, the MLP uses 100 hidden nodes, which is quite a lot for this small dataset.\\nWe can reduce the number (which reduces the complexity of the model) and still get\\na good result (Figure 2-49):\\nIn[94]:\\nmlp = MLPClassifier(algorithm=\\'l-bfgs\\', random_state=0, hidden_layer_sizes=[10])\\nmlp.fit(X_train, y_train)\\nmglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nSupervised Machine Learning Algorithms | 109'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 123, 'page_label': '110'}, page_content='Figure 2-49. Decision boundary learned by a neural network with 10 hidden units on\\nthe two_moons dataset\\nWith only 10 hidden units, the decision boundary looks somewhat more ragged. The\\ndefault nonlinearity is relu, shown in Figure 2-46 . With a single hidden layer, this\\nmeans the decision function will be made up of 10 straight line segments. If we want\\na smoother decision boundary, we could add more hidden units (as in Figure 2-49),\\nadd a second hidden layer (Figure 2-50), or use the tanh nonlinearity (Figure 2-51):\\nIn[95]:\\n# using two hidden layers, with 10 units each\\nmlp = MLPClassifier(algorithm=\\'l-bfgs\\', random_state=0,\\n                    hidden_layer_sizes=[10, 10])\\nmlp.fit(X_train, y_train)\\nmglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\n110 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 124, 'page_label': '111'}, page_content='In[96]:\\n# using two hidden layers, with 10 units each, now with tanh nonlinearity\\nmlp = MLPClassifier(algorithm=\\'l-bfgs\\', activation=\\'tanh\\',\\n                    random_state=0, hidden_layer_sizes=[10, 10])\\nmlp.fit(X_train, y_train)\\nmglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nFigure 2-50. Decision boundary learned using 2 hidden layers with 10 hidden units\\neach, with rect activation function\\nSupervised Machine Learning Algorithms | 111'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 125, 'page_label': '112'}, page_content='Figure 2-51. Decision boundary learned using 2 hidden layers with 10 hidden units\\neach, with tanh activation function\\nFinally, we can also control the complexity of a neural network by using an l2 penalty\\nto shrink the weights toward zero, as we did in ridge regression and the linear classifi‐\\ners. The parameter for this in the MLPClassifier is alpha (as in the linear regression\\nmodels), and it’s set to a very low value (little regularization) by default. Figure 2-52\\nshows the effect of different values of alpha on the two_moons dataset, using two hid‐\\nden layers of 10 or 100 units each:\\nIn[97]:\\nfig, axes = plt.subplots(2, 4, figsize=(20, 8))\\nfor axx, n_hidden_nodes in zip(axes, [10, 100]):\\n    for ax, alpha in zip(axx, [0.0001, 0.01, 0.1, 1]):\\n        mlp = MLPClassifier(algorithm=\\'l-bfgs\\', random_state=0,\\n                            hidden_layer_sizes=[n_hidden_nodes, n_hidden_nodes],\\n                            alpha=alpha)\\n        mlp.fit(X_train, y_train)\\n        mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax)\\n        mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)\\n        ax.set_title(\"n_hidden=[{}, {}]\\\\nalpha={:.4f}\".format(\\n                      n_hidden_nodes, n_hidden_nodes, alpha))\\n112 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 126, 'page_label': '113'}, page_content=\"Figure 2-52. Decision functions for different numbers of hidden units and different set‐\\ntings of the alpha parameter\\nAs you probably have realized by now, there are many ways to control the complexity\\nof a neural network: the number of hidden layers, the number of units in each hidden\\nlayer, and the regularization ( alpha). There are actually even more, which we won’t\\ngo into here.\\nAn important property of neural networks is that their weights are set randomly\\nbefore learning is started, and this random initialization affects the model that is\\nlearned. That means that even when using exactly the same parameters, we can\\nobtain very different models when using different random seeds. If the networks are\\nlarge, and their complexity is chosen properly, this should not affect accuracy too\\nmuch, but it is worth keeping in mind (particularly for smaller networks).\\nFigure 2-53 shows plots of several models, all learned with the same settings of the\\nparameters:\\nIn[98]:\\nfig, axes = plt.subplots(2, 4, figsize=(20, 8))\\nfor i, ax in enumerate(axes.ravel()):\\n    mlp = MLPClassifier(algorithm='l-bfgs', random_state=i,\\n                        hidden_layer_sizes=[100, 100])\\n    mlp.fit(X_train, y_train)\\n    mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax)\\n    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)\\nSupervised Machine Learning Algorithms | 113\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 127, 'page_label': '114'}, page_content='Figure 2-53. Decision functions learned with the same parameters but different random\\ninitializations\\nTo get a better understanding of neural networks on real-world data, let’s apply the\\nMLPClassifier to the Breast Cancer dataset. We start with the default parameters:\\nIn[99]:\\nprint(\"Cancer data per-feature maxima:\\\\n{}\".format(cancer.data.max(axis=0)))\\nOut[99]:\\nCancer data per-feature maxima:\\n[   28.110    39.280   188.500  2501.000     0.163     0.345     0.427\\n     0.201     0.304     0.097     2.873     4.885    21.980   542.200\\n     0.031     0.135     0.396     0.053     0.079     0.030    36.040\\n    49.540   251.200  4254.000     0.223     1.058     1.252     0.291\\n     0.664     0.207]\\nIn[100]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, random_state=0)\\nmlp = MLPClassifier(random_state=42)\\nmlp.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.2f}\".format(mlp.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.2f}\".format(mlp.score(X_test, y_test)))\\nOut[100]:\\nAccuracy on training set: 0.92\\nAccuracy on test set: 0.90\\nThe accuracy of the MLP is quite good, but not as good as the other models. As in the\\nearlier SVC example, this is likely due to scaling of the data. Neural networks also\\nexpect all input features to vary in a similar way, and ideally to have a mean of 0, and\\n114 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 128, 'page_label': '115'}, page_content='a variance of 1. We must rescale our data so that it fulfills these requirements. Again,\\nwe will do this by hand here, but we’ll introduce the StandardScaler to do this auto‐\\nmatically in Chapter 3:\\nIn[101]:\\n# compute the mean value per feature on the training set\\nmean_on_train = X_train.mean(axis=0)\\n# compute the standard deviation of each feature on the training set\\nstd_on_train = X_train.std(axis=0)\\n# subtract the mean, and scale by inverse standard deviation\\n# afterward, mean=0 and std=1\\nX_train_scaled = (X_train - mean_on_train) / std_on_train\\n# use THE SAME transformation (using training mean and std) on the test set\\nX_test_scaled = (X_test - mean_on_train) / std_on_train\\nmlp = MLPClassifier(random_state=0)\\nmlp.fit(X_train_scaled, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(\\n    mlp.score(X_train_scaled, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\\nOut[101]:\\nAccuracy on training set: 0.991\\nAccuracy on test set: 0.965\\nConvergenceWarning:\\n    Stochastic Optimizer: Maximum iterations reached and the optimization\\n    hasn\\'t converged yet.\\nThe results are much better after scaling, and already quite competitive. We got a\\nwarning from the model, though, that tells us that the maximum number of iterations\\nhas been reached. This is part of the adam algorithm for learning the model, and tells\\nus that we should increase the number of iterations:\\nIn[102]:\\nmlp = MLPClassifier(max_iter=1000, random_state=0)\\nmlp.fit(X_train_scaled, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(\\n    mlp.score(X_train_scaled, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\\nOut[102]:\\nAccuracy on training set: 0.995\\nAccuracy on test set: 0.965\\nSupervised Machine Learning Algorithms | 115'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 129, 'page_label': '116'}, page_content='12 Y ou might have noticed at this point that many of the well-performing models achieved exactly the same\\naccuracy of 0.972. This means that all of the models make exactly the same number of mistakes, which is four.\\nIf you compare the actual predictions, you can even see that they make exactly the same mistakes! This might\\nbe a consequence of the dataset being very small, or it may be because these points are really different from\\nthe rest.\\nIncreasing the number of iterations only increased the training set performance, not\\nthe generalization performance. Still, the model is performing quite well. As there is\\nsome gap between the training and the test performance, we might try to decrease the\\nmodel’s complexity to get better generalization performance. Here, we choose to\\nincrease the alpha parameter (quite aggressively, from 0.0001 to 1) to add stronger\\nregularization of the weights:\\nIn[103]:\\nmlp = MLPClassifier(max_iter=1000, alpha=1, random_state=0)\\nmlp.fit(X_train_scaled, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(\\n    mlp.score(X_train_scaled, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\\nOut[103]:\\nAccuracy on training set: 0.988\\nAccuracy on test set: 0.972\\nThis leads to a performance on par with the best models so far.12\\nWhile it is possible to analyze what a neural network has learned, this is usually much\\ntrickier than analyzing a linear model or a tree-based model. One way to introspect\\nwhat was learned is to look at the weights in the model. Y ou can see an example of\\nthis in the scikit-learn example gallery . For the Breast Cancer dataset, this might\\nbe a bit hard to understand. The following plot ( Figure 2-54) shows the weights that\\nwere learned connecting the input to the first hidden layer. The rows in this plot cor‐\\nrespond to the 30 input features, while the columns correspond to the 100 hidden\\nunits. Light colors represent large positive values, while dark colors represent nega‐\\ntive values:\\nIn[104]:\\nplt.figure(figsize=(20, 5))\\nplt.imshow(mlp.coefs_[0], interpolation=\\'none\\', cmap=\\'viridis\\')\\nplt.yticks(range(30), cancer.feature_names)\\nplt.xlabel(\"Columns in weight matrix\")\\nplt.ylabel(\"Input feature\")\\nplt.colorbar()\\n116 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 130, 'page_label': '117'}, page_content='Figure 2-54. Heat map of the first layer weights in a neural network learned on the\\nBreast Cancer dataset\\nOne possible inference we can make is that features that have very small weights for\\nall of the hidden units are “less important” to the model. We can see that “mean\\nsmoothness” and “mean compactness, ” in addition to the features found between\\n“smoothness error” and “fractal dimension error, ” have relatively low weights com‐\\npared to other features. This could mean that these are less important features or pos‐\\nsibly that we didn’t represent them in a way that the neural network could use.\\nWe could also visualize the weights connecting the hidden layer to the output layer,\\nbut those are even harder to interpret.\\nWhile the MLPClassifier and MLPRegressor provide easy-to-use interfaces for the\\nmost common neural network architectures, they only capture a small subset of what\\nis possible with neural networks. If you are interested in working with more flexible\\nor larger models, we encourage you to look beyond scikit-learn into the fantastic\\ndeep learning libraries that are out there. For Python users, the most well-established\\nare keras, lasagna, and tensor-flow. lasagna builds on the theano library, while\\nkeras can use either tensor-flow or theano. These libraries provide a much more\\nflexible interface to build neural networks and track the rapid progress in deep learn‐\\ning research. All of the popular deep learning libraries also allow the use of high-\\nperformance graphics processing units (GPUs), which scikit-learn does not\\nsupport. Using GPUs allows us to accelerate computations by factors of 10x to 100x,\\nand they are essential for applying deep learning methods to large-scale datasets.\\nStrengths, weaknesses, and parameters\\nNeural networks have reemerged as state-of-the-art models in many applications of\\nmachine learning. One of their main advantages is that they are able to capture infor‐\\nmation contained in large amounts of data and build incredibly complex models.\\nGiven enough computation time, data, and careful tuning of the parameters, neural\\nnetworks often beat other machine learning algorithms (for classification and regres‐\\nsion tasks).\\nSupervised Machine Learning Algorithms | 117'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 131, 'page_label': '118'}, page_content=\"This brings us to the downsides. Neural networks—particularly the large and power‐\\nful ones—often take a long time to train. They also require careful preprocessing of\\nthe data, as we saw here. Similarly to SVMs, they work best with “homogeneous”\\ndata, where all the features have similar meanings. For data that has very different\\nkinds of features, tree-based models might work better. Tuning neural network\\nparameters is also an art unto itself. In our experiments, we barely scratched the sur‐\\nface of possible ways to adjust neural network models and how to train them.\\nEstimating complexity in neural networks.    The most important parameters are the num‐\\nber of layers and the number of hidden units per layer. Y ou should start with one or\\ntwo hidden layers, and possibly expand from there. The number of nodes per hidden\\nlayer is often similar to the number of input features, but rarely higher than in the low\\nto mid-thousands.\\nA helpful measure when thinking about the model complexity of a neural network is\\nthe number of weights or coefficients that are learned. If you have a binary classifica‐\\ntion dataset with 100 features, and you have 100 hidden units, then there are 100 *\\n100 = 10,000 weights between the input and the first hidden layer. There are also\\n100 * 1 = 100 weights between the hidden layer and the output layer, for a total of\\naround 10,100 weights. If you add a second hidden layer with 100 hidden units, there\\nwill be another 100 * 100 = 10,000 weights from the first hidden layer to the second\\nhidden layer, resulting in a total of 20,100 weights. If instead you use one layer with\\n1,000 hidden units, you are learning 100 * 1,000 = 100,000 weights from the input to\\nthe hidden layer and 1,000 x 1 weights from the hidden layer to the output layer, for a\\ntotal of 101,000. If you add a second hidden layer you add 1,000 * 1,000 = 1,000,000\\nweights, for a whopping total of 1,101,000—50 times larger than the model with two\\nhidden layers of size 100.\\nA common way to adjust parameters in a neural network is to first create a network\\nthat is large enough to overfit, making sure that the task can actually be learned by\\nthe network. Then, once you know the training data can be learned, either shrink the\\nnetwork or increase alpha to add regularization, which will improve generalization\\nperformance.\\nIn our experiments, we focused mostly on the definition of the model: the number of\\nlayers and nodes per layer, the regularization, and the nonlinearity. These define the\\nmodel we want to learn. There is also the question of how to learn the model, or the\\nalgorithm that is used for learning the parameters, which is set using the algorithm\\nparameter. There are two easy-to-use choices for algorithm. The default is 'adam',\\nwhich works well in most situations but is quite sensitive to the scaling of the data (so\\nit is important to always scale your data to 0 mean and unit variance). The other one\\nis 'l-bfgs', which is quite robust but might take a long time on larger models or\\nlarger datasets. There is also the more advanced 'sgd' option, which is what many\\ndeep learning researchers use. The 'sgd' option comes with many additional param‐\\n118 | Chapter 2: Supervised Learning\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 132, 'page_label': '119'}, page_content='eters that need to be tuned for best results. Y ou can find all of these parameters and\\ntheir definitions in the user guide. When starting to work with MLPs, we recommend\\nsticking to \\'adam\\' and \\'l-bfgs\\'.\\nfit  Resets a Model\\nAn important property of scikit-learn models is that calling fit\\nwill always reset everything a model previously learned. So if you\\nbuild a model on one dataset, and then call fit again on a different\\ndataset, the model will “forget” everything it learned from the first\\ndataset. Y ou can call fit as often as you like on a model, and the\\noutcome will be the same as calling fit on a “new” model.\\nUncertainty Estimates from Classifiers\\nAnother useful part of the scikit-learn interface that we haven’t talked about yet is\\nthe ability of classifiers to provide uncertainty estimates of predictions. Often, you are\\nnot only interested in which class a classifier predicts for a certain test point, but also\\nhow certain it is that this is the right class. In practice, different kinds of mistakes lead\\nto very different outcomes in real-world applications. Imagine a medical application\\ntesting for cancer. Making a false positive prediction might lead to a patient undergo‐\\ning additional tests, while a false negative prediction might lead to a serious disease\\nnot being treated. We will go into this topic in more detail in Chapter 6.\\nThere are two different functions in scikit-learn that can be used to obtain uncer‐\\ntainty estimates from classifiers: decision_function and predict_proba. Most (but\\nnot all) classifiers have at least one of them, and many classifiers have both. Let’s look\\nat what these two functions do on a synthetic two-dimensional dataset, when build‐\\ning a GradientBoostingClassifier classifier, which has both a decision_function\\nand a predict_proba method:\\nIn[105]:\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.datasets import make_blobs, make_circles\\nX, y = make_circles(noise=0.25, factor=0.5, random_state=1)\\n# we rename the classes \"blue\" and \"red\" for illustration purposes\\ny_named = np.array([\"blue\", \"red\"])[y]\\n# we can call train_test_split with arbitrarily many arrays;\\n# all will be split in a consistent manner\\nX_train, X_test, y_train_named, y_test_named, y_train, y_test = \\\\\\n    train_test_split(X, y_named, y, random_state=0)\\n# build the gradient boosting model\\ngbrt = GradientBoostingClassifier(random_state=0)\\ngbrt.fit(X_train, y_train_named)\\nUncertainty Estimates from Classifiers  | 119'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 133, 'page_label': '120'}, page_content='The Decision Function\\nIn the binary classification case, the return value of decision_function is of shape\\n(n_samples,), and it returns one floating-point number for each sample:\\nIn[106]:\\nprint(\"X_test.shape: {}\".format(X_test.shape))\\nprint(\"Decision function shape: {}\".format(\\n    gbrt.decision_function(X_test).shape))\\nOut[106]:\\nX_test.shape: (25, 2)\\nDecision function shape: (25,)\\nThis value encodes how strongly the model believes a data point to belong to the\\n“positive” class, in this case class 1. Positive values indicate a preference for the posi‐\\ntive class, and negative values indicate a preference for the “negative” (other) class:\\nIn[107]:\\n# show the first few entries of decision_function\\nprint(\"Decision function:\\\\n{}\".format(gbrt.decision_function(X_test)[:6]))\\nOut[107]:\\nDecision function:\\n[ 4.136 -1.683 -3.951 -3.626  4.29   3.662]\\nWe can recover the prediction by looking only at the sign of the decision function:\\nIn[108]:\\nprint(\"Thresholded decision function:\\\\n{}\".format(\\n    gbrt.decision_function(X_test) > 0))\\nprint(\"Predictions:\\\\n{}\".format(gbrt.predict(X_test)))\\nOut[108]:\\nThresholded decision function:\\n[ True False False False  True  True False  True  True  True False  True\\n  True False  True False False False  True  True  True  True  True False\\n  False]\\nPredictions:\\n[\\'red\\' \\'blue\\' \\'blue\\' \\'blue\\' \\'red\\' \\'red\\' \\'blue\\' \\'red\\' \\'red\\' \\'red\\' \\'blue\\'\\n \\'red\\' \\'red\\' \\'blue\\' \\'red\\' \\'blue\\' \\'blue\\' \\'blue\\' \\'red\\' \\'red\\' \\'red\\' \\'red\\'\\n \\'red\\' \\'blue\\' \\'blue\\']\\nFor binary classification, the “negative” class is always the first entry of the classes_\\nattribute, and the “positive” class is the second entry of classes_. So if you want to\\nfully recover the output of predict, you need to make use of the classes_ attribute:\\n120 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 134, 'page_label': '121'}, page_content='In[109]:\\n# make the boolean True/False into 0 and 1\\ngreater_zero = (gbrt.decision_function(X_test) > 0).astype(int)\\n# use 0 and 1 as indices into classes_\\npred = gbrt.classes_[greater_zero]\\n# pred is the same as the output of gbrt.predict\\nprint(\"pred is equal to predictions: {}\".format(\\n    np.all(pred == gbrt.predict(X_test))))\\nOut[109]:\\npred is equal to predictions: True\\nThe range of decision_function can be arbitrary, and depends on the data and the\\nmodel parameters:\\nIn[110]:\\ndecision_function = gbrt.decision_function(X_test)\\nprint(\"Decision function minimum: {:.2f} maximum: {:.2f}\".format(\\n    np.min(decision_function), np.max(decision_function)))\\nOut[110]:\\nDecision function minimum: -7.69 maximum: 4.29\\nThis arbitrary scaling makes the output of decision_function often hard to\\ninterpret.\\nIn the following example we plot the decision_function for all points in the 2D\\nplane using a color coding, next to a visualization of the decision boundary, as we saw\\nearlier. We show training points as circles and test data as triangles (Figure 2-55):\\nIn[111]:\\nfig, axes = plt.subplots(1, 2, figsize=(13, 5))\\nmglearn.tools.plot_2d_separator(gbrt, X, ax=axes[0], alpha=.4,\\n                                fill=True, cm=mglearn.cm2)\\nscores_image = mglearn.tools.plot_2d_scores(gbrt, X, ax=axes[1],\\n                                            alpha=.4, cm=mglearn.ReBl)\\nfor ax in axes:\\n    # plot training and test points\\n    mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test,\\n                             markers=\\'^\\', ax=ax)\\n    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train,\\n                             markers=\\'o\\', ax=ax)\\n    ax.set_xlabel(\"Feature 0\")\\n    ax.set_ylabel(\"Feature 1\")\\ncbar = plt.colorbar(scores_image, ax=axes.tolist())\\naxes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\",\\n                \"Train class 1\"], ncol=4, loc=(.1, 1.1))\\nUncertainty Estimates from Classifiers  | 121'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 135, 'page_label': '122'}, page_content='Figure 2-55. Decision boundary (left) and decision function (right) for a gradient boost‐\\ning model on a two-dimensional toy dataset\\nEncoding not only the predicted outcome but also how certain the classifier is pro‐\\nvides additional information. However, in this visualization, it is hard to make out the\\nboundary between the two classes.\\nPredicting Probabilities\\nThe output of predict_proba is a probability for each class, and is often more easily\\nunderstood than the output of decision_function. It is always of shape (n_samples,\\n2) for binary classification:\\nIn[112]:\\nprint(\"Shape of probabilities: {}\".format(gbrt.predict_proba(X_test).shape))\\nOut[112]:\\nShape of probabilities: (25, 2)\\nThe first entry in each row is the estimated probability of the first class, and the sec‐\\nond entry is the estimated probability of the second class. Because it is a probability,\\nthe output of predict_proba is always between 0 and 1, and the sum of the entries\\nfor both classes is always 1:\\nIn[113]:\\n# show the first few entries of predict_proba\\nprint(\"Predicted probabilities:\\\\n{}\".format(\\n    gbrt.predict_proba(X_test[:6])))\\n122 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 136, 'page_label': '123'}, page_content='13 Because the probabilities are floating-point numbers, it is unlikely that they will both be exactly 0.500. How‐\\never, if that happens, the prediction is made at random.\\nOut[113]:\\nPredicted probabilities:\\n[[ 0.016  0.984]\\n [ 0.843  0.157]\\n [ 0.981  0.019]\\n [ 0.974  0.026]\\n [ 0.014  0.986]\\n [ 0.025  0.975]]\\nBecause the probabilities for the two classes sum to 1, exactly one of the classes will\\nbe above 50% certainty. That class is the one that is predicted.13\\nY ou can see in the previous output that the classifier is relatively certain for most\\npoints. How well the uncertainty actually reflects uncertainty in the data depends on\\nthe model and the parameters. A model that is more overfitted tends to make more\\ncertain predictions, even if they might be wrong. A model with less complexity usu‐\\nally has more uncertainty in its predictions. A model is called calibrated if the\\nreported uncertainty actually matches how correct it is—in a calibrated model, a pre‐\\ndiction made with 70% certainty would be correct 70% of the time.\\nIn the following example ( Figure 2-56) we again show the decision boundary on the\\ndataset, next to the class probabilities for the class 1:\\nIn[114]:\\nfig, axes = plt.subplots(1, 2, figsize=(13, 5))\\nmglearn.tools.plot_2d_separator(\\n    gbrt, X, ax=axes[0], alpha=.4, fill=True, cm=mglearn.cm2)\\nscores_image = mglearn.tools.plot_2d_scores(\\n    gbrt, X, ax=axes[1], alpha=.5, cm=mglearn.ReBl, function=\\'predict_proba\\')\\nfor ax in axes:\\n    # plot training and test points\\n    mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test,\\n                             markers=\\'^\\', ax=ax)\\n    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train,\\n                             markers=\\'o\\', ax=ax)\\n    ax.set_xlabel(\"Feature 0\")\\n    ax.set_ylabel(\"Feature 1\")\\ncbar = plt.colorbar(scores_image, ax=axes.tolist())\\naxes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\",\\n                \"Train class 1\"], ncol=4, loc=(.1, 1.1))\\nUncertainty Estimates from Classifiers  | 123'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 137, 'page_label': '124'}, page_content='Figure 2-56. Decision boundary (left) and predicted probabilities for the gradient boost‐\\ning model shown in Figure 2-55\\nThe boundaries in this plot are much more well-defined, and the small areas of\\nuncertainty are clearly visible.\\nThe scikit-learn website has a great comparison of many models and what their\\nuncertainty estimates look like. We’ve reproduced this in Figure 2-57, and we encour‐\\nage you to go though the example there.\\nFigure 2-57. Comparison of several classifiers in scikit-learn on synthetic datasets (image\\ncourtesy http://scikit-learn.org)\\nUncertainty in Multiclass Classification\\nSo far, we’ve only talked about uncertainty estimates in binary classification. But the\\ndecision_function and predict_proba methods also work in the multiclass setting.\\nLet’s apply them on the Iris dataset, which is a three-class classification dataset:\\n124 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 138, 'page_label': '125'}, page_content='In[115]:\\nfrom sklearn.datasets import load_iris\\niris = load_iris()\\nX_train, X_test, y_train, y_test = train_test_split(\\n    iris.data, iris.target, random_state=42)\\ngbrt = GradientBoostingClassifier(learning_rate=0.01, random_state=0)\\ngbrt.fit(X_train, y_train)\\nIn[116]:\\nprint(\"Decision function shape: {}\".format(gbrt.decision_function(X_test).shape))\\n# plot the first few entries of the decision function\\nprint(\"Decision function:\\\\n{}\".format(gbrt.decision_function(X_test)[:6, :]))\\nOut[116]:\\nDecision function shape: (38, 3)\\nDecision function:\\n[[-0.529  1.466 -0.504]\\n [ 1.512 -0.496 -0.503]\\n [-0.524 -0.468  1.52 ]\\n [-0.529  1.466 -0.504]\\n [-0.531  1.282  0.215]\\n [ 1.512 -0.496 -0.503]]\\nIn the multiclass case, the decision_function has the shape (n_samples,\\nn_classes) and each column provides a “certainty score” for each class, where a large\\nscore means that a class is more likely and a small score means the class is less likely.\\nY ou can recover the predictions from these scores by finding the maximum entry for\\neach data point:\\nIn[117]:\\nprint(\"Argmax of decision function:\\\\n{}\".format(\\n      np.argmax(gbrt.decision_function(X_test), axis=1)))\\nprint(\"Predictions:\\\\n{}\".format(gbrt.predict(X_test)))\\nOut[117]:\\nArgmax of decision function:\\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\\nPredictions:\\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\\nThe output of predict_proba has the same shape, (n_samples, n_classes). Again,\\nthe probabilities for the possible classes for each data point sum to 1:\\nUncertainty Estimates from Classifiers  | 125'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 139, 'page_label': '126'}, page_content='In[118]:\\n# show the first few entries of predict_proba\\nprint(\"Predicted probabilities:\\\\n{}\".format(gbrt.predict_proba(X_test)[:6]))\\n# show that sums across rows are one\\nprint(\"Sums: {}\".format(gbrt.predict_proba(X_test)[:6].sum(axis=1)))\\nOut[118]:\\nPredicted probabilities:\\n[[ 0.107  0.784  0.109]\\n [ 0.789  0.106  0.105]\\n [ 0.102  0.108  0.789]\\n [ 0.107  0.784  0.109]\\n [ 0.108  0.663  0.228]\\n [ 0.789  0.106  0.105]]\\nSums: [ 1.  1.  1.  1.  1.  1.]\\nWe can again recover the predictions by computing the argmax of predict_proba:\\nIn[119]:\\nprint(\"Argmax of predicted probabilities:\\\\n{}\".format(\\n    np.argmax(gbrt.predict_proba(X_test), axis=1)))\\nprint(\"Predictions:\\\\n{}\".format(gbrt.predict(X_test)))\\nOut[119]:\\nArgmax of predicted probabilities:\\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\\nPredictions:\\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\\nTo summarize, predict_proba and decision_function always have shape (n_sam\\nples, n_classes)—apart from decision_function in the special binary case. In the\\nbinary case, decision_function only has one column, corresponding to the “posi‐\\ntive” class classes_[1]. This is mostly for historical reasons.\\nY ou can recover the prediction when there are n_classes many columns by comput‐\\ning the argmax across columns. Be careful, though, if your classes are strings, or you\\nuse integers but they are not consecutive and starting from 0. If you want to compare\\nresults obtained with predict to results obtained via decision_function or pre\\ndict_proba, make sure to use the classes_ attribute of the classifier to get the actual\\nclass names:\\n126 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 140, 'page_label': '127'}, page_content='In[120]:\\nlogreg = LogisticRegression()\\n# represent each target by its class name in the iris dataset\\nnamed_target = iris.target_names[y_train]\\nlogreg.fit(X_train, named_target)\\nprint(\"unique classes in training data: {}\".format(logreg.classes_))\\nprint(\"predictions: {}\".format(logreg.predict(X_test)[:10]))\\nargmax_dec_func = np.argmax(logreg.decision_function(X_test), axis=1)\\nprint(\"argmax of decision function: {}\".format(argmax_dec_func[:10]))\\nprint(\"argmax combined with classes_: {}\".format(\\n        logreg.classes_[argmax_dec_func][:10]))\\nOut[120]:\\nunique classes in training data: [\\'setosa\\' \\'versicolor\\' \\'virginica\\']\\npredictions: [\\'versicolor\\' \\'setosa\\' \\'virginica\\' \\'versicolor\\' \\'versicolor\\'\\n \\'setosa\\' \\'versicolor\\' \\'virginica\\' \\'versicolor\\' \\'versicolor\\']\\nargmax of decision function: [1 0 2 1 1 0 1 2 1 1]\\nargmax combined with classes_: [\\'versicolor\\' \\'setosa\\' \\'virginica\\' \\'versicolor\\'\\n \\'versicolor\\' \\'setosa\\' \\'versicolor\\' \\'virginica\\' \\'versicolor\\' \\'versicolor\\']\\nSummary and Outlook\\nWe started this chapter with a discussion of model complexity, then discussed gener‐\\nalization, or learning a model that is able to perform well on new, previously unseen\\ndata. This led us to the concepts of underfitting, which describes a model that cannot\\ncapture the variations present in the training data, and overfitting, which describes a\\nmodel that focuses too much on the training data and is not able to generalize to new\\ndata very well.\\nWe then discussed a wide array of machine learning models for classification and\\nregression, what their advantages and disadvantages are, and how to control model\\ncomplexity for each of them. We saw that for many of the algorithms, setting the right\\nparameters is important for good performance. Some of the algorithms are also sensi‐\\ntive to how we represent the input data, and in particular to how the features are\\nscaled. Therefore, blindly applying an algorithm to a dataset without understanding\\nthe assumptions the model makes and the meanings of the parameter settings will\\nrarely lead to an accurate model.\\nThis chapter contains a lot of information about the algorithms, and it is not neces‐\\nsary for you to remember all of these details for the following chapters. However,\\nsome knowledge of the models described here—and which to use in a specific situa‐\\ntion—is important for successfully applying machine learning in practice. Here is a\\nquick summary of when to use each model:\\nSummary and Outlook | 127'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 141, 'page_label': '128'}, page_content='Nearest neighbors\\nFor small datasets, good as a baseline, easy to explain.\\nLinear models\\nGo-to as a first algorithm to try, good for very large datasets, good for very high-\\ndimensional data.\\nNaive Bayes\\nOnly for classification. Even faster than linear models, good for very large data‐\\nsets and high-dimensional data. Often less accurate than linear models.\\nDecision trees\\nVery fast, don’t need scaling of the data, can be visualized and easily explained.\\nRandom forests\\nNearly always perform better than a single decision tree, very robust and power‐\\nful. Don’t need scaling of data. Not good for very high-dimensional sparse data.\\nGradient boosted decision trees\\nOften slightly more accurate than random forests. Slower to train but faster to\\npredict than random forests, and smaller in memory. Need more parameter tun‐\\ning than random forests.\\nSupport vector machines\\nPowerful for medium-sized datasets of features with similar meaning. Require\\nscaling of data, sensitive to parameters.\\nNeural networks\\nCan build very complex models, particularly for large datasets. Sensitive to scal‐\\ning of the data and to the choice of parameters. Large models need a long time to\\ntrain.\\nWhen working with a new dataset, it is in general a good idea to start with a simple\\nmodel, such as a linear model or a naive Bayes or nearest neighbors classifier, and see\\nhow far you can get. After understanding more about the data, you can consider\\nmoving to an algorithm that can build more complex models, such as random forests,\\ngradient boosted decision trees, SVMs, or neural networks.\\nY ou should now be in a position where you have some idea of how to apply, tune, and\\nanalyze the models we discussed here. In this chapter, we focused on the binary clas‐\\nsification case, as this is usually easiest to understand. Most of the algorithms presen‐\\nted have classification and regression variants, however, and all of the classification\\nalgorithms support both binary and multiclass classification. Try applying any of\\nthese algorithms to the built-in datasets in scikit-learn, like the boston_housing or\\ndiabetes datasets for regression, or the digits dataset for multiclass classification.\\nPlaying around with the algorithms on different datasets will give you a better feel for\\n128 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 142, 'page_label': '129'}, page_content='how long they need to train, how easy it is to analyze the models, and how sensitive\\nthey are to the representation of the data.\\nWhile we analyzed the consequences of different parameter settings for the algo‐\\nrithms we investigated, building a model that actually generalizes well to new data in\\nproduction is a bit trickier than that. We will see how to properly adjust parameters\\nand how to find good parameters automatically in Chapter 6.\\nFirst, though, we will dive in more detail into unsupervised learning and preprocess‐\\ning in the next chapter.\\nSummary and Outlook | 129'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 143, 'page_label': '130'}, page_content=''),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 144, 'page_label': '131'}, page_content='CHAPTER 3\\nUnsupervised Learning and Preprocessing\\nThe second family of machine learning algorithms that we will discuss is unsuper‐\\nvised learning algorithms. Unsupervised learning subsumes all kinds of machine\\nlearning where there is no known output, no teacher to instruct the learning algo‐\\nrithm. In unsupervised learning, the learning algorithm is just shown the input data\\nand asked to extract knowledge from this data.\\nTypes of Unsupervised Learning\\nWe will look into two kinds of unsupervised learning in this chapter: transformations\\nof the dataset and clustering.\\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\\ntion of the data which might be easier for humans or other machine learning algo‐\\nrithms to understand compared to the original representation of the data. A common\\napplication of unsupervised transformations is dimensionality reduction, which takes\\na high-dimensional representation of the data, consisting of many features, and finds\\na new way to represent this data that summarizes the essential characteristics with\\nfewer features. A common application for dimensionality reduction is reduction to\\ntwo dimensions for visualization purposes.\\nAnother application for unsupervised transformations is finding the parts or compo‐\\nnents that “make up” the data. An example of this is topic extraction on collections of\\ntext documents. Here, the task is to find the unknown topics that are talked about in\\neach document, and to learn what topics appear in each document. This can be useful\\nfor tracking the discussion of themes like elections, gun control, or pop stars on social\\nmedia.\\nClustering algorithms, on the other hand, partition data into distinct groups of similar\\nitems. Consider the example of uploading photos to a social media site. To allow you\\n131'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 145, 'page_label': '132'}, page_content='to organize your pictures, the site might want to group together pictures that show\\nthe same person. However, the site doesn’t know which pictures show whom, and it\\ndoesn’t know how many different people appear in your photo collection. A sensible\\napproach would be to extract all the faces and divide them into groups of faces that\\nlook similar. Hopefully, these correspond to the same person, and the images can be\\ngrouped together for you.\\nChallenges in Unsupervised Learning\\nA major challenge in unsupervised learning is evaluating whether the algorithm\\nlearned something useful. Unsupervised learning algorithms are usually applied to\\ndata that does not contain any label information, so we don’t know what the right\\noutput should be. Therefore, it is very hard to say whether a model “did well. ” For\\nexample, our hypothetical clustering algorithm could have grouped together all the\\npictures that show faces in profile and all the full-face pictures. This would certainly\\nbe a possible way to divide a collection of pictures of people’s faces, but it’s not the one\\nwe were looking for. However, there is no way for us to “tell” the algorithm what we\\nare looking for, and often the only way to evaluate the result of an unsupervised algo‐\\nrithm is to inspect it manually.\\nAs a consequence, unsupervised algorithms are used often in an exploratory setting,\\nwhen a data scientist wants to understand the data better, rather than as part of a\\nlarger automatic system. Another common application for unsupervised algorithms\\nis as a preprocessing step for supervised algorithms. Learning a new representation of\\nthe data can sometimes improve the accuracy of supervised algorithms, or can lead to\\nreduced memory and time consumption.\\nBefore we start with “real” unsupervised algorithms, we will briefly discuss some sim‐\\nple preprocessing methods that often come in handy. Even though preprocessing and\\nscaling are often used in tandem with supervised learning algorithms, scaling meth‐\\nods don’t make use of the supervised information, making them unsupervised.\\nPreprocessing and Scaling\\nIn the previous chapter we saw that some algorithms, like neural networks and SVMs,\\nare very sensitive to the scaling of the data. Therefore, a common practice is to adjust\\nthe features so that the data representation is more suitable for these algorithms.\\nOften, this is a simple per-feature rescaling and shift of the data. The following code\\n(Figure 3-1) shows a simple example:\\nIn[2]:\\nmglearn.plots.plot_scaling()\\n132 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 146, 'page_label': '133'}, page_content='1 The median of a set of numbers is the number x such that half of the numbers are smaller than x and half of\\nthe numbers are larger than x. The lower quartile is the number x such that one-fourth of the numbers are\\nsmaller than x, and the upper quartile is the number x such that one-fourth of the numbers are larger than x.\\nFigure 3-1. Different ways to rescale and preprocess a dataset\\nDifferent  Kinds of Preprocessing\\nThe first plot in Figure 3-1 shows a synthetic two-class classification dataset with two\\nfeatures. The first feature (the x-axis value) is between 10 and 15. The second feature\\n(the y-axis value) is between around 1 and 9.\\nThe following four plots show four different ways to transform the data that yield\\nmore standard ranges. The StandardScaler in scikit-learn ensures that for each\\nfeature the mean is 0 and the variance is 1, bringing all features to the same magni‐\\ntude. However, this scaling does not ensure any particular minimum and maximum\\nvalues for the features. The RobustScaler works similarly to the StandardScaler in\\nthat it ensures statistical properties for each feature that guarantee that they are on the\\nsame scale. However, the RobustScaler uses the median and quartiles, 1 instead of\\nmean and variance. This makes the RobustScaler ignore data points that are very\\ndifferent from the rest (like measurement errors). These odd data points are also\\ncalled outliers, and can lead to trouble for other scaling techniques.\\nThe MinMaxScaler, on the other hand, shifts the data such that all features are exactly\\nbetween 0 and 1. For the two-dimensional dataset this means all of the data is con‐\\nPreprocessing and Scaling | 133'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 147, 'page_label': '134'}, page_content='tained within the rectangle created by the x-axis between 0 and 1 and the y-axis\\nbetween 0 and 1.\\nFinally, the Normalizer does a very different kind of rescaling. It scales each data\\npoint such that the feature vector has a Euclidean length of 1. In other words, it\\nprojects a data point on the circle (or sphere, in the case of higher dimensions) with a\\nradius of 1. This means every data point is scaled by a different number (by the\\ninverse of its length). This normalization is often used when only the direction (or\\nangle) of the data matters, not the length of the feature vector.\\nApplying Data Transformations\\nNow that we’ve seen what the different kinds of transformations do, let’s apply them\\nusing scikit-learn. We will use the cancer dataset that we saw in Chapter 2. Pre‐\\nprocessing methods like the scalers are usually applied before applying a supervised\\nmachine learning algorithm. As an example, say we want to apply the kernel SVM\\n(SVC) to the cancer dataset, and use MinMaxScaler for preprocessing the data. We\\nstart by loading our dataset and splitting it into a training set and a test set (we need\\nseparate training and test sets to evaluate the supervised model we will build after the\\npreprocessing):\\nIn[3]:\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\ncancer = load_breast_cancer()\\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\\n                                                    random_state=1)\\nprint(X_train.shape)\\nprint(X_test.shape)\\nOut[3]:\\n(426, 30)\\n(143, 30)\\nAs a reminder, the dataset contains 569 data points, each represented by 30 measure‐\\nments. We split the dataset into 426 samples for the training set and 143 samples for\\nthe test set.\\nAs with the supervised models we built earlier, we first import the class that imple‐\\nments the preprocessing, and then instantiate it:\\nIn[4]:\\nfrom sklearn.preprocessing import MinMaxScaler\\nscaler = MinMaxScaler()\\n134 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 148, 'page_label': '135'}, page_content='We then fit the scaler using the fit method, applied to the training data. For the Min\\nMaxScaler, the fit method computes the minimum and maximum value of each fea‐\\nture on the training set. In contrast to the classifiers and regressors of Chapter 2, the\\nscaler is only provided with the data (X_train) when fit is called, and y_train is not\\nused:\\nIn[5]:\\nscaler.fit(X_train)\\nOut[5]:\\nMinMaxScaler(copy=True, feature_range=(0, 1))\\nTo apply the transformation that we just learned—that is, to actually scale the training\\ndata—we use the transform method of the scaler. The transform method is used in\\nscikit-learn whenever a model returns a new representation of the data:\\nIn[6]:\\n# transform data\\nX_train_scaled = scaler.transform(X_train)\\n# print dataset properties before and after scaling\\nprint(\"transformed shape: {}\".format(X_train_scaled.shape))\\nprint(\"per-feature minimum before scaling:\\\\n {}\".format(X_train.min(axis=0)))\\nprint(\"per-feature maximum before scaling:\\\\n {}\".format(X_train.max(axis=0)))\\nprint(\"per-feature minimum after scaling:\\\\n {}\".format(\\n    X_train_scaled.min(axis=0)))\\nprint(\"per-feature maximum after scaling:\\\\n {}\".format(\\n    X_train_scaled.max(axis=0)))\\nOut[6]:\\ntransformed shape: (426, 30)\\nper-feature minimum before scaling:\\n [   6.98    9.71   43.79  143.50    0.05    0.02    0.      0.      0.11\\n     0.05    0.12    0.36    0.76    6.80    0.      0.      0.      0.\\n     0.01    0.      7.93   12.02   50.41  185.20    0.07    0.03    0.\\n     0.      0.16    0.06]\\nper-feature maximum before scaling:\\n [   28.11    39.28   188.5   2501.0     0.16     0.29     0.43     0.2\\n     0.300    0.100    2.87     4.88    21.98   542.20     0.03     0.14\\n     0.400    0.050    0.06     0.03    36.04    49.54   251.20  4254.00\\n     0.220    0.940    1.17     0.29     0.58     0.15]\\nper-feature minimum after scaling:\\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\\n   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\\nper-feature maximum after scaling:\\n [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\\n   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\\nPreprocessing and Scaling | 135'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 149, 'page_label': '136'}, page_content='The transformed data has the same shape as the original data—the features are simply\\nshifted and scaled. Y ou can see that all of the features are now between 0 and 1, as\\ndesired.\\nTo apply the SVM to the scaled data, we also need to transform the test set. This is\\nagain done by calling the transform method, this time on X_test:\\nIn[7]:\\n# transform test data\\nX_test_scaled = scaler.transform(X_test)\\n# print test data properties after scaling\\nprint(\"per-feature minimum after scaling:\\\\n{}\".format(X_test_scaled.min(axis=0)))\\nprint(\"per-feature maximum after scaling:\\\\n{}\".format(X_test_scaled.max(axis=0)))\\nOut[7]:\\nper-feature minimum after scaling:\\n[ 0.034  0.023  0.031  0.011  0.141  0.044  0.     0.     0.154 -0.006\\n -0.001  0.006  0.004  0.001  0.039  0.011  0.     0.    -0.032  0.007\\n  0.027  0.058  0.02   0.009  0.109  0.026  0.     0.    -0.    -0.002]\\nper-feature maximum after scaling:\\n[ 0.958  0.815  0.956  0.894  0.811  1.22   0.88   0.933  0.932  1.037\\n  0.427  0.498  0.441  0.284  0.487  0.739  0.767  0.629  1.337  0.391\\n  0.896  0.793  0.849  0.745  0.915  1.132  1.07   0.924  1.205  1.631]\\nMaybe somewhat surprisingly, you can see that for the test set, after scaling, the mini‐\\nmum and maximum are not 0 and 1. Some of the features are even outside the 0–1\\nrange! The explanation is that the MinMaxScaler (and all the other scalers) always\\napplies exactly the same transformation to the training and the test set. This means\\nthe transform method always subtracts the training set minimum and divides by the\\ntraining set range, which might be different from the minimum and range for the test\\nset.\\nScaling Training and Test Data the Same Way\\nIt is important to apply exactly the same transformation to the training set and the\\ntest set for the supervised model to work on the test set. The following example\\n(Figure 3-2) illustrates what would happen if we were to use the minimum and range\\nof the test set instead:\\nIn[8]:\\nfrom sklearn.datasets import make_blobs\\n# make synthetic data\\nX, _ = make_blobs(n_samples=50, centers=5, random_state=4, cluster_std=2)\\n# split it into training and test sets\\nX_train, X_test = train_test_split(X, random_state=5, test_size=.1)\\n# plot the training and test sets\\nfig, axes = plt.subplots(1, 3, figsize=(13, 4))\\n136 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 150, 'page_label': '137'}, page_content='axes[0].scatter(X_train[:, 0], X_train[:, 1],\\n                c=mglearn.cm2(0), label=\"Training set\", s=60)\\naxes[0].scatter(X_test[:, 0], X_test[:, 1], marker=\\'^\\',\\n                c=mglearn.cm2(1), label=\"Test set\", s=60)\\naxes[0].legend(loc=\\'upper left\\')\\naxes[0].set_title(\"Original Data\")\\n# scale the data using MinMaxScaler\\nscaler = MinMaxScaler()\\nscaler.fit(X_train)\\nX_train_scaled = scaler.transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n# visualize the properly scaled data\\naxes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\\n                c=mglearn.cm2(0), label=\"Training set\", s=60)\\naxes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker=\\'^\\',\\n                c=mglearn.cm2(1), label=\"Test set\", s=60)\\naxes[1].set_title(\"Scaled Data\")\\n# rescale the test set separately\\n# so test set min is 0 and test set max is 1\\n# DO NOT DO THIS! For illustration purposes only.\\ntest_scaler = MinMaxScaler()\\ntest_scaler.fit(X_test)\\nX_test_scaled_badly = test_scaler.transform(X_test)\\n# visualize wrongly scaled data\\naxes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\\n                c=mglearn.cm2(0), label=\"training set\", s=60)\\naxes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1],\\n                marker=\\'^\\', c=mglearn.cm2(1), label=\"test set\", s=60)\\naxes[2].set_title(\"Improperly Scaled Data\")\\nfor ax in axes:\\n    ax.set_xlabel(\"Feature 0\")\\n    ax.set_ylabel(\"Feature 1\")\\nFigure 3-2. Effect of scaling training and test data shown on the left together (center) and\\nseparately (right)\\nPreprocessing and Scaling | 137'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 151, 'page_label': '138'}, page_content='The first panel is an unscaled two-dimensional dataset, with the training set shown as\\ncircles and the test set shown as triangles. The second panel is the same data, but\\nscaled using the MinMaxScaler. Here, we called fit on the training set, and then\\ncalled transform on the training and test sets. Y ou can see that the dataset in the sec‐\\nond panel looks identical to the first; only the ticks on the axes have changed. Now all\\nthe features are between 0 and 1. Y ou can also see that the minimum and maximum\\nfeature values for the test data (the triangles) are not 0 and 1.\\nThe third panel shows what would happen if we scaled the training set and test set\\nseparately. In this case, the minimum and maximum feature values for both the train‐\\ning and the test set are 0 and 1. But now the dataset looks different. The test points\\nmoved incongruously to the training set, as they were scaled differently. We changed\\nthe arrangement of the data in an arbitrary way. Clearly this is not what we want to\\ndo.\\nAs another way to think about this, imagine your test set is a single point. There is no\\nway to scale a single point correctly, to fulfill the minimum and maximum require‐\\nments of the MinMaxScaler. But the size of your test set should not change your\\nprocessing.\\nShortcuts and \\nEfficient  Alternatives\\nOften, you want to fit a model on some dataset, and then transform it. This is a very\\ncommon task, which can often be computed more efficiently than by simply calling\\nfit and then transform. For this use case, all models that have a transform method\\nalso have a fit_transform method. Here is an example using StandardScaler:\\nIn[9]:\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\n# calling fit and transform in sequence (using method chaining)\\nX_scaled = scaler.fit(X).transform(X)\\n# same result, but more efficient computation\\nX_scaled_d = scaler.fit_transform(X)\\nWhile fit_transform is not necessarily more efficient for all models, it is still good\\npractice to use this method when trying to transform the training set.\\nThe \\nEffect  of Preprocessing on Supervised Learning\\nNow let’s go back to the cancer dataset and see the effect of using the MinMaxScaler\\non learning the SVC (this is a different way of doing the same scaling we did in Chap‐\\nter 2). First, let’s fit the SVC on the original data again for comparison:\\n138 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 152, 'page_label': '139'}, page_content='In[10]:\\nfrom sklearn.svm import SVC\\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\\n                                                    random_state=0)\\nsvm = SVC(C=100)\\nsvm.fit(X_train, y_train)\\nprint(\"Test set accuracy: {:.2f}\".format(svm.score(X_test, y_test)))\\nOut[10]:\\nTest set accuracy: 0.63\\nNow, let’s scale the data using MinMaxScaler before fitting the SVC:\\nIn[11]:\\n# preprocessing using 0-1 scaling\\nscaler = MinMaxScaler()\\nscaler.fit(X_train)\\nX_train_scaled = scaler.transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n# learning an SVM on the scaled training data\\nsvm.fit(X_train_scaled, y_train)\\n# scoring on the scaled test set\\nprint(\"Scaled test set accuracy: {:.2f}\".format(\\n    svm.score(X_test_scaled, y_test)))\\nOut[11]:\\nScaled test set accuracy: 0.97\\nAs we saw before, the effect of scaling the data is quite significant. Even though scal‐\\ning the data doesn’t involve any complicated math, it is good practice to use the scal‐\\ning mechanisms provided by scikit-learn instead of reimplementing them yourself,\\nas it’s easy to make mistakes even in these simple computations.\\nY ou can also easily replace one preprocessing algorithm with another by changing the\\nclass you use, as all of the preprocessing classes have the same interface, consisting of\\nthe fit and transform methods:\\nIn[12]:\\n# preprocessing using zero mean and unit variance scaling\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\nscaler.fit(X_train)\\nX_train_scaled = scaler.transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\nPreprocessing and Scaling | 139'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 153, 'page_label': '140'}, page_content='# learning an SVM on the scaled training data\\nsvm.fit(X_train_scaled, y_train)\\n# scoring on the scaled test set\\nprint(\"SVM test accuracy: {:.2f}\".format(svm.score(X_test_scaled, y_test)))\\nOut[12]:\\nSVM test accuracy: 0.96\\nNow that we’ve seen how simple data transformations for preprocessing work, let’s\\nmove on to more interesting transformations using unsupervised learning.\\nDimensionality Reduction, Feature Extraction, and\\nManifold Learning\\nAs we discussed earlier, transforming data using unsupervised learning can have\\nmany motivations. The most common motivations are visualization, compressing the\\ndata, and finding a representation that is more informative for further processing.\\nOne of the simplest and most widely used algorithms for all of these is principal com‐\\nponent analysis. We’ll also look at two other algorithms: non-negative matrix factori‐\\nzation (NMF), which is commonly used for feature extraction, and t-SNE, which is\\ncommonly used for visualization using two-dimensional scatter plots.\\nPrincipal Component Analysis (PCA)\\nPrincipal component analysis is a method that rotates the dataset in a way such that\\nthe rotated features are statistically uncorrelated. This rotation is often followed by\\nselecting only a subset of the new features, according to how important they are for\\nexplaining the data. The following example ( Figure 3-3) illustrates the effect of PCA\\non a synthetic two-dimensional dataset:\\nIn[13]:\\nmglearn.plots.plot_pca_illustration()\\nThe first plot (top left) shows the original data points, colored to distinguish among\\nthem. The algorithm proceeds by first finding the direction of maximum variance,\\nlabeled “Component 1. ” This is the direction (or vector) in the data that contains most\\nof the information, or in other words, the direction along which the features are most\\ncorrelated with each other. Then, the algorithm finds the direction that contains the\\nmost information while being orthogonal (at a right angle) to the first direction. In\\ntwo dimensions, there is only one possible orientation that is at a right angle, but in\\nhigher-dimensional spaces there would be (infinitely) many orthogonal directions.\\nAlthough the two components are drawn as arrows, it doesn’t really matter where the\\nhead and the tail are; we could have drawn the first component from the center up to\\n140 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 154, 'page_label': '141'}, page_content='the top left instead of down to the bottom right. The directions found using this pro‐\\ncess are called principal components, as they are the main directions of variance in the\\ndata. In general, there are as many principal components as original features.\\nFigure 3-3. Transformation of data with PCA\\nThe second plot (top right) shows the same data, but now rotated so that the first\\nprincipal component aligns with the x-axis and the second principal component\\naligns with the y-axis. Before the rotation, the mean was subtracted from the data, so\\nthat the transformed data is centered around zero. In the rotated representation\\nfound by PCA, the two axes are uncorrelated, meaning that the correlation matrix of\\nthe data in this representation is zero except for the diagonal.\\nWe can use PCA for dimensionality reduction by retaining only some of the principal\\ncomponents. In this example, we might keep only the first principal component, as\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 141'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 155, 'page_label': '142'}, page_content='shown in the third panel in Figure 3-3  (bottom left). This reduces the data from a\\ntwo-dimensional dataset to a one-dimensional dataset. Note, however, that instead of\\nkeeping only one of the original features, we found the most interesting direction\\n(top left to bottom right in the first panel) and kept this direction, the first principal\\ncomponent.\\nFinally, we can undo the rotation and add the mean back to the data. This will result\\nin the data shown in the last panel in Figure 3-3. These points are in the original fea‐\\nture space, but we kept only the information contained in the first principal compo‐\\nnent. This transformation is sometimes used to remove noise effects from the data or\\nvisualize what part of the information is retained using the principal components.\\nApplying PCA to the cancer dataset for visualization\\nOne of the most common applications of PCA is visualizing high-dimensional data‐\\nsets. As we saw in Chapter 1, it is hard to create scatter plots of data that has more\\nthan two features. For the Iris dataset, we were able to create a pair plot (Figure 1-3 in\\nChapter 1) that gave us a partial picture of the data by showing us all the possible\\ncombinations of two features. But if we want to look at the Breast Cancer dataset,\\neven using a pair plot is tricky. This dataset has 30 features, which would result in\\n30 * 14 = 420 scatter plots! We’ d never be able to look at all these plots in detail, let\\nalone try to understand them.\\nThere is an even simpler visualization we can use, though—computing histograms of\\neach of the features for the two classes, benign and malignant cancer (Figure 3-4):\\nIn[14]:\\nfig, axes = plt.subplots(15, 2, figsize=(10, 20))\\nmalignant = cancer.data[cancer.target == 0]\\nbenign = cancer.data[cancer.target == 1]\\nax = axes.ravel()\\nfor i in range(30):\\n    _, bins = np.histogram(cancer.data[:, i], bins=50)\\n    ax[i].hist(malignant[:, i], bins=bins, color=mglearn.cm3(0), alpha=.5)\\n    ax[i].hist(benign[:, i], bins=bins, color=mglearn.cm3(2), alpha=.5)\\n    ax[i].set_title(cancer.feature_names[i])\\n    ax[i].set_yticks(())\\nax[0].set_xlabel(\"Feature magnitude\")\\nax[0].set_ylabel(\"Frequency\")\\nax[0].legend([\"malignant\", \"benign\"], loc=\"best\")\\nfig.tight_layout()\\n142 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 156, 'page_label': '143'}, page_content='Figure 3-4. Per-class feature histograms on the Breast Cancer dataset\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 143'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 157, 'page_label': '144'}, page_content='Here we create a histogram for each of the features, counting how often a data point\\nappears with a feature in a certain range (called a bin). Each plot overlays two histo‐\\ngrams, one for all of the points in the benign class (blue) and one for all the points in\\nthe malignant class (red). This gives us some idea of how each feature is distributed\\nacross the two classes, and allows us to venture a guess as to which features are better\\nat distinguishing malignant and benign samples. For example, the feature “smooth‐\\nness error” seems quite uninformative, because the two histograms mostly overlap,\\nwhile the feature “worst concave points” seems quite informative, because the histo‐\\ngrams are quite disjoint.\\nHowever, this plot doesn’t show us anything about the interactions between variables\\nand how these relate to the classes. Using PCA, we can capture the main interactions\\nand get a slightly more complete picture. We can find the first two principal compo‐\\nnents, and visualize the data in this new two-dimensional space with a single scatter\\nplot.\\nBefore we apply PCA, we scale our data so that each feature has unit variance using\\nStandardScaler:\\nIn[15]:\\nfrom sklearn.datasets import load_breast_cancer\\ncancer = load_breast_cancer()\\nscaler = StandardScaler()\\nscaler.fit(cancer.data)\\nX_scaled = scaler.transform(cancer.data)\\nLearning the PCA transformation and applying it is as simple as applying a prepro‐\\ncessing transformation. We instantiate the PCA object, find the principal components\\nby calling the fit method, and then apply the rotation and dimensionality reduction\\nby calling transform. By default, PCA only rotates (and shifts) the data, but keeps all\\nprincipal components. To reduce the dimensionality of the data, we need to specify\\nhow many components we want to keep when creating the PCA object:\\nIn[16]:\\nfrom sklearn.decomposition import PCA\\n# keep the first two principal components of the data\\npca = PCA(n_components=2)\\n# fit PCA model to breast cancer data\\npca.fit(X_scaled)\\n# transform data onto the first two principal components\\nX_pca = pca.transform(X_scaled)\\nprint(\"Original shape: {}\".format(str(X_scaled.shape)))\\nprint(\"Reduced shape: {}\".format(str(X_pca.shape)))\\n144 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 158, 'page_label': '145'}, page_content='Out[16]:\\nOriginal shape: (569, 30)\\nReduced shape: (569, 2)\\nWe can now plot the first two principal components (Figure 3-5):\\nIn[17]:\\n# plot first vs. second principal component, colored by class\\nplt.figure(figsize=(8, 8))\\nmglearn.discrete_scatter(X_pca[:, 0], X_pca[:, 1], cancer.target)\\nplt.legend(cancer.target_names, loc=\"best\")\\nplt.gca().set_aspect(\"equal\")\\nplt.xlabel(\"First principal component\")\\nplt.ylabel(\"Second principal component\")\\nFigure 3-5. Two-dimensional scatter plot of the Breast Cancer dataset using the first two\\nprincipal components\\nIt is important to note that PCA is an unsupervised method, and does not use any class\\ninformation when finding the rotation. It simply looks at the correlations in the data.\\nFor the scatter plot shown here, we plotted the first principal component against the\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 145'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 159, 'page_label': '146'}, page_content='second principal component, and then used the class information to color the points.\\nY ou can see that the two classes separate quite well in this two-dimensional space.\\nThis leads us to believe that even a linear classifier (that would learn a line in this\\nspace) could do a reasonably good job at distinguishing the two classes. We can also\\nsee that the malignant (red) points are more spread out than the benign (blue) points\\n—something that we could already see a bit from the histograms in Figure 3-4.\\nA downside of PCA is that the two axes in the plot are often not very easy to interpret.\\nThe principal components correspond to directions in the original data, so they are\\ncombinations of the original features. However, these combinations are usually very\\ncomplex, as we’ll see shortly. The principal components themselves are stored in the\\ncomponents_ attribute of the PCA object during fitting:\\nIn[18]:\\nprint(\"PCA component shape: {}\".format(pca.components_.shape))\\nOut[18]:\\nPCA component shape: (2, 30)\\nEach row in components_ corresponds to one principal component, and they are sor‐\\nted by their importance (the first principal component comes first, etc.). The columns\\ncorrespond to the original features attribute of the PCA in this example, “mean\\nradius, ” “mean texture, ” and so on. Let’s have a look at the content of components_:\\nIn[19]:\\nprint(\"PCA components:\\\\n{}\".format(pca.components_))\\nOut[19]:\\nPCA components:\\n[[ 0.219  0.104  0.228  0.221  0.143  0.239  0.258  0.261  0.138  0.064\\n   0.206  0.017  0.211  0.203  0.015  0.17   0.154  0.183  0.042  0.103\\n   0.228  0.104  0.237  0.225  0.128  0.21   0.229  0.251  0.123  0.132]\\n [-0.234 -0.06  -0.215 -0.231  0.186  0.152  0.06  -0.035  0.19   0.367\\n  -0.106  0.09  -0.089 -0.152  0.204  0.233  0.197  0.13   0.184  0.28\\n  -0.22  -0.045 -0.2   -0.219  0.172  0.144  0.098 -0.008  0.142  0.275]]\\nWe can also visualize the coefficients using a heat map ( Figure 3-6), which might be\\neasier to understand:\\nIn[20]:\\nplt.matshow(pca.components_, cmap=\\'viridis\\')\\nplt.yticks([0, 1], [\"First component\", \"Second component\"])\\nplt.colorbar()\\nplt.xticks(range(len(cancer.feature_names)),\\n           cancer.feature_names, rotation=60, ha=\\'left\\')\\nplt.xlabel(\"Feature\")\\nplt.ylabel(\"Principal components\")\\n146 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 160, 'page_label': '147'}, page_content=\"Figure 3-6. Heat map of the first two principal components on the Breast Cancer dataset\\nY ou can see that in the first component, all features have the same sign (it’s negative,\\nbut as we mentioned earlier, it doesn’t matter which direction the arrow points in).\\nThat means that there is a general correlation between all features. As one measure‐\\nment is high, the others are likely to be high as well. The second component has\\nmixed signs, and both of the components involve all of the 30 features. This mixing of\\nall features is what makes explaining the axes in Figure 3-6 so tricky.\\nEigenfaces for feature extraction\\nAnother application of PCA that we mentioned earlier is feature extraction. The idea\\nbehind feature extraction is that it is possible to find a representation of your data\\nthat is better suited to analysis than the raw representation you were given. A great\\nexample of an application where feature extraction is helpful is with images. Images\\nare made up of pixels, usually stored as red, green, and blue (RGB) intensities.\\nObjects in images are usually made up of thousands of pixels, and only together are\\nthey meaningful.\\nWe will give a very simple application of feature extraction on images using PCA, by\\nworking with face images from the Labeled Faces in the Wild dataset. This dataset\\ncontains face images of celebrities downloaded from the Internet, and it includes\\nfaces of politicians, singers, actors, and athletes from the early 2000s. We use gray‐\\nscale versions of these images, and scale them down for faster processing. Y ou can see\\nsome of the images in Figure 3-7:\\nIn[21]:\\nfrom sklearn.datasets import fetch_lfw_people\\npeople = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\\nimage_shape = people.images[0].shape\\nfix, axes = plt.subplots(2, 5, figsize=(15, 8),\\n                         subplot_kw={'xticks': (), 'yticks': ()})\\nfor target, image, ax in zip(people.target, people.images, axes.ravel()):\\n    ax.imshow(image)\\n    ax.set_title(people.target_names[target])\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 147\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 161, 'page_label': '148'}, page_content='Figure 3-7. Some images from the Labeled Faces in the Wild dataset\\nThere are 3,023 images, each 87×65 pixels large, belonging to 62 different people:\\nIn[22]:\\nprint(\"people.images.shape: {}\".format(people.images.shape))\\nprint(\"Number of classes: {}\".format(len(people.target_names)))\\nOut[22]:\\npeople.images.shape: (3023, 87, 65)\\nNumber of classes: 62\\nThe dataset is a bit skewed, however, containing a lot of images of George W . Bush\\nand Colin Powell, as you can see here:\\nIn[23]:\\n# count how often each target appears\\ncounts = np.bincount(people.target)\\n# print counts next to target names\\nfor i, (count, name) in enumerate(zip(counts, people.target_names)):\\n    print(\"{0:25} {1:3}\".format(name, count), end=\\'   \\')\\n    if (i + 1) % 3 == 0:\\n        print()\\n148 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 162, 'page_label': '149'}, page_content='Out[23]:\\nAlejandro Toledo           39   Alvaro Uribe               35\\nAmelie Mauresmo            21   Andre Agassi               36\\nAngelina Jolie             20   Arnold Schwarzenegger      42\\nAtal Bihari Vajpayee       24   Bill Clinton               29\\nCarlos Menem               21   Colin Powell              236\\nDavid Beckham              31   Donald Rumsfeld           121\\nGeorge W Bush             530   George Robertson           22\\nGerhard Schroeder         109   Gloria Macapagal Arroyo    44\\nGray Davis                 26   Guillermo Coria            30\\nHamid Karzai               22   Hans Blix                  39\\nHugo Chavez                71   Igor Ivanov                20\\n[...]                           [...]\\nLaura Bush                 41   Lindsay Davenport          22\\nLleyton Hewitt             41   Luiz Inacio Lula da Silva  48\\nMahmoud Abbas              29   Megawati Sukarnoputri      33\\nMichael Bloomberg          20   Naomi Watts                22\\nNestor Kirchner            37   Paul Bremer                20\\nPete Sampras               22   Recep Tayyip Erdogan       30\\nRicardo Lagos              27   Roh Moo-hyun               32\\nRudolph Giuliani           26   Saddam Hussein             23\\nSerena Williams            52   Silvio Berlusconi          33\\nTiger Woods                23   Tom Daschle                25\\nTom Ridge                  33   Tony Blair                144\\nVicente Fox                32   Vladimir Putin             49\\nWinona Ryder               24\\nTo make the data less skewed, we will only take up to 50 images of each person\\n(otherwise, the feature extraction would be overwhelmed by the likelihood of George\\nW . Bush):\\nIn[24]:\\nmask = np.zeros(people.target.shape, dtype=np.bool)\\nfor target in np.unique(people.target):\\n    mask[np.where(people.target == target)[0][:50]] = 1\\nX_people = people.data[mask]\\ny_people = people.target[mask]\\n# scale the grayscale values to be between 0 and 1\\n# instead of 0 and 255 for better numeric stability\\nX_people = X_people / 255.\\nA common task in face recognition is to ask if a previously unseen face belongs to a\\nknown person from a database. This has applications in photo collection, social\\nmedia, and security applications. One way to solve this problem would be to build a\\nclassifier where each person is a separate class. However, there are usually many dif‐\\nferent people in face databases, and very few images of the same person (i.e., very few\\ntraining examples per class). That makes it hard to train most classifiers. Additionally,\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 149'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 163, 'page_label': '150'}, page_content='you often want to be able to add new people easily, without needing to retrain a large\\nmodel.\\nA simple solution is to use a one-nearest-neighbor classifier that looks for the most\\nsimilar face image to the face you are classifying. This classifier could in principle\\nwork with only a single training example per class. Let’s take a look at how well\\nKNeighborsClassifier does here:\\nIn[25]:\\nfrom sklearn.neighbors import KNeighborsClassifier\\n# split the data into training and test sets\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X_people, y_people, stratify=y_people, random_state=0)\\n# build a KNeighborsClassifier using one neighbor\\nknn = KNeighborsClassifier(n_neighbors=1)\\nknn.fit(X_train, y_train)\\nprint(\"Test set score of 1-nn: {:.2f}\".format(knn.score(X_test, y_test)))\\nOut[25]:\\nTest set score of 1-nn: 0.27\\nWe obtain an accuracy of 26.6%, which is not actually that bad for a 62-class classifi‐\\ncation problem (random guessing would give you around 1/62 = 1.5% accuracy), but\\nis also not great. We only correctly identify a person every fourth time.\\nThis is where PCA comes in. Computing distances in the original pixel space is quite\\na bad way to measure similarity between faces. When using a pixel representation to\\ncompare two images, we compare the grayscale value of each individual pixel to the\\nvalue of the pixel in the corresponding position in the other image. This representa‐\\ntion is quite different from how humans would interpret the image of a face, and it is\\nhard to capture the facial features using this raw representation. For example, using\\npixel distances means that shifting a face by one pixel to the right corresponds to a\\ndrastic change, with a completely different representation. We hope that using distan‐\\nces along principal components can improve our accuracy. Here, we enable the\\nwhitening option of PCA, which rescales the principal components to have the same\\nscale. This is the same as using StandardScaler after the transformation. Reusing the\\ndata from Figure 3-3 again, whitening corresponds to not only rotating the data, but\\nalso rescaling it so that the center panel is a circle instead of an ellipse (see\\nFigure 3-8):\\nIn[26]:\\nmglearn.plots.plot_pca_whitening()150 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 164, 'page_label': '151'}, page_content='Figure 3-8. Transformation of data with PCA using whitening\\nWe fit the PCA object to the training data and extract the first 100 principal compo‐\\nnents. Then we transform the training and test data:\\nIn[27]:\\npca = PCA(n_components=100, whiten=True, random_state=0).fit(X_train)\\nX_train_pca = pca.transform(X_train)\\nX_test_pca = pca.transform(X_test)\\nprint(\"X_train_pca.shape: {}\".format(X_train_pca.shape))\\nOut[27]:\\nX_train_pca.shape: (1537, 100)\\nThe new data has 100 features, the first 100 principal components. Now, we can use\\nthe new representation to classify our images using a one-nearest-neighbors classifier:\\nIn[28]:\\nknn = KNeighborsClassifier(n_neighbors=1)\\nknn.fit(X_train_pca, y_train)\\nprint(\"Test set accuracy: {:.2f}\".format(knn.score(X_test_pca, y_test)))\\nOut[28]:\\nTest set accuracy: 0.36\\nOur accuracy improved quite significantly, from 26.6% to 35.7%, confirming our\\nintuition that the principal components might provide a better representation of the\\ndata.\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 151'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 165, 'page_label': '152'}, page_content='For image data, we can also easily visualize the principal components that are found.\\nRemember that components correspond to directions in the input space. The input\\nspace here is 50×37-pixel grayscale images, so directions within this space are also\\n50×37-pixel grayscale images.\\nLet’s look at the first couple of principal components (Figure 3-9):\\nIn[29]:\\nprint(\"pca.components_.shape: {}\".format(pca.components_.shape))\\nOut[29]:\\npca.components_.shape: (100, 5655)\\nIn[30]:\\nfix, axes = plt.subplots(3, 5, figsize=(15, 12),\\n                         subplot_kw={\\'xticks\\': (), \\'yticks\\': ()})\\nfor i, (component, ax) in enumerate(zip(pca.components_, axes.ravel())):\\n    ax.imshow(component.reshape(image_shape),\\n              cmap=\\'viridis\\')\\n    ax.set_title(\"{}. component\".format((i + 1)))\\nWhile we certainly cannot understand all aspects of these components, we can guess\\nwhich aspects of the face images some of the components are capturing. The first\\ncomponent seems to mostly encode the contrast between the face and the back‐\\nground, the second component encodes differences in lighting between the right and\\nthe left half of the face, and so on. While this representation is slightly more semantic\\nthan the raw pixel values, it is still quite far from how a human might perceive a face.\\nAs the PCA model is based on pixels, the alignment of the face (the position of eyes,\\nchin, and nose) and the lighting both have a strong influence on how similar two\\nimages are in their pixel representation. But alignment and lighting are probably not\\nwhat a human would perceive first. When asking people to rate similarity of faces,\\nthey are more likely to use attributes like age, gender, facial expression, and hair style,\\nwhich are attributes that are hard to infer from the pixel intensities. It’s important to\\nkeep in mind that algorithms often interpret data (particularly visual data, such as\\nimages, which humans are very familiar with) quite differently from how a human\\nwould.\\n152 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 166, 'page_label': '153'}, page_content='Figure 3-9. Component vectors of the first 15 principal components of the faces dataset\\nLet’s come back to the specific case of PCA, though. We introduced the PCA transfor‐\\nmation as rotating the data and then dropping the components with low variance.\\nAnother useful interpretation is to try to find some numbers (the new feature values\\nafter the PCA rotation) so that we can express the test points as a weighted sum of the\\nprincipal components (see Figure 3-10).\\nFigure 3-10. Schematic view of PCA as decomposing an image into a weighted sum of\\ncomponents\\nHere, x0, x1, and so on are the coefficients of the principal components for this data\\npoint; in other words, they are the representation of the image in the rotated space.\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 153'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 167, 'page_label': '154'}, page_content='Another way we can try to understand what a PCA model is doing is by looking at\\nthe reconstructions of the original data using only some components. In Figure 3-3,\\nafter dropping the second component and arriving at the third panel, we undid the\\nrotation and added the mean back to obtain new points in the original space with the\\nsecond component removed, as shown in the last panel. We can do a similar transfor‐\\nmation for the faces by reducing the data to only some principal components and\\nthen rotating back into the original space. This return to the original feature space\\ncan be done using the inverse_transform method. Here, we visualize the recon‐\\nstruction of some faces using 10, 50, 100, 500, or 2,000 components (Figure 3-11):\\nIn[32]:\\nmglearn.plots.plot_pca_faces(X_train, X_test, image_shape)\\nFigure 3-11. Reconstructing three face images using increasing numbers of principal\\ncomponents\\nY ou can see that when we use only the first 10 principal components, only the essence\\nof the picture, like the face orientation and lighting, is captured. By using more and\\nmore principal components, more and more details in the image are preserved. This\\n154 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 168, 'page_label': '155'}, page_content='corresponds to extending the sum in Figure 3-10 to include more and more terms.\\nUsing as many components as there are pixels would mean that we would not discard\\nany information after the rotation, and we would reconstruct the image perfectly.\\nWe can also try to use PCA to visualize all the faces in the dataset in a scatter plot\\nusing the first two principal components ( Figure 3-12), with classes given by who is\\nshown in the image, similarly to what we did for the cancer dataset:\\nIn[33]:\\nmglearn.discrete_scatter(X_train_pca[:, 0], X_train_pca[:, 1], y_train)\\nplt.xlabel(\"First principal component\")\\nplt.ylabel(\"Second principal component\")\\nFigure 3-12. Scatter plot of the faces dataset using the first two principal components (see\\nFigure 3-5 for the corresponding image for the cancer dataset)\\nAs you can see, when we use only the first two principal components the whole data\\nis just a big blob, with no separation of classes visible. This is not very surprising,\\ngiven that even with 10 components, as shown earlier in Figure 3-11, PCA only cap‐\\ntures very rough characteristics of the faces.\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 155'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 169, 'page_label': '156'}, page_content='Non-Negative Matrix Factorization (NMF)\\nNon-negative matrix factorization is another unsupervised learning algorithm that\\naims to extract useful features. It works similarly to PCA and can also be used for\\ndimensionality reduction. As in PCA, we are trying to write each data point as a\\nweighted sum of some components, as illustrated in Figure 3-10. But whereas in PCA\\nwe wanted components that were orthogonal and that explained as much variance of\\nthe data as possible, in NMF , we want the components and the coefficients to be non-\\nnegative; that is, we want both the components and the coefficients to be greater than\\nor equal to zero. Consequently, this method can only be applied to data where each\\nfeature is non-negative, as a non-negative sum of non-negative components cannot\\nbecome negative.\\nThe process of decomposing data into a non-negative weighted sum is particularly\\nhelpful for data that is created as the addition (or overlay) of several independent\\nsources, such as an audio track of multiple people speaking, or music with many\\ninstruments. In these situations, NMF can identify the original components that\\nmake up the combined data. Overall, NMF leads to more interpretable components\\nthan PCA, as negative components and coefficients can lead to hard-to-interpret can‐\\ncellation effects. The eigenfaces in Figure 3-9, for example, contain both positive and\\nnegative parts, and as we mentioned in the description of PCA, the sign is actually\\narbitrary. Before we apply NMF to the face dataset, let’s briefly revisit the synthetic\\ndata.\\nApplying NMF to synthetic data\\nIn contrast to when using PCA, we need to ensure that our data is positive for NMF\\nto be able to operate on the data. This means where the data lies relative to the origin\\n(0, 0) actually matters for NMF . Therefore, you can think of the non-negative compo‐\\nnents that are extracted as directions from (0, 0) toward the data.\\nThe following example ( Figure 3-13 ) shows the results of NMF on the two-\\ndimensional toy data:\\nIn[34]:\\nmglearn.plots.plot_nmf_illustration()\\n156 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 170, 'page_label': '157'}, page_content='Figure 3-13. Components found by non-negative matrix factorization with two compo‐\\nnents (left) and one component (right)\\nFor NMF with two components, as shown on the left, it is clear that all points in the\\ndata can be written as a positive combination of the two components. If there are\\nenough components to perfectly reconstruct the data (as many components as there\\nare features), the algorithm will choose directions that point toward the extremes of\\nthe data.\\nIf we only use a single component, NMF creates a component that points toward the\\nmean, as pointing there best explains the data. Y ou can see that in contrast with PCA,\\nreducing the number of components not only removes some directions, but creates\\nan entirely different set of components! Components in NMF are also not ordered in\\nany specific way, so there is no “first non-negative component”: all components play\\nan equal part.\\nNMF uses a random initialization, which might lead to different results depending on\\nthe random seed. In relatively simple cases such as the synthetic data with two com‐\\nponents, where all the data can be explained perfectly, the randomness has little effect\\n(though it might change the order or scale of the components). In more complex sit‐\\nuations, there might be more drastic changes.\\nApplying NMF to face images\\nNow, let’s apply NMF to the Labeled Faces in the Wild dataset we used earlier. The\\nmain parameter of NMF is how many components we want to extract. Usually this is\\nlower than the number of input features (otherwise, the data could be explained by\\nmaking each pixel a separate component).\\nFirst, let’s inspect how the number of components impacts how well the data can be\\nreconstructed using NMF (Figure 3-14):\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 157'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 171, 'page_label': '158'}, page_content='In[35]:\\nmglearn.plots.plot_nmf_faces(X_train, X_test, image_shape)\\nFigure 3-14. Reconstructing three face images using increasing numbers of components\\nfound by NMF\\nThe quality of the back-transformed data is similar to when using PCA, but slightly\\nworse. This is expected, as PCA finds the optimum directions in terms of reconstruc‐\\ntion. NMF is usually not used for its ability to reconstruct or encode data, but rather\\nfor finding interesting patterns within the data.\\nAs a first look into the data, let’s try extracting only a few components (say, 15).\\nFigure 3-15 shows the result:\\n158 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 172, 'page_label': '159'}, page_content='In[36]:\\nfrom sklearn.decomposition import NMF\\nnmf = NMF(n_components=15, random_state=0)\\nnmf.fit(X_train)\\nX_train_nmf = nmf.transform(X_train)\\nX_test_nmf = nmf.transform(X_test)\\nfix, axes = plt.subplots(3, 5, figsize=(15, 12),\\n                         subplot_kw={\\'xticks\\': (), \\'yticks\\': ()})\\nfor i, (component, ax) in enumerate(zip(nmf.components_, axes.ravel())):\\n    ax.imshow(component.reshape(image_shape))\\n    ax.set_title(\"{}. component\".format(i))\\nFigure 3-15. The components found by NMF on the faces dataset when using 15 compo‐\\nnents\\nThese components are all positive, and so resemble prototypes of faces much more so\\nthan the components shown for PCA in Figure 3-9. For example, one can clearly see\\nthat component 3 shows a face rotated somewhat to the right, while component 7\\nshows a face somewhat rotated to the left. Let’s look at the images for which these\\ncomponents are particularly strong, shown in Figures 3-16 and 3-17:\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 159'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 173, 'page_label': '160'}, page_content=\"In[37]:\\ncompn = 3\\n# sort by 3rd component, plot first 10 images\\ninds = np.argsort(X_train_nmf[:, compn])[::-1]\\nfig, axes = plt.subplots(2, 5, figsize=(15, 8),\\n                         subplot_kw={'xticks': (), 'yticks': ()})\\nfor i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\\n    ax.imshow(X_train[ind].reshape(image_shape))\\ncompn = 7\\n# sort by 7th component, plot first 10 images\\ninds = np.argsort(X_train_nmf[:, compn])[::-1]\\nfig, axes = plt.subplots(2, 5, figsize=(15, 8),\\n                         subplot_kw={'xticks': (), 'yticks': ()})\\nfor i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\\n    ax.imshow(X_train[ind].reshape(image_shape))\\nFigure 3-16. Faces that have a large coefficient for component 3\\n160 | Chapter 3: Unsupervised Learning and Preprocessing\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 174, 'page_label': '161'}, page_content='Figure 3-17. Faces that have a large coefficient for component 7\\nAs expected, faces that have a high coefficient for component 3 are faces looking to\\nthe right (Figure 3-16), while faces with a high coefficient for component 7 are look‐\\ning to the left (Figure 3-17). As mentioned earlier, extracting patterns like these works\\nbest for data with additive structure, including audio, gene expression, and text data.\\nLet’s walk through one example on synthetic data to see what this might look like.\\nLet’s say we are interested in a signal that is a combination of three different sources\\n(Figure 3-18):\\nIn[38]:\\nS = mglearn.datasets.make_signals()\\nplt.figure(figsize=(6, 1))\\nplt.plot(S, \\'-\\')\\nplt.xlabel(\"Time\")\\nplt.ylabel(\"Signal\")\\nFigure 3-18. Original signal sources\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 161'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 175, 'page_label': '162'}, page_content='Unfortunately we cannot observe the original signals, but only an additive mixture of\\nall three of them. We want to recover the decomposition of the mixed signal into the\\noriginal components. We assume that we have many different ways to observe the\\nmixture (say 100 measurement devices), each of which provides us with a series of\\nmeasurements:\\nIn[39]:\\n# mix data into a 100-dimensional state\\nA = np.random.RandomState(0).uniform(size=(100, 3))\\nX = np.dot(S, A.T)\\nprint(\"Shape of measurements: {}\".format(X.shape))\\nOut[39]:\\nShape of measurements: (2000, 100)\\nWe can use NMF to recover the three signals:\\nIn[40]:\\nnmf = NMF(n_components=3, random_state=42)\\nS_ = nmf.fit_transform(X)\\nprint(\"Recovered signal shape: {}\".format(S_.shape))\\nOut[40]:\\nRecovered signal shape: (2000, 3)\\nFor comparison, we also apply PCA:\\nIn[41]:\\npca = PCA(n_components=3)\\nH = pca.fit_transform(X)\\nFigure 3-19 shows the signal activity that was discovered by NMF and PCA:\\nIn[42]:\\nmodels = [X, S, S_, H]\\nnames = [\\'Observations (first three measurements)\\',\\n         \\'True sources\\',\\n         \\'NMF recovered signals\\',\\n         \\'PCA recovered signals\\']\\nfig, axes = plt.subplots(4, figsize=(8, 4), gridspec_kw={\\'hspace\\': .5},\\n                         subplot_kw={\\'xticks\\': (), \\'yticks\\': ()})\\nfor model, name, ax in zip(models, names, axes):\\n    ax.set_title(name)\\n    ax.plot(model[:, :3], \\'-\\')\\n162 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 176, 'page_label': '163'}, page_content='Figure 3-19. Recovering mixed sources using NMF and PCA\\nThe figure includes 3 of the 100 measurements from X for reference. As you can see,\\nNMF did a reasonable job of discovering the original sources, while PCA failed and\\nused the first component to explain the majority of the variation in the data. Keep in\\nmind that the components produced by NMF have no natural ordering. In this exam‐\\nple, the ordering of the NMF components is the same as in the original signal (see the\\nshading of the three curves), but this is purely accidental.\\nThere are many other algorithms that can be used to decompose each data point into\\na weighted sum of a fixed set of components, as PCA and NMF do. Discussing all of\\nthem is beyond the scope of this book, and describing the constraints made on the\\ncomponents and coefficients often involves probability theory. If you are interested in\\nthis kind of pattern extraction, we recommend that you study the sections of the sci\\nkit_learn user guide on independent component analysis (ICA), factor analysis\\n(FA), and sparse coding (dictionary learning), all of which you can find on the page\\nabout decomposition methods.\\nManifold Learning with t-SNE\\nWhile PCA is often a good first approach for transforming your data so that you\\nmight be able to visualize it using a scatter plot, the nature of the method (applying a\\nrotation and then dropping directions) limits its usefulness, as we saw with the scatter\\nplot of the Labeled Faces in the Wild dataset. There is a class of algorithms for visuali‐\\nzation called manifold learning algorithms that allow for much more complex map‐\\npings, and often provide better visualizations. A particularly useful one is the t-SNE\\nalgorithm.\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 163'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 177, 'page_label': '164'}, page_content=\"2 Not to be confused with the much larger MNIST dataset.\\nManifold learning algorithms are mainly aimed at visualization, and so are rarely\\nused to generate more than two new features. Some of them, including t-SNE, com‐\\npute a new representation of the training data, but don’t allow transformations of new\\ndata. This means these algorithms cannot be applied to a test set: rather, they can only\\ntransform the data they were trained for. Manifold learning can be useful for explora‐\\ntory data analysis, but is rarely used if the final goal is supervised learning. The idea\\nbehind t-SNE is to find a two-dimensional representation of the data that preserves\\nthe distances between points as best as possible. t-SNE starts with a random two-\\ndimensional representation for each data point, and then tries to make points that are\\nclose in the original feature space closer, and points that are far apart in the original\\nfeature space farther apart. t-SNE puts more emphasis on points that are close by,\\nrather than preserving distances between far-apart points. In other words, it tries to\\npreserve the information indicating which points are neighbors to each other.\\nWe will apply the t-SNE manifold learning algorithm on a dataset of handwritten dig‐\\nits that is included in scikit-learn.2 Each data point in this dataset is an 8×8 gray‐\\nscale image of a handwritten digit between 0 and 1. Figure 3-20 shows an example\\nimage for each class:\\nIn[43]:\\nfrom sklearn.datasets import load_digits\\ndigits = load_digits()\\nfig, axes = plt.subplots(2, 5, figsize=(10, 5),\\n                         subplot_kw={'xticks':(), 'yticks': ()})\\nfor ax, img in zip(axes.ravel(), digits.images):\\n    ax.imshow(img)\\n164 | Chapter 3: Unsupervised Learning and Preprocessing\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 178, 'page_label': '165'}, page_content='Figure 3-20. Example images from the digits dataset\\nLet’s use PCA to visualize the data reduced to two dimensions. We plot the first two\\nprincipal components, and color each dot by its class (see Figure 3-21):\\nIn[44]:\\n# build a PCA model\\npca = PCA(n_components=2)\\npca.fit(digits.data)\\n# transform the digits data onto the first two principal components\\ndigits_pca = pca.transform(digits.data)\\ncolors = [\"#476A2A\", \"#7851B8\", \"#BD3430\", \"#4A2D4E\", \"#875525\",\\n          \"#A83683\", \"#4E655E\", \"#853541\", \"#3A3120\", \"#535D8E\"]\\nplt.figure(figsize=(10, 10))\\nplt.xlim(digits_pca[:, 0].min(), digits_pca[:, 0].max())\\nplt.ylim(digits_pca[:, 1].min(), digits_pca[:, 1].max())\\nfor i in range(len(digits.data)):\\n    # actually plot the digits as text instead of using scatter\\n    plt.text(digits_pca[i, 0], digits_pca[i, 1], str(digits.target[i]),\\n             color = colors[digits.target[i]],\\n             fontdict={\\'weight\\': \\'bold\\', \\'size\\': 9})\\nplt.xlabel(\"First principal component\")\\nplt.ylabel(\"Second principal component\")\\nHere, we actually used the true digit classes as glyphs, to show which class is where.\\nThe digits zero, six, and four are relatively well separated using the first two principal\\ncomponents, though they still overlap. Most of the other digits overlap significantly.\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 165'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 179, 'page_label': '166'}, page_content='Figure 3-21. Scatter plot of the digits dataset using the first two principal components\\nLet’s apply t-SNE to the same dataset, and compare the results. As t-SNE does not\\nsupport transforming new data, the TSNE class has no transform method. Instead, we\\ncan call the fit_transform method, which will build the model and immediately\\nreturn the transformed data (see Figure 3-22):\\nIn[45]:\\nfrom sklearn.manifold import TSNE\\ntsne = TSNE(random_state=42)\\n# use fit_transform instead of fit, as TSNE has no transform method\\ndigits_tsne = tsne.fit_transform(digits.data)\\n166 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 180, 'page_label': '167'}, page_content='In[46]:\\nplt.figure(figsize=(10, 10))\\nplt.xlim(digits_tsne[:, 0].min(), digits_tsne[:, 0].max() + 1)\\nplt.ylim(digits_tsne[:, 1].min(), digits_tsne[:, 1].max() + 1)\\nfor i in range(len(digits.data)):\\n    # actually plot the digits as text instead of using scatter\\n    plt.text(digits_tsne[i, 0], digits_tsne[i, 1], str(digits.target[i]),\\n             color = colors[digits.target[i]],\\n             fontdict={\\'weight\\': \\'bold\\', \\'size\\': 9})\\nplt.xlabel(\"t-SNE feature 0\")\\nplt.xlabel(\"t-SNE feature 1\")\\nFigure 3-22. Scatter plot of the digits dataset using two components found by t-SNE\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 167'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 181, 'page_label': '168'}, page_content='The result of t-SNE is quite remarkable. All the classes are quite clearly separated.\\nThe ones and nines are somewhat split up, but most of the classes form a single dense\\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\\npletely unsupervised. Still, it can find a representation of the data in two dimensions\\nthat clearly separates the classes, based solely on how close points are in the original\\nspace.\\nThe t-SNE algorithm has some tuning parameters, though it often works well with\\nthe default settings. Y ou can try playing with perplexity and early_exaggeration,\\nbut the effects are usually minor.\\nClustering\\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\\ncalled clusters. The goal is to split up the data in such a way that points within a single\\ncluster are very similar and points in different clusters are different. Similarly to clas‐\\nsification algorithms, clustering algorithms assign (or predict) a number to each data\\npoint, indicating which cluster a particular point belongs to.\\nk-Means Clustering\\nk-means clustering is one of the simplest and most commonly used clustering algo‐\\nrithms. It tries to find cluster centers that are representative of certain regions of the\\ndata. The algorithm alternates between two steps: assigning each data point to the\\nclosest cluster center, and then setting each cluster center as the mean of the data\\npoints that are assigned to it. The algorithm is finished when the assignment of\\ninstances to clusters no longer changes. The following example ( Figure 3-23 ) illus‐\\ntrates the algorithm on a synthetic dataset:\\nIn[47]:\\nmglearn.plots.plot_kmeans_algorithm()\\n168 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 182, 'page_label': '169'}, page_content='Figure 3-23. Input data and three steps of the k-means algorithm\\nCluster centers are shown as triangles, while data points are shown as circles. Colors\\nindicate cluster membership. We specified that we are looking for three clusters, so\\nthe algorithm was initialized by declaring three data points randomly as cluster cen‐\\nters (see “Initialization”). Then the iterative algorithm starts. First, each data point is\\nassigned to the cluster center it is closest to (see “ Assign Points (1)”). Next, the cluster\\ncenters are updated to be the mean of the assigned points (see “Recompute Centers\\n(1)”). Then the process is repeated two more times. After the third iteration, the\\nassignment of points to cluster centers remained unchanged, so the algorithm stops.\\nGiven new data points, k-means will assign each to the closest cluster center. The next\\nexample (Figure 3-24) shows the boundaries of the cluster centers that were learned\\nin Figure 3-23:\\nIn[48]:\\nmglearn.plots.plot_kmeans_boundaries()\\nClustering | 169'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 183, 'page_label': '170'}, page_content='3 If you don’t provide n_clusters, it is set to 8 by default. There is no particular reason why you should use this\\nvalue.\\nFigure 3-24. Cluster centers and cluster boundaries found by the k-means algorithm\\nApplying k-means with scikit-learn is quite straightforward. Here, we apply it to\\nthe synthetic data that we used for the preceding plots. We instantiate the KMeans\\nclass, and set the number of clusters we are looking for. 3 Then we call the fit method\\nwith the data:\\nIn[49]:\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.cluster import KMeans\\n# generate synthetic two-dimensional data\\nX, y = make_blobs(random_state=1)\\n# build the clustering model\\nkmeans = KMeans(n_clusters=3)\\nkmeans.fit(X)\\nDuring the algorithm, each training data point in X is assigned a cluster label. Y ou can\\nfind these labels in the kmeans.labels_ attribute:\\n170 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 184, 'page_label': '171'}, page_content='In[50]:\\nprint(\"Cluster memberships:\\\\n{}\".format(kmeans.labels_))\\nOut[50]:\\nCluster memberships:\\n[1 2 2 2 0 0 0 2 1 1 2 2 0 1 0 0 0 1 2 2 0 2 0 1 2 0 0 1 1 0 1 1 0 1 2 0 2\\n 2 2 0 0 2 1 2 2 0 1 1 1 1 2 0 0 0 1 0 2 2 1 1 2 0 0 2 2 0 1 0 1 2 2 2 0 1\\n 1 2 0 0 1 2 1 2 2 0 1 1 1 1 2 1 0 1 1 2 2 0 0 1 0 1]\\nAs we asked for three clusters, the clusters are numbered 0 to 2.\\nY ou can also assign cluster labels to new points, using the predict method. Each new\\npoint is assigned to the closest cluster center when predicting, but the existing model\\nis not changed. Running predict on the training set returns the same result as\\nlabels_:\\nIn[51]:\\nprint(kmeans.predict(X))\\nOut[51]:\\n[1 2 2 2 0 0 0 2 1 1 2 2 0 1 0 0 0 1 2 2 0 2 0 1 2 0 0 1 1 0 1 1 0 1 2 0 2\\n 2 2 0 0 2 1 2 2 0 1 1 1 1 2 0 0 0 1 0 2 2 1 1 2 0 0 2 2 0 1 0 1 2 2 2 0 1\\n 1 2 0 0 1 2 1 2 2 0 1 1 1 1 2 1 0 1 1 2 2 0 0 1 0 1]\\nY ou can see that clustering is somewhat similar to classification, in that each item gets\\na label. However, there is no ground truth, and consequently the labels themselves\\nhave no a priori meaning. Let’s go back to the example of clustering face images that\\nwe discussed before. It might be that the cluster 3 found by the algorithm contains\\nonly faces of your friend Bela. Y ou can only know that after you look at the pictures,\\nthough, and the number 3 is arbitrary. The only information the algorithm gives you\\nis that all faces labeled as 3 are similar.\\nFor the clustering we just computed on the two-dimensional toy dataset, that means\\nthat we should not assign any significance to the fact that one group was labeled 0\\nand another one was labeled 1. Running the algorithm again might result in a differ‐\\nent numbering of clusters because of the random nature of the initialization.\\nHere is a plot of this data again ( Figure 3-25). The cluster centers are stored in the\\ncluster_centers_ attribute, and we plot them as triangles:\\nIn[52]:\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], kmeans.labels_, markers=\\'o\\')\\nmglearn.discrete_scatter(\\n    kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2],\\n    markers=\\'^\\', markeredgewidth=2)\\nClustering | 171'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 185, 'page_label': '172'}, page_content='Figure 3-25. Cluster assignments and cluster centers found by k-means with three\\nclusters\\nWe can also use more or fewer cluster centers (Figure 3-26):\\nIn[53]:\\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\\n# using two cluster centers:\\nkmeans = KMeans(n_clusters=2)\\nkmeans.fit(X)\\nassignments = kmeans.labels_\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[0])\\n# using five cluster centers:\\nkmeans = KMeans(n_clusters=5)\\nkmeans.fit(X)\\nassignments = kmeans.labels_\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[1])\\n172 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 186, 'page_label': '173'}, page_content='Figure 3-26. Cluster assignments found by k-means using two clusters (left) and five\\nclusters (right)\\nFailure cases of k-means\\nEven if you know the “right” number of clusters for a given dataset, k-means might\\nnot always be able to recover them. Each cluster is defined solely by its center, which\\nmeans that each cluster is a convex shape. As a result of this, k-means can only cap‐\\nture relatively simple shapes. k-means also assumes that all clusters have the same\\n“diameter” in some sense; it always draws the boundary between clusters to be exactly\\nin the middle between the cluster centers. That can sometimes lead to surprising\\nresults, as shown in Figure 3-27:\\nIn[54]:\\nX_varied, y_varied = make_blobs(n_samples=200,\\n                                cluster_std=[1.0, 2.5, 0.5],\\n                                random_state=170)\\ny_pred = KMeans(n_clusters=3, random_state=0).fit_predict(X_varied)\\nmglearn.discrete_scatter(X_varied[:, 0], X_varied[:, 1], y_pred)\\nplt.legend([\"cluster 0\", \"cluster 1\", \"cluster 2\"], loc=\\'best\\')\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nClustering | 173'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 187, 'page_label': '174'}, page_content='Figure 3-27. Cluster assignments found by k-means when clusters have different\\ndensities\\nOne might have expected the dense region in the lower left to be the first cluster, the\\ndense region in the upper right to be the second, and the less dense region in the cen‐\\nter to be the third. Instead, both cluster 0 and cluster 1 have some points that are far\\naway from all the other points in these clusters that “reach” toward the center.\\nk-means also assumes that all directions are equally important for each cluster. The\\nfollowing plot ( Figure 3-28) shows a two-dimensional dataset where there are three\\nclearly separated parts in the data. However, these groups are stretched toward the\\ndiagonal. As k-means only considers the distance to the nearest cluster center, it can’t\\nhandle this kind of data:\\nIn[55]:\\n# generate some random cluster data\\nX, y = make_blobs(random_state=170, n_samples=600)\\nrng = np.random.RandomState(74)\\n# transform the data to be stretched\\ntransformation = rng.normal(size=(2, 2))\\nX = np.dot(X, transformation)\\n174 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 188, 'page_label': '175'}, page_content='# cluster the data into three clusters\\nkmeans = KMeans(n_clusters=3)\\nkmeans.fit(X)\\ny_pred = kmeans.predict(X)\\n# plot the cluster assignments and cluster centers\\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm3)\\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\\n            marker=\\'^\\', c=[0, 1, 2], s=100, linewidth=2, cmap=mglearn.cm3)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nFigure 3-28. k-means fails to identify nonspherical clusters\\nk-means also performs poorly if the clusters have more complex shapes, like the\\ntwo_moons data we encountered in Chapter 2 (see Figure 3-29):\\nIn[56]:\\n# generate synthetic two_moons data (with less noise this time)\\nfrom sklearn.datasets import make_moons\\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\\n# cluster the data into two clusters\\nkmeans = KMeans(n_clusters=2)\\nkmeans.fit(X)\\ny_pred = kmeans.predict(X)\\nClustering | 175'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 189, 'page_label': '176'}, page_content='# plot the cluster assignments and cluster centers\\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm2, s=60)\\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\\n            marker=\\'^\\', c=[mglearn.cm2(0), mglearn.cm2(1)], s=100, linewidth=2)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nFigure 3-29. k-means fails to identify clusters with complex shapes\\nHere, we would hope that the clustering algorithm can discover the two half-moon\\nshapes. However, this is not possible using the k-means algorithm.\\nVector quantization, or seeing k-means as decomposition\\nEven though k-means is a clustering algorithm, there are interesting parallels between\\nk-means and the decomposition methods like PCA and NMF that we discussed ear‐\\nlier. Y ou might remember that PCA tries to find directions of maximum variance in\\nthe data, while NMF tries to find additive components, which often correspond to\\n“extremes” or “parts” of the data (see Figure 3-13). Both methods tried to express the\\ndata points as a sum over some components. k-means, on the other hand, tries to rep‐\\nresent each data point using a cluster center. Y ou can think of that as each point being\\nrepresented using only a single component, which is given by the cluster center. This\\nview of k-means as a decomposition method, where each point is represented using a\\nsingle component, is called vector quantization.\\n176 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 190, 'page_label': '177'}, page_content='Let’s do a side-by-side comparison of PCA, NMF , and k-means, showing the compo‐\\nnents extracted ( Figure 3-30 ), as well as reconstructions of faces from the test set\\nusing 100 components ( Figure 3-31). For k-means, the reconstruction is the closest\\ncluster center found on the training set:\\nIn[57]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X_people, y_people, stratify=y_people, random_state=0)\\nnmf = NMF(n_components=100, random_state=0)\\nnmf.fit(X_train)\\npca = PCA(n_components=100, random_state=0)\\npca.fit(X_train)\\nkmeans = KMeans(n_clusters=100, random_state=0)\\nkmeans.fit(X_train)\\nX_reconstructed_pca = pca.inverse_transform(pca.transform(X_test))\\nX_reconstructed_kmeans = kmeans.cluster_centers_[kmeans.predict(X_test)]\\nX_reconstructed_nmf = np.dot(nmf.transform(X_test), nmf.components_)\\nIn[58]:\\nfig, axes = plt.subplots(3, 5, figsize=(8, 8),\\n                         subplot_kw={\\'xticks\\': (), \\'yticks\\': ()})\\nfig.suptitle(\"Extracted Components\")\\nfor ax, comp_kmeans, comp_pca, comp_nmf in zip(\\n        axes.T, kmeans.cluster_centers_, pca.components_, nmf.components_):\\n    ax[0].imshow(comp_kmeans.reshape(image_shape))\\n    ax[1].imshow(comp_pca.reshape(image_shape), cmap=\\'viridis\\')\\n    ax[2].imshow(comp_nmf.reshape(image_shape))\\naxes[0, 0].set_ylabel(\"kmeans\")\\naxes[1, 0].set_ylabel(\"pca\")\\naxes[2, 0].set_ylabel(\"nmf\")\\nfig, axes = plt.subplots(4, 5, subplot_kw={\\'xticks\\': (), \\'yticks\\': ()},\\n                         figsize=(8, 8))\\nfig.suptitle(\"Reconstructions\")\\nfor ax, orig, rec_kmeans, rec_pca, rec_nmf in zip(\\n        axes.T, X_test, X_reconstructed_kmeans, X_reconstructed_pca,\\n        X_reconstructed_nmf):\\n    ax[0].imshow(orig.reshape(image_shape))\\n    ax[1].imshow(rec_kmeans.reshape(image_shape))\\n    ax[2].imshow(rec_pca.reshape(image_shape))\\n    ax[3].imshow(rec_nmf.reshape(image_shape))\\naxes[0, 0].set_ylabel(\"original\")\\naxes[1, 0].set_ylabel(\"kmeans\")\\naxes[2, 0].set_ylabel(\"pca\")\\naxes[3, 0].set_ylabel(\"nmf\")\\nClustering | 177'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 191, 'page_label': '178'}, page_content='Figure 3-30. Comparing k-means cluster centers to components found by PCA and NMF\\n178 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 192, 'page_label': '179'}, page_content='Figure 3-31. Comparing image reconstructions using k-means, PCA, and NMF with 100\\ncomponents (or cluster centers)—k-means uses only a single cluster center per image\\nAn interesting aspect of vector quantization using k-means is that we can use many\\nmore clusters than input dimensions to encode our data. Let’s go back to the\\ntwo_moons data. Using PCA or NMF , there is nothing much we can do to this data, as\\nit lives in only two dimensions. Reducing it to one dimension with PCA or NMF\\nwould completely destroy the structure of the data. But we can find a more expressive\\nrepresentation with k-means, by using more cluster centers (see Figure 3-32):\\nClustering | 179'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 193, 'page_label': '180'}, page_content='In[59]:\\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\\nkmeans = KMeans(n_clusters=10, random_state=0)\\nkmeans.fit(X)\\ny_pred = kmeans.predict(X)\\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, s=60, cmap=\\'Paired\\')\\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=60,\\n            marker=\\'^\\', c=range(kmeans.n_clusters), linewidth=2, cmap=\\'Paired\\')\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nprint(\"Cluster memberships:\\\\n{}\".format(y_pred))\\nOut[59]:\\nCluster memberships:\\n[9 2 5 4 2 7 9 6 9 6 1 0 2 6 1 9 3 0 3 1 7 6 8 6 8 5 2 7 5 8 9 8 6 5 3 7 0\\n 9 4 5 0 1 3 5 2 8 9 1 5 6 1 0 7 4 6 3 3 6 3 8 0 4 2 9 6 4 8 2 8 4 0 4 0 5\\n 6 4 5 9 3 0 7 8 0 7 5 8 9 8 0 7 3 9 7 1 7 2 2 0 4 5 6 7 8 9 4 5 4 1 2 3 1\\n 8 8 4 9 2 3 7 0 9 9 1 5 8 5 1 9 5 6 7 9 1 4 0 6 2 6 4 7 9 5 5 3 8 1 9 5 6\\n 3 5 0 2 9 3 0 8 6 0 3 3 5 6 3 2 0 2 3 0 2 6 3 4 4 1 5 6 7 1 1 3 2 4 7 2 7\\n 3 8 6 4 1 4 3 9 9 5 1 7 5 8 2]\\nFigure 3-32. Using many k-means clusters to cover the variation in a complex dataset\\n180 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 194, 'page_label': '181'}, page_content='4 In this case, “best” means that the sum of variances of the clusters is small.\\nWe used 10 cluster centers, which means each point is now assigned a number\\nbetween 0 and 9. We can see this as the data being represented using 10 components\\n(that is, we have 10 new features), with all features being 0, apart from the one that\\nrepresents the cluster center the point is assigned to. Using this 10-dimensional repre‐\\nsentation, it would now be possible to separate the two half-moon shapes using a lin‐\\near model, which would not have been possible using the original two features. It is\\nalso possible to get an even more expressive representation of the data by using the\\ndistances to each of the cluster centers as features. This can be accomplished using\\nthe transform method of kmeans:\\nIn[60]:\\ndistance_features = kmeans.transform(X)\\nprint(\"Distance feature shape: {}\".format(distance_features.shape))\\nprint(\"Distance features:\\\\n{}\".format(distance_features))\\nOut[60]:\\nDistance feature shape: (200, 10)\\nDistance features:\\n[[ 0.922  1.466  1.14  ...,  1.166  1.039  0.233]\\n [ 1.142  2.517  0.12  ...,  0.707  2.204  0.983]\\n [ 0.788  0.774  1.749 ...,  1.971  0.716  0.944]\\n ...,\\n [ 0.446  1.106  1.49  ...,  1.791  1.032  0.812]\\n [ 1.39   0.798  1.981 ...,  1.978  0.239  1.058]\\n [ 1.149  2.454  0.045 ...,  0.572  2.113  0.882]]\\nk-means is a very popular algorithm for clustering, not only because it is relatively\\neasy to understand and implement, but also because it runs relatively quickly. k-\\nmeans scales easily to large datasets, and scikit-learn even includes a more scalable\\nvariant in the MiniBatchKMeans class, which can handle very large datasets.\\nOne of the drawbacks of k-means is that it relies on a random initialization, which\\nmeans the outcome of the algorithm depends on a random seed. By default, scikit-\\nlearn runs the algorithm 10 times with 10 different random initializations, and\\nreturns the best result. 4 Further downsides of k-means are the relatively restrictive\\nassumptions made on the shape of clusters, and the requirement to specify the num‐\\nber of clusters you are looking for (which might not be known in a real-world\\napplication).\\nNext, we will look at two more clustering algorithms that improve upon these proper‐\\nties in some ways.\\nClustering | 181'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 195, 'page_label': '182'}, page_content='Agglomerative Clustering\\nAgglomerative clustering refers to a collection of clustering algorithms that all build\\nupon the same principles: the algorithm starts by declaring each point its own cluster,\\nand then merges the two most similar clusters until some stopping criterion is satis‐\\nfied. The stopping criterion implemented in scikit-learn is the number of clusters,\\nso similar clusters are merged until only the specified number of clusters are left.\\nThere are several linkage criteria that specify how exactly the “most similar cluster” is\\nmeasured. This measure is always defined between two existing clusters.\\nThe following three choices are implemented in scikit-learn:\\nward\\nThe default choice, ward picks the two clusters to merge such that the variance\\nwithin all clusters increases the least. This often leads to clusters that are rela‐\\ntively equally sized.\\naverage\\naverage linkage merges the two clusters that have the smallest average distance\\nbetween all their points.\\ncomplete\\ncomplete linkage (also known as maximum linkage) merges the two clusters that\\nhave the smallest maximum distance between their points.\\nward works on most datasets, and we will use it in our examples. If the clusters have\\nvery dissimilar numbers of members (if one is much bigger than all the others, for\\nexample), average or complete might work better.\\nThe following plot ( Figure 3-33) illustrates the progression of agglomerative cluster‐\\ning on a two-dimensional dataset, looking for three clusters:\\nIn[61]:\\nmglearn.plots.plot_agglomerative_algorithm()\\n182 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 196, 'page_label': '183'}, page_content='5 We could also use the labels_ attribute, as we did for k-means.\\nFigure 3-33. Agglomerative clustering iteratively joins the two closest clusters\\nInitially, each point is its own cluster. Then, in each step, the two clusters that are\\nclosest are merged. In the first four steps, two single-point clusters are picked and\\nthese are joined into two-point clusters. In step 5, one of the two-point clusters is\\nextended to a third point, and so on. In step 9, there are only three clusters remain‐\\ning. As we specified that we are looking for three clusters, the algorithm then stops.\\nLet’s have a look at how agglomerative clustering performs on the simple three-\\ncluster data we used here. Because of the way the algorithm works, agglomerative\\nclustering cannot make predictions for new data points. Therefore, Agglomerative\\nClustering has no predict method. To build the model and get the cluster member‐\\nships on the training set, use the fit_predict method instead. 5 The result is shown\\nin Figure 3-34:\\nIn[62]:\\nfrom sklearn.cluster import AgglomerativeClustering\\nX, y = make_blobs(random_state=1)\\nagg = AgglomerativeClustering(n_clusters=3)\\nassignment = agg.fit_predict(X)\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], assignment)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nClustering | 183'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 197, 'page_label': '184'}, page_content='Figure 3-34. Cluster assignment using agglomerative clustering with three clusters\\nAs expected, the algorithm recovers the clustering perfectly. While the scikit-learn\\nimplementation of agglomerative clustering requires you to specify the number of\\nclusters you want the algorithm to find, agglomerative clustering methods provide\\nsome help with choosing the right number, which we will discuss next.\\nHierarchical clustering and dendrograms\\nAgglomerative clustering produces what is known as a hierarchical clustering. The\\nclustering proceeds iteratively, and every point makes a journey from being a single\\npoint cluster to belonging to some final cluster. Each intermediate step provides a\\nclustering of the data (with a different number of clusters). It is sometimes helpful to\\nlook at all possible clusterings jointly. The next example (Figure 3-35) shows an over‐\\nlay of all the possible clusterings shown in Figure 3-33, providing some insight into\\nhow each cluster breaks up into smaller clusters:\\nIn[63]:\\nmglearn.plots.plot_agglomerative()\\n184 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 198, 'page_label': '185'}, page_content='Figure 3-35. Hierarchical cluster assignment (shown as lines) generated with agglomera‐\\ntive clustering, with numbered data points (cf. Figure 3-36)\\nWhile this visualization provides a very detailed view of the hierarchical clustering, it\\nrelies on the two-dimensional nature of the data and therefore cannot be used on\\ndatasets that have more than two features. There is, however, another tool to visualize\\nhierarchical clustering, called a dendrogram, that can handle multidimensional\\ndatasets.\\nUnfortunately, scikit-learn currently does not have the functionality to draw den‐\\ndrograms. However, you can generate them easily using SciPy. The SciPy clustering\\nalgorithms have a slightly different interface to the scikit-learn clustering algo‐\\nrithms. SciPy provides a function that takes a data array X and computes a linkage\\narray, which encodes hierarchical cluster similarities. We can then feed this linkage\\narray into the scipy dendrogram function to plot the dendrogram (Figure 3-36):\\nIn[64]:\\n# Import the dendrogram function and the ward clustering function from SciPy\\nfrom scipy.cluster.hierarchy import dendrogram, ward\\nX, y = make_blobs(random_state=0, n_samples=12)\\n# Apply the ward clustering to the data array X\\n# The SciPy ward function returns an array that specifies the distances\\n# bridged when performing agglomerative clustering\\nlinkage_array = ward(X)\\nClustering | 185'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 199, 'page_label': '186'}, page_content='# Now we plot the dendrogram for the linkage_array containing the distances\\n# between clusters\\ndendrogram(linkage_array)\\n# Mark the cuts in the tree that signify two or three clusters\\nax = plt.gca()\\nbounds = ax.get_xbound()\\nax.plot(bounds, [7.25, 7.25], \\'--\\', c=\\'k\\')\\nax.plot(bounds, [4, 4], \\'--\\', c=\\'k\\')\\nax.text(bounds[1], 7.25, \\' two clusters\\', va=\\'center\\', fontdict={\\'size\\': 15})\\nax.text(bounds[1], 4, \\' three clusters\\', va=\\'center\\', fontdict={\\'size\\': 15})\\nplt.xlabel(\"Sample index\")\\nplt.ylabel(\"Cluster distance\")\\nFigure 3-36. Dendrogram of the clustering shown in Figure 3-35 with lines indicating\\nsplits into two and three clusters\\nThe dendrogram shows data points as points on the bottom (numbered from 0 to\\n11). Then, a tree is plotted with these points (representing single-point clusters) as the\\nleaves, and a new node parent is added for each two clusters that are joined.\\nReading from bottom to top, the data points 1 and 4 are joined first (as you could see\\nin Figure 3-33). Next, points 6 and 9 are joined into a cluster, and so on. At the top\\nlevel, there are two branches, one consisting of points 11, 0, 5, 10, 7, 6, and 9, and the\\nother consisting of points 1, 4, 3, 2, and 8. These correspond to the two largest clus‐\\nters in the lefthand side of the plot.\\n186 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 200, 'page_label': '187'}, page_content='The y-axis in the dendrogram doesn’t just specify when in the agglomerative algo‐\\nrithm two clusters get merged. The length of each branch also shows how far apart\\nthe merged clusters are. The longest branches in this dendrogram are the three lines\\nthat are marked by the dashed line labeled “three clusters. ” That these are the longest\\nbranches indicates that going from three to two clusters meant merging some very\\nfar-apart points. We see this again at the top of the chart, where merging the two\\nremaining clusters into a single cluster again bridges a relatively large distance.\\nUnfortunately, agglomerative clustering still fails at separating complex shapes like\\nthe two_moons dataset. But the same is not true for the next algorithm we will look at,\\nDBSCAN.\\nDBSCAN\\nAnother very useful clustering algorithm is DBSCAN (which stands for “density-\\nbased spatial clustering of applications with noise”). The main benefits of DBSCAN\\nare that it does not require the user to set the number of clusters a priori, it can cap‐\\nture clusters of complex shapes, and it can identify points that are not part of any\\ncluster. DBSCAN is somewhat slower than agglomerative clustering and k-means, but\\nstill scales to relatively large datasets.\\nDBSCAN works by identifying points that are in “crowded” regions of the feature\\nspace, where many data points are close together. These regions are referred to as\\ndense regions in feature space. The idea behind DBSCAN is that clusters form dense\\nregions of data, separated by regions that are relatively empty.\\nPoints that are within a dense region are called core samples (or core points), and they\\nare defined as follows. There are two parameters in DBSCAN: min_samples and eps.\\nIf there are at least min_samples many data points within a distance of eps to a given\\ndata point, that data point is classified as a core sample. Core samples that are closer\\nto each other than the distance eps are put into the same cluster by DBSCAN.\\nThe algorithm works by picking an arbitrary point to start with. It then finds all\\npoints with distance eps or less from that point. If there are less than min_samples\\npoints within distance eps of the starting point, this point is labeled as noise, meaning\\nthat it doesn’t belong to any cluster. If there are more than min_samples points within\\na distance of eps, the point is labeled a core sample and assigned a new cluster label.\\nThen, all neighbors (within eps) of the point are visited. If they have not been\\nassigned a cluster yet, they are assigned the new cluster label that was just created. If\\nthey are core samples, their neighbors are visited in turn, and so on. The cluster\\ngrows until there are no more core samples within distance eps of the cluster. Then\\nanother point that hasn’t yet been visited is picked, and the same procedure is\\nrepeated.\\nClustering | 187'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 201, 'page_label': '188'}, page_content='In the end, there are three kinds of points: core points, points that are within distance\\neps of core points (called boundary points), and noise. When the DBSCAN algorithm\\nis run on a particular dataset multiple times, the clustering of the core points is always\\nthe same, and the same points will always be labeled as noise. However, a boundary\\npoint might be neighbor to core samples of more than one cluster. Therefore, the\\ncluster membership of boundary points depends on the order in which points are vis‐\\nited. Usually there are only few boundary points, and this slight dependence on the\\norder of points is not important.\\nLet’s apply DBSCAN on the synthetic dataset we used to demonstrate agglomerative\\nclustering. Like agglomerative clustering, DBSCAN does not allow predictions on\\nnew test data, so we will use the fit_predict method to perform clustering and\\nreturn the cluster labels in one step:\\nIn[65]:\\nfrom sklearn.cluster import DBSCAN\\nX, y = make_blobs(random_state=0, n_samples=12)\\ndbscan = DBSCAN()\\nclusters = dbscan.fit_predict(X)\\nprint(\"Cluster memberships:\\\\n{}\".format(clusters))\\nOut[65]:\\nCluster memberships:\\n[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\nAs you can see, all data points were assigned the label -1, which stands for noise. This\\nis a consequence of the default parameter settings for eps and min_samples, which\\nare not tuned for small toy datasets. The cluster assignments for different values of\\nmin_samples and eps are shown below, and visualized in Figure 3-37:\\nIn[66]:\\nmglearn.plots.plot_dbscan()\\nOut[66]:\\nmin_samples: 2 eps: 1.000000  cluster: [-1  0  0 -1  0 -1  1  1  0  1 -1 -1]\\nmin_samples: 2 eps: 1.500000  cluster: [0 1 1 1 1 0 2 2 1 2 2 0]\\nmin_samples: 2 eps: 2.000000  cluster: [0 1 1 1 1 0 0 0 1 0 0 0]\\nmin_samples: 2 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\\nmin_samples: 3 eps: 1.000000  cluster: [-1  0  0 -1  0 -1  1  1  0  1 -1 -1]\\nmin_samples: 3 eps: 1.500000  cluster: [0 1 1 1 1 0 2 2 1 2 2 0]\\nmin_samples: 3 eps: 2.000000  cluster: [0 1 1 1 1 0 0 0 1 0 0 0]\\nmin_samples: 3 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\\nmin_samples: 5 eps: 1.000000  cluster: [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\nmin_samples: 5 eps: 1.500000  cluster: [-1  0  0  0  0 -1 -1 -1  0 -1 -1 -1]\\nmin_samples: 5 eps: 2.000000  cluster: [-1  0  0  0  0 -1 -1 -1  0 -1 -1 -1]\\nmin_samples: 5 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\\n188 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 202, 'page_label': '189'}, page_content='Figure 3-37. Cluster assignments found by DBSCAN with varying settings for the\\nmin_samples and eps parameters\\nIn this plot, points that belong to clusters are solid, while the noise points are shown\\nin white. Core samples are shown as large markers, while boundary points are dis‐\\nplayed as smaller markers. Increasing eps (going from left to right in the figure)\\nmeans that more points will be included in a cluster. This makes clusters grow, but\\nmight also lead to multiple clusters joining into one. Increasing min_samples (going\\nfrom top to bottom in the figure) means that fewer points will be core points, and\\nmore points will be labeled as noise.\\nThe parameter eps is somewhat more important, as it determines what it means for\\npoints to be “close. ” Setting eps to be very small will mean that no points are core\\nsamples, and may lead to all points being labeled as noise. Setting eps to be very large\\nwill result in all points forming a single cluster.\\nThe min_samples setting mostly determines whether points in less dense regions will\\nbe labeled as outliers or as their own clusters. If you decrease min_samples, anything\\nthat would have been a cluster with less than min_samples many samples will now be\\nlabeled as noise. min_samples therefore determines the minimum cluster size. Y ou\\ncan see this very clearly in Figure 3-37, when going from min_samples=3 to min_sam\\nples=5 with eps=1.5. With min_samples=3, there are three clusters: one of four\\nClustering | 189'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 203, 'page_label': '190'}, page_content='points, one of five points, and one of three points. Using min_samples=5, the two\\nsmaller clusters (with three and four points) are now labeled as noise, and only the\\ncluster with five samples remains.\\nWhile DBSCAN doesn’t require setting the number of clusters explicitly, setting eps\\nimplicitly controls how many clusters will be found. Finding a good setting for eps is\\nsometimes easier after scaling the data using StandardScaler or MinMaxScaler, as\\nusing these scaling techniques will ensure that all features have similar ranges.\\nFigure 3-38  shows the result of running DBSCAN on the two_moons dataset. The\\nalgorithm actually finds the two half-circles and separates them using the default\\nsettings:\\nIn[67]:\\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\\n# rescale the data to zero mean and unit variance\\nscaler = StandardScaler()\\nscaler.fit(X)\\nX_scaled = scaler.transform(X)\\ndbscan = DBSCAN()\\nclusters = dbscan.fit_predict(X_scaled)\\n# plot the cluster assignments\\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=mglearn.cm2, s=60)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nAs the algorithm produced the desired number of clusters (two), the parameter set‐\\ntings seem to work well. If we decrease eps to 0.2 (from the default of 0.5), we will\\nget eight clusters, which is clearly too many. Increasing eps to 0.7 results in a single\\ncluster.\\nWhen using DBSCAN, you need to be careful about handling the returned cluster\\nassignments. The use of -1 to indicate noise might result in unexpected effects when\\nusing the cluster labels to index another array.\\n190 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 204, 'page_label': '191'}, page_content='Figure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\\nComparing and Evaluating Clustering Algorithms\\nOne of the challenges in applying clustering algorithms is that it is very hard to assess\\nhow well an algorithm worked, and to compare outcomes between different algo‐\\nrithms. After talking about the algorithms behind k-means, agglomerative clustering,\\nand DBSCAN, we will now compare them on some real-world datasets.\\nEvaluating clustering with ground truth\\nThere are metrics that can be used to assess the outcome of a clustering algorithm\\nrelative to a ground truth clustering, the most important ones being the adjusted rand\\nindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐\\ntative measure between 0 and 1.\\nHere, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\\nusing ARI. We also include what it looks like when we randomly assign points to two\\nclusters for comparison (see Figure 3-39):\\nClustering | 191'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 205, 'page_label': '192'}, page_content='In[68]:\\nfrom sklearn.metrics.cluster import adjusted_rand_score\\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\\n# rescale the data to zero mean and unit variance\\nscaler = StandardScaler()\\nscaler.fit(X)\\nX_scaled = scaler.transform(X)\\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\\n                         subplot_kw={\\'xticks\\': (), \\'yticks\\': ()})\\n# make a list of algorithms to use\\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\\n              DBSCAN()]\\n# create a random cluster assignment for reference\\nrandom_state = np.random.RandomState(seed=0)\\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\\n# plot random assignment\\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\\n                cmap=mglearn.cm3, s=60)\\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\\n        adjusted_rand_score(y, random_clusters)))\\nfor ax, algorithm in zip(axes[1:], algorithms):\\n    # plot the cluster assignments and cluster centers\\n    clusters = algorithm.fit_predict(X_scaled)\\n    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters,\\n               cmap=mglearn.cm3, s=60)\\n    ax.set_title(\"{} - ARI: {:.2f}\".format(algorithm.__class__.__name__,\\n                                           adjusted_rand_score(y, clusters)))\\nFigure 3-39. Comparing random assignment, k-means, agglomerative clustering, and\\nDBSCAN on the two_moons dataset using the supervised ARI score\\nThe adjusted rand index provides intuitive results, with a random cluster assignment\\nhaving a score of 0 and DBSCAN (which recovers the desired clustering perfectly)\\nhaving a score of 1.\\n192 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 206, 'page_label': '193'}, page_content='A common mistake when evaluating clustering in this way is to use accuracy_score\\ninstead of adjusted_rand_score, normalized_mutual_info_score, or some other\\nclustering metric. The problem in using accuracy is that it requires the assigned clus‐\\nter labels to exactly match the ground truth. However, the cluster labels themselves\\nare meaningless—the only thing that matters is which points are in the same cluster:\\nIn[69]:\\nfrom sklearn.metrics import accuracy_score\\n# these two labelings of points correspond to the same clustering\\nclusters1 = [0, 0, 1, 1, 0]\\nclusters2 = [1, 1, 0, 0, 1]\\n# accuracy is zero, as none of the labels are the same\\nprint(\"Accuracy: {:.2f}\".format(accuracy_score(clusters1, clusters2)))\\n# adjusted rand score is 1, as the clustering is exactly the same\\nprint(\"ARI: {:.2f}\".format(adjusted_rand_score(clusters1, clusters2)))\\nOut[69]:\\nAccuracy: 0.00\\nARI: 1.00\\nEvaluating clustering without ground truth\\nAlthough we have just shown one way to evaluate clustering algorithms, in practice,\\nthere is a big problem with using measures like ARI. When applying clustering algo‐\\nrithms, there is usually no ground truth to which to compare the results. If we knew\\nthe right clustering of the data, we could use this information to build a supervised\\nmodel like a classifier. Therefore, using metrics like ARI and NMI usually only helps\\nin developing algorithms, not in assessing success in an application.\\nThere are scoring metrics for clustering that don’t require ground truth, like the sil‐\\nhouette \\ncoefficient. However, these often don’t work well in practice. The silhouette\\nscore computes the compactness of a cluster, where higher is better, with a perfect\\nscore of 1. While compact clusters are good, compactness doesn’t allow for complex\\nshapes.\\nHere is an example comparing the outcome of k-means, agglomerative clustering,\\nand DBSCAN on the two-moons dataset using the silhouette score (Figure 3-40):\\nIn[70]:\\nfrom sklearn.metrics.cluster import silhouette_score\\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\\n# rescale the data to zero mean and unit variance\\nscaler = StandardScaler()\\nscaler.fit(X)\\nX_scaled = scaler.transform(X)\\nClustering | 193'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 207, 'page_label': '194'}, page_content='fig, axes = plt.subplots(1, 4, figsize=(15, 3),\\n                         subplot_kw={\\'xticks\\': (), \\'yticks\\': ()})\\n# create a random cluster assignment for reference\\nrandom_state = np.random.RandomState(seed=0)\\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\\n# plot random assignment\\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\\n    cmap=mglearn.cm3, s=60)\\naxes[0].set_title(\"Random assignment: {:.2f}\".format(\\n    silhouette_score(X_scaled, random_clusters)))\\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\\n              DBSCAN()]\\nfor ax, algorithm in zip(axes[1:], algorithms):\\n    clusters = algorithm.fit_predict(X_scaled)\\n    # plot the cluster assignments and cluster centers\\n    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=mglearn.cm3,\\n               s=60)\\n    ax.set_title(\"{} : {:.2f}\".format(algorithm.__class__.__name__,\\n                                      silhouette_score(X_scaled, clusters)))\\nFigure 3-40. Comparing random assignment, k-means, agglomerative clustering, and\\nDBSCAN on the two_moons dataset using the unsupervised silhouette score—the more\\nintuitive result of DBSCAN has a lower silhouette score than the assignments found by\\nk-means\\nAs you can see, k-means gets the highest silhouette score, even though we might pre‐\\nfer the result produced by DBSCAN. A slightly better strategy for evaluating clusters\\nis using robustness-based clustering metrics. These run an algorithm after adding\\nsome noise to the data, or using different parameter settings, and compare the out‐\\ncomes. The idea is that if many algorithm parameters and many perturbations of the\\ndata return the same result, it is likely to be trustworthy. Unfortunately, this strategy is\\nnot implemented in scikit-learn at the time of writing.\\nEven if we get a very robust clustering, or a very high silhouette score, we still don’t\\nknow if there is any semantic meaning in the clustering, or whether the clustering\\n194 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 208, 'page_label': '195'}, page_content='reflects an aspect of the data that we are interested in. Let’s go back to the example of\\nface images. We hope to find groups of similar faces—say, men and women, or old\\npeople and young people, or people with beards and without. Let’s say we cluster the\\ndata into two clusters, and all algorithms agree about which points should be clus‐\\ntered together. We still don’t know if the clusters that are found correspond in any\\nway to the concepts we are interested in. It could be that they found side views versus\\nfront views, or pictures taken at night versus pictures taken during the day, or pic‐\\ntures taken with iPhones versus pictures taken with Android phones. The only way to\\nknow whether the clustering corresponds to anything we are interested in is to ana‐\\nlyze the clusters manually.\\nComparing algorithms on the faces dataset\\nLet’s apply the k-means, DBSCAN, and agglomerative clustering algorithms to the\\nLabeled Faces in the Wild dataset, and see if any of them find interesting structure.\\nWe will use the eigenface representation of the data, as produced by\\nPCA(whiten=True), with 100 components:\\nIn[71]:\\n# extract eigenfaces from lfw data and transform data\\nfrom sklearn.decomposition import PCA\\npca = PCA(n_components=100, whiten=True, random_state=0)\\npca.fit_transform(X_people)\\nX_pca = pca.transform(X_people)\\nWe saw earlier that this is a more semantic representation of the face images than the\\nraw pixels. It will also make computation faster. A good exercise would be for you to\\nrun the following experiments on the original data, without PCA, and see if you find\\nsimilar clusters.\\nAnalyzing the faces dataset with DBSCAN.    We will start by applying DBSCAN, which we\\njust discussed:\\nIn[72]:\\n# apply DBSCAN with default parameters\\ndbscan = DBSCAN()\\nlabels = dbscan.fit_predict(X_pca)\\nprint(\"Unique labels: {}\".format(np.unique(labels)))\\nOut[72]:\\nUnique labels: [-1]\\nWe see that all the returned labels are –1, so all of the data was labeled as “noise” by\\nDBSCAN. There are two things we can change to help this: we can make eps higher,\\nto expand the neighborhood of each point, and set min_samples lower, to consider\\nsmaller groups of points as clusters. Let’s try changing min_samples first:\\nClustering | 195'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 209, 'page_label': '196'}, page_content='In[73]:\\ndbscan = DBSCAN(min_samples=3)\\nlabels = dbscan.fit_predict(X_pca)\\nprint(\"Unique labels: {}\".format(np.unique(labels)))\\nOut[73]:\\nUnique labels: [-1]\\nEven when considering groups of three points, everything is labeled as noise. So, we\\nneed to increase eps:\\nIn[74]:\\ndbscan = DBSCAN(min_samples=3, eps=15)\\nlabels = dbscan.fit_predict(X_pca)\\nprint(\"Unique labels: {}\".format(np.unique(labels)))\\nOut[74]:\\nUnique labels: [-1  0]\\nUsing a much larger eps of 15, we get only a single cluster and noise points. We can\\nuse this result to find out what the “noise” looks like compared to the rest of the data.\\nTo understand better what’s happening, let’s look at how many points are noise, and\\nhow many points are inside the cluster:\\nIn[75]:\\n# Count number of points in all clusters and noise.\\n# bincount doesn\\'t allow negative numbers, so we need to add 1.\\n# The first number in the result corresponds to noise points.\\nprint(\"Number of points per cluster: {}\".format(np.bincount(labels + 1)))\\nOut[75]:\\nNumber of points per cluster: [  27 2036]\\nThere are very few noise points—only 27—so we can look at all of them (see\\nFigure 3-41):\\nIn[76]:\\nnoise = X_people[labels==-1]\\nfig, axes = plt.subplots(3, 9, subplot_kw={\\'xticks\\': (), \\'yticks\\': ()},\\n                         figsize=(12, 4))\\nfor image, ax in zip(noise, axes.ravel()):\\n    ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\\n196 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 210, 'page_label': '197'}, page_content='Figure 3-41. Samples from the faces dataset labeled as noise by DBSCAN\\nComparing these images to the random sample of face images from Figure 3-7, we\\ncan guess why they were labeled as noise: the fifth image in the first row shows a per‐\\nson drinking from a glass, there are images of people wearing hats, and in the last\\nimage there’s a hand in front of the person’s face. The other images contain odd angles\\nor crops that are too close or too wide.\\nThis kind of analysis—trying to find “the odd one out”—is called outlier detection. If\\nthis was a real application, we might try to do a better job of cropping images, to get\\nmore homogeneous data. There is little we can do about people in photos sometimes\\nwearing hats, drinking, or holding something in front of their faces, but it’s good to\\nknow that these are issues in the data that any algorithm we might apply needs to\\nhandle.\\nIf we want to find more interesting clusters than just one large one, we need to set eps\\nsmaller, somewhere between 15 and 0.5 (the default). Let’s have a look at what differ‐\\nent values of eps result in:\\nIn[77]:\\nfor eps in [1, 3, 5, 7, 9, 11, 13]:\\n    print(\"\\\\neps={}\".format(eps))\\n    dbscan = DBSCAN(eps=eps, min_samples=3)\\n    labels = dbscan.fit_predict(X_pca)\\n    print(\"Clusters present: {}\".format(np.unique(labels)))\\n    print(\"Cluster sizes: {}\".format(np.bincount(labels + 1)))\\nOut[78]:\\neps=1\\nClusters present: [-1]\\nCluster sizes: [2063]\\neps=3\\nClusters present: [-1]\\nCluster sizes: [2063]\\nClustering | 197'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 211, 'page_label': '198'}, page_content=\"eps=5\\nClusters present: [-1]\\nCluster sizes: [2063]\\neps=7\\nClusters present: [-1  0  1  2  3  4  5  6  7  8  9 10 11 12]\\nCluster sizes: [2006  4  6  6  6  9  3  3  4  3  3  3  3  4]\\neps=9\\nClusters present: [-1  0  1  2]\\nCluster sizes: [1269  788    3    3]\\neps=11\\nClusters present: [-1  0]\\nCluster sizes: [ 430 1633]\\neps=13\\nClusters present: [-1  0]\\nCluster sizes: [ 112 1951]\\nFor low settings of eps, all points are labeled as noise. For eps=7, we get many noise\\npoints and many smaller clusters. For eps=9 we still get many noise points, but we get\\none big cluster and some smaller clusters. Starting from eps=11, we get only one large\\ncluster and noise.\\nWhat is interesting to note is that there is never more than one large cluster. At most,\\nthere is one large cluster containing most of the points, and some smaller clusters.\\nThis indicates that there are not two or three different kinds of face images in the data\\nthat are very distinct, but rather that all images are more or less equally similar to (or\\ndissimilar from) the rest.\\nThe results for eps=7 look most interesting, with many small clusters. We can investi‐\\ngate this clustering in more detail by visualizing all of the points in each of the 13\\nsmall clusters (Figure 3-42):\\nIn[78]:\\ndbscan = DBSCAN(min_samples=3, eps=7)\\nlabels = dbscan.fit_predict(X_pca)\\nfor cluster in range(max(labels) + 1):\\n    mask = labels == cluster\\n    n_images =  np.sum(mask)\\n    fig, axes = plt.subplots(1, n_images, figsize=(n_images * 1.5, 4),\\n                             subplot_kw={'xticks': (), 'yticks': ()})\\n    for image, label, ax in zip(X_people[mask], y_people[mask], axes):\\n        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\\n        ax.set_title(people.target_names[label].split()[-1])\\n198 | Chapter 3: Unsupervised Learning and Preprocessing\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 212, 'page_label': '199'}, page_content='Figure 3-42. Clusters found by DBSCAN with eps=7\\nSome of the clusters correspond to people with very distinct faces (within this data‐\\nset), such as Sharon or Koizumi. Within each cluster, the orientation of the face is also\\nClustering | 199'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 213, 'page_label': '200'}, page_content='quite fixed, as well as the facial expression. Some of the clusters contain faces of mul‐\\ntiple people, but they share a similar orientation and expression.\\nThis concludes our analysis of the DBSCAN algorithm applied to the faces dataset. As\\nyou can see, we are doing a manual analysis here, different from the much more auto‐\\nmatic search approach we could use for supervised learning based on R2 score or\\naccuracy.\\nLet’s move on to applying k-means and agglomerative clustering.\\nAnalyzing the faces dataset with k-means.    We saw that it was not possible to create\\nmore than one big cluster using DBSCAN. Agglomerative clustering and k-means are\\nmuch more likely to create clusters of even size, but we do need to set a target num‐\\nber of clusters. We could set the number of clusters to the known number of people in\\nthe dataset, though it is very unlikely that an unsupervised clustering algorithm will\\nrecover them. Instead, we can start with a low number of clusters, like 10, which\\nmight allow us to analyze each of the clusters:\\nIn[79]:\\n# extract clusters with k-means\\nkm = KMeans(n_clusters=10, random_state=0)\\nlabels_km = km.fit_predict(X_pca)\\nprint(\"Cluster sizes k-means: {}\".format(np.bincount(labels_km)))\\nOut[79]:\\nCluster sizes k-means: [269 128 170 186 386 222 237  64 253 148]\\nAs you can see, k-means clustering partitioned the data into relatively similarly sized\\nclusters from 64 to 386. This is quite different from the result of DBSCAN.\\nWe can further analyze the outcome of k-means by visualizing the cluster centers\\n(Figure 3-43 ). As we clustered in the representation produced by PCA, we need to\\nrotate the cluster centers back into the original space to visualize them, using\\npca.inverse_transform:\\nIn[80]:\\nfig, axes = plt.subplots(2, 5, subplot_kw={\\'xticks\\': (), \\'yticks\\': ()},\\n                         figsize=(12, 4))\\nfor center, ax in zip(km.cluster_centers_, axes.ravel()):\\n    ax.imshow(pca.inverse_transform(center).reshape(image_shape),\\n              vmin=0, vmax=1)\\n200 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 214, 'page_label': '201'}, page_content='Figure 3-43. Cluster centers found by k-means when setting the number of clusters to 10\\nThe cluster centers found by k-means are very smooth versions of faces. This is not\\nvery surprising, given that each center is an average of 64 to 386 face images. Working\\nwith a reduced PCA representation adds to the smoothness of the images (compared\\nto the faces reconstructed using 100 PCA dimensions in Figure 3-11). The clustering\\nseems to pick up on different orientations of the face, different expressions (the third\\ncluster center seems to show a smiling face), and the presence of shirt collars (see the\\nsecond-to-last cluster center).\\nFor a more detailed view, in Figure 3-44 we show for each cluster center the five most\\ntypical images in the cluster (the images assigned to the cluster that are closest to the\\ncluster center) and the five most atypical images in the cluster (the images assigned to\\nthe cluster that are furthest from the cluster center):\\nIn[81]:\\nmglearn.plots.plot_kmeans_faces(km, pca, X_pca, X_people,\\n                                y_people, people.target_names)\\nClustering | 201'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 215, 'page_label': '202'}, page_content='Figure 3-44. Sample images for each cluster found by k-means—the cluster centers are\\non the left, followed by the five closest points to each center and the five points that are\\nassigned to the cluster but are furthest away from the center\\n202 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 216, 'page_label': '203'}, page_content='Figure 3-44 confirms our intuition about smiling faces for the third cluster, and also\\nthe importance of orientation for the other clusters. The “atypical” points are not very\\nsimilar to the cluster centers, though, and their assignment seems somewhat arbi‐\\ntrary. This can be attributed to the fact that k-means partitions all the data points and\\ndoesn’t have a concept of “noise” points, as DBSCAN does. Using a larger number of\\nclusters, the algorithm could find finer distinctions. However, adding more clusters\\nmakes manual inspection even harder.\\nAnalyzing the faces dataset with agglomerative clustering.    Now, let’s look at the results of\\nagglomerative clustering:\\nIn[82]:\\n# extract clusters with ward agglomerative clustering\\nagglomerative = AgglomerativeClustering(n_clusters=10)\\nlabels_agg = agglomerative.fit_predict(X_pca)\\nprint(\"Cluster sizes agglomerative clustering: {}\".format(\\n    np.bincount(labels_agg)))\\nOut[82]:\\nCluster sizes agglomerative clustering: [255 623  86 102 122 199 265  26 230 155]\\nAgglomerative clustering also produces relatively equally sized clusters, with cluster\\nsizes between 26 and 623. These are more uneven than those produced by k-means,\\nbut much more even than the ones produced by DBSCAN.\\nWe can compute the ARI to measure whether the two partitions of the data given by\\nagglomerative clustering and k-means are similar:\\nIn[83]:\\nprint(\"ARI: {:.2f}\".format(adjusted_rand_score(labels_agg, labels_km)))\\nOut[83]:\\nARI: 0.13\\nAn ARI of only 0.13 means that the two clusterings labels_agg and labels_km have\\nlittle in common. This is not very surprising, given the fact that points further away\\nfrom the cluster centers seem to have little in common for k-means.\\nNext, we might want to plot the dendrogram ( Figure 3-45). We’ll limit the depth of\\nthe tree in the plot, as branching down to the individual 2,063 data points would\\nresult in an unreadably dense plot:\\nClustering | 203'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 217, 'page_label': '204'}, page_content='In[84]:\\nlinkage_array = ward(X_pca)\\n# now we plot the dendrogram for the linkage_array\\n# containing the distances between clusters\\nplt.figure(figsize=(20, 5))\\ndendrogram(linkage_array, p=7, truncate_mode=\\'level\\', no_labels=True)\\nplt.xlabel(\"Sample index\")\\nplt.ylabel(\"Cluster distance\")\\nFigure 3-45. Dendrogram of agglomerative clustering on the faces dataset\\nCreating 10 clusters, we cut across the tree at the very top, where there are 10 vertical\\nlines. In the dendrogram for the toy data shown in Figure 3-36, you could see by the\\nlength of the branches that two or three clusters might capture the data appropriately.\\nFor the faces data, there doesn’t seem to be a very natural cutoff point. There are\\nsome branches that represent more distinct groups, but there doesn’t appear to be a\\nparticular number of clusters that is a good fit. This is not surprising, given the results\\nof DBSCAN, which tried to cluster all points together.\\nLet’s visualize the 10 clusters, as we did for k-means earlier ( Figure 3-46). Note that\\nthere is no notion of cluster center in agglomerative clustering (though we could\\ncompute the mean), and we simply show the first couple of points in each cluster. We\\nshow the number of points in each cluster to the left of the first image:\\nIn[85]:\\nn_clusters = 10\\nfor cluster in range(n_clusters):\\n    mask = labels_agg == cluster\\n    fig, axes = plt.subplots(1, 10, subplot_kw={\\'xticks\\': (), \\'yticks\\': ()},\\n                             figsize=(15, 8))\\n    axes[0].set_ylabel(np.sum(mask))\\n    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\\n                                      labels_agg[mask], axes):\\n        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\\n        ax.set_title(people.target_names[label].split()[-1],\\n                     fontdict={\\'fontsize\\': 9})\\n204 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 218, 'page_label': '205'}, page_content='Figure 3-46. Random images from the clusters generated by In[82]—each row corre‐\\nsponds to one cluster; the number to the left lists the number of images in each cluster\\nClustering | 205'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 219, 'page_label': '206'}, page_content='While some of the clusters seem to have a semantic theme, many of them are too\\nlarge to be actually homogeneous. To get more homogeneous clusters, we can run the\\nalgorithm again, this time with 40 clusters, and pick out some of the clusters that are\\nparticularly interesting (Figure 3-47):\\nIn[86]:\\n# extract clusters with ward agglomerative clustering\\nagglomerative = AgglomerativeClustering(n_clusters=40)\\nlabels_agg = agglomerative.fit_predict(X_pca)\\nprint(\"cluster sizes agglomerative clustering: {}\".format(np.bincount(labels_agg)))\\nn_clusters = 40\\nfor cluster in [10, 13, 19, 22, 36]: # hand-picked \"interesting\" clusters\\n    mask = labels_agg == cluster\\n    fig, axes = plt.subplots(1, 15, subplot_kw={\\'xticks\\': (), \\'yticks\\': ()},\\n                             figsize=(15, 8))\\n    cluster_size = np.sum(mask)\\n    axes[0].set_ylabel(\"#{}: {}\".format(cluster, cluster_size))\\n    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\\n                                      labels_agg[mask], axes):\\n        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\\n        ax.set_title(people.target_names[label].split()[-1],\\n                     fontdict={\\'fontsize\\': 9})\\n    for i in range(cluster_size, 15):\\n        axes[i].set_visible(False)\\nOut[86]:\\ncluster sizes agglomerative clustering:\\n [ 58  80  79  40 222  50  55  78 172  28  26  34  14  11  60  66 152  27\\n  47  31  54   5   8  56   3   5   8  18  22  82  37  89  28  24  41  40\\n  21  10 113  69]\\n206 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 220, 'page_label': '207'}, page_content='Figure 3-47. Images from selected clusters found by agglomerative clustering when set‐\\nting the number of clusters to 40—the text to the left shows the index of the cluster and\\nthe total number of points in the cluster\\nHere, the clustering seems to have picked up on “dark skinned and smiling, ” “collared\\nshirt, ” “smiling woman, ” “Hussein, ” and “high forehead. ” We could also find these\\nhighly similar clusters using the dendrogram, if we did more a detailed analysis.\\nSummary of Clustering Methods\\nThis section has shown that applying and evaluating clustering is a highly qualitative\\nprocedure, and often most helpful in the exploratory phase of data analysis. We\\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\\ning. All three have a way of controlling the granularity of clustering. k-means and\\nagglomerative clustering allow you to specify the number of desired clusters, while\\nDBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\\nences cluster size. All three methods can be used on large, real-world datasets, are rel‐\\natively easy to understand, and allow for clustering into many clusters.\\nEach of the algorithms has somewhat different strengths. k-means allows for a char‐\\nacterization of the clusters using the cluster means. It can also be viewed as a decom‐\\nposition method, where each data point is represented by its cluster center. DBSCAN\\nallows for the detection of “noise points” that are not assigned any cluster, and it can\\nhelp automatically determine the number of clusters. In contrast to the other two\\nmethods, it allow for complex cluster shapes, as we saw in the two_moons example.\\nDBSCAN sometimes produces clusters of very differing size, which can be a strength\\nor a weakness. Agglomerative clustering can provide a whole hierarchy of possible\\npartitions of the data, which can be easily inspected via dendrograms.\\nClustering | 207'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 221, 'page_label': '208'}, page_content='Summary and Outlook\\nThis chapter introduced a range of unsupervised learning algorithms that can be\\napplied for exploratory data analysis and preprocessing. Having the right representa‐\\ntion of the data is often crucial for supervised or unsupervised learning to succeed,\\nand preprocessing and decomposition methods play an important part in data prepa‐\\nration.\\nDecomposition, manifold learning, and clustering are essential tools to further your\\nunderstanding of your data, and can be the only ways to make sense of your data in\\nthe absence of supervision information. Even in a supervised setting, exploratory\\ntools are important for a better understanding of the properties of the data. Often it is\\nhard to quantify the usefulness of an unsupervised algorithm, though this shouldn’t\\ndeter you from using them to gather insights from your data. With these methods\\nunder your belt, you are now equipped with all the essential learning algorithms that\\nmachine learning practitioners use every day.\\nWe encourage you to try clustering and decomposition methods both on two-\\ndimensional toy data and on real-world datasets included in scikit-learn, like the\\ndigits, iris, and cancer datasets.\\n208 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 222, 'page_label': '209'}, page_content='Summary of the Estimator Interface\\nLet’s briefly review the API that we introduced in Chapters 2 and 3. All algorithms in\\nscikit-learn, whether preprocessing, supervised learning, or unsupervised learning\\nalgorithms, are implemented as classes. These classes are called estimators in scikit-\\nlearn. To apply an algorithm, you first have to instantiate an object of the particular\\nclass:\\nIn[87]:\\nfrom sklearn.linear_model import LogisticRegression\\nlogreg = LogisticRegression()\\nThe estimator class contains the algorithm, and also stores the model that is learned\\nfrom data using the algorithm.\\nY ou should set any parameters of the model when constructing the model object.\\nThese parameters include regularization, complexity control, number of clusters to\\nfind, etc. All estimators have a fit method, which is used to build the model. The fit\\nmethod always requires as its first argument the data X, represented as a NumPy array\\nor a SciPy sparse matrix, where each row represents a single data point. The data X is\\nalways assumed to be a NumPy array or SciPy sparse matrix that has continuous\\n(floating-point) entries. Supervised algorithms also require a y argument, which is a\\none-dimensional NumPy array containing target values for regression or classifica‐\\ntion (i.e., the known output labels or responses).\\nThere are two main ways to apply a learned model in scikit-learn. To create a pre‐\\ndiction in the form of a new output like y, you use the predict method. To create a\\nnew representation of the input data X, you use the transform method. Table 3-1\\nsummarizes the use cases of the predict and transform methods.\\nTable 3-1. scikit-learn API summary\\nestimator.fit(x_train, [y_train])\\nestimator.predict(X_text) estimator.transform(X_test)\\nClassification Preprocessing\\nRegression Dimensionality reduction\\nClustering Feature extraction\\n Feature selection\\nAdditionally, all supervised models have a score(X_test, y_test)  method that\\nallows an evaluation of the model. In Table 3-1, X_train and y_train refer to the\\ntraining data and training labels, while X_test and y_test refer to the test data and\\ntest labels (if applicable).\\nSummary and Outlook | 209'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 223, 'page_label': '210'}, page_content=''),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 224, 'page_label': '211'}, page_content='CHAPTER 4\\nRepresenting Data and\\nEngineering Features\\nSo far, we’ve assumed that our data comes in as a two-dimensional array of floating-\\npoint numbers, where each column is a continuous feature that describes the data\\npoints. For many applications, this is not how the data is collected. A particularly\\ncommon type of feature is the categorical features. Also known as discrete features,\\nthese are usually not numeric. The distinction between categorical features and con‐\\ntinuous features is analogous to the distinction between classification and regression,\\nonly on the input side rather than the output side. Examples of continuous features\\nthat we have seen are pixel brightnesses and size measurements of plant flowers.\\nExamples of categorical features are the brand of a product, the color of a product, or\\nthe department (books, clothing, hardware) it is sold in. These are all properties that\\ncan describe a product, but they don’t vary in a continuous way. A product belongs\\neither in the clothing department or in the books department. There is no middle\\nground between books and clothing, and no natural order for the different categories\\n(books is not greater or less than clothing, hardware is not between books and cloth‐\\ning, etc.).\\nRegardless of the types of features your data consists of, how you represent them can\\nhave an enormous effect on the performance of machine learning models. We saw in\\nChapters 2 and 3 that scaling of the data is important. In other words, if you don’t\\nrescale your data (say, to unit variance), then it makes a difference whether you repre‐\\nsent a measurement in centimeters or inches. We also saw in Chapter 2 that it can be\\nhelpful to augment your data with additional features, like adding interactions (prod‐\\nucts) of features or more general polynomials.\\nThe question of how to represent your data best for a particular application is known\\nas feature engineering, and it is one of the main tasks of data scientists and machine\\n211'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 225, 'page_label': '212'}, page_content='learning practitioners trying to solve real-world problems. Representing your data in\\nthe right way can have a bigger influence on the performance of a supervised model\\nthan the exact parameters you choose.\\nIn this chapter, we will first go over the important and very common case of categori‐\\ncal features, and then give some examples of helpful transformations for specific\\ncombinations of features and models.\\nCategorical Variables\\nAs an example, we will use the dataset of adult incomes in the United States, derived\\nfrom the 1994 census database. The task of the adult dataset is to predict whether a\\nworker has an income of over $50,000 or under $50,000. The features in this dataset\\ninclude the workers’ ages, how they are employed (self employed, private industry\\nemployee, government employee, etc.), their education, their gender, their working\\nhours per week, occupation, and more. Table 4-1 shows the first few entries in the\\ndataset.\\nTable 4-1. The first few entries in the adult dataset\\nage workclass education gender hours-per-week occupation income\\n0 39 State-gov Bachelors Male 40 Adm-clerical <=50K\\n1 50 Self-emp-not-inc Bachelors Male 13 Exec-managerial <=50K\\n2 38 Private HS-grad Male 40 Handlers-cleaners <=50K\\n3 53 Private 11th Male 40 Handlers-cleaners <=50K\\n4 28 Private Bachelors Female 40 Prof-specialty <=50K\\n5 37 Private Masters Female 40 Exec-managerial <=50K\\n6 49 Private 9th Female 16 Other-service <=50K\\n7 52 Self-emp-not-inc HS-grad Male 45 Exec-managerial >50K\\n8 31 Private Masters Female 50 Prof-specialty >50K\\n9 42 Private Bachelors Male 40 Exec-managerial >50K\\n10 37 Private Some-college Male 80 Exec-managerial >50K\\nThe task is phrased as a classification task with the two classes being income <=50k\\nand >50k. It would also be possible to predict the exact income, and make this a\\nregression task. However, that would be much more difficult, and the 50K division is\\ninteresting to understand on its own.\\nIn this dataset, age and hours-per-week are continuous features, which we know\\nhow to treat. The workclass, education, sex, and occupation features are categori‐\\ncal, however. All of them come from a fixed list of possible values, as opposed to a\\nrange, and denote a qualitative property, as opposed to a quantity.\\n212 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 226, 'page_label': '213'}, page_content='As a starting point, let’s say we want to learn a logistic regression classifier on this\\ndata. We know from Chapter 2 that a logistic regression makes predictions, ŷ, using\\nthe following formula:\\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0\\nwhere w[i] and b are coefficients learned from the training set and x[i] are the input\\nfeatures. This formula makes sense when x[i] are numbers, but not when x[2] is\\n\"Masters\" or \"Bachelors\". Clearly we need to represent our data in some different\\nway when applying logistic regression. The next section will explain how we can\\novercome this problem.\\nOne-Hot-Encoding (Dummy Variables)\\nBy far the most common way to represent categorical variables is using the one-hot-\\nencoding or one-out-of-N encoding, also known as dummy variables. The idea behind\\ndummy variables is to replace a categorical variable with one or more new features\\nthat can have the values 0 and 1. The values 0 and 1 make sense in the formula for\\nlinear binary classification (and for all other models in scikit-learn), and we can\\nrepresent any number of categories by introducing one new feature per category, as\\ndescribed here.\\nLet’s say for the workclass feature we have possible values of \"Government\\nEmployee\", \"Private Employee\", \"Self Employed\", and \"Self Employed Incorpo\\nrated\". To encode these four possible values, we create four new features, called \"Gov\\nernment Employee\", \"Private Employee\", \"Self Employed\", and \"Self Employed\\nIncorporated\". A feature is 1 if workclass for this person has the corresponding\\nvalue and 0 otherwise, so exactly one of the four new features will be 1 for each data\\npoint. This is why this is called one-hot or one-out-of-N encoding.\\nThe principle is illustrated in Table 4-2. A single feature is encoded using four new\\nfeatures. When using this data in a machine learning algorithm, we would drop the\\noriginal workclass feature and only keep the 0–1 features.\\nTable 4-2. Encoding the workclass feature using one-hot encoding\\nworkclass Government Employee Private Employee Self Employed Self Employed Incorporated\\nGovernment Employee 1 0 0 0\\nPrivate Employee 0 1 0 0\\nSelf Employed 0 0 1 0\\nSelf Employed Incorporated 0 0 0 1\\nCategorical Variables | 213'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 227, 'page_label': '214'}, page_content='The one-hot encoding we use is quite similar, but not identical, to\\nthe dummy encoding used in statistics. For simplicity, we encode\\neach category with a different binary feature. In statistics, it is com‐\\nmon to encode a categorical feature with k different possible values\\ninto k–1 features (the last one is represented as all zeros). This is\\ndone to simplify the analysis (more technically, this will avoid mak‐\\ning the data matrix rank-deficient).\\nThere are two ways to convert your data to a one-hot encoding of categorical vari‐\\nables, using either pandas or scikit-learn. At the time of writing, using pandas is\\nslightly easier, so let’s go this route. First we load the data using pandas from a\\ncomma-separated values (CSV) file:\\nIn[2]:\\nimport pandas as pd\\n# The file has no headers naming the columns, so we pass header=None\\n# and provide the column names explicitly in \"names\"\\ndata = pd.read_csv(\\n    \"/home/andy/datasets/adult.data\", header=None, index_col=False,\\n    names=[\\'age\\', \\'workclass\\', \\'fnlwgt\\', \\'education\\',  \\'education-num\\',\\n           \\'marital-status\\', \\'occupation\\', \\'relationship\\', \\'race\\', \\'gender\\',\\n           \\'capital-gain\\', \\'capital-loss\\', \\'hours-per-week\\', \\'native-country\\',\\n           \\'income\\'])\\n# For illustration purposes, we only select some of the columns\\ndata = data[[\\'age\\', \\'workclass\\', \\'education\\', \\'gender\\', \\'hours-per-week\\',\\n             \\'occupation\\', \\'income\\']]\\n# IPython.display allows nice output formatting within the Jupyter notebook\\ndisplay(data.head())\\nTable 4-3 shows the result.\\nTable 4-3. The first five rows of the adult dataset\\nage workclass education gender hours-per-week occupation income\\n0 39 State-gov Bachelors Male 40 Adm-clerical <=50K\\n1 50 Self-emp-not-inc Bachelors Male 13 Exec-managerial <=50K\\n2 38 Private HS-grad Male 40 Handlers-cleaners <=50K\\n3 53 Private 11th Male 40 Handlers-cleaners <=50K\\n4 28 Private Bachelors Female 40 Prof-specialty <=50K\\nChecking string-encoded categorical data\\nAfter reading a dataset like this, it is often good to first check if a column actually\\ncontains meaningful categorical data. When working with data that was input by\\nhumans (say, users on a website), there might not be a fixed set of categories, and dif‐\\nferences in spelling and capitalization might require preprocessing. For example, it\\nmight be that some people specified gender as “male” and some as “man, ” and we\\n214 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 228, 'page_label': '215'}, page_content='might want to represent these two inputs using the same category. A good way to\\ncheck the contents of a column is using the value_counts function of a pandas\\nSeries (the type of a single column in a DataFrame), to show us what the unique val‐\\nues are and how often they appear:\\nIn[3]:\\nprint(data.gender.value_counts())\\nOut[3]:\\n Male      21790\\n Female    10771\\nName: gender, dtype: int64\\nWe can see that there are exactly two values for gender in this dataset, Male and\\nFemale, meaning the data is already in a good format to be represented using one-\\nhot-encoding. In a real application, you should look at all columns and check their\\nvalues. We will skip this here for brevity’s sake.\\nThere is a very simple way to encode the data in pandas, using the get_dummies func‐\\ntion. The get_dummies function automatically transforms all columns that have\\nobject type (like strings) or are categorical (which is a special pandas concept that we\\nhaven’t talked about yet):\\nIn[4]:\\nprint(\"Original features:\\\\n\", list(data.columns), \"\\\\n\")\\ndata_dummies = pd.get_dummies(data)\\nprint(\"Features after get_dummies:\\\\n\", list(data_dummies.columns))\\nOut[4]:\\nOriginal features:\\n [\\'age\\', \\'workclass\\', \\'education\\', \\'gender\\', \\'hours-per-week\\', \\'occupation\\',\\n  \\'income\\']\\nFeatures after get_dummies:\\n [\\'age\\', \\'hours-per-week\\', \\'workclass_ ?\\', \\'workclass_ Federal-gov\\',\\n  \\'workclass_ Local-gov\\', \\'workclass_ Never-worked\\', \\'workclass_ Private\\',\\n  \\'workclass_ Self-emp-inc\\', \\'workclass_ Self-emp-not-inc\\',\\n  \\'workclass_ State-gov\\', \\'workclass_ Without-pay\\', \\'education_ 10th\\',\\n  \\'education_ 11th\\', \\'education_ 12th\\', \\'education_ 1st-4th\\',\\n   ...\\n  \\'education_ Preschool\\', \\'education_ Prof-school\\', \\'education_ Some-college\\',\\n  \\'gender_ Female\\', \\'gender_ Male\\', \\'occupation_ ?\\',\\n  \\'occupation_ Adm-clerical\\', \\'occupation_ Armed-Forces\\',\\n  \\'occupation_ Craft-repair\\', \\'occupation_ Exec-managerial\\',\\n  \\'occupation_ Farming-fishing\\', \\'occupation_ Handlers-cleaners\\',\\n  ...\\n  \\'occupation_ Tech-support\\', \\'occupation_ Transport-moving\\',\\n  \\'income_ <=50K\\', \\'income_ >50K\\']\\nCategorical Variables | 215'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 229, 'page_label': '216'}, page_content='Y ou can see that the continuous features age and hours-per-week were not touched,\\nwhile the categorical features were expanded into one new feature for each possible\\nvalue:\\nIn[5]:\\ndata_dummies.head()\\nOut[5]:\\nage hours-\\nper-\\nweek\\nworkclass_ ? workclass_\\nFederal-\\ngov\\nworkclass_\\nLocal-gov\\n… occupation_\\nTech-\\nsupport\\noccupation_\\nTransport-\\nmoving\\nincome_\\n<=50K\\nincome_\\n>50K\\n0 39 40 0.0 0.0 0.0 … 0.0 0.0 1.0 0.0\\n1 50 13 0.0 0.0 0.0 … 0.0 0.0 1.0 0.0\\n2 38 40 0.0 0.0 0.0 … 0.0 0.0 1.0 0.0\\n3 53 40 0.0 0.0 0.0 … 0.0 0.0 1.0 0.0\\n4 28 40 0.0 0.0 0.0 … 0.0 0.0 1.0 0.0\\n5 rows × 46 columns\\nWe can now use the values attribute to convert the data_dummies DataFrame into a\\nNumPy array, and then train a machine learning model on it. Be careful to separate\\nthe target variable (which is now encoded in two income columns) from the data\\nbefore training a model. Including the output variable, or some derived property of\\nthe output variable, into the feature representation is a very common mistake in\\nbuilding supervised machine learning models.\\nBe careful: column indexing in pandas includes the end of the\\nrange, so \\'age\\':\\'occupation_ Transport-moving\\' is inclusive of\\noccupation_ Transport-moving . This is different from slicing a\\nNumPy array, where the end of a range is not included: for exam‐\\nple, np.arange(11)[0:10] doesn’t include the entry with index 10.\\nIn this case, we extract only the columns containing features—that is, all columns\\nfrom age to occupation_ Transport-moving. This range contains all the features but\\nnot the target:\\nIn[6]:\\nfeatures = data_dummies.ix[:, \\'age\\':\\'occupation_ Transport-moving\\']\\n# Extract NumPy arrays\\nX = features.values\\ny = data_dummies[\\'income_ >50K\\'].values\\nprint(\"X.shape: {}  y.shape: {}\".format(X.shape, y.shape))\\n216 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 230, 'page_label': '217'}, page_content='Out[6]:\\nX.shape: (32561, 44)  y.shape: (32561,)\\nNow the data is represented in a way that scikit-learn can work with, and we can\\nproceed as usual:\\nIn[7]:\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\nlogreg = LogisticRegression()\\nlogreg.fit(X_train, y_train)\\nprint(\"Test score: {:.2f}\".format(logreg.score(X_test, y_test)))\\nOut[7]:\\nTest score: 0.81\\nIn this example, we called get_dummies on a DataFrame containing\\nboth the training and the test data. This is important to ensure cat‐\\negorical values are represented in the same way in the training set\\nand the test set.\\nImagine we have the training and test sets in two different Data\\nFrames. If the \"Private Employee\" value for the workclass feature\\ndoes not appear in the test set, pandas will assume there are only\\nthree possible values for this feature and will create only three new\\ndummy features. Now our training and test sets have different\\nnumbers of features, and we can’t apply the model we learned on\\nthe training set to the test set anymore. Even worse, imagine the\\nworkclass feature has the values \"Government Employee\"  and\\n\"Private Employee\" in the training set, and \"Self Employed\" and\\n\"Self Employed Incorporated\"  in the test set. In both cases,\\npandas will create two new dummy features, so the encoded Data\\nFrames will have the same number of features. However, the two\\ndummy features have entirely different meanings in the training\\nand test sets. The column that means \"Government Employee\" for\\nthe training set would encode \"Self Employed\" for the test set.\\nIf we built a machine learning model on this data it would work\\nvery badly, because it would assume the columns mean the same\\nthings (because they are in the same position) when in fact they\\nmean very different things. To fix this, either call get_dummies on a\\nDataFrame that contains both the training and the test data points,\\nor make sure that the column names are the same for the training\\nand test sets after calling get_dummies, to ensure they have the\\nsame semantics.\\nCategorical Variables | 217'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 231, 'page_label': '218'}, page_content='Numbers Can Encode Categoricals\\nIn the example of the adult dataset, the categorical variables were encoded as strings.\\nOn the one hand, that opens up the possibility of spelling errors, but on the other\\nhand, it clearly marks a variable as categorical. Often, whether for ease of storage or\\nbecause of the way the data is collected, categorical variables are encoded as integers.\\nFor example, imagine the census data in the adult dataset was collected using a ques‐\\ntionnaire, and the answers for workclass were recorded as 0 (first box ticked), 1 (sec‐\\nond box ticked), 2 (third box ticked), and so on. Now the column will contain\\nnumbers from 0 to 8, instead of strings like \"Private\", and it won’t be immediately\\nobvious to someone looking at the table representing the dataset whether they should\\ntreat this variable as continuous or categorical. Knowing that the numbers indicate\\nemployment status, however, it is clear that these are very distinct states and should\\nnot be modeled by a single continuous variable.\\nCategorical features are often encoded using integers. That they are\\nnumbers doesn’t mean that they should necessarily be treated as\\ncontinuous features. It is not always clear whether an integer fea‐\\nture should be treated as continuous or discrete (and one-hot-\\nencoded). If there is no ordering between the semantics that are\\nencoded (like in the workclass example), the feature must be\\ntreated as discrete. For other cases, like five-star ratings, the better\\nencoding depends on the particular task and data and which\\nmachine learning algorithm is used.\\nThe get_dummies function in pandas treats all numbers as continuous and will not\\ncreate dummy variables for them. To get around this, you can either use scikit-\\nlearn’s OneHotEncoder, for which you can specify which variables are continuous\\nand which are discrete, or convert numeric columns in the DataFrame to strings. To\\nillustrate, let’s create a DataFrame object with two columns, one containing strings\\nand one containing integers:\\nIn[8]:\\n# create a DataFrame with an integer feature and a categorical string feature\\ndemo_df = pd.DataFrame({\\'Integer Feature\\': [0, 1, 2, 1],\\n                        \\'Categorical Feature\\': [\\'socks\\', \\'fox\\', \\'socks\\', \\'box\\']})\\ndisplay(demo_df)\\nTable 4-4 shows the result.\\n218 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 232, 'page_label': '219'}, page_content=\"Table 4-4. DataFrame containing categorical string features and integer features\\nCategorical Feature Integer Feature\\n0 socks 0\\n1 fox 1\\n2 socks 2\\n3 box 1\\nUsing get_dummies will only encode the string feature and will not change the integer\\nfeature, as you can see in Table 4-5:\\nIn[9]:\\npd.get_dummies(demo_df)\\nTable 4-5. One-hot-encoded version of the data from Table 4-4, leaving the integer feature\\nunchanged\\nInteger Feature Categorical Feature_box Categorical Feature_fox Categorical Feature_socks\\n0 0 0.0 0.0 1.0\\n1 1 0.0 1.0 0.0\\n2 2 0.0 0.0 1.0\\n3 1 1.0 0.0 0.0\\nIf you want dummy variables to be created for the “Integer Feature” column, you can\\nexplicitly list the columns you want to encode using the columns parameter. Then,\\nboth features will be treated as categorical (see Table 4-6):\\nIn[10]:\\ndemo_df['Integer Feature'] = demo_df['Integer Feature'].astype(str)\\npd.get_dummies(demo_df, columns=['Integer Feature', 'Categorical Feature'])\\nTable 4-6. One-hot encoding of the data shown in Table 4-4, encoding the integer and string\\nfeatures\\nInteger\\nFeature_0\\nInteger\\nFeature_1\\nInteger\\nFeature_2\\nCategorical\\nFeature_box\\nCategorical\\nFeature_fox\\nCategorical\\nFeature_socks\\n0 1.0 0.0 0.0 0.0 0.0 1.0\\n1 0.0 1.0 0.0 0.0 1.0 0.0\\n2 0.0 0.0 1.0 0.0 0.0 1.0\\n3 0.0 1.0 0.0 1.0 0.0 0.0\\nCategorical Variables | 219\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 233, 'page_label': '220'}, page_content='Binning, Discretization, Linear Models, and Trees\\nThe best way to represent data depends not only on the semantics of the data, but also\\non the kind of model you are using. Linear models and tree-based models (such as\\ndecision trees, gradient boosted trees, and random forests), two large and very com‐\\nmonly used families, have very different properties when it comes to how they work\\nwith different feature representations. Let’s go back to the wave regression dataset that\\nwe used in Chapter 2. It has only a single input feature. Here is a comparison of a\\nlinear regression model and a decision tree regressor on this dataset (see Figure 4-1):\\nIn[11]:\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.tree import DecisionTreeRegressor\\nX, y = mglearn.datasets.make_wave(n_samples=100)\\nline = np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)\\nreg = DecisionTreeRegressor(min_samples_split=3).fit(X, y)\\nplt.plot(line, reg.predict(line), label=\"decision tree\")\\nreg = LinearRegression().fit(X, y)\\nplt.plot(line, reg.predict(line), label=\"linear regression\")\\nplt.plot(X[:, 0], y, \\'o\\', c=\\'k\\')\\nplt.ylabel(\"Regression output\")\\nplt.xlabel(\"Input feature\")\\nplt.legend(loc=\"best\")\\nAs you know, linear models can only model linear relationships, which are lines in\\nthe case of a single feature. The decision tree can build a much more complex model\\nof the data. However, this is strongly dependent on the representation of the data.\\nOne way to make linear models more powerful on continuous data is to use binning\\n(also known as discretization) of the feature to split it up into multiple features, as\\ndescribed here.\\n220 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 234, 'page_label': '221'}, page_content='Figure 4-1. Comparing linear regression and a decision tree on the wave dataset\\nWe imagine a partition of the input range for the feature (in this case, the numbers\\nfrom –3 to 3) into a fixed number of bins—say, 10. A data point will then be repre‐\\nsented by which bin it falls into. To determine this, we first have to define the bins. In\\nthis case, we’ll define 10 bins equally spaced between –3 and 3. We use the\\nnp.linspace function for this, creating 11 entries, which will create 10 bins—they are\\nthe spaces in between two consecutive boundaries:\\nIn[12]:\\nbins = np.linspace(-3, 3, 11)\\nprint(\"bins: {}\".format(bins))\\nOut[12]:\\nbins: [-3.  -2.4 -1.8 -1.2 -0.6  0.   0.6  1.2  1.8  2.4  3. ]\\nHere, the first bin contains all data points with feature values –3 to –2.68, the second\\nbin contains all points with feature values from –2.68 to –2.37, and so on.\\nNext, we record for each data point which bin it falls into. This can be easily compu‐\\nted using the np.digitize function:\\nBinning, Discretization, Linear Models, and Trees | 221'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 235, 'page_label': '222'}, page_content='In[13]:\\nwhich_bin = np.digitize(X, bins=bins)\\nprint(\"\\\\nData points:\\\\n\", X[:5])\\nprint(\"\\\\nBin membership for data points:\\\\n\", which_bin[:5])\\nOut[13]:\\nData points:\\n [[-0.753]\\n  [ 2.704]\\n  [ 1.392]\\n  [ 0.592]\\n [-2.064]]\\nBin membership for data points:\\n [[ 4]\\n  [10]\\n  [ 8]\\n  [ 6]\\n [ 2]]\\nWhat we did here is transform the single continuous input feature in the wave dataset\\ninto a categorical feature that encodes which bin a data point is in. To use a scikit-\\nlearn model on this data, we transform this discrete feature to a one-hot encoding\\nusing the OneHotEncoder from the preprocessing module. The OneHotEncoder does\\nthe same encoding as pandas.get_dummies, though it currently only works on cate‐\\ngorical variables that are integers:\\nIn[14]:\\nfrom sklearn.preprocessing import OneHotEncoder\\n# transform using the OneHotEncoder\\nencoder = OneHotEncoder(sparse=False)\\n# encoder.fit finds the unique values that appear in which_bin\\nencoder.fit(which_bin)\\n# transform creates the one-hot encoding\\nX_binned = encoder.transform(which_bin)\\nprint(X_binned[:5])\\nOut[14]:\\n[[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\\n [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\\n [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\\n [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]\\nBecause we specified 10 bins, the transformed dataset X_binned now is made up of 10\\nfeatures:\\n222 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 236, 'page_label': '223'}, page_content='In[15]:\\nprint(\"X_binned.shape: {}\".format(X_binned.shape))\\nOut[15]:\\nX_binned.shape: (100, 10)\\nNow we build a new linear regression model and a new decision tree model on the\\none-hot-encoded data. The result is visualized in Figure 4-2 , together with the bin\\nboundaries, shown as dotted black lines:\\nIn[16]:\\nline_binned = encoder.transform(np.digitize(line, bins=bins))\\nreg = LinearRegression().fit(X_binned, y)\\nplt.plot(line, reg.predict(line_binned), label=\\'linear regression binned\\')\\nreg = DecisionTreeRegressor(min_samples_split=3).fit(X_binned, y)\\nplt.plot(line, reg.predict(line_binned), label=\\'decision tree binned\\')\\nplt.plot(X[:, 0], y, \\'o\\', c=\\'k\\')\\nplt.vlines(bins, -3, 3, linewidth=1, alpha=.2)\\nplt.legend(loc=\"best\")\\nplt.ylabel(\"Regression output\")\\nplt.xlabel(\"Input feature\")\\nFigure 4-2. Comparing linear regression and decision tree regression on binned features\\nBinning, Discretization, Linear Models, and Trees | 223'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 237, 'page_label': '224'}, page_content=\"The dashed line and solid line are exactly on top of each other, meaning the linear\\nregression model and the decision tree make exactly the same predictions. For each\\nbin, they predict a constant value. As features are constant within each bin, any\\nmodel must predict the same value for all points within a bin. Comparing what the\\nmodels learned before binning the features and after, we see that the linear model\\nbecame much more flexible, because it now has a different value for each bin, while\\nthe decision tree model got much less flexible. Binning features generally has no ben‐\\neficial effect for tree-based models, as these models can learn to split up the data any‐\\nwhere. In a sense, that means decision trees can learn whatever binning is most useful\\nfor predicting on this data. Additionally, decision trees look at multiple features at\\nonce, while binning is usually done on a per-feature basis. However, the linear model\\nbenefited greatly in expressiveness from the transformation of the data.\\nIf there are good reasons to use a linear model for a particular dataset—say, because it\\nis very large and high-dimensional, but some features have nonlinear relations with\\nthe output—binning can be a great way to increase modeling power.\\nInteractions and Polynomials\\nAnother way to enrich a feature representation, particularly for linear models, is\\nadding interaction features and polynomial features of the original data. This kind of\\nfeature engineering is often used in statistical modeling, but it’s also common in many\\npractical machine learning applications.\\nAs a first example, look again at Figure 4-2. The linear model learned a constant value\\nfor each bin in the wave dataset. We know, however, that linear models can learn not\\nonly offsets, but also slopes. One way to add a slope to the linear model on the binned\\ndata is to add the original feature (the x-axis in the plot) back in. This leads to an 11-\\ndimensional dataset, as seen in Figure 4-3:\\nIn[17]:\\nX_combined = np.hstack([X, X_binned])\\nprint(X_combined.shape)\\nOut[17]:\\n(100, 11)\\nIn[18]:\\nreg = LinearRegression().fit(X_combined, y)\\nline_combined = np.hstack([line, line_binned])\\nplt.plot(line, reg.predict(line_combined), label='linear regression combined')\\nfor bin in bins:\\n    plt.plot([bin, bin], [-3, 3], ':', c='k')\\n224 | Chapter 4: Representing Data and Engineering Features\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 238, 'page_label': '225'}, page_content='plt.legend(loc=\"best\")\\nplt.ylabel(\"Regression output\")\\nplt.xlabel(\"Input feature\")\\nplt.plot(X[:, 0], y, \\'o\\', c=\\'k\\')\\nFigure 4-3. Linear regression using binned features and a single global slope\\nIn this example, the model learned an offset for each bin, together with a slope. The\\nlearned slope is downward, and shared across all the bins—there is a single x-axis fea‐\\nture, which has a single slope. Because the slope is shared across all bins, it doesn’t\\nseem to be very helpful. We would rather have a separate slope for each bin! We can\\nachieve this by adding an interaction or product feature that indicates which bin a\\ndata point is in and where it lies on the x-axis. This feature is a product of the bin\\nindicator and the original feature. Let’s create this dataset:\\nIn[19]:\\nX_product = np.hstack([X_binned, X * X_binned])\\nprint(X_product.shape)\\nOut[19]:\\n(100, 20)\\nThe dataset now has 20 features: the indicators for which bin a data point is in, and a\\nproduct of the original feature and the bin indicator. Y ou can think of the product\\nInteractions and Polynomials | 225'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 239, 'page_label': '226'}, page_content='feature as a separate copy of the x-axis feature for each bin. It is the original feature\\nwithin the bin, and zero everywhere else. Figure 4-4  shows the result of the linear\\nmodel on this new representation:\\nIn[20]:\\nreg = LinearRegression().fit(X_product, y)\\nline_product = np.hstack([line_binned, line * line_binned])\\nplt.plot(line, reg.predict(line_product), label=\\'linear regression product\\')\\nfor bin in bins:\\n    plt.plot([bin, bin], [-3, 3], \\':\\', c=\\'k\\')\\nplt.plot(X[:, 0], y, \\'o\\', c=\\'k\\')\\nplt.ylabel(\"Regression output\")\\nplt.xlabel(\"Input feature\")\\nplt.legend(loc=\"best\")\\nFigure 4-4. Linear regression with a separate slope per bin\\nAs you can see, now each bin has its own offset and slope in this model.\\n226 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 240, 'page_label': '227'}, page_content='Using binning is one way to expand a continuous feature. Another one is to use poly‐\\nnomials of the original features. For a given feature x, we might want to consider\\nx ** 2, x ** 3, x ** 4, and so on. This is implemented in PolynomialFeatures in\\nthe preprocessing module:\\nIn[21]:\\nfrom sklearn.preprocessing import PolynomialFeatures\\n# include polynomials up to x ** 10:\\n# the default \"include_bias=True\" adds a feature that\\'s constantly 1\\npoly = PolynomialFeatures(degree=10, include_bias=False)\\npoly.fit(X)\\nX_poly = poly.transform(X)\\nUsing a degree of 10 yields 10 features:\\nIn[22]:\\nprint(\"X_poly.shape: {}\".format(X_poly.shape))\\nOut[22]:\\nX_poly.shape: (100, 10)\\nLet’s compare the entries of X_poly to those of X:\\nIn[23]:\\nprint(\"Entries of X:\\\\n{}\".format(X[:5]))\\nprint(\"Entries of X_poly:\\\\n{}\".format(X_poly[:5]))\\nOut[23]:\\nEntries of X:\\n[[-0.753]\\n [ 2.704]\\n [ 1.392]\\n [ 0.592]\\n [-2.064]]\\nEntries of X_poly:\\n[[    -0.753      0.567     -0.427      0.321     -0.242      0.182\\n      -0.137      0.103     -0.078      0.058]\\n [     2.704      7.313     19.777     53.482    144.632    391.125\\n    1057.714   2860.360   7735.232  20918.278]\\n [     1.392      1.938      2.697      3.754      5.226      7.274\\n      10.125     14.094     19.618     27.307]\\n [     0.592      0.350      0.207      0.123      0.073      0.043\\n       0.025      0.015      0.009      0.005]\\n [    -2.064      4.260     -8.791     18.144    -37.448     77.289\\n    -159.516    329.222   -679.478   1402.367]]\\nY ou can obtain the semantics of the features by calling the get_feature_names\\nmethod, which provides the exponent for each feature:\\nInteractions and Polynomials | 227'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 241, 'page_label': '228'}, page_content='In[24]:\\nprint(\"Polynomial feature names:\\\\n{}\".format(poly.get_feature_names()))\\nOut[24]:\\nPolynomial feature names:\\n[\\'x0\\', \\'x0^2\\', \\'x0^3\\', \\'x0^4\\', \\'x0^5\\', \\'x0^6\\', \\'x0^7\\', \\'x0^8\\', \\'x0^9\\', \\'x0^10\\']\\nY ou can see that the first column of X_poly corresponds exactly to X, while the other\\ncolumns are the powers of the first entry. It’s interesting to see how large some of the\\nvalues can get. The second column has entries above 20,000, orders of magnitude dif‐\\nferent from the rest.\\nUsing polynomial features together with a linear regression model yields the classical\\nmodel of polynomial regression (see Figure 4-5):\\nIn[26]:\\nreg = LinearRegression().fit(X_poly, y)\\nline_poly = poly.transform(line)\\nplt.plot(line, reg.predict(line_poly), label=\\'polynomial linear regression\\')\\nplt.plot(X[:, 0], y, \\'o\\', c=\\'k\\')\\nplt.ylabel(\"Regression output\")\\nplt.xlabel(\"Input feature\")\\nplt.legend(loc=\"best\")\\nFigure 4-5. Linear regression with tenth-degree polynomial features\\n228 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 242, 'page_label': '229'}, page_content='As you can see, polynomial features yield a very smooth fit on this one-dimensional\\ndata. However, polynomials of high degree tend to behave in extreme ways on the\\nboundaries or in regions with little data.\\nAs a comparison, here is a kernel SVM model learned on the original data, without\\nany transformation (see Figure 4-6):\\nIn[26]:\\nfrom sklearn.svm import SVR\\nfor gamma in [1, 10]:\\n    svr = SVR(gamma=gamma).fit(X, y)\\n    plt.plot(line, svr.predict(line), label=\\'SVR gamma={}\\'.format(gamma))\\nplt.plot(X[:, 0], y, \\'o\\', c=\\'k\\')\\nplt.ylabel(\"Regression output\")\\nplt.xlabel(\"Input feature\")\\nplt.legend(loc=\"best\")\\nFigure 4-6. Comparison of different gamma parameters for an SVM with RBF kernel\\nUsing a more complex model, a kernel SVM, we are able to learn a similarly complex\\nprediction to the polynomial regression without an explicit transformation of the\\nfeatures.\\nInteractions and Polynomials | 229'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 243, 'page_label': '230'}, page_content='As a more realistic application of interactions and polynomials, let’s look again at the\\nBoston Housing dataset. We already used polynomial features on this dataset in\\nChapter 2. Now let’s have a look at how these features were constructed, and at how\\nmuch the polynomial features help. First we load the data, and rescale it to be\\nbetween 0 and 1 using MinMaxScaler:\\nIn[27]:\\nfrom sklearn.datasets import load_boston\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import MinMaxScaler\\nboston = load_boston()\\nX_train, X_test, y_train, y_test = train_test_split\\n    (boston.data, boston.target, random_state=0)\\n# rescale data\\nscaler = MinMaxScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\nNow, we extract polynomial features and interactions up to a degree of 2:\\nIn[28]:\\npoly = PolynomialFeatures(degree=2).fit(X_train_scaled)\\nX_train_poly = poly.transform(X_train_scaled)\\nX_test_poly = poly.transform(X_test_scaled)\\nprint(\"X_train.shape: {}\".format(X_train.shape))\\nprint(\"X_train_poly.shape: {}\".format(X_train_poly.shape))\\nOut[28]:\\nX_train.shape: (379, 13)\\nX_train_poly.shape: (379, 105)\\nThe data originally had 13 features, which were expanded into 105 interaction fea‐\\ntures. These new features represent all possible interactions between two different\\noriginal features, as well as the square of each original feature. degree=2 here means\\nthat we look at all features that are the product of up to two original features. The\\nexact correspondence between input and output features can be found using the\\nget_feature_names method:\\nIn[29]:\\nprint(\"Polynomial feature names:\\\\n{}\".format(poly.get_feature_names()))\\nOut[29]:\\nPolynomial feature names:\\n[\\'1\\', \\'x0\\', \\'x1\\', \\'x2\\', \\'x3\\', \\'x4\\', \\'x5\\', \\'x6\\', \\'x7\\', \\'x8\\', \\'x9\\', \\'x10\\',\\n\\'x11\\', \\'x12\\', \\'x0^2\\', \\'x0 x1\\', \\'x0 x2\\', \\'x0 x3\\', \\'x0 x4\\', \\'x0 x5\\', \\'x0 x6\\',\\n\\'x0 x7\\', \\'x0 x8\\', \\'x0 x9\\', \\'x0 x10\\', \\'x0 x11\\', \\'x0 x12\\', \\'x1^2\\', \\'x1 x2\\',\\n230 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 244, 'page_label': '231'}, page_content='\\'x1 x3\\', \\'x1 x4\\', \\'x1 x5\\', \\'x1 x6\\', \\'x1 x7\\', \\'x1 x8\\', \\'x1 x9\\', \\'x1 x10\\',\\n\\'x1 x11\\', \\'x1 x12\\', \\'x2^2\\', \\'x2 x3\\', \\'x2 x4\\', \\'x2 x5\\', \\'x2 x6\\', \\'x2 x7\\',\\n\\'x2 x8\\', \\'x2 x9\\', \\'x2 x10\\', \\'x2 x11\\', \\'x2 x12\\', \\'x3^2\\', \\'x3 x4\\', \\'x3 x5\\',\\n\\'x3 x6\\', \\'x3 x7\\', \\'x3 x8\\', \\'x3 x9\\', \\'x3 x10\\', \\'x3 x11\\', \\'x3 x12\\', \\'x4^2\\',\\n\\'x4 x5\\', \\'x4 x6\\', \\'x4 x7\\', \\'x4 x8\\', \\'x4 x9\\', \\'x4 x10\\', \\'x4 x11\\', \\'x4 x12\\',\\n\\'x5^2\\', \\'x5 x6\\', \\'x5 x7\\', \\'x5 x8\\', \\'x5 x9\\', \\'x5 x10\\', \\'x5 x11\\', \\'x5 x12\\',\\n\\'x6^2\\', \\'x6 x7\\', \\'x6 x8\\', \\'x6 x9\\', \\'x6 x10\\', \\'x6 x11\\', \\'x6 x12\\', \\'x7^2\\',\\n\\'x7 x8\\', \\'x7 x9\\', \\'x7 x10\\', \\'x7 x11\\', \\'x7 x12\\', \\'x8^2\\', \\'x8 x9\\', \\'x8 x10\\',\\n\\'x8 x11\\', \\'x8 x12\\', \\'x9^2\\', \\'x9 x10\\', \\'x9 x11\\', \\'x9 x12\\', \\'x10^2\\', \\'x10 x11\\',\\n\\'x10 x12\\', \\'x11^2\\', \\'x11 x12\\', \\'x12^2\\']\\nThe first new feature is a constant feature, called \"1\" here. The next 13 features are\\nthe original features (called \"x0\" to \"x12\"). Then follows the first feature squared\\n(\"x0^2\") and combinations of the first and the other features.\\nLet’s compare the performance using Ridge on the data with and without interac‐\\ntions:\\nIn[30]:\\nfrom sklearn.linear_model import Ridge\\nridge = Ridge().fit(X_train_scaled, y_train)\\nprint(\"Score without interactions: {:.3f}\".format(\\n    ridge.score(X_test_scaled, y_test)))\\nridge = Ridge().fit(X_train_poly, y_train)\\nprint(\"Score with interactions: {:.3f}\".format(\\n    ridge.score(X_test_poly, y_test)))\\nOut[30]:\\nScore without interactions: 0.621\\nScore with interactions: 0.753\\nClearly, the interactions and polynomial features gave us a good boost in perfor‐\\nmance when using Ridge. When using a more complex model like a random forest,\\nthe story is a bit different, though:\\nIn[31]:\\nfrom sklearn.ensemble import RandomForestRegressor\\nrf = RandomForestRegressor(n_estimators=100).fit(X_train_scaled, y_train)\\nprint(\"Score without interactions: {:.3f}\".format(\\n    rf.score(X_test_scaled, y_test)))\\nrf = RandomForestRegressor(n_estimators=100).fit(X_train_poly, y_train)\\nprint(\"Score with interactions: {:.3f}\".format(rf.score(X_test_poly, y_test)))\\nOut[31]:\\nScore without interactions: 0.799\\nScore with interactions: 0.763\\nInteractions and Polynomials | 231'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 245, 'page_label': '232'}, page_content='Y ou can see that even without additional features, the random forest beats the\\nperformance of Ridge. Adding interactions and polynomials actually decreases per‐\\nformance slightly.\\nUnivariate Nonlinear Transformations\\nWe just saw that adding squared or cubed features can help linear models for regres‐\\nsion. There are other transformations that often prove useful for transforming certain\\nfeatures: in particular, applying mathematical functions like log, exp, or sin. While\\ntree-based models only care about the ordering of the features, linear models and\\nneural networks are very tied to the scale and distribution of each feature, and if there\\nis a nonlinear relation between the feature and the target, that becomes hard to model\\n—particularly in regression. The functions log and exp can help by adjusting the rel‐\\native scales in the data so that they can be captured better by a linear model or neural\\nnetwork. We saw an application of that in Chapter 2 with the memory price data. The\\nsin and cos functions can come in handy when dealing with data that encodes peri‐\\nodic patterns.\\nMost models work best when each feature (and in regression also the target) is loosely\\nGaussian distributed—that is, a histogram of each feature should have something\\nresembling the familiar “bell curve” shape. Using transformations like log and exp is\\na hacky but simple and efficient way to achieve this. A particularly common case\\nwhen such a transformation can be helpful is when dealing with integer count data.\\nBy count data, we mean features like “how often did user A log in?” Counts are never\\nnegative, and often follow particular statistical patterns. We are using a synthetic\\ndataset of counts here that has properties similar to those you can find in the wild.\\nThe features are all integer-valued, while the response is continuous:\\nIn[32]:\\nrnd = np.random.RandomState(0)\\nX_org = rnd.normal(size=(1000, 3))\\nw = rnd.normal(size=3)\\nX = rnd.poisson(10 * np.exp(X_org))\\ny = np.dot(X_org, w)\\nLet’s look at the first 10 entries of the first feature. All are integer values and positive,\\nbut apart from that it’s hard to make out a particular pattern.\\nIf we count the appearance of each value, the distribution of values becomes clearer:\\n232 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 246, 'page_label': '233'}, page_content='In[33]:\\nprint(\"Number of feature appearances:\\\\n{}\".format(np.bincount(X[:, 0])))\\nOut[33]:\\nNumber of feature appearances:\\n[28 38 68 48 61 59 45 56 37 40 35 34 36 26 23 26 27 21 23 23 18 21 10  9 17\\n  9  7 14 12  7  3  8  4  5  5  3  4  2  4  1  1  3  2  5  3  8  2  5  2  1\\n  2  3  3  2  2  3  3  0  1  2  1  0  0  3  1  0  0  0  1  3  0  1  0  2  0\\n  1  1  0  0  0  0  1  0  0  2  2  0  1  1  0  0  0  0  1  1  0  0  0  0  0\\n  0  0  1  0  0  0  0  0  1  1  0  0  1  0  0  0  0  0  0  0  1  0  0  0  0\\n  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]\\nThe value 2 seems to be the most common, with 62 appearances ( bincount always\\nstarts at 0), and the counts for higher values fall quickly. However, there are some\\nvery high values, like 134 appearing twice. We visualize the counts in Figure 4-7:\\nIn[34]:\\nbins = np.bincount(X[:, 0])\\nplt.bar(range(len(bins)), bins, color=\\'w\\')\\nplt.ylabel(\"Number of appearances\")\\nplt.xlabel(\"Value\")\\nFigure 4-7. Histogram of feature values for X[0]\\nUnivariate Nonlinear Transformations | 233'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 247, 'page_label': '234'}, page_content='1 This is a Poisson distribution, which is quite fundamental to count data.\\nFeatures X[:, 1] and X[:, 2] have similar properties. This kind of distribution of\\nvalues (many small ones and a few very large ones) is very common in practice. 1\\nHowever, it is something most linear models can’t handle very well. Let’s try to fit a\\nridge regression to this model:\\nIn[35]:\\nfrom sklearn.linear_model import Ridge\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\nscore = Ridge().fit(X_train, y_train).score(X_test, y_test)\\nprint(\"Test score: {:.3f}\".format(score))\\nOut[35]:\\nTest score: 0.622\\nAs you can see from the relatively low R2 score, Ridge was not able to really capture\\nthe relationship between X and y. Applying a logarithmic transformation can help,\\nthough. Because the value 0 appears in the data (and the logarithm is not defined at\\n0), we can’t actually just apply log, but we have to compute log(X + 1):\\nIn[36]:\\nX_train_log = np.log(X_train + 1)\\nX_test_log = np.log(X_test + 1)\\nAfter the transformation, the distribution of the data is less asymmetrical and doesn’t\\nhave very large outliers anymore (see Figure 4-8):\\nIn[37]:\\nplt.hist(np.log(X_train_log[:, 0] + 1), bins=25, color=\\'gray\\')\\nplt.ylabel(\"Number of appearances\")\\nplt.xlabel(\"Value\")\\n234 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 248, 'page_label': '235'}, page_content='2 This is a very crude approximation of using Poisson regression, which would be the proper solution from a\\nprobabilistic standpoint.\\nFigure 4-8. Histogram of feature values for X[0] after logarithmic transformation\\nBuilding a ridge model on the new data provides a much better fit:\\nIn[38]:\\nscore = Ridge().fit(X_train_log, y_train).score(X_test_log, y_test)\\nprint(\"Test score: {:.3f}\".format(score))\\nOut[38]:\\nTest score: 0.875\\nFinding the transformation that works best for each combination of dataset and\\nmodel is somewhat of an art. In this example, all the features had the same properties.\\nThis is rarely the case in practice, and usually only a subset of the features should be\\ntransformed, or sometimes each feature needs to be transformed in a different way.\\nAs we mentioned earlier, these kinds of transformations are irrelevant for tree-based\\nmodels but might be essential for linear models. Sometimes it is also a good idea to\\ntransform the target variable y in regression. Trying to predict counts (say, number of\\norders) is a fairly common task, and using the log(y + 1)  transformation often\\nhelps.2\\nUnivariate Nonlinear Transformations | 235'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 249, 'page_label': '236'}, page_content='As you saw in the previous examples, binning, polynomials, and interactions can\\nhave a huge influence on how models perform on a given dataset. This is particularly\\ntrue for less complex models like linear models and naive Bayes models. Tree-based\\nmodels, on the other hand, are often able to discover important interactions them‐\\nselves, and don’t require transforming the data explicitly most of the time. Other\\nmodels, like SVMs, nearest neighbors, and neural networks, might sometimes benefit\\nfrom using binning, interactions, or polynomials, but the implications there are usu‐\\nally much less clear than in the case of linear models.\\nAutomatic Feature Selection\\nWith so many ways to create new features, you might get tempted to increase the\\ndimensionality of the data way beyond the number of original features. However,\\nadding more features makes all models more complex, and so increases the chance of\\noverfitting. When adding new features, or with high-dimensional datasets in general,\\nit can be a good idea to reduce the number of features to only the most useful ones,\\nand discard the rest. This can lead to simpler models that generalize better. But how\\ncan you know how good each feature is? There are three basic strategies: univariate\\nstatistics, model-based selection , and iterative selection. We will discuss all three of\\nthem in detail. All of these methods are supervised methods, meaning they need the\\ntarget for fitting the model. This means we need to split the data into training and test\\nsets, and fit the feature selection only on the training part of the data.\\nUnivariate Statistics\\nIn univariate statistics, we compute whether there is a statistically significant relation‐\\nship between each feature and the target. Then the features that are related with the\\nhighest confidence are selected. In the case of classification, this is also known as\\nanalysis of variance (ANOV A). A key property of these tests is that they are univari‐\\nate, meaning that they only consider each feature individually. Consequently, a fea‐\\nture will be discarded if it is only informative when combined with another feature.\\nUnivariate tests are often very fast to compute, and don’t require building a model.\\nOn the other hand, they are completely independent of the model that you might\\nwant to apply after the feature selection.\\nTo use univariate feature selection in scikit-learn, you need to choose a test, usu‐\\nally either f_classif (the default) for classification or f_regression for regression,\\nand a method to discard features based on the p-values determined in the test. All\\nmethods for discarding parameters use a threshold to discard all features with too\\nhigh a p-value (which means they are unlikely to be related to the target). The meth‐\\nods differ in how they compute this threshold, with the simplest ones being SelectKB\\nest, which selects a fixed number k of features, and SelectPercentile, which selects\\na fixed percentage of features. Let’s apply the feature selection for classification on the\\n236 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 250, 'page_label': '237'}, page_content='cancer dataset. To make the task a bit harder, we’ll add some noninformative noise\\nfeatures to the data. We expect the feature selection to be able to identify the features\\nthat are noninformative and remove them:\\nIn[39]:\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.feature_selection import SelectPercentile\\nfrom sklearn.model_selection import train_test_split\\ncancer = load_breast_cancer()\\n# get deterministic random numbers\\nrng = np.random.RandomState(42)\\nnoise = rng.normal(size=(len(cancer.data), 50))\\n# add noise features to the data\\n# the first 30 features are from the dataset, the next 50 are noise\\nX_w_noise = np.hstack([cancer.data, noise])\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X_w_noise, cancer.target, random_state=0, test_size=.5)\\n# use f_classif (the default) and SelectPercentile to select 50% of features\\nselect = SelectPercentile(percentile=50)\\nselect.fit(X_train, y_train)\\n# transform training set\\nX_train_selected = select.transform(X_train)\\nprint(\"X_train.shape: {}\".format(X_train.shape))\\nprint(\"X_train_selected.shape: {}\".format(X_train_selected.shape))\\nOut[39]:\\nX_train.shape: (284, 80)\\nX_train_selected.shape: (284, 40)\\nAs you can see, the number of features was reduced from 80 to 40 (50 percent of the\\noriginal number of features). We can find out which features have been selected using\\nthe get_support method, which returns a Boolean mask of the selected features\\n(visualized in Figure 4-9):\\nIn[40]:\\nmask = select.get_support()\\nprint(mask)\\n# visualize the mask -- black is True, white is False\\nplt.matshow(mask.reshape(1, -1), cmap=\\'gray_r\\')\\nplt.xlabel(\"Sample index\")\\nOut[40]:\\n[ True  True  True  True  True  True  True  True  True False  True False\\n  True  True  True  True  True  True False False  True  True  True  True\\n  True  True  True  True  True  True False False False  True False  True\\nAutomatic Feature Selection | 237'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 251, 'page_label': '238'}, page_content='False False  True False False False False  True False False  True False\\n False  True False  True False False False False False False  True False\\n  True False False False False  True False  True False False False False\\n  True  True False  True False False False False]\\nFigure 4-9. Features selected by SelectPercentile\\nAs you can see from the visualization of the mask, most of the selected features are\\nthe original features, and most of the noise features were removed. However, the\\nrecovery of the original features is not perfect. Let’s compare the performance of\\nlogistic regression on all features against the performance using only the selected\\nfeatures:\\nIn[41]:\\nfrom sklearn.linear_model import LogisticRegression\\n# transform test data\\nX_test_selected = select.transform(X_test)\\nlr = LogisticRegression()\\nlr.fit(X_train, y_train)\\nprint(\"Score with all features: {:.3f}\".format(lr.score(X_test, y_test)))\\nlr.fit(X_train_selected, y_train)\\nprint(\"Score with only selected features: {:.3f}\".format(\\n    lr.score(X_test_selected, y_test)))\\nOut[41]:\\nScore with all features: 0.930\\nScore with only selected features: 0.940\\nIn this case, removing the noise features improved performance, even though some\\nof the original features were lost. This was a very simple synthetic example, and out‐\\ncomes on real data are usually mixed. Univariate feature selection can still be very\\nhelpful, though, if there is such a large number of features that building a model on\\nthem is infeasible, or if you suspect that many features are completely uninformative.\\nModel-Based Feature Selection\\nModel-based feature selection uses a supervised machine learning model to judge the\\nimportance of each feature, and keeps only the most important ones. The supervised\\nmodel that is used for feature selection doesn’t need to be the same model that is used\\nfor the final supervised modeling. The feature selection model needs to provide some\\nmeasure of importance for each feature, so that they can be ranked by this measure.\\nDecision trees and decision tree–based models provide a feature_importances_\\n238 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 252, 'page_label': '239'}, page_content='attribute, which directly encodes the importance of each feature. Linear models have\\ncoefficients, which can also be used to capture feature importances by considering the\\nabsolute values. As we saw in Chapter 2, linear models with L1 penalty learn sparse\\ncoefficients, which only use a small subset of features. This can be viewed as a form of\\nfeature selection for the model itself, but can also be used as a preprocessing step to\\nselect features for another model. In contrast to univariate selection, model-based\\nselection considers all features at once, and so can capture interactions (if the model\\ncan capture them). To use model-based feature selection, we need to use the\\nSelectFromModel transformer:\\nIn[42]:\\nfrom sklearn.feature_selection import SelectFromModel\\nfrom sklearn.ensemble import RandomForestClassifier\\nselect = SelectFromModel(\\n    RandomForestClassifier(n_estimators=100, random_state=42),\\n    threshold=\"median\")\\nThe SelectFromModel class selects all features that have an importance measure of\\nthe feature (as provided by the supervised model) greater than the provided thresh‐\\nold. To get a comparable result to what we got with univariate feature selection, we\\nused the median as a threshold, so that half of the features will be selected. We use a\\nrandom forest classifier with 100 trees to compute the feature importances. This is a\\nquite complex model and much more powerful than using univariate tests. Now let’s\\nactually fit the model:\\nIn[43]:\\nselect.fit(X_train, y_train)\\nX_train_l1 = select.transform(X_train)\\nprint(\"X_train.shape: {}\".format(X_train.shape))\\nprint(\"X_train_l1.shape: {}\".format(X_train_l1.shape))\\nOut[43]:\\nX_train.shape: (284, 80)\\nX_train_l1.shape: (284, 40)\\nAgain, we can have a look at the features that were selected (Figure 4-10):\\nIn[44]:\\nmask = select.get_support()\\n# visualize the mask -- black is True, white is False\\nplt.matshow(mask.reshape(1, -1), cmap=\\'gray_r\\')\\nplt.xlabel(\"Sample index\")\\nFigure 4-10. Features selected by SelectFromModel using the RandomForestClassifier\\nAutomatic Feature Selection | 239'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 253, 'page_label': '240'}, page_content='This time, all but two of the original features were selected. Because we specified to\\nselect 40 features, some of the noise features are also selected. Let’s take a look at the\\nperformance:\\nIn[45]:\\nX_test_l1 = select.transform(X_test)\\nscore = LogisticRegression().fit(X_train_l1, y_train).score(X_test_l1, y_test)\\nprint(\"Test score: {:.3f}\".format(score))\\nOut[45]:\\nTest score: 0.951\\nWith the better feature selection, we also gained some improvements here.\\nIterative Feature Selection\\nIn univariate testing we used no model, while in model-based selection we used a sin‐\\ngle model to select features. In iterative feature selection, a series of models are built,\\nwith varying numbers of features. There are two basic methods: starting with no fea‐\\ntures and adding features one by one until some stopping criterion is reached, or\\nstarting with all features and removing features one by one until some stopping crite‐\\nrion is reached. Because a series of models are built, these methods are much more\\ncomputationally expensive than the methods we discussed previously. One particular\\nmethod of this kind is recursive feature elimination (RFE), which starts with all fea‐\\ntures, builds a model, and discards the least important feature according to the\\nmodel. Then a new model is built using all but the discarded feature, and so on until\\nonly a prespecified number of features are left. For this to work, the model used for\\nselection needs to provide some way to determine feature importance, as was the case\\nfor the model-based selection. Here, we use the same random forest model that we\\nused earlier, and get the results shown in Figure 4-11:\\nIn[46]:\\nfrom sklearn.feature_selection import RFE\\nselect = RFE(RandomForestClassifier(n_estimators=100, random_state=42),\\n             n_features_to_select=40)\\nselect.fit(X_train, y_train)\\n# visualize the selected features:\\nmask = select.get_support()\\nplt.matshow(mask.reshape(1, -1), cmap=\\'gray_r\\')\\nplt.xlabel(\"Sample index\")\\n240 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 254, 'page_label': '241'}, page_content='Figure 4-11. Features selected by recursive feature elimination with the random forest\\nclassifier model\\nThe feature selection got better compared to the univariate and model-based selec‐\\ntion, but one feature was still missed. Running this code also takes significantly longer\\nthan that for the model-based selection, because a random forest model is trained 40\\ntimes, once for each feature that is dropped. Let’s test the accuracy of the logistic\\nregression model when using RFE for feature selection:\\nIn[47]:\\nX_train_rfe= select.transform(X_train)\\nX_test_rfe= select.transform(X_test)\\nscore = LogisticRegression().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)\\nprint(\"Test score: {:.3f}\".format(score))\\nOut[47]:\\nTest score: 0.951\\nWe can also use the model used inside the RFE to make predictions. This uses only\\nthe feature set that was selected:\\nIn[48]:\\nprint(\"Test score: {:.3f}\".format(select.score(X_test, y_test)))\\nOut[48]:\\nTest score: 0.951\\nHere, the performance of the random forest used inside the RFE is the same as that\\nachieved by training a logistic regression model on top of the selected features. In\\nother words, once we’ve selected the right features, the linear model performs as well\\nas the random forest.\\nIf you are unsure when selecting what to use as input to your machine learning algo‐\\nrithms, automatic feature selection can be quite helpful. It is also great for reducing\\nthe amount of features needed—for example, to speed up prediction or to allow for\\nmore interpretable models. In most real-world cases, applying feature selection is\\nunlikely to provide large gains in performance. However, it is still a valuable tool in\\nthe toolbox of the feature engineer.\\nAutomatic Feature Selection | 241'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 255, 'page_label': '242'}, page_content='Utilizing Expert Knowledge\\nFeature engineering is often an important place to use expert knowledge for a particu‐\\nlar application. While the purpose of machine learning in many cases is to avoid hav‐\\ning to create a set of expert-designed rules, that doesn’t mean that prior knowledge of\\nthe application or domain should be discarded. Often, domain experts can help in\\nidentifying useful features that are much more informative than the initial represen‐\\ntation of the data. Imagine you work for a travel agency and want to predict flight\\nprices. Let’s say you have a record of prices together with dates, airlines, start loca‐\\ntions, and destinations. A machine learning model might be able to build a decent\\nmodel from that. Some important factors in flight prices, however, cannot be learned.\\nFor example, flights are usually more expensive during peak vacation months and\\naround holidays. While the dates of some holidays (like Christmas) are fixed, and\\ntheir effect can therefore be learned from the date, others might depend on the phases\\nof the moon (like Hanukkah and Easter) or be set by authorities (like school holi‐\\ndays). These events cannot be learned from the data if each flight is only recorded\\nusing the (Gregorian) date. However, it is easy to add a feature that encodes whether a\\nflight was on, preceding, or following a public or school holiday. In this way, prior\\nknowledge about the nature of the task can be encoded in the features to aid a\\nmachine learning algorithm. Adding a feature does not force a machine learning\\nalgorithm to use it, and even if the holiday information turns out to be noninforma‐\\ntive for flight prices, augmenting the data with this information doesn’t hurt.\\nWe’ll now look at one particular case of using expert knowledge—though in this case\\nit might be more rightfully called “common sense. ” The task is predicting bicycle rent‐\\nals in front of Andreas’s house.\\nIn New Y ork, Citi Bike operates a network of bicycle rental stations with a subscrip‐\\ntion system. The stations are all over the city and provide a convenient way to get\\naround. Bike rental data is made public in an anonymized form  and has been ana‐\\nlyzed in various ways. The task we want to solve is to predict for a given time and day\\nhow many people will rent a bike in front of Andreas’s house—so he knows if any\\nbikes will be left for him.\\nWe first load the data for August 2015 for this particular station as a pandas Data\\nFrame. We resample the data into three-hour intervals to obtain the main trends for\\neach day:\\nIn[49]:\\ncitibike = mglearn.datasets.load_citibike()\\n242 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 256, 'page_label': '243'}, page_content='In[50]:\\nprint(\"Citi Bike data:\\\\n{}\".format(citibike.head()))\\nOut[50]:\\nCiti Bike data:\\nstarttime\\n2015-08-01 00:00:00     3.0\\n2015-08-01 03:00:00     0.0\\n2015-08-01 06:00:00     9.0\\n2015-08-01 09:00:00    41.0\\n2015-08-01 12:00:00    39.0\\nFreq: 3H, Name: one, dtype: float64\\nThe following example shows a visualization of the rental frequencies for the whole\\nmonth (Figure 4-12):\\nIn[51]:\\nplt.figure(figsize=(10, 3))\\nxticks = pd.date_range(start=citibike.index.min(), end=citibike.index.max(),\\n                       freq=\\'D\\')\\nplt.xticks(xticks, xticks.strftime(\"%a %m-%d\"), rotation=90, ha=\"left\")\\nplt.plot(citibike, linewidth=1)\\nplt.xlabel(\"Date\")\\nplt.ylabel(\"Rentals\")\\nFigure 4-12. Number of bike rentals over time for a selected Citi Bike station\\nLooking at the data, we can clearly distinguish day and night for each 24-hour inter‐\\nval. The patterns for weekdays and weekends also seem to be quite different. When\\nevaluating a prediction task on a time series like this, we usually want to learn from\\nthe past and predict for the future. This means when doing a split into a training and a\\ntest set, we want to use all the data up to a certain date as the training set and all the\\ndata past that date as the test set. This is how we would usually use time series predic‐\\ntion: given everything that we know about rentals in the past, what do we think will\\nUtilizing Expert Knowledge | 243'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 257, 'page_label': '244'}, page_content='happen tomorrow? We will use the first 184 data points, corresponding to the first 23\\ndays, as our training set, and the remaining 64 data points, corresponding to the\\nremaining 8 days, as our test set.\\nThe only feature that we are using in our prediction task is the date and time when a\\nparticular number of rentals occurred. So, the input feature is the date and time—say,\\n2015-08-01 00:00:00 —and the output is the number of rentals in the following\\nthree hours (three in this case, according to our DataFrame).\\nA (surprisingly) common way that dates are stored on computers is using POSIX\\ntime, which is the number of seconds since January 1970 00:00:00 (aka the beginning\\nof Unix time). As a first try, we can use this single integer feature as our data repre‐\\nsentation:\\nIn[52]:\\n# extract the target values (number of rentals)\\ny = citibike.values\\n# convert the time to POSIX time using \"%s\"\\nX = citibike.index.strftime(\"%s\").astype(\"int\").reshape(-1, 1)\\nWe first define a function to split the data into training and test sets, build the model,\\nand visualize the result:\\nIn[54]:\\n# use the first 184 data points for training, and the rest for testing\\nn_train = 184\\n# function to evaluate and plot a regressor on a given feature set\\ndef eval_on_features(features, target, regressor):\\n    # split the given features into a training and a test set\\n    X_train, X_test = features[:n_train], features[n_train:]\\n    # also split the target array\\n    y_train, y_test = target[:n_train], target[n_train:]\\n    regressor.fit(X_train, y_train)\\n    print(\"Test-set R^2: {:.2f}\".format(regressor.score(X_test, y_test)))\\n    y_pred = regressor.predict(X_test)\\n    y_pred_train = regressor.predict(X_train)\\n    plt.figure(figsize=(10, 3))\\n    plt.xticks(range(0, len(X), 8), xticks.strftime(\"%a %m-%d\"), rotation=90,\\n               ha=\"left\")\\n    plt.plot(range(n_train), y_train, label=\"train\")\\n    plt.plot(range(n_train, len(y_test) + n_train), y_test, \\'-\\', label=\"test\")\\n    plt.plot(range(n_train), y_pred_train, \\'--\\', label=\"prediction train\")\\n    plt.plot(range(n_train, len(y_test) + n_train), y_pred, \\'--\\',\\n             label=\"prediction test\")\\n    plt.legend(loc=(1.01, 0))\\n    plt.xlabel(\"Date\")\\n    plt.ylabel(\"Rentals\")\\n244 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 258, 'page_label': '245'}, page_content='We saw earlier that random forests require very little preprocessing of the data, which\\nmakes this seem like a good model to start with. We use the POSIX time feature X and\\npass a random forest regressor to our eval_on_features function. Figure 4-13 shows\\nthe result:\\nIn[55]:\\nfrom sklearn.ensemble import RandomForestRegressor\\nregressor = RandomForestRegressor(n_estimators=100, random_state=0)\\nplt.figure()\\neval_on_features(X, y, regressor)\\nOut[55]:\\nTest-set R^2: -0.04\\nFigure 4-13. Predictions made by a random forest using only the POSIX time\\nThe predictions on the training set are quite good, as is usual for random forests.\\nHowever, for the test set, a constant line is predicted. The R2 is –0.03, which means\\nthat we learned nothing. What happened?\\nThe problem lies in the combination of our feature and the random forest. The value\\nof the POSIX time feature for the test set is outside of the range of the feature values\\nin the training set: the points in the test set have timestamps that are later than all the\\npoints in the training set. Trees, and therefore random forests, cannot extrapolate to\\nfeature ranges outside the training set. The result is that the model simply predicts the\\ntarget value of the closest point in the training set—which is the last time it observed\\nany data.\\nClearly we can do better than this. This is where our “expert knowledge” comes in.\\nFrom looking at the rental figures in the training data, two factors seem to be very\\nimportant: the time of day and the day of the week. So, let’s add these two features.\\nWe can’t really learn anything from the POSIX time, so we drop that feature. First,\\nlet’s use only the hour of the day. As Figure 4-14 shows, now the predictions have the\\nsame pattern for each day of the week:\\nUtilizing Expert Knowledge | 245'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 259, 'page_label': '246'}, page_content='In[56]:\\nX_hour = citibike.index.hour.reshape(-1, 1)\\neval_on_features(X_hour, y, regressor)\\nOut[56]:\\nTest-set R^2: 0.60\\nFigure 4-14. Predictions made by a random forest using only the hour of the day\\nThe R2 is already much better, but the predictions clearly miss the weekly pattern.\\nNow let’s also add the day of the week (see Figure 4-15):\\nIn[57]:\\nX_hour_week = np.hstack([citibike.index.dayofweek.reshape(-1, 1),\\n                         citibike.index.hour.reshape(-1, 1)])\\neval_on_features(X_hour_week, y, regressor)\\nOut[57]:\\nTest-set R^2: 0.84\\nFigure 4-15. Predictions with a random forest using day of week and hour of day\\nfeatures\\n246 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 260, 'page_label': '247'}, page_content='Now we have a model that captures the periodic behavior by considering the day of\\nweek and time of day. It has an R2 of 0.84, and shows pretty good predictive perfor‐\\nmance. What this model likely is learning is the mean number of rentals for each\\ncombination of weekday and time of day from the first 23 days of August. This\\nactually does not require a complex model like a random forest, so let’s try with a\\nsimpler model, LinearRegression (see Figure 4-16):\\nIn[58]:\\nfrom sklearn.linear_model import LinearRegression\\neval_on_features(X_hour_week, y, LinearRegression())\\nOut[58]:\\nTest-set R^2: 0.13\\nFigure 4-16. Predictions made by linear regression using day of week and hour of day as\\nfeatures\\nLinearRegression works much worse, and the periodic pattern looks odd. The rea‐\\nson for this is that we encoded day of week and time of day using integers, which are\\ninterpreted as categorical variables. Therefore, the linear model can only learn a lin‐\\near function of the time of day—and it learned that later in the day, there are more\\nrentals. However, the patterns are much more complex than that. We can capture this\\nby interpreting the integers as categorical variables, by transforming them using One\\nHotEncoder (see Figure 4-17):\\nIn[59]:\\nenc = OneHotEncoder()\\nX_hour_week_onehot = enc.fit_transform(X_hour_week).toarray()\\nUtilizing Expert Knowledge | 247'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 261, 'page_label': '248'}, page_content='In[60]:\\neval_on_features(X_hour_week_onehot, y, Ridge())\\nOut[60]:\\nTest-set R^2: 0.62\\nFigure 4-17. Predictions made by linear regression using a one-hot encoding of hour of\\nday and day of week\\nThis gives us a much better match than the continuous feature encoding. Now the\\nlinear model learns one coefficient for each day of the week, and one coefficient for\\neach time of the day. That means that the “time of day” pattern is shared over all days\\nof the week, though.\\nUsing interaction features, we can allow the model to learn one coefficient for each\\ncombination of day and time of day (see Figure 4-18):\\nIn[61]:\\npoly_transformer = PolynomialFeatures(degree=2, interaction_only=True,\\n                                      include_bias=False)\\nX_hour_week_onehot_poly = poly_transformer.fit_transform(X_hour_week_onehot)\\nlr = Ridge()\\neval_on_features(X_hour_week_onehot_poly, y, lr)\\nOut[61]:\\nTest-set R^2: 0.85\\n248 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 262, 'page_label': '249'}, page_content='Figure 4-18. Predictions made by linear regression using a product of the day of week\\nand hour of day features\\nThis transformation finally yields a model that performs similarly well to the random\\nforest. A big benefit of this model is that it is very clear what is learned: one coeffi‐\\ncient for each day and time. We can simply plot the coefficients learned by the model,\\nsomething that would not be possible for the random forest.\\nFirst, we create feature names for the hour and day features:\\nIn[62]:\\nhour = [\"%02d:00\" % i for i in range(0, 24, 3)]\\nday = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\\nfeatures =  day + hour\\nThen we name all the interaction features extracted by PolynomialFeatures, using\\nthe get_feature_names method, and keep only the features with nonzero coeffi‐\\ncients:\\nIn[63]:\\nfeatures_poly = poly_transformer.get_feature_names(features)\\nfeatures_nonzero = np.array(features_poly)[lr.coef_ != 0]\\ncoef_nonzero = lr.coef_[lr.coef_ != 0]\\nNow we can visualize the coefficients learned by the linear model, as seen in\\nFigure 4-19:\\nIn[64]:\\nplt.figure(figsize=(15, 2))\\nplt.plot(coef_nonzero, \\'o\\')\\nplt.xticks(np.arange(len(coef_nonzero)), features_nonzero, rotation=90)\\nplt.xlabel(\"Feature magnitude\")\\nplt.ylabel(\"Feature\")\\nUtilizing Expert Knowledge | 249'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 263, 'page_label': '250'}, page_content='Figure 4-19. Coefficients of the linear regression model using a product of hour and day\\nSummary and Outlook\\nIn this chapter, we discussed how to deal with different data types (in particular, with\\ncategorical variables). We emphasized the importance of representing data in a way\\nthat is suitable for the machine learning algorithm—for example, by one-hot-\\nencoding categorical variables. We also discussed the importance of engineering new\\nfeatures, and the possibility of utilizing expert knowledge in creating derived features\\nfrom your data. In particular, linear models might benefit greatly from generating\\nnew features via binning and adding polynomials and interactions, while more com‐\\nplex, nonlinear models like random forests and SVMs might be able to learn more\\ncomplex tasks without explicitly expanding the feature space. In practice, the features\\nthat are used (and the match between features and method) is often the most impor‐\\ntant piece in making a machine learning approach work well.\\nNow that you have a good idea of how to represent your data in an appropriate way\\nand which algorithm to use for which task, the next chapter will focus on evaluating\\nthe performance of machine learning models and selecting the right parameter\\nsettings.\\n250 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 264, 'page_label': '251'}, page_content='CHAPTER 5\\nModel Evaluation and Improvement\\nHaving discussed the fundamentals of supervised and unsupervised learning, and\\nhaving explored a variety of machine learning algorithms, we will now dive more\\ndeeply into evaluating models and selecting parameters.\\nWe will focus on the supervised methods, regression and classification, as evaluating\\nand selecting models in unsupervised learning is often a very qualitative process (as\\nwe saw in Chapter 3).\\nTo evaluate our supervised models, so far we have split our dataset into a training set\\nand a test set using the train_test_split function, built a model on the training set\\nby calling the fit method, and evaluated it on the test set using the score method,\\nwhich for classification computes the fraction of correctly classified samples. Here’s\\nan example of that process:\\nIn[2]:\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\n# create a synthetic dataset\\nX, y = make_blobs(random_state=0)\\n# split data and labels into a training and a test set\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n# instantiate a model and fit it to the training set\\nlogreg = LogisticRegression().fit(X_train, y_train)\\n# evaluate the model on the test set\\nprint(\"Test set score: {:.2f}\".format(logreg.score(X_test, y_test)))\\nOut[2]:\\nTest set score: 0.88\\n251'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 265, 'page_label': '252'}, page_content='Remember, the reason we split our data into training and test sets is that we are inter‐\\nested in measuring how well our model generalizes to new, previously unseen data.\\nWe are not interested in how well our model fit the training set, but rather in how\\nwell it can make predictions for data that was not observed during training.\\nIn this chapter, we will expand on two aspects of this evaluation. We will first intro‐\\nduce cross-validation, a more robust way to assess generalization performance, and\\ndiscuss methods to evaluate classification and regression performance that go beyond\\nthe default measures of accuracy and R2 provided by the score method.\\nWe will also discuss grid search, an effective method for adjusting the parameters in\\nsupervised models for the best generalization performance.\\nCross-Validation\\nCross-validation is a statistical method of evaluating generalization performance that\\nis more stable and thorough than using a split into a training and a test set. In cross-\\nvalidation, the data is instead split repeatedly and multiple models are trained. The\\nmost commonly used version of cross-validation is k-fold cross-validation, where k is\\na user-specified number, usually 5 or 10. When performing five-fold cross-validation,\\nthe data is first partitioned into five parts of (approximately) equal size, called folds.\\nNext, a sequence of models is trained. The first model is trained using the first fold as\\nthe test set, and the remaining folds (2–5) are used as the training set. The model is\\nbuilt using the data in folds 2–5, and then the accuracy is evaluated on fold 1. Then\\nanother model is built, this time using fold 2 as the test set and the data in folds 1, 3,\\n4, and 5 as the training set. This process is repeated using folds 3, 4, and 5 as test sets.\\nFor each of these five splits of the data into training and test sets, we compute the\\naccuracy. In the end, we have collected five accuracy values. The process is illustrated\\nin Figure 5-1:\\nIn[3]:\\nmglearn.plots.plot_cross_validation()\\nFigure 5-1. Data splitting in five-fold cross-validation\\nUsually, the first fifth of the data is the first fold, the second fifth of the data is the\\nsecond fold, and so on.\\n252 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 266, 'page_label': '253'}, page_content='Cross-Validation in scikit-learn\\nCross-validation is implemented in scikit-learn using the cross_val_score func‐\\ntion from the model_selection module. The parameters of the cross_val_score\\nfunction are the model we want to evaluate, the training data, and the ground-truth\\nlabels. Let’s evaluate LogisticRegression on the iris dataset:\\nIn[4]:\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.linear_model import LogisticRegression\\niris = load_iris()\\nlogreg = LogisticRegression()\\nscores = cross_val_score(logreg, iris.data, iris.target)\\nprint(\"Cross-validation scores: {}\".format(scores))\\nOut[4]:\\nCross-validation scores: [ 0.961  0.922  0.958]\\nBy default, cross_val_score performs three-fold cross-validation, returning three\\naccuracy values. We can change the number of folds used by changing the cv parame‐\\nter:\\nIn[5]:\\nscores = cross_val_score(logreg, iris.data, iris.target, cv=5)\\nprint(\"Cross-validation scores: {}\".format(scores))\\nOut[5]:\\nCross-validation scores: [ 1.     0.967  0.933  0.9    1.   ]\\nA common way to summarize the cross-validation accuracy is to compute the mean:\\nIn[6]:\\nprint(\"Average cross-validation score: {:.2f}\".format(scores.mean()))\\nOut[6]:\\nAverage cross-validation score: 0.96\\nUsing the mean cross-validation we can conclude that we expect the model to be\\naround 96% accurate on average. Looking at all five scores produced by the five-fold\\ncross-validation, we can also conclude that there is a relatively high variance in the\\naccuracy between folds, ranging from 100% accuracy to 90% accuracy. This could\\nimply that the model is very dependent on the particular folds used for training, but it\\ncould also just be a consequence of the small size of the dataset.\\nCross-Validation | 253'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 267, 'page_label': '254'}, page_content='Benefits  of Cross-Validation\\nThere are several benefits to using cross-validation instead of a single split into a\\ntraining and a test set. First, remember that train_test_split performs a random\\nsplit of the data. Imagine that we are “lucky” when randomly splitting the data, and\\nall examples that are hard to classify end up in the training set. In that case, the test\\nset will only contain “easy” examples, and our test set accuracy will be unrealistically\\nhigh. Conversely, if we are “unlucky, ” we might have randomly put all the hard-to-\\nclassify examples in the test set and consequently obtain an unrealistically low score.\\nHowever, when using cross-validation, each example will be in the training set exactly\\nonce: each example is in one of the folds, and each fold is the test set once. Therefore,\\nthe model needs to generalize well to all of the samples in the dataset for all of the\\ncross-validation scores (and their mean) to be high.\\nHaving multiple splits of the data also provides some information about how sensi‐\\ntive our model is to the selection of the training dataset. For the iris dataset, we saw\\naccuracies between 90% and 100%. This is quite a range, and it provides us with an\\nidea about how the model might perform in the worst case and best case scenarios\\nwhen applied to new data.\\nAnother benefit of cross-validation as compared to using a single split of the data is\\nthat we use our data more effectively. When using train_test_split, we usually use\\n75% of the data for training and 25% of the data for evaluation. When using five-fold\\ncross-validation, in each iteration we can use four-fifths of the data (80%) to fit the\\nmodel. When using 10-fold cross-validation, we can use nine-tenths of the data\\n(90%) to fit the model. More data will usually result in more accurate models.\\nThe main disadvantage of cross-validation is increased computational cost. As we are\\nnow training k models instead of a single model, cross-validation will be roughly k\\ntimes slower than doing a single split of the data.\\nIt is important to keep in mind that cross-validation is not a way to\\nbuild a model that can be applied to new data. Cross-validation\\ndoes not return a model. When calling cross_val_score, multiple\\nmodels are built internally, but the purpose of cross-validation is\\nonly to evaluate how well a given algorithm will generalize when\\ntrained on a specific dataset.\\nStratified  k-Fold Cross-Validation and Other Strategies\\nSplitting the dataset into k folds by starting with the first one- k-th part of the data, as\\ndescribed in the previous section, might not always be a good idea. For example, let’s\\nhave a look at the iris dataset:\\n254 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 268, 'page_label': '255'}, page_content='In[7]:\\nfrom sklearn.datasets import load_iris\\niris = load_iris()\\nprint(\"Iris labels:\\\\n{}\".format(iris.target))\\nOut[7]:\\nIris labels:\\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\\n 2 2]\\nAs you can see, the first third of the data is the class 0, the second third is the class 1,\\nand the last third is the class 2. Imagine doing three-fold cross-validation on this\\ndataset. The first fold would be only class 0, so in the first split of the data, the test set\\nwould be only class 0, and the training set would be only classes 1 and 2. As the\\nclasses in training and test sets would be different for all three splits, the three-fold\\ncross-validation accuracy would be zero on this dataset. That is not very helpful, as\\nwe can do much better than 0% accuracy on iris.\\nAs the simple k-fold strategy fails here, scikit-learn does not use it for classifica‐\\ntion, but rather uses \\nstratified k-fold cross-validation. In stratified cross-validation, we\\nsplit the data such that the proportions between classes are the same in each fold as\\nthey are in the whole dataset, as illustrated in Figure 5-2:\\nIn[8]:\\nmglearn.plots.plot_stratified_cross_validation()\\nFigure 5-2. Comparison of standard cross-validation and stratified cross-validation\\nwhen the data is ordered by class label\\nCross-Validation | 255'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 269, 'page_label': '256'}, page_content='For example, if 90% of your samples belong to class A and 10% of your samples\\nbelong to class B, then stratified cross-validation ensures that in each fold, 90% of\\nsamples belong to class A and 10% of samples belong to class B.\\nIt is usually a good idea to use stratified k-fold cross-validation instead of k-fold\\ncross-validation to evaluate a classifier, because it results in more reliable estimates of\\ngeneralization performance. In the case of only 10% of samples belonging to class B,\\nusing standard k-fold cross-validation it might easily happen that one fold only con‐\\ntains samples of class A. Using this fold as a test set would not be very informative\\nabout the overall performance of the classifier.\\nFor regression, scikit-learn uses the standard k-fold cross-validation by default. It\\nwould be possible to also try to make each fold representative of the different values\\nthe regression target has, but this is not a commonly used strategy and would be sur‐\\nprising to most users.\\nMore control over cross-validation\\nWe saw earlier that we can adjust the number of folds that are used in\\ncross_val_score using the cv parameter. However, scikit-learn allows for much\\nfiner control over what happens during the splitting of the data by providing a cross-\\nvalidation splitter as the cv parameter. For most use cases, the defaults of k-fold cross-\\nvalidation for regression and stratified k-fold for classification work well, but there\\nare some cases where you might want to use a different strategy. Say, for example, we\\nwant to use the standard k-fold cross-validation on a classification dataset to repro‐\\nduce someone else’s results. To do this, we first have to import the KFold splitter class\\nfrom the model_selection module and instantiate it with the number of folds we\\nwant to use:\\nIn[9]:\\nfrom sklearn.model_selection import KFold\\nkfold = KFold(n_splits=5)\\nThen, we can pass the kfold splitter object as the cv parameter to cross_val_score:\\nIn[10]:\\nprint(\"Cross-validation scores:\\\\n{}\".format(\\n      cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\\nOut[10]:\\nCross-validation scores:\\n[ 1.     0.933  0.433  0.967  0.433]\\nThis way, we can verify that it is indeed a really bad idea to use three-fold (nonstrati‐\\nfied) cross-validation on the iris dataset:\\n256 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 270, 'page_label': '257'}, page_content='In[11]:\\nkfold = KFold(n_splits=3)\\nprint(\"Cross-validation scores:\\\\n{}\".format(\\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\\nOut[11]:\\nCross-validation scores:\\n[ 0.  0.  0.]\\nRemember: each fold corresponds to one of the classes in the iris dataset, and so\\nnothing can be learned. Another way to resolve this problem is to shuffle the data\\ninstead of stratifying the folds, to remove the ordering of the samples by label. We can\\ndo that by setting the shuffle parameter of KFold to True. If we shuffle the data, we\\nalso need to fix the random_state to get a reproducible shuffling. Otherwise, each run\\nof cross_val_score would yield a different result, as each time a different split would\\nbe used (this might not be a problem, but can be surprising). Shuffling the data before\\nsplitting it yields a much better result:\\nIn[12]:\\nkfold = KFold(n_splits=3, shuffle=True, random_state=0)\\nprint(\"Cross-validation scores:\\\\n{}\".format(\\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\\nOut[12]:\\nCross-validation scores:\\n[ 0.9   0.96  0.96]\\nLeave-one-out cross-validation\\nAnother frequently used cross-validation method is leave-one-out. Y ou can think of\\nleave-one-out cross-validation as k-fold cross-validation where each fold is a single\\nsample. For each split, you pick a single data point to be the test set. This can be very\\ntime consuming, particularly for large datasets, but sometimes provides better esti‐\\nmates on small datasets:\\nIn[13]:\\nfrom sklearn.model_selection import LeaveOneOut\\nloo = LeaveOneOut()\\nscores = cross_val_score(logreg, iris.data, iris.target, cv=loo)\\nprint(\"Number of cv iterations: \", len(scores))\\nprint(\"Mean accuracy: {:.2f}\".format(scores.mean()))\\nOut[13]:\\nNumber of cv iterations:  150\\nMean accuracy: 0.95\\nCross-Validation | 257'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 271, 'page_label': '258'}, page_content='Shuffle-split  cross-validation\\nAnother, very flexible strategy for cross-validation is shuffle-split cross-validation. In\\nshuffle-split cross-validation, each split samples train_size many points for the\\ntraining set and test_size many (disjoint) point for the test set. This splitting is\\nrepeated n_iter times. Figure 5-3  illustrates running four iterations of splitting a\\ndataset consisting of 10 points, with a training set of 5 points and test sets of 2 points\\neach (you can use integers for train_size and test_size to use absolute sizes for\\nthese sets, or floating-point numbers to use fractions of the whole dataset):\\nIn[14]:\\nmglearn.plots.plot_shuffle_split()\\nFigure 5-3. ShuffleSplit with 10 points, train_size=5, test_size=2, and n_iter=4\\nThe following code splits the dataset into 50% training set and 50% test set for 10\\niterations:\\nIn[15]:\\nfrom sklearn.model_selection import ShuffleSplit\\nshuffle_split = ShuffleSplit(test_size=.5, train_size=.5, n_splits=10)\\nscores = cross_val_score(logreg, iris.data, iris.target, cv=shuffle_split)\\nprint(\"Cross-validation scores:\\\\n{}\".format(scores))\\nOut[15]:\\nCross-validation scores:\\n[ 0.96   0.907  0.947  0.96   0.96   0.907  0.893  0.907  0.92   0.973]\\nShuffle-split cross-validation allows for control over the number of iterations inde‐\\npendently of the training and test sizes, which can sometimes be helpful. It also allows\\nfor using only part of the data in each iteration, by providing train_size and\\ntest_size settings that don’t add up to one. Subsampling the data in this way can be\\nuseful for experimenting with large datasets.\\nThere is also a stratified variant of ShuffleSplit, aptly named StratifiedShuffleS\\nplit, which can provide more reliable results for classification tasks.\\n258 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 272, 'page_label': '259'}, page_content='Cross-validation with groups\\nAnother very common setting for cross-validation is when there are groups in the\\ndata that are highly related. Say you want to build a system to recognize emotions\\nfrom pictures of faces, and you collect a dataset of pictures of 100 people where each\\nperson is captured multiple times, showing various emotions. The goal is to build a\\nclassifier that can correctly identify emotions of people not in the dataset. Y ou could\\nuse the default stratified cross-validation to measure the performance of a classifier\\nhere. However, it is likely that pictures of the same person will be in both the training\\nand the test set. It will be much easier for a classifier to detect emotions in a face that\\nis part of the training set, compared to a completely new face. To accurately evaluate\\nthe generalization to new faces, we must therefore ensure that the training and test\\nsets contain images of different people.\\nTo achieve this, we can use GroupKFold, which takes an array of groups as argument\\nthat we can use to indicate which person is in the image. The groups array here indi‐\\ncates groups in the data that should not be split when creating the training and test\\nsets, and should not be confused with the class label.\\nThis example of groups in the data is common in medical applications, where you\\nmight have multiple samples from the same patient, but are interested in generalizing\\nto new patients. Similarly, in speech recognition, you might have multiple recordings\\nof the same speaker in your dataset, but are interested in recognizing speech of new\\nspeakers.\\nThe following is an example of using a synthetic dataset with a grouping given by the\\ngroups array. The dataset consists of 12 data points, and for each of the data points,\\ngroups specifies which group (think patient) the point belongs to. The groups specify\\nthat there are four groups, and the first three samples belong to the first group, the\\nnext four samples belong to the second group, and so on:\\nIn[17]:\\nfrom sklearn.model_selection import GroupKFold\\n# create synthetic dataset\\nX, y = make_blobs(n_samples=12, random_state=0)\\n# assume the first three samples belong to the same group,\\n# then the next four, etc.\\ngroups = [0, 0, 0, 1, 1, 1, 1, 2, 2, 3, 3, 3]\\nscores = cross_val_score(logreg, X, y, groups, cv=GroupKFold(n_splits=3))\\nprint(\"Cross-validation scores:\\\\n{}\".format(scores))\\nOut[17]:\\nCross-validation scores:\\n[ 0.75   0.8    0.667]\\nThe samples don’t need to be ordered by group; we just did this for illustration pur‐\\nposes. The splits that are calculated based on these labels are visualized in Figure 5-4.\\nCross-Validation | 259'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 273, 'page_label': '260'}, page_content='As you can see, for each split, each group is either entirely in the training set or\\nentirely in the test set:\\nIn[16]:\\nmglearn.plots.plot_label_kfold()\\nFigure 5-4. Label-dependent splitting with GroupKFold\\nThere are more splitting strategies for cross-validation in scikit-learn, which allow\\nfor an even greater variety of use cases (you can find these in the scikit-learn user\\nguide). However, the standard KFold, StratifiedKFold, and GroupKFold are by far\\nthe most commonly used ones.\\nGrid Search\\nNow that we know how to evaluate how well a model generalizes, we can take the\\nnext step and improve the model’s generalization performance by tuning its parame‐\\nters. We discussed the parameter settings of many of the algorithms in scikit-learn\\nin Chapters 2 and 3, and it is important to understand what the parameters mean\\nbefore trying to adjust them. Finding the values of the important parameters of a\\nmodel (the ones that provide the best generalization performance) is a tricky task, but\\nnecessary for almost all models and datasets. Because it is such a common task, there\\nare standard methods in scikit-learn to help you with it. The most commonly used\\nmethod is grid search, which basically means trying all possible combinations of the\\nparameters of interest.\\nConsider the case of a kernel SVM with an RBF (radial basis function) kernel, as\\nimplemented in the SVC class. As we discussed in Chapter 2, there are two important\\nparameters: the kernel bandwidth, gamma, and the regularization parameter, C. Say we\\nwant to try the values 0.001, 0.01, 0.1, 1, 10, and 100 for the parameter C, and the\\nsame for gamma. Because we have six different settings for C and gamma that we want to\\ntry, we have 36 combinations of parameters in total. Looking at all possible combina‐\\ntions creates a table (or grid) of parameter settings for the SVM, as shown here:\\n260 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 274, 'page_label': '261'}, page_content='C = 0.001 C = 0.01 … C = 10\\ngamma=0.001 SVC(C=0.001, gamma=0.001) SVC(C=0.01, gamma=0.001) … SVC(C=10, gamma=0.001)\\ngamma=0.01 SVC(C=0.001, gamma=0.01) SVC(C=0.01, gamma=0.01) … SVC(C=10, gamma=0.01)\\n… … … … …\\ngamma=100 SVC(C=0.001, gamma=100) SVC(C=0.01, gamma=100) … SVC(C=10, gamma=100)\\nSimple Grid Search\\nWe can implement a simple grid search just as for loops over the two parameters,\\ntraining and evaluating a classifier for each combination:\\nIn[18]:\\n# naive grid search implementation\\nfrom sklearn.svm import SVC\\nX_train, X_test, y_train, y_test = train_test_split(\\n    iris.data, iris.target, random_state=0)\\nprint(\"Size of training set: {}   size of test set: {}\".format(\\n      X_train.shape[0], X_test.shape[0]))\\nbest_score = 0\\nfor gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\\n    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\\n        # for each combination of parameters, train an SVC\\n        svm = SVC(gamma=gamma, C=C)\\n        svm.fit(X_train, y_train)\\n        # evaluate the SVC on the test set\\n        score = svm.score(X_test, y_test)\\n        # if we got a better score, store the score and parameters\\n        if score > best_score:\\n            best_score = score\\n            best_parameters = {\\'C\\': C, \\'gamma\\': gamma}\\nprint(\"Best score: {:.2f}\".format(best_score))\\nprint(\"Best parameters: {}\".format(best_parameters))\\nOut[18]:\\nSize of training set: 112   size of test set: 38\\nBest score: 0.97\\nBest parameters: {\\'C\\': 100, \\'gamma\\': 0.001}\\nThe Danger of \\nOverfitting  the Parameters and the Validation Set\\nGiven this result, we might be tempted to report that we found a model that performs\\nwith 97% accuracy on our dataset. However, this claim could be overly optimistic (or\\njust wrong), for the following reason: we tried many different parameters and\\nGrid Search | 261'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 275, 'page_label': '262'}, page_content='selected the one with best accuracy on the test set, but this accuracy won’t necessarily\\ncarry over to new data. Because we used the test data to adjust the parameters, we can\\nno longer use it to assess how good the model is. This is the same reason we needed\\nto split the data into training and test sets in the first place; we need an independent\\ndataset to evaluate, one that was not used to create the model.\\nOne way to resolve this problem is to split the data again, so we have three sets: the\\ntraining set to build the model, the validation (or development) set to select the\\nparameters of the model, and the test set to evaluate the performance of the selected\\nparameters. Figure 5-5 shows what this looks like:\\nIn[19]:\\nmglearn.plots.plot_threefold_split()\\nFigure 5-5. A threefold split of data into training set, validation set, and test set\\nAfter selecting the best parameters using the validation set, we can rebuild a model\\nusing the parameter settings we found, but now training on both the training data\\nand the validation data. This way, we can use as much data as possible to build our\\nmodel. This leads to the following implementation:\\nIn[20]:\\nfrom sklearn.svm import SVC\\n# split data into train+validation set and test set\\nX_trainval, X_test, y_trainval, y_test = train_test_split(\\n    iris.data, iris.target, random_state=0)\\n# split train+validation set into training and validation sets\\nX_train, X_valid, y_train, y_valid = train_test_split(\\n    X_trainval, y_trainval, random_state=1)\\nprint(\"Size of training set: {}   size of validation set: {}   size of test set:\"\\n      \" {}\\\\n\".format(X_train.shape[0], X_valid.shape[0], X_test.shape[0]))\\nbest_score = 0\\nfor gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\\n    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\\n        # for each combination of parameters, train an SVC\\n        svm = SVC(gamma=gamma, C=C)\\n        svm.fit(X_train, y_train)\\n        # evaluate the SVC on the test set\\n        score = svm.score(X_valid, y_valid)\\n        # if we got a better score, store the score and parameters\\n        if score > best_score:\\n            best_score = score\\n            best_parameters = {\\'C\\': C, \\'gamma\\': gamma}\\n262 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 276, 'page_label': '263'}, page_content='# rebuild a model on the combined training and validation set,\\n# and evaluate it on the test set\\nsvm = SVC(**best_parameters)\\nsvm.fit(X_trainval, y_trainval)\\ntest_score = svm.score(X_test, y_test)\\nprint(\"Best score on validation set: {:.2f}\".format(best_score))\\nprint(\"Best parameters: \", best_parameters)\\nprint(\"Test set score with best parameters: {:.2f}\".format(test_score))\\nOut[20]:\\nSize of training set: 84   size of validation set: 28   size of test set: 38\\nBest score on validation set: 0.96\\nBest parameters:  {\\'C\\': 10, \\'gamma\\': 0.001}\\nTest set score with best parameters: 0.92\\nThe best score on the validation set is 96%: slightly lower than before, probably\\nbecause we used less data to train the model (X_train is smaller now because we split\\nour dataset twice). However, the score on the test set—the score that actually tells us\\nhow well we generalize—is even lower, at 92%. So we can only claim to classify new\\ndata 92% correctly, not 97% correctly as we thought before!\\nThe distinction between the training set, validation set, and test set is fundamentally\\nimportant to applying machine learning methods in practice. Any choices made\\nbased on the test set accuracy “leak” information from the test set into the model.\\nTherefore, it is important to keep a separate test set, which is only used for the final\\nevaluation. It is good practice to do all exploratory analysis and model selection using\\nthe combination of a training and a validation set, and reserve the test set for a final\\nevaluation—this is even true for exploratory visualization. Strictly speaking, evaluat‐\\ning more than one model on the test set and choosing the better of the two will result\\nin an overly optimistic estimate of how accurate the model is.\\nGrid Search with Cross-Validation\\nWhile the method of splitting the data into a training, a validation, and a test set that\\nwe just saw is workable, and relatively commonly used, it is quite sensitive to how\\nexactly the data is split. From the output of the previous code snippet we can see that\\nGridSearchCV selects \\'C\\': 10, \\'gamma\\': 0.001 as the best parameters, while the\\noutput of the code in the previous section selects \\'C\\': 100, \\'gamma\\': 0.001 as the\\nbest parameters. For a better estimate of the generalization performance, instead of\\nusing a single split into a training and a validation set, we can use cross-validation to\\nevaluate the performance of each parameter combination. This method can be coded\\nup as follows:\\nGrid Search | 263'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 277, 'page_label': '264'}, page_content=\"In[21]:\\nfor gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\\n    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\\n        # for each combination of parameters,\\n        # train an SVC\\n        svm = SVC(gamma=gamma, C=C)\\n        # perform cross-validation\\n        scores = cross_val_score(svm, X_trainval, y_trainval, cv=5)\\n        # compute mean cross-validation accuracy\\n        score = np.mean(scores)\\n        # if we got a better score, store the score and parameters\\n        if score > best_score:\\n            best_score = score\\n            best_parameters = {'C': C, 'gamma': gamma}\\n# rebuild a model on the combined training and validation set\\nsvm = SVC(**best_parameters)\\nsvm.fit(X_trainval, y_trainval)\\nTo evaluate the accuracy of the SVM using a particular setting of C and gamma using\\nfive-fold cross-validation, we need to train 36 * 5 = 180 models. As you can imagine,\\nthe main downside of the use of cross-validation is the time it takes to train all these\\nmodels.\\nThe following visualization ( Figure 5-6) illustrates how the best parameter setting is\\nselected in the preceding code:\\nIn[22]:\\nmglearn.plots.plot_cross_val_selection()\\nFigure 5-6. Results of grid search with cross-validation\\nFor each parameter setting (only a subset is shown), five accuracy values are compu‐\\nted, one for each split in the cross-validation. Then the mean validation accuracy is\\ncomputed for each parameter setting. The parameters with the highest mean valida‐\\ntion accuracy are chosen, marked by the circle.\\n264 | Chapter 5: Model Evaluation and Improvement\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 278, 'page_label': '265'}, page_content='As we said earlier, cross-validation is a way to evaluate a given algo‐\\nrithm on a specific dataset. However, it is often used in conjunction\\nwith parameter search methods like grid search. For this reason,\\nmany people use the term cross-validation colloquially to refer to\\ngrid search with cross-validation.\\nThe overall process of splitting the data, running the grid search, and evaluating the\\nfinal parameters is illustrated in Figure 5-7:\\nIn[23]:\\nmglearn.plots.plot_grid_search_overview()\\nFigure 5-7. Overview of the process of parameter selection and model evaluation with\\nGridSearchCV\\nBecause grid search with cross-validation is such a commonly used method to adjust\\nparameters, scikit-learn provides the GridSearchCV class, which implements it in\\nthe form of an estimator. To use the GridSearchCV class, you first need to specify the\\nparameters you want to search over using a dictionary. GridSearchCV will then per‐\\nform all the necessary model fits. The keys of the dictionary are the names of parame‐\\nters we want to adjust (as given when constructing the model—in this case, C and\\ngamma), and the values are the parameter settings we want to try out. Trying the val‐\\nues 0.001, 0.01, 0.1, 1, 10, and 100 for C and gamma translates to the following\\ndictionary:\\nIn[24]:\\nparam_grid = {\\'C\\': [0.001, 0.01, 0.1, 1, 10, 100],\\n              \\'gamma\\': [0.001, 0.01, 0.1, 1, 10, 100]}\\nprint(\"Parameter grid:\\\\n{}\".format(param_grid))\\nOut[24]:\\nParameter grid:\\n{\\'C\\': [0.001, 0.01, 0.1, 1, 10, 100], \\'gamma\\': [0.001, 0.01, 0.1, 1, 10, 100]}\\nGrid Search | 265'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 279, 'page_label': '266'}, page_content='1 A scikit-learn estimator that is created using another estimator is called a meta-estimator. GridSearchCV is\\nthe most commonly used meta-estimator, but we will see more later.\\nWe can now instantiate the GridSearchCV class with the model ( SVC), the parameter\\ngrid to search ( param_grid), and the cross-validation strategy we want to use (say,\\nfive-fold stratified cross-validation):\\nIn[25]:\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.svm import SVC\\ngrid_search = GridSearchCV(SVC(), param_grid, cv=5)\\nGridSearchCV will use cross-validation in place of the split into a training and valida‐\\ntion set that we used before. However, we still need to split the data into a training\\nand a test set, to avoid overfitting the parameters:\\nIn[26]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    iris.data, iris.target, random_state=0)\\nThe grid_search object that we created behaves just like a classifier; we can call the\\nstandard methods fit, predict, and score on it. 1 However, when we call fit, it will\\nrun cross-validation for each combination of parameters we specified in param_grid:\\nIn[27]:\\ngrid_search.fit(X_train, y_train)\\nFitting the GridSearchCV object not only searches for the best parameters, but also\\nautomatically fits a new model on the whole training dataset with the parameters that\\nyielded the best cross-validation performance. What happens in fit is therefore\\nequivalent to the result of the In[21] code we saw at the beginning of this section. The\\nGridSearchCV class provides a very convenient interface to access the retrained\\nmodel using the predict and score methods. To evaluate how well the best found\\nparameters generalize, we can call score on the test set:\\nIn[28]:\\nprint(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))\\nOut[28]:\\nTest set score: 0.97\\nChoosing the parameters using cross-validation, we actually found a model that ach‐\\nieves 97% accuracy on the test set. The important thing here is that we did not use the\\ntest set to choose the parameters. The parameters that were found are scored in the\\n266 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 280, 'page_label': '267'}, page_content='best_params_ attribute, and the best cross-validation accuracy (the mean accuracy\\nover the different splits for this parameter setting) is stored in best_score_:\\nIn[29]:\\nprint(\"Best parameters: {}\".format(grid_search.best_params_))\\nprint(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\\nOut[29]:\\nBest parameters: {\\'C\\': 100, \\'gamma\\': 0.01}\\nBest cross-validation score: 0.97\\nAgain, be careful not to confuse best_score_ with the generaliza‐\\ntion performance of the model as computed by the score method\\non the test set. Using the score method (or evaluating the output of\\nthe predict method) employs a model trained on the whole train‐\\ning set. The best_score_ attribute stores the mean cross-validation\\naccuracy, with cross-validation performed on the training set.\\nSometimes it is helpful to have access to the actual model that was found—for exam‐\\nple, to look at coefficients or feature importances. Y ou can access the model with the\\nbest parameters trained on the whole training set using the best_estimator_\\nattribute:\\nIn[30]:\\nprint(\"Best estimator:\\\\n{}\".format(grid_search.best_estimator_))\\nOut[30]:\\nBest estimator:\\nSVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\\n   decision_function_shape=None, degree=3, gamma=0.01, kernel=\\'rbf\\',\\n   max_iter=-1, probability=False, random_state=None, shrinking=True,\\n   tol=0.001, verbose=False)\\nBecause grid_search itself has predict and score methods, using best_estimator_\\nis not needed to make predictions or evaluate the model.\\nAnalyzing the result of cross-validation\\nIt is often helpful to visualize the results of cross-validation, to understand how the\\nmodel generalization depends on the parameters we are searching. As grid searches\\nare quite computationally expensive to run, often it is a good idea to start with a rela‐\\ntively coarse and small grid. We can then inspect the results of the cross-validated\\ngrid search, and possibly expand our search. The results of a grid search can be found\\nin the cv_results_ attribute, which is a dictionary storing all aspects of the search. It\\nGrid Search | 267'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 281, 'page_label': '268'}, page_content='contains a lot of details, as you can see in the following output, and is best looked at\\nafter converting it to a pandas DataFrame:\\nIn[31]:\\nimport pandas as pd\\n# convert to DataFrame\\nresults = pd.DataFrame(grid_search.cv_results_)\\n# show the first 5 rows\\ndisplay(results.head())\\nOut[31]:\\n    param_C   param_gamma   params                        mean_test_score\\n0   0.001     0.001         {\\'C\\': 0.001, \\'gamma\\': 0.001}       0.366\\n1   0.001      0.01         {\\'C\\': 0.001, \\'gamma\\': 0.01}        0.366\\n2   0.001       0.1         {\\'C\\': 0.001, \\'gamma\\': 0.1}         0.366\\n3   0.001         1         {\\'C\\': 0.001, \\'gamma\\': 1}           0.366\\n4   0.001        10         {\\'C\\': 0.001, \\'gamma\\': 10}          0.366\\n       rank_test_score  split0_test_score  split1_test_score  split2_test_score\\n0               22              0.375           0.347           0.363\\n1               22              0.375           0.347           0.363\\n2               22              0.375           0.347           0.363\\n3               22              0.375           0.347           0.363\\n4               22              0.375           0.347           0.363\\n       split3_test_score  split4_test_score  std_test_score\\n0           0.363              0.380           0.011\\n1           0.363              0.380           0.011\\n2           0.363              0.380           0.011\\n3           0.363              0.380           0.011\\n4           0.363              0.380           0.011\\nEach row in results corresponds to one particular parameter setting. For each set‐\\nting, the results of all cross-validation splits are recorded, as well as the mean and\\nstandard deviation over all splits. As we were searching a two-dimensional grid of\\nparameters (C and gamma), this is best visualized as a heat map ( Figure 5-8). First we\\nextract the mean validation scores, then we reshape the scores so that the axes corre‐\\nspond to C and gamma:\\nIn[32]:\\nscores = np.array(results.mean_test_score).reshape(6, 6)\\n# plot the mean cross-validation scores\\nmglearn.tools.heatmap(scores, xlabel=\\'gamma\\', xticklabels=param_grid[\\'gamma\\'],\\n                      ylabel=\\'C\\', yticklabels=param_grid[\\'C\\'], cmap=\"viridis\")\\n268 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 282, 'page_label': '269'}, page_content='Figure 5-8. Heat map of mean cross-validation score as a function of C and gamma\\nEach point in the heat map corresponds to one run of cross-validation, with a partic‐\\nular parameter setting. The color encodes the cross-validation accuracy, with light\\ncolors meaning high accuracy and dark colors meaning low accuracy. Y ou can see\\nthat SVC is very sensitive to the setting of the parameters. For many of the parameter\\nsettings, the accuracy is around 40%, which is quite bad; for other settings the accu‐\\nracy is around 96%. We can take away from this plot several things. First, the parame‐\\nters we adjusted are very important for obtaining good performance. Both parameters\\n(C and gamma) matter a lot, as adjusting them can change the accuracy from 40% to\\n96%. Additionally, the ranges we picked for the parameters are ranges in which we\\nsee significant changes in the outcome. It’s also important to note that the ranges for\\nthe parameters are large enough: the optimum values for each parameter are not on\\nthe edges of the plot.\\nNow let’s look at some plots (shown in Figure 5-9 ) where the result is less ideal,\\nbecause the search ranges were not chosen properly:\\nFigure 5-9. Heat map visualizations of misspecified search grids\\nGrid Search | 269'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 283, 'page_label': '270'}, page_content='In[33]:\\nfig, axes = plt.subplots(1, 3, figsize=(13, 5))\\nparam_grid_linear = {\\'C\\': np.linspace(1, 2, 6),\\n                     \\'gamma\\':  np.linspace(1, 2, 6)}\\nparam_grid_one_log = {\\'C\\': np.linspace(1, 2, 6),\\n                      \\'gamma\\':  np.logspace(-3, 2, 6)}\\nparam_grid_range = {\\'C\\': np.logspace(-3, 2, 6),\\n                    \\'gamma\\':  np.logspace(-7, -2, 6)}\\nfor param_grid, ax in zip([param_grid_linear, param_grid_one_log,\\n                           param_grid_range], axes):\\n    grid_search = GridSearchCV(SVC(), param_grid, cv=5)\\n    grid_search.fit(X_train, y_train)\\n    scores = grid_search.cv_results_[\\'mean_test_score\\'].reshape(6, 6)\\n    # plot the mean cross-validation scores\\n    scores_image = mglearn.tools.heatmap(\\n        scores, xlabel=\\'gamma\\', ylabel=\\'C\\', xticklabels=param_grid[\\'gamma\\'],\\n        yticklabels=param_grid[\\'C\\'], cmap=\"viridis\", ax=ax)\\nplt.colorbar(scores_image, ax=axes.tolist())\\nThe first panel shows no changes at all, with a constant color over the whole parame‐\\nter grid. In this case, this is caused by improper scaling and range of the parameters C\\nand gamma. However, if no change in accuracy is visible over the different parameter\\nsettings, it could also be that a parameter is just not important at all. It is usually good\\nto try very extreme values first, to see if there are any changes in the accuracy as a\\nresult of changing a parameter.\\nThe second panel shows a vertical stripe pattern. This indicates that only the setting\\nof the gamma parameter makes any difference. This could mean that the gamma param‐\\neter is searching over interesting values but the C parameter is not—or it could mean\\nthe C parameter is not important.\\nThe third panel shows changes in both C and gamma. However, we can see that in the\\nentire bottom left of the plot, nothing interesting is happening. We can probably\\nexclude the very small values from future grid searches. The optimum parameter set‐\\nting is at the top right. As the optimum is in the border of the plot, we can expect that\\nthere might be even better values beyond this border, and we might want to change\\nour search range to include more parameters in this region.\\nTuning the parameter grid based on the cross-validation scores is perfectly fine, and a\\ngood way to explore the importance of different parameters. However, you should\\nnot test different parameter ranges on the final test set—as we discussed earlier, eval‐\\n270 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 284, 'page_label': '271'}, page_content='uation of the test set should happen only once we know exactly what model we want\\nto use.\\nSearch over spaces that are not grids\\nIn some cases, trying all possible combinations of all parameters as GridSearchCV\\nusually does, is not a good idea. For example, SVC has a kernel parameter, and\\ndepending on which kernel is chosen, other parameters will be relevant. If ker\\nnel=\\'linear\\', the model is linear, and only the C parameter is used. If kernel=\\'rbf\\',\\nboth the C and gamma parameters are used (but not other parameters like degree). In\\nthis case, searching over all possible combinations of C, gamma, and kernel wouldn’t\\nmake sense: if kernel=\\'linear\\', gamma is not used, and trying different values for\\ngamma would be a waste of time. To deal with these kinds of “conditional” parameters,\\nGridSearchCV allows the param_grid to be a list of dictionaries. Each dictionary in the\\nlist is expanded into an independent grid. A possible grid search involving kernel and\\nparameters could look like this:\\nIn[34]:\\nparam_grid = [{\\'kernel\\': [\\'rbf\\'],\\n               \\'C\\': [0.001, 0.01, 0.1, 1, 10, 100],\\n               \\'gamma\\': [0.001, 0.01, 0.1, 1, 10, 100]},\\n              {\\'kernel\\': [\\'linear\\'],\\n               \\'C\\': [0.001, 0.01, 0.1, 1, 10, 100]}]\\nprint(\"List of grids:\\\\n{}\".format(param_grid))\\nOut[34]:\\nList of grids:\\n[{\\'kernel\\': [\\'rbf\\'], \\'C\\': [0.001, 0.01, 0.1, 1, 10, 100],\\n  \\'gamma\\': [0.001, 0.01, 0.1, 1, 10, 100]},\\n {\\'kernel\\': [\\'linear\\'], \\'C\\': [0.001, 0.01, 0.1, 1, 10, 100]}]\\nIn the first grid, the kernel parameter is always set to \\'rbf\\' (not that the entry for\\nkernel is a list of length one), and both the C and gamma parameters are varied. In the\\nsecond grid, the kernel parameter is always set to linear, and only C is varied. Now\\nlet’s apply this more complex parameter search:\\nIn[35]:\\ngrid_search = GridSearchCV(SVC(), param_grid, cv=5)\\ngrid_search.fit(X_train, y_train)\\nprint(\"Best parameters: {}\".format(grid_search.best_params_))\\nprint(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\\nOut[35]:\\nBest parameters: {\\'C\\': 100, \\'kernel\\': \\'rbf\\', \\'gamma\\': 0.01}\\nBest cross-validation score: 0.97\\nGrid Search | 271'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 285, 'page_label': '272'}, page_content=\"Let’s look at the cv_results_ again. As expected, if kernel is 'linear', then only C is\\nvaried:\\nIn[36]:\\nresults = pd.DataFrame(grid_search.cv_results_)\\n# we display the transposed table so that it better fits on the page:\\ndisplay(results.T)\\nOut[36]:\\n0 1 2 3 … 38 39 40 41\\nparam_C 0.001 0.001 0.001 0.001 … 0.1 1 10 100\\nparam_gamma 0.001 0.01 0.1 1 … NaN NaN NaN NaN\\nparam_kernel rbf rbf rbf rbf … linear linear linear linear\\nparams {C: 0.001,\\nkernel: rbf,\\ngamma:\\n0.001}\\n{C: 0.001,\\nkernel: rbf,\\ngamma:\\n0.01}\\n{C: 0.001,\\nkernel: rbf,\\ngamma:\\n0.1}\\n{C: 0.001,\\nkernel: rbf,\\ngamma: 1}\\n… {C: 0.1,\\nkernel:\\nlinear}\\n{C: 1,\\nkernel:\\nlinear}\\n{C: 10,\\nkernel:\\nlinear}\\n{C: 100,\\nkernel:\\nlinear}\\nmean_test_score 0.37 0.37 0.37 0.37 … 0.95 0.97 0.96 0.96\\nrank_test_score 27 27 27 27 … 11 1 3 3\\nsplit0_test_score 0.38 0.38 0.38 0.38 … 0.96 1 0.96 0.96\\nsplit1_test_score 0.35 0.35 0.35 0.35 … 0.91 0.96 1 1\\nsplit2_test_score 0.36 0.36 0.36 0.36 … 1 1 1 1\\nsplit3_test_score 0.36 0.36 0.36 0.36 … 0.91 0.95 0.91 0.91\\nsplit4_test_score 0.38 0.38 0.38 0.38 … 0.95 0.95 0.95 0.95\\nstd_test_score 0.011 0.011 0.011 0.011 … 0.033 0.022 0.034 0.034\\n12 rows × 42 columns\\nUsing different  cross-validation strategies with grid search\\nSimilarly to cross_val_score, GridSearchCV uses stratified k-fold cross-validation\\nby default for classification, and k-fold cross-validation for regression. However, you\\ncan also pass any cross-validation splitter, as described in “More control over cross-\\nvalidation” on page 256, as the cv parameter in GridSearchCV. In particular, to get\\nonly a single split into a training and a validation set, you can use ShuffleSplit or\\nStratifiedShuffleSplit with n_iter=1. This might be helpful for very large data‐\\nsets, or very slow models.\\nNested cross-validation\\nIn the preceding examples, we went from using a single split of the data into training,\\nvalidation, and test sets to splitting the data into training and test sets and then per‐\\nforming cross-validation on the training set. But when using GridSearchCV as\\n272 | Chapter 5: Model Evaluation and Improvement\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 286, 'page_label': '273'}, page_content='described earlier, we still have a single split of the data into training and test sets,\\nwhich might make our results unstable and make us depend too much on this single\\nsplit of the data. We can go a step further, and instead of splitting the original data\\ninto training and test sets once, use multiple splits of cross-validation. This will result\\nin what is called nested cross-validation. In nested cross-validation, there is an outer\\nloop over splits of the data into training and test sets. For each of them, a grid search\\nis run (which might result in different best parameters for each split in the outer\\nloop). Then, for each outer split, the test set score using the best settings is reported.\\nThe result of this procedure is a list of scores—not a model, and not a parameter set‐\\nting. The scores tell us how well a model generalizes, given the best parameters found\\nby the grid. As it doesn’t provide a model that can be used on new data, nested cross-\\nvalidation is rarely used when looking for a predictive model to apply to future data.\\nHowever, it can be useful for evaluating how well a given model works on a particular\\ndataset.\\nImplementing nested cross-validation in scikit-learn is straightforward. We call\\ncross_val_score with an instance of GridSearchCV as the model:\\nIn[34]:\\nscores = cross_val_score(GridSearchCV(SVC(), param_grid, cv=5),\\n                         iris.data, iris.target, cv=5)\\nprint(\"Cross-validation scores: \", scores)\\nprint(\"Mean cross-validation score: \", scores.mean())\\nOut[34]:\\nCross-validation scores:  [ 0.967  1.     0.967  0.967  1.   ]\\nMean cross-validation score:  0.98\\nThe result of our nested cross-validation can be summarized as “SVC can achieve 98%\\nmean cross-validation accuracy on the iris dataset”—nothing more and nothing\\nless.\\nHere, we used stratified five-fold cross-validation in both the inner and the outer\\nloop. As our param_grid contains 36 combinations of parameters, this results in a\\nwhopping 36 * 5 * 5 = 900 models being built, making nested cross-validation a very\\nexpensive procedure. Here, we used the same cross-validation splitter in the inner\\nand the outer loop; however, this is not necessary and you can use any combination\\nof cross-validation strategies in the inner and outer loops. It can be a bit tricky to\\nunderstand what is happening in the single line given above, and it can be helpful to\\nvisualize it as for loops, as done in the following simplified implementation:\\nGrid Search | 273'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 287, 'page_label': '274'}, page_content='In[35]:\\ndef nested_cv(X, y, inner_cv, outer_cv, Classifier, parameter_grid):\\n    outer_scores = []\\n    # for each split of the data in the outer cross-validation\\n    # (split method returns indices)\\n    for training_samples, test_samples in outer_cv.split(X, y):\\n        # find best parameter using inner cross-validation\\n        best_parms = {}\\n        best_score = -np.inf\\n        # iterate over parameters\\n        for parameters in parameter_grid:\\n            # accumulate score over inner splits\\n            cv_scores = []\\n            # iterate over inner cross-validation\\n            for inner_train, inner_test in inner_cv.split(\\n                    X[training_samples], y[training_samples]):\\n                # build classifier given parameters and training data\\n                clf = Classifier(**parameters)\\n                clf.fit(X[inner_train], y[inner_train])\\n                # evaluate on inner test set\\n                score = clf.score(X[inner_test], y[inner_test])\\n                cv_scores.append(score)\\n            # compute mean score over inner folds\\n            mean_score = np.mean(cv_scores)\\n            if mean_score > best_score:\\n                # if better than so far, remember parameters\\n                best_score = mean_score\\n                best_params = parameters\\n        # build classifier on best parameters using outer training set\\n        clf = Classifier(**best_params)\\n        clf.fit(X[training_samples], y[training_samples])\\n        # evaluate\\n        outer_scores.append(clf.score(X[test_samples], y[test_samples]))\\n    return np.array(outer_scores)\\nNow, let’s run this function on the iris dataset:\\nIn[36]:\\nfrom sklearn.model_selection import ParameterGrid, StratifiedKFold\\nscores = nested_cv(iris.data, iris.target, StratifiedKFold(5),\\n          StratifiedKFold(5), SVC, ParameterGrid(param_grid))\\nprint(\"Cross-validation scores: {}\".format(scores))\\nOut[36]:\\nCross-validation scores: [ 0.967  1.     0.967  0.967  1.   ]\\nParallelizing cross-validation and grid search\\nWhile running a grid search over many parameters and on large datasets can be com‐\\nputationally challenging, it is also embarrassingly parallel. This means that building a\\n274 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 288, 'page_label': '275'}, page_content='model using a particular parameter setting on a particular cross-validation split can\\nbe done completely independently from the other parameter settings and models.\\nThis makes grid search and cross-validation ideal candidates for parallelization over\\nmultiple CPU cores or over a cluster. Y ou can make use of multiple cores in Grid\\nSearchCV and cross_val_score by setting the n_jobs parameter to the number of\\nCPU cores you want to use. Y ou can set n_jobs=-1 to use all available cores.\\nY ou should be aware that scikit-learn does not allow nesting of parallel operations .\\nSo, if you are using the n_jobs option on your model (for example, a random forest),\\nyou cannot use it in GridSearchCV to search over this model. If your dataset and\\nmodel are very large, it might be that using many cores uses up too much memory,\\nand you should monitor your memory usage when building large models in parallel.\\nIt is also possible to parallelize grid search and cross-validation over multiple\\nmachines in a cluster, although at the time of writing this is not supported within\\nscikit-learn. It is, however, possible to use the IPython parallel framework for par‐\\nallel grid searches, if you don’t mind writing the for loop over parameters as we did\\nin “Simple Grid Search” on page 261.\\nFor Spark users, there is also the recently developed spark-sklearn package, which\\nallows running a grid search over an already established Spark cluster.\\nEvaluation Metrics and Scoring\\nSo far, we have evaluated classification performance using accuracy (the fraction of\\ncorrectly classified samples) and regression performance using R2. However, these are\\nonly two of the many possible ways to summarize how well a supervised model per‐\\nforms on a given dataset. In practice, these evaluation metrics might not be appropri‐\\nate for your application, and it is important to choose the right metric when selecting\\nbetween models and adjusting parameters.\\nKeep the End Goal in Mind\\nWhen selecting a metric, you should always have the end goal of the machine learn‐\\ning application in mind. In practice, we are usually interested not just in making\\naccurate predictions, but in using these predictions as part of a larger decision-\\nmaking process. Before picking a machine learning metric, you should think about\\nthe high-level goal of the application, often called the business metric. The conse‐\\nquences of choosing a particular algorithm for a machine learning application are\\nEvaluation Metrics and Scoring | 275'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 289, 'page_label': '276'}, page_content='2 We ask scientifically minded readers to excuse the commercial language in this section. Not losing track of the\\nend goal is equally important in science, though the authors are not aware of a similar phrase to “business\\nimpact” being used in that realm.\\ncalled the business impact.2 Maybe the high-level goal is avoiding traffic accidents, or\\ndecreasing the number of hospital admissions. It could also be getting more users for\\nyour website, or having users spend more money in your shop. When choosing a\\nmodel or adjusting parameters, you should pick the model or parameter values that\\nhave the most positive influence on the business metric. Often this is hard, as assess‐\\ning the business impact of a particular model might require putting it in production\\nin a real-life system.\\nIn the early stages of development, and for adjusting parameters, it is often infeasible\\nto put models into production just for testing purposes, because of the high business\\nor personal risks that can be involved. Imagine evaluating the pedestrian avoidance\\ncapabilities of a self-driving car by just letting it drive around, without verifying it\\nfirst; if your model is bad, pedestrians will be in trouble! Therefore we often need to\\nfind some surrogate evaluation procedure, using an evaluation metric that is easier to\\ncompute. For example, we could test classifying images of pedestrians against non-\\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\\npays off to find the closest metric to the original business goal that is feasible to evalu‐\\nate. This closest metric should be used whenever possible for model evaluation and\\nselection. The result of this evaluation might not be a single number—the conse‐\\nquence of your algorithm could be that you have 10% more customers, but each cus‐\\ntomer will spend 15% less—but it should capture the expected business impact of\\nchoosing one model over another.\\nIn this section, we will first discuss metrics for the important special case of binary\\nclassification, then turn to multiclass classification and finally regression.\\nMetrics for Binary \\nClassification\\nBinary classification is arguably the most common and conceptually simple applica‐\\ntion of machine learning in practice. However, there are still a number of caveats in\\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\\nlook at the ways in which measuring accuracy might be misleading. Remember that\\nfor binary classification, we often speak of a positive class and a negative class, with\\nthe understanding that the positive class is the one we are looking for.\\nKinds of errors\\nOften, accuracy is not a good measure of predictive performance, as the number of\\nmistakes we make does not contain all the information we are interested in. Imagine\\nan application to screen for the early detection of cancer using an automated test. If\\n276 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 290, 'page_label': '277'}, page_content='the test is negative, the patient will be assumed healthy, while if the test is positive, the\\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\\ncation of cancer) the positive class, and a negative test the negative class. We can’t\\nassume that our model will always work perfectly, and it will make mistakes. For any\\napplication, we need to ask ourselves what the consequences of these mistakes might\\nbe in the real world.\\nOne possible mistake is that a healthy patient will be classified as positive, leading to\\nadditional testing. This leads to some costs and an inconvenience for the patient (and\\npossibly some mental distress). An incorrect positive prediction is called a false posi‐\\ntive. The other possible mistake is that a sick patient will be classified as negative, and\\nwill not receive further tests and treatment. The undiagnosed cancer might lead to\\nserious health issues, and could even be fatal. A mistake of this kind—an incorrect\\nnegative prediction—is called a false negative . In statistics, a false positive is also\\nknown as type I error, and a false negative as type II error. We will stick to “false nega‐\\ntive” and “false positive, ” as they are more explicit and easier to remember. In the can‐\\ncer diagnosis example, it is clear that we want to avoid false negatives as much as\\npossible, while false positives can be viewed as more of a minor nuisance.\\nWhile this is a particularly drastic example, the consequence of false positives and\\nfalse negatives are rarely the same. In commercial applications, it might be possible to\\nassign dollar values to both kinds of mistakes, which would allow measuring the error\\nof a particular prediction in dollars, instead of accuracy. This might be much more\\nmeaningful for making business decisions on which model to use.\\nImbalanced datasets\\nTypes of errors play an important role when one of two classes is much more frequent\\nthan the other one. This is very common in practice; a good example is click-through\\nprediction, where each data point represents an “impression, ” an item that was shown\\nto a user. This item might be an ad, or a related story, or a related person to follow on\\na social media site. The goal is to predict whether, if shown a particular item, a user\\nwill click on it (indicating they are interested). Most things users are shown on the\\nInternet (in particular, ads) will not result in a click. Y ou might need to show a user\\n100 ads or articles before they find something interesting enough to click on. This\\nresults in a dataset where for each 99 “no click” data points, there is 1 “clicked” data\\npoint; in other words, 99% of the samples belong to the “no click” class. Datasets in\\nwhich one class is much more frequent than the other are often called imbalanced\\ndatasets, or datasets with imbalanced classes. In reality, imbalanced data is the norm,\\nand it is rare that the events of interest have equal or even similar frequency in the\\ndata.\\nNow let’s say you build a classifier that is 99% accurate on the click prediction task.\\nWhat does that tell you? 99% accuracy sounds impressive, but this doesn’t take the\\nEvaluation Metrics and Scoring | 277'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 291, 'page_label': '278'}, page_content='class imbalance into account. Y ou can achieve 99% accuracy without building a\\nmachine learning model, by always predicting “no click. ” On the other hand, even\\nwith imbalanced data, a 99% accurate model could in fact be quite good. However,\\naccuracy doesn’t allow us to distinguish the constant “no click” model from a poten‐\\ntially good model.\\nTo illustrate, we’ll create a 9:1 imbalanced dataset from the digits dataset, by classify‐\\ning the digit 9 against the nine other classes:\\nIn[37]:\\nfrom sklearn.datasets import load_digits\\ndigits = load_digits()\\ny = digits.target == 9\\nX_train, X_test, y_train, y_test = train_test_split(\\n    digits.data, y, random_state=0)\\nWe can use the DummyClassifier to always predict the majority class (here\\n“not nine”) to see how uninformative accuracy can be:\\nIn[38]:\\nfrom sklearn.dummy import DummyClassifier\\ndummy_majority = DummyClassifier(strategy=\\'most_frequent\\').fit(X_train, y_train)\\npred_most_frequent = dummy_majority.predict(X_test)\\nprint(\"Unique predicted labels: {}\".format(np.unique(pred_most_frequent)))\\nprint(\"Test score: {:.2f}\".format(dummy_majority.score(X_test, y_test)))\\nOut[38]:\\nUnique predicted labels: [False]\\nTest score: 0.90\\nWe obtained close to 90% accuracy without learning anything. This might seem strik‐\\ning, but think about it for a minute. Imagine someone telling you their model is 90%\\naccurate. Y ou might think they did a very good job. But depending on the problem,\\nthat might be possible by just predicting one class! Let’s compare this against using an\\nactual classifier:\\nIn[39]:\\nfrom sklearn.tree import DecisionTreeClassifier\\ntree = DecisionTreeClassifier(max_depth=2).fit(X_train, y_train)\\npred_tree = tree.predict(X_test)\\nprint(\"Test score: {:.2f}\".format(tree.score(X_test, y_test)))\\nOut[39]:\\nTest score: 0.92\\n278 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 292, 'page_label': '279'}, page_content='According to accuracy, the DecisionTreeClassifier is only slightly better than the\\nconstant predictor. This could indicate either that something is wrong with how we\\nused DecisionTreeClassifier, or that accuracy is in fact not a good measure here.\\nFor comparison purposes, let’s evaluate two more classifiers, LogisticRegression\\nand the default DummyClassifier, which makes random predictions but produces\\nclasses with the same proportions as in the training set:\\nIn[40]:\\nfrom sklearn.linear_model import LogisticRegression\\ndummy = DummyClassifier().fit(X_train, y_train)\\npred_dummy = dummy.predict(X_test)\\nprint(\"dummy score: {:.2f}\".format(dummy.score(X_test, y_test)))\\nlogreg = LogisticRegression(C=0.1).fit(X_train, y_train)\\npred_logreg = logreg.predict(X_test)\\nprint(\"logreg score: {:.2f}\".format(logreg.score(X_test, y_test)))\\nOut[40]:\\ndummy score: 0.80\\nlogreg score: 0.98\\nThe dummy classifier that produces random output is clearly the worst of the lot\\n(according to accuracy), while LogisticRegression produces very good results.\\nHowever, even the random classifier yields over 80% accuracy. This makes it very\\nhard to judge which of these results is actually helpful. The problem here is that accu‐\\nracy is an inadequate measure for quantifying predictive performance in this imbal‐\\nanced setting. For the rest of this chapter, we will explore alternative metrics that\\nprovide better guidance in selecting models. In particular, we would like to have met‐\\nrics that tell us how much better a model is than making “most frequent” predictions\\nor random predictions, as they are computed in pred_most_frequent and\\npred_dummy. If we use a metric to assess our models, it should definitely be able to\\nweed out these nonsense predictions.\\nConfusion matrices\\nOne of the most comprehensive ways to represent the result of evaluating binary clas‐\\nsification is using confusion matrices. Let’s inspect the predictions of LogisticRegres\\nsion from the previous section using the confusion_matrix function. We already\\nstored the predictions on the test set in pred_logreg:\\nEvaluation Metrics and Scoring | 279'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 293, 'page_label': '280'}, page_content='In[41]:\\nfrom sklearn.metrics import confusion_matrix\\nconfusion = confusion_matrix(y_test, pred_logreg)\\nprint(\"Confusion matrix:\\\\n{}\".format(confusion))\\nOut[41]:\\nConfusion matrix:\\n[[401   2]\\n [  8  39]]\\nThe output of confusion_matrix is a two-by-two array, where the rows correspond\\nto the true classes and the columns correspond to the predicted classes. Each entry\\ncounts how often a sample that belongs to the class corresponding to the row (here,\\n“not nine” and “nine”) was classified as the class corresponding to the column. The\\nfollowing plot (Figure 5-10) illustrates this meaning:\\nIn[42]:\\nmglearn.plots.plot_confusion_matrix_illustration()\\nFigure 5-10. Confusion matrix of the “nine vs. rest” classification task\\n280 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 294, 'page_label': '281'}, page_content='3 The main diagonal of a two-dimensional array or matrix A is A[i, i].\\nEntries on the main diagonal 3 of the confusion matrix correspond to correct classifi‐\\ncations, while other entries tell us how many samples of one class got mistakenly clas‐\\nsified as another class.\\nIf we declare “a nine” the positive class, we can relate the entries of the confusion\\nmatrix with the terms false positive and false negative that we introduced earlier. To\\ncomplete the picture, we call correctly classified samples belonging to the positive\\nclass true positives and correctly classified samples belonging to the negative class true\\nnegatives. These terms are usually abbreviated FP , FN, TP , and TN and lead to the fol‐\\nlowing interpretation for the confusion matrix (Figure 5-11):\\nIn[43]:\\nmglearn.plots.plot_binary_confusion_matrix()\\nFigure 5-11. Confusion matrix for binary classification\\nNow let’s use the confusion matrix to compare the models we fitted earlier (the two\\ndummy models, the decision tree, and the logistic regression):\\nIn[44]:\\nprint(\"Most frequent class:\")\\nprint(confusion_matrix(y_test, pred_most_frequent))\\nprint(\"\\\\nDummy model:\")\\nprint(confusion_matrix(y_test, pred_dummy))\\nprint(\"\\\\nDecision tree:\")\\nprint(confusion_matrix(y_test, pred_tree))\\nprint(\"\\\\nLogistic Regression\")\\nprint(confusion_matrix(y_test, pred_logreg))\\nEvaluation Metrics and Scoring | 281'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 295, 'page_label': '282'}, page_content='Out[44]:\\nMost frequent class:\\n[[403   0]\\n [ 47   0]]\\nDummy model:\\n[[361  42]\\n [ 43   4]]\\nDecision tree:\\n[[390  13]\\n [ 24  23]]\\nLogistic Regression\\n[[401   2]\\n [  8  39]]\\nLooking at the confusion matrix, it is quite clear that something is wrong with\\npred_most_frequent, because it always predicts the same class. pred_dummy, on the\\nother hand, has a very small number of true positives (4), particularly compared to\\nthe number of false negatives and false positives—there are many more false positives\\nthan true positives! The predictions made by the decision tree make much more\\nsense than the dummy predictions, even though the accuracy was nearly the same.\\nFinally, we can see that logistic regression does better than pred_tree in all aspects: it\\nhas more true positives and true negatives while having fewer false positives and false\\nnegatives. From this comparison, it is clear that only the decision tree and the logistic\\nregression give reasonable results, and that the logistic regression works better than\\nthe tree on all accounts. However, inspecting the full confusion matrix is a bit cum‐\\nbersome, and while we gained a lot of insight from looking at all aspects of the\\nmatrix, the process was very manual and qualitative. There are several ways to sum‐\\nmarize the information in the confusion matrix, which we will discuss next.\\nRelation to accuracy.    We already saw one way to summarize the result in the confu‐\\nsion matrix—by computing accuracy, which can be expressed as:\\nAccuracy = TP+TN\\nTP+TN + FP + FN\\nIn other words, accuracy is the number of correct predictions (TP and TN) divided\\nby the number of all samples (all entries of the confusion matrix summed up).\\nPrecision, recall, and f-score.    There are several other ways to summarize the confusion\\nmatrix, with the most common ones being precision and recall. Precision measures\\nhow many of the samples predicted as positive are actually positive:\\n282 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 296, 'page_label': '283'}, page_content='Precision = TP\\nTP+FP\\nPrecision is used as a performance metric when the goal is to limit the number of\\nfalse positives. As an example, imagine a model for predicting whether a new drug\\nwill be effective in treating a disease in clinical trials. Clinical trials are notoriously\\nexpensive, and a pharmaceutical company will only want to run an experiment if it is\\nvery sure that the drug will actually work. Therefore, it is important that the model\\ndoes not produce many false positives—in other words, that it has a high precision.\\nPrecision is also known as positive predictive value (PPV).\\nRecall, on the other hand, measures how many of the positive samples are captured\\nby the positive predictions:\\nRecall = TP\\nTP+FN\\nRecall is used as performance metric when we need to identify all positive samples;\\nthat is, when it is important to avoid false negatives. The cancer diagnosis example\\nfrom earlier in this chapter is a good example for this: it is important to find all peo‐\\nple that are sick, possibly including healthy patients in the prediction. Other names\\nfor recall are sensitivity, hit rate, or true positive rate (TPR).\\nThere is a trade-off between optimizing recall and optimizing precision. Y ou can triv‐\\nially obtain a perfect recall if you predict all samples to belong to the positive class—\\nthere will be no false negatives, and no true negatives either. However, predicting all\\nsamples as positive will result in many false positives, and therefore the precision will\\nbe very low. On the other hand, if you find a model that predicts only the single data\\npoint it is most sure about as positive and the rest as negative, then precision will be\\nperfect (assuming this data point is in fact positive), but recall will be very bad.\\nPrecision and recall are only two of many classification measures\\nderived from TP , FP , TN, and FN. Y ou can find a great summary of\\nall the measures on Wikipedia. In the machine learning commu‐\\nnity, precision and recall are arguably the most commonly used\\nmeasures for binary classification, but other communities might\\nuse other related metrics.\\nSo, while precision and recall are very important measures, looking at only one of\\nthem will not provide you with the full picture. One way to summarize them is the\\nf-score or f-measure, which is with the harmonic mean of precision and recall:\\nF = 2 · precision·recall\\nprecision+recall\\nEvaluation Metrics and Scoring | 283'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 297, 'page_label': '284'}, page_content='This particular variant is also known as the f1-score. As it takes precision and recall\\ninto account, it can be a better measure than accuracy on imbalanced binary classifi‐\\ncation datasets. Let’s run it on the predictions for the “nine vs. rest” dataset that we\\ncomputed earlier. Here, we will assume that the “nine” class is the positive class (it is\\nlabeled as True while the rest is labeled as False), so the positive class is the minority\\nclass:\\nIn[45]:\\nfrom sklearn.metrics import f1_score\\nprint(\"f1 score most frequent: {:.2f}\".format(\\n        f1_score(y_test, pred_most_frequent)))\\nprint(\"f1 score dummy: {:.2f}\".format(f1_score(y_test, pred_dummy)))\\nprint(\"f1 score tree: {:.2f}\".format(f1_score(y_test, pred_tree)))\\nprint(\"f1 score logistic regression: {:.2f}\".format(\\n        f1_score(y_test, pred_logreg)))\\nOut[45]:\\nf1 score most frequent: 0.00\\nf1 score dummy: 0.10\\nf1 score tree: 0.55\\nf1 score logistic regression: 0.89\\nWe can note two things here. First, we get an error message for the most_frequent\\nprediction, as there were no predictions of the positive class (which makes the\\ndenominator in the f-score zero). Also, we can see a pretty strong distinction between\\nthe dummy predictions and the tree predictions, which wasn’t clear when looking at\\naccuracy alone. Using the f-score for evaluation, we summarized the predictive per‐\\nformance again in one number. However, the f-score seems to capture our intuition\\nof what makes a good model much better than accuracy did. A disadvantage of the\\nf-score, however, is that it is harder to interpret and explain than accuracy.\\nIf we want a more comprehensive summary of precision, recall, and f1-score, we can\\nuse the classification_report convenience function to compute all three at once,\\nand print them in a nice format:\\nIn[46]:\\nfrom sklearn.metrics import classification_report\\nprint(classification_report(y_test, pred_most_frequent,\\n                            target_names=[\"not nine\", \"nine\"]))\\n284 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 298, 'page_label': '285'}, page_content='Out[46]:\\n             precision    recall  f1-score   support\\n   not nine       0.90      1.00      0.94       403\\n       nine       0.00      0.00      0.00        47\\navg / total       0.80      0.90      0.85       450\\nThe classification_report function produces one line per class (here, True and\\nFalse) and reports precision, recall, and f-score with this class as the positive class.\\nBefore, we assumed the minority “nine” class was the positive class. If we change the\\npositive class to “not nine, ” we can see from the output of classification_report\\nthat we obtain an f-score of 0.94 with the most_frequent model. Furthermore, for the\\n“not nine” class we have a recall of 1, as we classified all samples as “not nine. ” The\\nlast column next to the f-score provides the support of each class, which simply means\\nthe number of samples in this class according to the ground truth.\\nThe last row in the classification report shows a weighted (by the number of samples\\nin the class) average of the numbers for each class. Here are two more reports, one for\\nthe dummy classifier and one for the logistic regression:\\nIn[47]:\\nprint(classification_report(y_test, pred_dummy,\\n                            target_names=[\"not nine\", \"nine\"]))\\nOut[47]:\\n             precision    recall  f1-score   support\\n   not nine       0.90      0.92      0.91       403\\n       nine       0.11      0.09      0.10        47\\navg / total       0.81      0.83      0.82       450\\nIn[48]:\\nprint(classification_report(y_test, pred_logreg,\\n                            target_names=[\"not nine\", \"nine\"]))\\nOut[48]:\\n             precision    recall  f1-score   support\\n   not nine       0.98      1.00      0.99       403\\n       nine       0.95      0.83      0.89        47\\navg / total       0.98      0.98      0.98       450\\nEvaluation Metrics and Scoring | 285'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 299, 'page_label': '286'}, page_content='As you may notice when looking at the reports, the differences between the dummy\\nmodels and a very good model are not as clear any more. Picking which class is\\ndeclared the positive class has a big impact on the metrics. While the f-score for the\\ndummy classification is 0.13 (vs. 0.89 for the logistic regression) on the “nine” class,\\nfor the “not nine” class it is 0.90 vs. 0.99, which both seem like reasonable results.\\nLooking at all the numbers together paints a pretty accurate picture, though, and we\\ncan clearly see the superiority of the logistic regression model.\\nTaking uncertainty into account\\nThe confusion matrix and the classification report provide a very detailed analysis of\\na particular set of predictions. However, the predictions themselves already threw\\naway a lot of information that is contained in the model. As we discussed in Chap‐\\nter 2, most classifiers provide a decision_function or a predict_proba method to\\nassess degrees of certainty about predictions. Making predictions can be seen as\\nthresholding the output of decision_function or predict_proba at a certain fixed\\npoint—in binary classification we use 0 for the decision function and 0.5 for\\npredict_proba.\\nThe following is an example of an imbalanced binary classification task, with 400\\npoints in the negative class classified against 50 points in the positive class. The train‐\\ning data is shown on the left in Figure 5-12. We train a kernel SVM model on this\\ndata, and the plots to the right of the training data illustrate the values of the decision\\nfunction as a heat map. Y ou can see a black circle in the plot in the top center, which\\ndenotes the threshold of the decision_function being exactly zero. Points inside this\\ncircle will be classified as the positive class, and points outside as the negative class:\\nIn[49]:\\nfrom mglearn.datasets import make_blobs\\nX, y = make_blobs(n_samples=(400, 50), centers=2, cluster_std=[7.0, 2],\\n                  random_state=22)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\nsvc = SVC(gamma=.05).fit(X_train, y_train)\\nIn[50]:\\nmglearn.plots.plot_decision_threshold()\\n286 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 300, 'page_label': '287'}, page_content='Figure 5-12. Heatmap of the decision function and the impact of changing the decision\\nthreshold\\nWe can use the classification_report function to evaluate precision and recall for\\nboth classes:\\nIn[51]:\\nprint(classification_report(y_test, svc.predict(X_test)))\\nOut[51]:\\n             precision    recall  f1-score   support\\n          0       0.97      0.89      0.93       104\\n          1       0.35      0.67      0.46         9\\navg / total       0.92      0.88      0.89       113\\nFor class 1, we get a fairly small recall, and precision is mixed. Because class 0 is so\\nmuch larger, the classifier focuses on getting class 0 right, and not the smaller class 1.\\nLet’s assume in our application it is more important to have a high recall for class 1, as\\nin the cancer screening example earlier. This means we are willing to risk more false\\npositives (false class 1) in exchange for more true positives (which will increase the\\nrecall). The predictions generated by svc.predict really do not fulfill this require‐\\nment, but we can adjust the predictions to focus on a higher recall of class 1 by\\nchanging the decision threshold away from 0. By default, points with a deci\\nsion_function value greater than 0 will be classified as class 1. We want more points\\nto be classified as class 1, so we need to decrease the threshold:\\nEvaluation Metrics and Scoring | 287'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 301, 'page_label': '288'}, page_content='In[52]:\\ny_pred_lower_threshold = svc.decision_function(X_test) > -.8\\nLet’s look at the classification report for this prediction:\\nIn[53]:\\nprint(classification_report(y_test, y_pred_lower_threshold))\\nOut[53]:\\n             precision    recall  f1-score   support\\n          0       1.00      0.82      0.90       104\\n          1       0.32      1.00      0.49         9\\navg / total       0.95      0.83      0.87       113\\nAs expected, the recall of class 1 went up, and the precision went down. We are now\\nclassifying a larger region of space as class 1, as illustrated in the top-right panel of\\nFigure 5-12. If you value precision over recall or the other way around, or your data is\\nheavily imbalanced, changing the decision threshold is the easiest way to obtain bet‐\\nter results. As the decision_function can have arbitrary ranges, it is hard to provide\\na rule of thumb regarding how to pick a threshold.\\nIf you do set a threshold, you need to be careful not to do so using\\nthe test set. As with any other parameter, setting a decision thresh‐\\nold on the test set is likely to yield overly optimistic results. Use a\\nvalidation set or cross-validation instead.\\nPicking a threshold for models that implement the predict_proba method can be\\neasier, as the output of predict_proba is on a fixed 0 to 1 scale, and models probabil‐\\nities. By default, the threshold of 0.5 means that if the model is more than 50% “sure”\\nthat a point is of the positive class, it will be classified as such. Increasing the thresh‐\\nold means that the model needs to be more confident to make a positive decision\\n(and less confident to make a negative decision). While working with probabilities\\nmay be more intuitive than working with arbitrary thresholds, not all models provide\\nrealistic models of uncertainty (a DecisionTree that is grown to its full depth is\\nalways 100% sure of its decisions, even though it might often be wrong). This relates\\nto the concept of calibration: a calibrated model is a model that provides an accurate\\nmeasure of its uncertainty. Discussing calibration in detail is beyond the scope of this\\nbook, but you can find more details in the paper “Predicting Good Probabilities with\\nSupervised Learning” by Alexandru Niculescu-Mizil and Rich Caruana.\\n288 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 302, 'page_label': '289'}, page_content='Precision-recall curves and ROC curves\\nAs we just discussed, changing the threshold that is used to make a classification deci‐\\nsion in a model is a way to adjust the trade-off of precision and recall for a given clas‐\\nsifier. Maybe you want to miss less than 10% of positive samples, meaning a desired\\nrecall of 90%. This decision depends on the application, and it should be driven by\\nbusiness goals. Once a particular goal is set—say, a particular recall or precision value\\nfor a class—a threshold can be set appropriately. It is always possible to set a thresh‐\\nold to fulfill a particular target, like 90% recall. The hard part is to develop a model\\nthat still has reasonable precision with this threshold—if you classify everything as\\npositive, you will have 100% recall, but your model will be useless.\\nSetting a requirement on a classifier like 90% recall is often called setting the operat‐\\ning point. Fixing an operating point is often helpful in business settings to make per‐\\nformance guarantees to customers or other groups inside your organization.\\nOften, when developing a new model, it is not entirely clear what the operating point\\nwill be. For this reason, and to understand a modeling problem better, it is instructive\\nto look at all possible thresholds, or all possible trade-offs of precision and recalls at\\nonce. This is possible using a tool called the precision-recall curve. Y ou can find the\\nfunction to compute the precision-recall curve in the sklearn.metrics module. It\\nneeds the ground truth labeling and predicted uncertainties, created via either\\ndecision_function or predict_proba:\\nIn[54]:\\nfrom sklearn.metrics import precision_recall_curve\\nprecision, recall, thresholds = precision_recall_curve(\\n    y_test, svc.decision_function(X_test))\\nThe precision_recall_curve function returns a list of precision and recall values\\nfor all possible thresholds (all values that appear in the decision function) in sorted\\norder, so we can plot a curve, as seen in Figure 5-13:\\nIn[55]:\\n# Use more data points for a smoother curve\\nX, y = make_blobs(n_samples=(4000, 500), centers=2, cluster_std=[7.0, 2],\\n                  random_state=22)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\nsvc = SVC(gamma=.05).fit(X_train, y_train)\\nprecision, recall, thresholds = precision_recall_curve(\\n    y_test, svc.decision_function(X_test))\\n# find threshold closest to zero\\nclose_zero = np.argmin(np.abs(thresholds))\\nplt.plot(precision[close_zero], recall[close_zero], \\'o\\', markersize=10,\\n         label=\"threshold zero\", fillstyle=\"none\", c=\\'k\\', mew=2)\\nplt.plot(precision, recall, label=\"precision recall curve\")\\nplt.xlabel(\"Precision\")\\nplt.ylabel(\"Recall\")\\nEvaluation Metrics and Scoring | 289'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 303, 'page_label': '290'}, page_content='Figure 5-13. Precision recall curve for SVC(gamma=0.05)\\nEach point along the curve in Figure 5-13 corresponds to a possible threshold of the\\ndecision_function. We can see, for example, that we can achieve a recall of 0.4 at a\\nprecision of about 0.75. The black circle marks the point that corresponds to a thresh‐\\nold of 0, the default threshold for decision_function. This point is the trade-off that\\nis chosen when calling the predict method.\\nThe closer a curve stays to the upper-right corner, the better the classifier. A point at\\nthe upper right means high precision and high recall for the same threshold. The\\ncurve starts at the top-left corner, corresponding to a very low threshold, classifying\\neverything as the positive class. Raising the threshold moves the curve toward higher\\nprecision, but also lower recall. Raising the threshold more and more, we get to a sit‐\\nuation where most of the points classified as being positive are true positives, leading\\nto a very high precision but lower recall. The more the model keeps recall high as\\nprecision goes up, the better.\\nLooking at this particular curve a bit more, we can see that with this model it is possi‐\\nble to get a precision of up to around 0.5 with very high recall. If we want a much\\nhigher precision, we have to sacrifice a lot of recall. In other words, on the left the\\ncurve is relatively flat, meaning that recall does not go down a lot when we require\\nincreased precision. For precision greater than 0.5, each gain in precision costs us a\\nlot of recall.\\nDifferent classifiers can work well in different parts of the curve—that is, at different\\noperating points. Let’s compare the SVM we trained to a random forest trained on the\\nsame dataset. The RandomForestClassifier doesn’t have a decision_function, only\\npredict_proba. The precision_recall_curve function expects as its second argu‐\\nment a certainty measure for the positive class (class 1), so we pass the probability of\\na sample being class 1—that is, rf.predict_proba(X_test)[:, 1] . The default\\nthreshold for predict_proba in binary classification is 0.5, so this is the point we\\nmarked on the curve (see Figure 5-14):\\n290 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 304, 'page_label': '291'}, page_content='In[56]:\\nfrom sklearn.ensemble import RandomForestClassifier\\nrf = RandomForestClassifier(n_estimators=100, random_state=0, max_features=2)\\nrf.fit(X_train, y_train)\\n# RandomForestClassifier has predict_proba, but not decision_function\\nprecision_rf, recall_rf, thresholds_rf = precision_recall_curve(\\n    y_test, rf.predict_proba(X_test)[:, 1])\\nplt.plot(precision, recall, label=\"svc\")\\nplt.plot(precision[close_zero], recall[close_zero], \\'o\\', markersize=10,\\n         label=\"threshold zero svc\", fillstyle=\"none\", c=\\'k\\', mew=2)\\nplt.plot(precision_rf, recall_rf, label=\"rf\")\\nclose_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))\\nplt.plot(precision_rf[close_default_rf], recall_rf[close_default_rf], \\'^\\', c=\\'k\\',\\n         markersize=10, label=\"threshold 0.5 rf\", fillstyle=\"none\", mew=2)\\nplt.xlabel(\"Precision\")\\nplt.ylabel(\"Recall\")\\nplt.legend(loc=\"best\")\\nFigure 5-14. Comparing precision recall curves of SVM and random forest\\nFrom the comparison plot we can see that the random forest performs better at the\\nextremes, for very high recall or very high precision requirements. Around the mid‐\\ndle (approximately precision=0.7), the SVM performs better. If we only looked at the\\nf1-score to compare overall performance, we would have missed these subtleties. The\\nf1-score only captures one point on the precision-recall curve, the one given by the\\ndefault threshold:\\nEvaluation Metrics and Scoring | 291'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 305, 'page_label': '292'}, page_content='4 There are some minor technical differences between the area under the precision-recall curve and average\\nprecision. However, this explanation conveys the general idea.\\nIn[57]:\\nprint(\"f1_score of random forest: {:.3f}\".format(\\n    f1_score(y_test, rf.predict(X_test))))\\nprint(\"f1_score of svc: {:.3f}\".format(f1_score(y_test, svc.predict(X_test))))\\nOut[57]:\\nf1_score of random forest: 0.610\\nf1_score of svc: 0.656\\nComparing two precision-recall curves provides a lot of detailed insight, but is a fairly\\nmanual process. For automatic model comparison, we might want to summarize the\\ninformation contained in the curve, without limiting ourselves to a particular thresh‐\\nold or operating point. One particular way to summarize the precision-recall curve is\\nby computing the integral or area under the curve of the precision-recall curve, also\\nknown as the average precision.4 Y ou can use the average_precision_score function\\nto compute the average precision. Because we need to compute the ROC curve and\\nconsider multiple thresholds, the result of decision_function or predict_proba\\nneeds to be passed to average_precision_score, not the result of predict:\\nIn[58]:\\nfrom sklearn.metrics import average_precision_score\\nap_rf = average_precision_score(y_test, rf.predict_proba(X_test)[:, 1])\\nap_svc = average_precision_score(y_test, svc.decision_function(X_test))\\nprint(\"Average precision of random forest: {:.3f}\".format(ap_rf))\\nprint(\"Average precision of svc: {:.3f}\".format(ap_svc))\\nOut[58]:\\nAverage precision of random forest: 0.666\\nAverage precision of svc: 0.663\\nWhen averaging over all possible thresholds, we see that the random forest and SVC\\nperform similarly well, with the random forest even slightly ahead. This is quite dif‐\\nferent from the result we got from f1_score earlier. Because average precision is the\\narea under a curve that goes from 0 to 1, average precision always returns a value\\nbetween 0 (worst) and 1 (best). The average precision of a classifier that assigns\\ndecision_function at random is the fraction of positive samples in the dataset.\\nReceiver operating characteristics (ROC) and AUC\\nThere is another tool that is commonly used to analyze the behavior of classifiers at\\ndifferent thresholds: the receiver operating characteristics curve , or ROC curve  for\\nshort. Similar to the precision-recall curve, the ROC curve considers all possible\\n292 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 306, 'page_label': '293'}, page_content='thresholds for a given classifier, but instead of reporting precision and recall, it shows\\nthe false positive rate (FPR) against the true positive rate (TPR). Recall that the true\\npositive rate is simply another name for recall, while the false positive rate is the frac‐\\ntion of false positives out of all negative samples:\\nFPR = FP\\nFP+TN\\nThe ROC curve can be computed using the roc_curve function (see Figure 5-15):\\nIn[59]:\\nfrom sklearn.metrics import roc_curve\\nfpr, tpr, thresholds = roc_curve(y_test, svc.decision_function(X_test))\\nplt.plot(fpr, tpr, label=\"ROC Curve\")\\nplt.xlabel(\"FPR\")\\nplt.ylabel(\"TPR (recall)\")\\n# find threshold closest to zero\\nclose_zero = np.argmin(np.abs(thresholds))\\nplt.plot(fpr[close_zero], tpr[close_zero], \\'o\\', markersize=10,\\n         label=\"threshold zero\", fillstyle=\"none\", c=\\'k\\', mew=2)\\nplt.legend(loc=4)\\nFigure 5-15. ROC curve for SVM\\nFor the ROC curve, the ideal curve is close to the top left: you want a classifier that\\nproduces a high recall while keeping a low false positive rate. Compared to the default\\nthreshold of 0, the curve shows that we can achieve a significantly higher recall\\n(around 0.9) while only increasing the FPR slightly. The point closest to the top left\\nmight be a better operating point than the one chosen by default. Again, be aware that\\nchoosing a threshold should not be done on the test set, but on a separate validation\\nset.\\nEvaluation Metrics and Scoring | 293'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 307, 'page_label': '294'}, page_content='Y ou can find a comparison of the random forest and the SVM using ROC curves in\\nFigure 5-16:\\nIn[60]:\\nfrom sklearn.metrics import roc_curve\\nfpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, rf.predict_proba(X_test)[:, 1])\\nplt.plot(fpr, tpr, label=\"ROC Curve SVC\")\\nplt.plot(fpr_rf, tpr_rf, label=\"ROC Curve RF\")\\nplt.xlabel(\"FPR\")\\nplt.ylabel(\"TPR (recall)\")\\nplt.plot(fpr[close_zero], tpr[close_zero], \\'o\\', markersize=10,\\n         label=\"threshold zero SVC\", fillstyle=\"none\", c=\\'k\\', mew=2)\\nclose_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))\\nplt.plot(fpr_rf[close_default_rf], tpr[close_default_rf], \\'^\\', markersize=10,\\n         label=\"threshold 0.5 RF\", fillstyle=\"none\", c=\\'k\\', mew=2)\\nplt.legend(loc=4)\\nFigure 5-16. Comparing ROC curves for SVM and random forest\\nAs for the precision-recall curve, we often want to summarize the ROC curve using a\\nsingle number, the area under the curve (this is commonly just referred to as the\\nAUC, and it is understood that the curve in question is the ROC curve). We can com‐\\npute the area under the ROC curve using the roc_auc_score function:\\n294 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 308, 'page_label': '295'}, page_content='In[61]:\\nfrom sklearn.metrics import roc_auc_score\\nrf_auc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])\\nsvc_auc = roc_auc_score(y_test, svc.decision_function(X_test))\\nprint(\"AUC for Random Forest: {:.3f}\".format(rf_auc))\\nprint(\"AUC for SVC: {:.3f}\".format(svc_auc))\\nOut[61]:\\nAUC for Random Forest: 0.937\\nAUC for SVC: 0.916\\nComparing the random forest and SVM using the AUC score, we find that the ran‐\\ndom forest performs quite a bit better than the SVM. Recall that because average pre‐\\ncision is the area under a curve that goes from 0 to 1, average precision always returns\\na value between 0 (worst) and 1 (best). Predicting randomly always produces an AUC\\nof 0.5, no matter how imbalanced the classes in a dataset are. This makes AUC a\\nmuch better metric for imbalanced classification problems than accuracy. The AUC\\ncan be interpreted as evaluating the ranking of positive samples. It’s equivalent to the\\nprobability that a randomly picked point of the positive class will have a higher score\\naccording to the classifier than a randomly picked point from the negative class. So, a\\nperfect AUC of 1 means that all positive points have a higher score than all negative\\npoints. For classification problems with imbalanced classes, using AUC for model\\nselection is often much more meaningful than using accuracy.\\nLet’s go back to the problem we studied earlier of classifying all nines in the digits\\ndataset versus all other digits. We will classify the dataset with an SVM with three dif‐\\nferent settings of the kernel bandwidth, gamma (see Figure 5-17):\\nIn[62]:\\ny = digits.target == 9\\nX_train, X_test, y_train, y_test = train_test_split(\\n    digits.data, y, random_state=0)\\nplt.figure()\\nfor gamma in [1, 0.05, 0.01]:\\n    svc = SVC(gamma=gamma).fit(X_train, y_train)\\n    accuracy = svc.score(X_test, y_test)\\n    auc = roc_auc_score(y_test, svc.decision_function(X_test))\\n    fpr, tpr, _ = roc_curve(y_test , svc.decision_function(X_test))\\n    print(\"gamma = {:.2f}  accuracy = {:.2f}  AUC = {:.2f}\".format(\\n    gamma, accuracy, auc))\\n    plt.plot(fpr, tpr, label=\"gamma={:.3f}\".format(gamma))\\nplt.xlabel(\"FPR\")\\nplt.ylabel(\"TPR\")\\nplt.xlim(-0.01, 1)\\nplt.ylim(0, 1.02)\\nplt.legend(loc=\"best\")\\nEvaluation Metrics and Scoring | 295'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 309, 'page_label': '296'}, page_content='5 Looking at the curve for gamma=0.01 in detail, you can see a small kink close to the top left. That means that at\\nleast one point was not ranked correctly. The AUC of 1.0 is a consequence of rounding to the second decimal\\npoint.\\nOut[62]:\\ngamma = 1.00  accuracy = 0.90  AUC = 0.50\\ngamma = 0.05  accuracy = 0.90  AUC = 0.90\\ngamma = 0.01  accuracy = 0.90  AUC = 1.00\\nFigure 5-17. Comparing ROC curves of SVMs with different settings of gamma\\nThe accuracy of all three settings of gamma is the same, 90%. This might be the same\\nas chance performance, or it might not. Looking at the AUC and the corresponding\\ncurve, however, we see a clear distinction between the three models. With gamma=1.0,\\nthe AUC is actually at chance level, meaning that the output of the decision_func\\ntion is as good as random. With gamma=0.05, performance drastically improves to an\\nAUC of 0.5. Finally, with gamma=0.01, we get a perfect AUC of 1.0. That means that\\nall positive points are ranked higher than all negative points according to the decision\\nfunction. In other words, with the right threshold, this model can classify the data\\nperfectly!5 Knowing this, we can adjust the threshold on this model and obtain great\\npredictions. If we had only used accuracy, we would never have discovered this.\\nFor this reason, we highly recommend using AUC when evaluating models on imbal‐\\nanced data. Keep in mind that AUC does not make use of the default threshold,\\nthough, so adjusting the decision threshold might be necessary to obtain useful classi‐\\nfication results from a model with a high AUC.\\nMetrics for Multiclass Classification\\nNow that we have discussed evaluation of binary classification tasks in depth, let’s\\nmove on to metrics to evaluate multiclass classification. Basically, all metrics for\\nmulticlass classification are derived from binary classification metrics, but averaged\\n296 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 310, 'page_label': '297'}, page_content='over all classes. Accuracy for multiclass classification is again defined as the fraction\\nof correctly classified examples. And again, when classes are imbalanced, accuracy is\\nnot a great evaluation measure. Imagine a three-class classification problem with 85%\\nof points belonging to class A, 10% belonging to class B, and 5% belonging to class C.\\nWhat does being 85% accurate mean on this dataset? In general, multiclass classifica‐\\ntion results are harder to understand than binary classification results. Apart from\\naccuracy, common tools are the confusion matrix and the classification report we saw\\nin the binary case in the previous section. Let’s apply these two detailed evaluation\\nmethods on the task of classifying the 10 different handwritten digits in the digits\\ndataset:\\nIn[63]:\\nfrom sklearn.metrics import accuracy_score\\nX_train, X_test, y_train, y_test = train_test_split(\\n    digits.data, digits.target, random_state=0)\\nlr = LogisticRegression().fit(X_train, y_train)\\npred = lr.predict(X_test)\\nprint(\"Accuracy: {:.3f}\".format(accuracy_score(y_test, pred)))\\nprint(\"Confusion matrix:\\\\n{}\".format(confusion_matrix(y_test, pred)))\\nOut[63]:\\nAccuracy: 0.953\\nConfusion matrix:\\n[[37  0  0  0  0  0  0  0  0  0]\\n [ 0 39  0  0  0  0  2  0  2  0]\\n [ 0  0 41  3  0  0  0  0  0  0]\\n [ 0  0  1 43  0  0  0  0  0  1]\\n [ 0  0  0  0 38  0  0  0  0  0]\\n [ 0  1  0  0  0 47  0  0  0  0]\\n [ 0  0  0  0  0  0 52  0  0  0]\\n [ 0  1  0  1  1  0  0 45  0  0]\\n [ 0  3  1  0  0  0  0  0 43  1]\\n [ 0  0  0  1  0  1  0  0  1 44]]\\nThe model has an accuracy of 95.3%, which already tells us that we are doing pretty\\nwell. The confusion matrix provides us with some more detail. As for the binary case,\\neach row corresponds to a true label, and each column corresponds to a predicted\\nlabel. Y ou can find a visually more appealing plot in Figure 5-18:\\nIn[64]:\\nscores_image = mglearn.tools.heatmap(\\n    confusion_matrix(y_test, pred), xlabel=\\'Predicted label\\',\\n    ylabel=\\'True label\\', xticklabels=digits.target_names,\\n    yticklabels=digits.target_names, cmap=plt.cm.gray_r, fmt=\"%d\")\\nplt.title(\"Confusion matrix\")\\nplt.gca().invert_yaxis()\\nEvaluation Metrics and Scoring | 297'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 311, 'page_label': '298'}, page_content='Figure 5-18. Confusion matrix for the 10-digit classification task\\nFor the first class, the digit 0, there are 37 samples in the class, and all of these sam‐\\nples were classified as class 0 (there are no false negatives for class 0). We can see that\\nbecause all other entries in the first row of the confusion matrix are 0. We can also see\\nthat no other digits were mistakenly classified as 0, because all other entries in the\\nfirst column of the confusion matrix are 0 (there are no false positives for class 0).\\nSome digits were confused with others, though—for example, the digit 2 (third row),\\nthree of which were classified as the digit 3 (fourth column). There was also one digit\\n3 that was classified as 2 (third column, fourth row) and one digit 8 that was classified\\nas 2 (thrid column, fourth row).\\nWith the classification_report function, we can compute the precision, recall,\\nand f-score for each class:\\nIn[65]:\\nprint(classification_report(y_test, pred))\\nOut[65]:\\n             precision    recall  f1-score   support\\n          0       1.00      1.00      1.00        37\\n          1       0.89      0.91      0.90        43\\n          2       0.95      0.93      0.94        44\\n          3       0.90      0.96      0.92        45\\n          4       0.97      1.00      0.99        38\\n          5       0.98      0.98      0.98        48\\n          6       0.96      1.00      0.98        52\\n          7       1.00      0.94      0.97        48\\n          8       0.93      0.90      0.91        48\\n          9       0.96      0.94      0.95        47\\navg / total       0.95      0.95      0.95       450\\n298 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 312, 'page_label': '299'}, page_content='Unsurprisingly, precision and recall are a perfect 1 for class 0, as there are no confu‐\\nsions with this class. For class 7, on the other hand, precision is 1 because no other\\nclass was mistakenly classified as 7, while for class 6, there are no false negatives, so\\nthe recall is 1. We can also see that the model has particular difficulties with classes 8\\nand 3.\\nThe most commonly used metric for imbalanced datasets in the multiclass setting is\\nthe multiclass version of the f-score. The idea behind the multiclass f-score is to com‐\\npute one binary f-score per class, with that class being the positive class and the other\\nclasses making up the negative classes. Then, these per-class f-scores are averaged\\nusing one of the following strategies:\\n• \"macro\" averaging computes the unweighted per-class f-scores. This gives equal\\nweight to all classes, no matter what their size is.\\n• \"weighted\" averaging computes the mean of the per-class f-scores, weighted by\\ntheir support. This is what is reported in the classification report.\\n• \"micro\" averaging computes the total number of false positives, false negatives,\\nand true positives over all classes, and then computes precision, recall, and f-\\nscore using these counts.\\nIf you care about each sample equally much, it is recommended to use the \"micro\"\\naverage f1-score; if you care about each class equally much, it is recommended to use\\nthe \"macro\" average f1-score:\\nIn[66]:\\nprint(\"Micro average f1 score: {:.3f}\".format\\n       (f1_score(y_test, pred, average=\"micro\")))\\nprint(\"Macro average f1 score: {:.3f}\".format\\n       (f1_score(y_test, pred, average=\"macro\")))\\nOut[66]:\\nMicro average f1 score: 0.953\\nMacro average f1 score: 0.954\\nRegression Metrics\\nEvaluation for regression can be done in similar detail as we did for classification—\\nfor example, by analyzing overpredicting the target versus underpredicting the target.\\nHowever, in most applications we’ve seen, using the default R2 used in the score\\nmethod of all regressors is enough. Sometimes business decisions are made on the\\nbasis of mean squared error or mean absolute error, which might give incentive to\\ntune models using these metrics. In general, though, we have found R2 to be a more\\nintuitive metric to evaluate regression models.\\nEvaluation Metrics and Scoring | 299'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 313, 'page_label': '300'}, page_content='Using Evaluation Metrics in Model Selection\\nWe have discussed many evaluation methods in detail, and how to apply them given\\nthe ground truth and a model. However, we often want to use metrics like AUC in\\nmodel selection using GridSearchCV or cross_val_score. Luckily scikit-learn\\nprovides a very simple way to achieve this, via the scoring argument that can be used\\nin both GridSearchCV and cross_val_score. Y ou can simply provide a string\\ndescribing the evaluation metric you want to use. Say, for example, we want to evalu‐\\nate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUC\\nscore. Changing the score from the default (accuracy) to AUC can be done by provid‐\\ning \"roc_auc\" as the scoring parameter:\\nIn[67]:\\n# default scoring for classification is accuracy\\nprint(\"Default scoring: {}\".format(\\n    cross_val_score(SVC(), digits.data, digits.target == 9)))\\n# providing scoring=\"accuracy\" doesn\\'t change the results\\nexplicit_accuracy =  cross_val_score(SVC(), digits.data, digits.target == 9,\\n                                     scoring=\"accuracy\")\\nprint(\"Explicit accuracy scoring: {}\".format(explicit_accuracy))\\nroc_auc =  cross_val_score(SVC(), digits.data, digits.target == 9,\\n                           scoring=\"roc_auc\")\\nprint(\"AUC scoring: {}\".format(roc_auc))\\nOut[67]:\\nDefault scoring: [ 0.9  0.9  0.9]\\nExplicit accuracy scoring: [ 0.9  0.9  0.9]\\nAUC scoring: [ 0.994  0.99   0.996]\\nSimilarly, we can change the metric used to pick the best parameters in Grid\\nSearchCV:\\nIn[68]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    digits.data, digits.target == 9, random_state=0)\\n# we provide a somewhat bad grid to illustrate the point:\\nparam_grid = {\\'gamma\\': [0.0001, 0.01, 0.1, 1, 10]}\\n# using the default scoring of accuracy:\\ngrid = GridSearchCV(SVC(), param_grid=param_grid)\\ngrid.fit(X_train, y_train)\\nprint(\"Grid-Search with accuracy\")\\nprint(\"Best parameters:\", grid.best_params_)\\nprint(\"Best cross-validation score (accuracy)): {:.3f}\".format(grid.best_score_))\\nprint(\"Test set AUC: {:.3f}\".format(\\n    roc_auc_score(y_test, grid.decision_function(X_test))))\\nprint(\"Test set accuracy: {:.3f}\".format(grid.score(X_test, y_test)))\\n300 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 314, 'page_label': '301'}, page_content='6 Finding a higher-accuracy solution using AUC is likely a consequence of accuracy being a bad measure of\\nmodel performance on imbalanced data.\\nOut[68]:\\nGrid-Search with accuracy\\nBest parameters: {\\'gamma\\': 0.0001}\\nBest cross-validation score (accuracy)): 0.970\\nTest set AUC: 0.992\\nTest set accuracy: 0.973\\nIn[69]:\\n# using AUC scoring instead:\\ngrid = GridSearchCV(SVC(), param_grid=param_grid, scoring=\"roc_auc\")\\ngrid.fit(X_train, y_train)\\nprint(\"\\\\nGrid-Search with AUC\")\\nprint(\"Best parameters:\", grid.best_params_)\\nprint(\"Best cross-validation score (AUC): {:.3f}\".format(grid.best_score_))\\nprint(\"Test set AUC: {:.3f}\".format(\\n    roc_auc_score(y_test, grid.decision_function(X_test))))\\nprint(\"Test set accuracy: {:.3f}\".format(grid.score(X_test, y_test)))\\nOut[69]:\\nGrid-Search with AUC\\nBest parameters: {\\'gamma\\': 0.01}\\nBest cross-validation score (AUC): 0.997\\nTest set AUC: 1.000\\nTest set accuracy: 1.000\\nWhen using accuracy, the parameter gamma=0.0001 is selected, while gamma=0.01 is\\nselected when using AUC. The cross-validation accuracy is consistent with the test set\\naccuracy in both cases. However, using AUC found a better parameter setting in\\nterms of AUC and even in terms of accuracy.6\\nThe most important values for the scoring parameter for classification are accuracy\\n(the default); roc_auc for the area under the ROC curve; average_precision for the\\narea under the precision-recall curve; f1, f1_macro, f1_micro, and f1_weighted for\\nthe binary f1-score and the different weighted variants. For regression, the most com‐\\nmonly used values are r2 for the R2 score, mean_squared_error for mean squared\\nerror, and mean_absolute_error for mean absolute error. Y ou can find a full list of\\nsupported arguments in the documentation or by looking at the SCORER dictionary\\ndefined in the metrics.scorer module:\\nEvaluation Metrics and Scoring | 301'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 315, 'page_label': '302'}, page_content='7 We highly recommend Foster Provost and Tom Fawcett’s book Data Science for Business (O’Reilly) for more\\ninformation on this topic.\\nIn[70]:\\nfrom sklearn.metrics.scorer import SCORERS\\nprint(\"Available scorers:\\\\n{}\".format(sorted(SCORERS.keys())))\\nOut[70]:\\nAvailable scorers:\\n[\\'accuracy\\', \\'adjusted_rand_score\\', \\'average_precision\\', \\'f1\\', \\'f1_macro\\',\\n \\'f1_micro\\', \\'f1_samples\\', \\'f1_weighted\\', \\'log_loss\\', \\'mean_absolute_error\\',\\n \\'mean_squared_error\\', \\'median_absolute_error\\', \\'precision\\', \\'precision_macro\\',\\n \\'precision_micro\\', \\'precision_samples\\', \\'precision_weighted\\', \\'r2\\', \\'recall\\',\\n \\'recall_macro\\', \\'recall_micro\\', \\'recall_samples\\', \\'recall_weighted\\', \\'roc_auc\\']\\nSummary and Outlook\\nIn this chapter we discussed cross-validation, grid search, and evaluation metrics, the\\ncornerstones of evaluating and improving machine learning algorithms. The tools\\ndescribed in this chapter, together with the algorithms described in Chapters 2 and 3,\\nare the bread and butter of every machine learning practitioner.\\nThere are two particular points that we made in this chapter that warrant repeating,\\nbecause they are often overlooked by new practitioners. The first has to do with\\ncross-validation. Cross-validation or the use of a test set allow us to evaluate a\\nmachine learning model as it will perform in the future. However, if we use the test\\nset or cross-validation to select a model or select model parameters, we “use up” the\\ntest data, and using the same data to evaluate how well our model will do in the future\\nwill lead to overly optimistic estimates. We therefore need to resort to a split into\\ntraining data for model building, validation data for model and parameter selection,\\nand test data for model evaluation. Instead of a simple split, we can replace each of\\nthese splits with cross-validation. The most commonly used form (as described ear‐\\nlier) is a training/test split for evaluation, and using cross-validation on the training\\nset for model and parameter selection.\\nThe second point has to do with the importance of the evaluation metric or scoring\\nfunction used for model selection and model evaluation. The theory of how to make\\nbusiness decisions from the predictions of a machine learning model is somewhat\\nbeyond the scope of this book. 7 However, it is rarely the case that the end goal of a\\nmachine learning task is building a model with a high accuracy. Make sure that the\\nmetric you choose to evaluate and select a model for is a good stand-in for what the\\nmodel will actually be used for. In reality, classification problems rarely have balanced\\nclasses, and often false positives and false negatives have very different consequences.\\n302 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 316, 'page_label': '303'}, page_content='Make sure you understand what these consequences are, and pick an evaluation met‐\\nric accordingly.\\nThe model evaluation and selection techniques we have described so far are the most\\nimportant tools in a data scientist’s toolbox. Grid search and cross-validation as we’ve\\ndescribed them in this chapter can only be applied to a single supervised model. We\\nhave seen before, however, that many models require preprocessing, and that in some\\napplications, like the face recognition example in Chapter 3 , extracting a different\\nrepresentation of the data can be useful. In the next chapter, we will introduce the\\nPipeline class, which allows us to use grid search and cross-validation on these com‐\\nplex chains of algorithms.\\nSummary and Outlook | 303'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 317, 'page_label': '304'}, page_content=''),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 318, 'page_label': '305'}, page_content='CHAPTER 6\\nAlgorithm Chains and Pipelines\\nFor many machine learning algorithms, the particular representation of the data that\\nyou provide is very important, as we discussed in Chapter 4. This starts with scaling\\nthe data and combining features by hand and goes all the way to learning features\\nusing unsupervised machine learning, as we saw in Chapter 3. Consequently, most\\nmachine learning applications require not only the application of a single algorithm,\\nbut the chaining together of many different processing steps and machine learning\\nmodels. In this chapter, we will cover how to use the Pipeline class to simplify the\\nprocess of building chains of transformations and models. In particular, we will see\\nhow we can combine Pipeline and GridSearchCV to search over parameters for all\\nprocessing steps at once.\\nAs an example of the importance of chaining models, we noticed that we can greatly\\nimprove the performance of a kernel SVM on the cancer dataset by using the Min\\nMaxScaler for preprocessing. Here’s code for splitting the data, computing the mini‐\\nmum and maximum, scaling the data, and training the SVM:\\nIn[1]:\\nfrom sklearn.svm import SVC\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import MinMaxScaler\\n# load and split the data\\ncancer = load_breast_cancer()\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, random_state=0)\\n# compute minimum and maximum on the training data\\nscaler = MinMaxScaler().fit(X_train)\\n305'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 319, 'page_label': '306'}, page_content='In[2]:\\n# rescale the training data\\nX_train_scaled = scaler.transform(X_train)\\nsvm = SVC()\\n# learn an SVM on the scaled training data\\nsvm.fit(X_train_scaled, y_train)\\n# scale the test data and score the scaled data\\nX_test_scaled = scaler.transform(X_test)\\nprint(\"Test score: {:.2f}\".format(svm.score(X_test_scaled, y_test)))\\nOut[2]:\\nTest score: 0.95\\nParameter Selection with Preprocessing\\nNow let’s say we want to find better parameters for SVC using GridSearchCV, as dis‐\\ncussed in Chapter 5. How should we go about doing this? A naive approach might\\nlook like this:\\nIn[3]:\\nfrom sklearn.model_selection import GridSearchCV\\n# for illustration purposes only, don\\'t use this code!\\nparam_grid = {\\'C\\': [0.001, 0.01, 0.1, 1, 10, 100],\\n              \\'gamma\\': [0.001, 0.01, 0.1, 1, 10, 100]}\\ngrid = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\\ngrid.fit(X_train_scaled, y_train)\\nprint(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\\nprint(\"Best set score: {:.2f}\".format(grid.score(X_test_scaled, y_test)))\\nprint(\"Best parameters: \", grid.best_params_)\\nOut[3]:\\nBest cross-validation accuracy: 0.98\\nBest set score: 0.97\\nBest parameters:  {\\'gamma\\': 1, \\'C\\': 1}\\nHere, we ran the grid search over the parameters of SVC using the scaled data. How‐\\never, there is a subtle catch in what we just did. When scaling the data, we used all the\\ndata in the training set to find out how to train it. We then use the scaled training data\\nto run our grid search using cross-validation. For each split in the cross-validation,\\nsome part of the original training set will be declared the training part of the split,\\nand some the test part of the split. The test part is used to measure what new data will\\nlook like to a model trained on the training part. However, we already used the infor‐\\nmation contained in the test part of the split, when scaling the data. Remember that\\nthe test part in each split in the cross-validation is part of the training set, and we\\nused the information from the entire training set to find the right scaling of the data.\\n306 | Chapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 320, 'page_label': '307'}, page_content='This is fundamentally different from how new data looks to the model.  If we observe\\nnew data (say, in form of our test set), this data will not have been used to scale the\\ntraining data, and it might have a different minimum and maximum than the train‐\\ning data. The following example ( Figure 6-1) shows how the data processing during\\ncross-validation and the final evaluation differ:\\nIn[4]:\\nmglearn.plots.plot_improper_processing()\\nFigure 6-1. Data usage when preprocessing outside the cross-validation loop\\nSo, the splits in the cross-validation no longer correctly mirror how new data will\\nlook to the modeling process. We already leaked information from these parts of the\\ndata into our modeling process. This will lead to overly optimistic results during\\ncross-validation, and possibly the selection of suboptimal parameters.\\nTo get around this problem, the splitting of the dataset during cross-validation should\\nbe done before doing any preprocessing. Any process that extracts knowledge from the\\ndataset should only ever be applied to the training portion of the dataset, so any\\ncross-validation should be the “outermost loop” in your processing.\\nTo achieve this in scikit-learn with the cross_val_score function and the Grid\\nSearchCV function, we can use the Pipeline class. The Pipeline class is a class that\\nallows “gluing” together multiple processing steps into a single scikit-learn estima‐\\nParameter Selection with Preprocessing | 307'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 321, 'page_label': '308'}, page_content='1 With one exception: the name can’t contain a double underscore, __.\\ntor. The Pipeline class itself has fit, predict, and score methods and behaves just\\nlike any other model in scikit-learn. The most common use case of the Pipeline\\nclass is in chaining preprocessing steps (like scaling of the data) together with a\\nsupervised model like a classifier.\\nBuilding Pipelines\\nLet’s look at how we can use the Pipeline class to express the workflow for training\\nan SVM after scaling the data with MinMaxScaler (for now without the grid search).\\nFirst, we build a pipeline object by providing it with a list of steps. Each step is a tuple\\ncontaining a name (any string of your choosing1) and an instance of an estimator:\\nIn[5]:\\nfrom sklearn.pipeline import Pipeline\\npipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])\\nHere, we created two steps: the first, called \"scaler\", is an instance of MinMaxScaler,\\nand the second, called \"svm\", is an instance of SVC. Now, we can fit the pipeline, like\\nany other scikit-learn estimator:\\nIn[6]:\\npipe.fit(X_train, y_train)\\nHere, pipe.fit first calls fit on the first step (the scaler), then transforms the train‐\\ning data using the scaler, and finally fits the SVM with the scaled data. To evaluate on\\nthe test data, we simply call pipe.score:\\nIn[7]:\\nprint(\"Test score: {:.2f}\".format(pipe.score(X_test, y_test)))\\nOut[7]:\\nTest score: 0.95\\nCalling the score method on the pipeline first transforms the test data using the\\nscaler, and then calls the score method on the SVM using the scaled test data. As you\\ncan see, the result is identical to the one we got from the code at the beginning of the\\nchapter, when doing the transformations by hand. Using the pipeline, we reduced the\\ncode needed for our “preprocessing + classification” process. The main benefit of\\nusing the pipeline, however, is that we can now use this single estimator in\\ncross_val_score or GridSearchCV.\\n308 | Chapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 322, 'page_label': '309'}, page_content='Using Pipelines in Grid Searches\\nUsing a pipeline in a grid search works the same way as using any other estimator. We\\ndefine a parameter grid to search over, and construct a GridSearchCV from the pipe‐\\nline and the parameter grid. When specifying the parameter grid, there is a slight\\nchange, though. We need to specify for each parameter which step of the pipeline it\\nbelongs to. Both parameters that we want to adjust, C and gamma, are parameters of\\nSVC, the second step. We gave this step the name \"svm\". The syntax to define a param‐\\neter grid for a pipeline is to specify for each parameter the step name, followed by __\\n(a double underscore), followed by the parameter name. To search over the C param‐\\neter of SVC we therefore have to use \"svm__C\" as the key in the parameter grid dictio‐\\nnary, and similarly for gamma:\\nIn[8]:\\nparam_grid = {\\'svm__C\\': [0.001, 0.01, 0.1, 1, 10, 100],\\n              \\'svm__gamma\\': [0.001, 0.01, 0.1, 1, 10, 100]}\\nWith this parameter grid we can use GridSearchCV as usual:\\nIn[9]:\\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\\ngrid.fit(X_train, y_train)\\nprint(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\\nprint(\"Test set score: {:.2f}\".format(grid.score(X_test, y_test)))\\nprint(\"Best parameters: {}\".format(grid.best_params_))\\nOut[9]:\\nBest cross-validation accuracy: 0.98\\nTest set score: 0.97\\nBest parameters: {\\'svm__C\\': 1, \\'svm__gamma\\': 1}\\nIn contrast to the grid search we did before, now for each split in the cross-validation,\\nthe MinMaxScaler is refit with only the training splits and no information is leaked\\nfrom the test split into the parameter search. Compare this ( Figure 6-2 ) with\\nFigure 6-1 earlier in this chapter:\\nIn[10]:\\nmglearn.plots.plot_proper_processing()\\nUsing Pipelines in Grid Searches | 309'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 323, 'page_label': '310'}, page_content='Figure 6-2. Data usage when preprocessing inside the cross-validation loop with a\\npipeline\\nThe impact of leaking information in the cross-validation varies depending on the\\nnature of the preprocessing step. Estimating the scale of the data using the test fold\\nusually doesn’t have a terrible impact, while using the test fold in feature extraction\\nand feature selection can lead to substantial differences in outcomes.\\nIllustrating Information Leakage\\nA great example of leaking information in cross-validation is given in Hastie, Tibshir‐\\nani, and Friedman’s book The Elements of Statistical Learning , and we reproduce an\\nadapted version here. Let’s consider a synthetic regression task with 100 samples and\\n1,000 features that are sampled independently from a Gaussian distribution. We also\\nsample the response from a Gaussian distribution:\\nIn[11]:\\nrnd = np.random.RandomState(seed=0)\\nX = rnd.normal(size=(100, 10000))\\ny = rnd.normal(size=(100,))\\nGiven the way we created the dataset, there is no relation between the data, X, and the\\ntarget, y (they are independent), so it should not be possible to learn anything from\\nthis dataset. We will now do the following. First, select the most informative of the 10\\nfeatures using SelectPercentile feature selection, and then we evaluate a Ridge\\nregressor using cross-validation:\\n310 | Chapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 324, 'page_label': '311'}, page_content='In[12]:\\nfrom sklearn.feature_selection import SelectPercentile, f_regression\\nselect = SelectPercentile(score_func=f_regression, percentile=5).fit(X, y)\\nX_selected = select.transform(X)\\nprint(\"X_selected.shape: {}\".format(X_selected.shape))\\nOut[12]:\\nX_selected.shape: (100, 500)\\nIn[13]:\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.linear_model import Ridge\\nprint(\"Cross-validation accuracy (cv only on ridge): {:.2f}\".format(\\n      np.mean(cross_val_score(Ridge(), X_selected, y, cv=5))))\\nOut[13]:\\nCross-validation accuracy (cv only on ridge): 0.91\\nThe mean R2 computed by cross-validation is 0.91, indicating a very good model.\\nThis clearly cannot be right, as our data is entirely random. What happened here is\\nthat our feature selection picked out some features among the 10,000 random features\\nthat are (by chance) very well correlated with the target. Because we fit the feature\\nselection outside of the cross-validation, it could find features that are correlated both\\non the training and the test folds. The information we leaked from the test folds was\\nvery informative, leading to highly unrealistic results. Let’s compare this to a proper\\ncross-validation using a pipeline:\\nIn[14]:\\npipe = Pipeline([(\"select\", SelectPercentile(score_func=f_regression,\\n                                             percentile=5)),\\n                 (\"ridge\", Ridge())])\\nprint(\"Cross-validation accuracy (pipeline): {:.2f}\".format(\\n      np.mean(cross_val_score(pipe, X, y, cv=5))))\\nOut[14]:\\nCross-validation accuracy (pipeline): -0.25\\nThis time, we get a negative R2 score, indicating a very poor model. Using the pipe‐\\nline, the feature selection is now inside the cross-validation loop. This means features\\ncan only be selected using the training folds of the data, not the test fold. The feature\\nselection finds features that are correlated with the target on the training set, but\\nbecause the data is entirely random, these features are not correlated with the target\\non the test set. In this example, rectifying the data leakage issue in the feature selec‐\\ntion makes the difference between concluding that a model works very well and con‐\\ncluding that a model works not at all.\\nUsing Pipelines in Grid Searches | 311'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 325, 'page_label': '312'}, page_content='2 Or just fit_transform.\\nThe General Pipeline Interface\\nThe Pipeline class is not restricted to preprocessing and classification, but can in\\nfact join any number of estimators together. For example, you could build a pipeline\\ncontaining feature extraction, feature selection, scaling, and classification, for a total\\nof four steps. Similarly, the last step could be regression or clustering instead of classi‐\\nfication.\\nThe only requirement for estimators in a pipeline is that all but the last step need to\\nhave a transform method, so they can produce a new representation of the data that\\ncan be used in the next step.\\nInternally, during the call to Pipeline.fit, the pipeline calls fit and then transform\\non each step in turn, 2 with the input given by the output of the transform method of\\nthe previous step. For the last step in the pipeline, just fit is called.\\nBrushing over some finer details, this is implemented as follows. Remember that pipe\\nline.steps is a list of tuples, so pipeline.steps[0][1] is the first estimator, pipe\\nline.steps[1][1] is the second estimator, and so on:\\nIn[15]:\\ndef fit(self, X, y):\\n    X_transformed = X\\n    for name, estimator in self.steps[:-1]:\\n        # iterate over all but the final step\\n        # fit and transform the data\\n        X_transformed = estimator.fit_transform(X_transformed, y)\\n    # fit the last step\\n    self.steps[-1][1].fit(X_transformed, y)\\n    return self\\nWhen predicting using Pipeline, we similarly transform the data using all but the\\nlast step, and then call predict on the last step:\\nIn[16]:\\ndef predict(self, X):\\n    X_transformed = X\\n    for step in self.steps[:-1]:\\n        # iterate over all but the final step\\n        # transform the data\\n        X_transformed = step[1].transform(X_transformed)\\n    # fit the last step\\n    return self.steps[-1][1].predict(X_transformed)\\n312 | Chapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 326, 'page_label': '313'}, page_content='The process is illustrated in Figure 6-3  for two transformers, T1 and T2, and a\\nclassifier (called Classifier).\\nFigure 6-3. Overview of the pipeline training and prediction process\\nThe pipeline is actually even more general than this. There is no requirement for the\\nlast step in a pipeline to have a predict function, and we could create a pipeline just\\ncontaining, for example, a scaler and PCA. Then, because the last step ( PCA) has a\\ntransform method, we could call transform on the pipeline to get the output of\\nPCA.transform applied to the data that was processed by the previous step. The last\\nstep of a pipeline is only required to have a fit method.\\nConvenient Pipeline Creation with make_pipeline\\nCreating a pipeline using the syntax described earlier is sometimes a bit cumbersome,\\nand we often don’t need user-specified names for each step. There is a convenience\\nfunction, make_pipeline, that will create a pipeline for us and automatically name\\neach step based on its class. The syntax for make_pipeline is as follows:\\nIn[17]:\\nfrom sklearn.pipeline import make_pipeline\\n# standard syntax\\npipe_long = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\\n# abbreviated syntax\\npipe_short = make_pipeline(MinMaxScaler(), SVC(C=100))\\nThe General Pipeline Interface | 313'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 327, 'page_label': '314'}, page_content='The pipeline objects pipe_long and pipe_short do exactly the same thing, but\\npipe_short has steps that were automatically named. We can see the names of the\\nsteps by looking at the steps attribute:\\nIn[18]:\\nprint(\"Pipeline steps:\\\\n{}\".format(pipe_short.steps))\\nOut[18]:\\nPipeline steps:\\n[(\\'minmaxscaler\\', MinMaxScaler(copy=True, feature_range=(0, 1))),\\n (\\'svc\\', SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\\n      decision_function_shape=None, degree=3, gamma=\\'auto\\',\\n             kernel=\\'rbf\\', max_iter=-1, probability=False,\\n             random_state=None, shrinking=True, tol=0.001,\\n             verbose=False))]\\nThe steps are named minmaxscaler and svc. In general, the step names are just low‐\\nercase versions of the class names. If multiple steps have the same class, a number is\\nappended:\\nIn[19]:\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.decomposition import PCA\\npipe = make_pipeline(StandardScaler(), PCA(n_components=2), StandardScaler())\\nprint(\"Pipeline steps:\\\\n{}\".format(pipe.steps))\\nOut[19]:\\nPipeline steps:\\n[(\\'standardscaler-1\\', StandardScaler(copy=True, with_mean=True, with_std=True)),\\n (\\'pca\\', PCA(copy=True, iterated_power=4, n_components=2, random_state=None,\\n             svd_solver=\\'auto\\', tol=0.0, whiten=False)),\\n (\\'standardscaler-2\\', StandardScaler(copy=True, with_mean=True, with_std=True))]\\nAs you can see, the first StandardScaler step was named standardscaler-1 and the\\nsecond standardscaler-2. However, in such settings it might be better to use the\\nPipeline construction with explicit names, to give more semantic names to each\\nstep.\\nAccessing Step Attributes\\nOften you will want to inspect attributes of one of the steps of the pipeline—say, the\\ncoefficients of a linear model or the components extracted by PCA. The easiest way to\\naccess the steps in a pipeline is via the named_steps attribute, which is a dictionary\\nfrom the step names to the estimators:\\n314 | Chapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 328, 'page_label': '315'}, page_content='In[20]:\\n# fit the pipeline defined before to the cancer dataset\\npipe.fit(cancer.data)\\n# extract the first two principal components from the \"pca\" step\\ncomponents = pipe.named_steps[\"pca\"].components_\\nprint(\"components.shape: {}\".format(components.shape))\\nOut[20]:\\ncomponents.shape: (2, 30)\\nAccessing Attributes in a Grid-Searched Pipeline\\nAs we discussed earlier in this chapter, one of the main reasons to use pipelines is for\\ndoing grid searches. A common task is to access some of the steps of a pipeline inside\\na grid search. Let’s grid search a LogisticRegression classifier on the cancer dataset,\\nusing Pipeline and StandardScaler to scale the data before passing it to the Logisti\\ncRegression classifier. First we create a pipeline using the make_pipeline function:\\nIn[21]:\\nfrom sklearn.linear_model import LogisticRegression\\npipe = make_pipeline(StandardScaler(), LogisticRegression())\\nNext, we create a parameter grid. As explained in Chapter 2 , the regularization\\nparameter to tune for LogisticRegression is the parameter C. We use a logarithmic\\ngrid for this parameter, searching between 0.01 and 100. Because we used the\\nmake_pipeline function, the name of the LogisticRegression step in the pipeline is\\nthe lowercased class name, logisticregression. To tune the parameter C, we there‐\\nfore have to specify a parameter grid for logisticregression__C:\\nIn[22]:\\nparam_grid = {\\'logisticregression__C\\': [0.01, 0.1, 1, 10, 100]}\\nAs usual, we split the cancer dataset into training and test sets, and fit a grid search:\\nIn[23]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, random_state=4)\\ngrid = GridSearchCV(pipe, param_grid, cv=5)\\ngrid.fit(X_train, y_train)\\nSo how do we access the coefficients of the best LogisticRegression model that was\\nfound by GridSearchCV? From Chapter 5  we know that the best model found by\\nGridSearchCV, trained on all the training data, is stored in grid.best_estimator_:\\nThe General Pipeline Interface | 315'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 329, 'page_label': '316'}, page_content='In[24]:\\nprint(\"Best estimator:\\\\n{}\".format(grid.best_estimator_))\\nOut[24]:\\nBest estimator:\\nPipeline(steps=[\\n    (\\'standardscaler\\', StandardScaler(copy=True, with_mean=True, with_std=True)),\\n    (\\'logisticregression\\', LogisticRegression(C=0.1, class_weight=None,\\n    dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100,\\n    multi_class=\\'ovr\\', n_jobs=1, penalty=\\'l2\\', random_state=None,\\n    solver=\\'liblinear\\', tol=0.0001, verbose=0, warm_start=False))])\\nThis best_estimator_ in our case is a pipeline with two steps, standardscaler and\\nlogisticregression. To access the logisticregression step, we can use the\\nnamed_steps attribute of the pipeline, as explained earlier:\\nIn[25]:\\nprint(\"Logistic regression step:\\\\n{}\".format(\\n      grid.best_estimator_.named_steps[\"logisticregression\"]))\\nOut[25]:\\nLogistic regression step:\\nLogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                  intercept_scaling=1, max_iter=100, multi_class=\\'ovr\\', n_jobs=1,\\n                  penalty=\\'l2\\', random_state=None, solver=\\'liblinear\\', tol=0.0001,\\n                  verbose=0, warm_start=False)\\nNow that we have the trained LogisticRegression instance, we can access the coeffi‐\\ncients (weights) associated with each input feature:\\nIn[26]:\\nprint(\"Logistic regression coefficients:\\\\n{}\".format(\\n      grid.best_estimator_.named_steps[\"logisticregression\"].coef_))\\nOut[26]:\\nLogistic regression coefficients:\\n[[-0.389 -0.375 -0.376 -0.396 -0.115  0.017 -0.355 -0.39  -0.058  0.209\\n  -0.495 -0.004 -0.371 -0.383 -0.045  0.198  0.004 -0.049  0.21   0.224\\n  -0.547 -0.525 -0.499 -0.515 -0.393 -0.123 -0.388 -0.417 -0.325 -0.139]]\\nThis might be a somewhat lengthy expression, but often it comes in handy in under‐\\nstanding your models.\\n316 | Chapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 330, 'page_label': '317'}, page_content='Grid-Searching Preprocessing Steps and Model\\nParameters\\nUsing pipelines, we can encapsulate all the processing steps in our machine learning\\nworkflow in a single scikit-learn estimator. Another benefit of doing this is that we\\ncan now adjust the parameters of the preprocessing using the outcome of a supervised\\ntask like regression or classification. In previous chapters, we used polynomial fea‐\\ntures on the boston dataset before applying the ridge regressor. Let’s model that using\\na pipeline instead. The pipeline contains three steps—scaling the data, computing\\npolynomial features, and ridge regression:\\nIn[27]:\\nfrom sklearn.datasets import load_boston\\nboston = load_boston()\\nX_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,\\n                                                    random_state=0)\\nfrom sklearn.preprocessing import PolynomialFeatures\\npipe = make_pipeline(\\n    StandardScaler(),\\n    PolynomialFeatures(),\\n    Ridge())\\nHow do we know which degrees of polynomials to choose, or whether to choose any\\npolynomials or interactions at all? Ideally we want to select the degree parameter\\nbased on the outcome of the classification. Using our pipeline, we can search over the\\ndegree parameter together with the parameter alpha of Ridge. To do this, we define a\\nparam_grid that contains both, appropriately prefixed by the step names:\\nIn[28]:\\nparam_grid = {\\'polynomialfeatures__degree\\': [1, 2, 3],\\n              \\'ridge__alpha\\': [0.001, 0.01, 0.1, 1, 10, 100]}\\nNow we can run our grid search again:\\nIn[29]:\\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1)\\ngrid.fit(X_train, y_train)\\nWe can visualize the outcome of the cross-validation using a heat map (Figure 6-4), as\\nwe did in Chapter 5:\\nIn[30]:\\nplt.matshow(grid.cv_results_[\\'mean_test_score\\'].reshape(3, -1),\\n            vmin=0, cmap=\"viridis\")\\nplt.xlabel(\"ridge__alpha\")\\nplt.ylabel(\"polynomialfeatures__degree\")\\nGrid-Searching Preprocessing Steps and Model Parameters | 317'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 331, 'page_label': '318'}, page_content='plt.xticks(range(len(param_grid[\\'ridge__alpha\\'])), param_grid[\\'ridge__alpha\\'])\\nplt.yticks(range(len(param_grid[\\'polynomialfeatures__degree\\'])),\\n           param_grid[\\'polynomialfeatures__degree\\'])\\nplt.colorbar()\\nFigure 6-4. Heat map of mean cross-validation score as a function of the degree of the\\npolynomial features and alpha parameter of Ridge\\nLooking at the results produced by the cross-validation, we can see that using polyno‐\\nmials of degree two helps, but that degree-three polynomials are much worse than\\neither degree one or two. This is reflected in the best parameters that were found:\\nIn[31]:\\nprint(\"Best parameters: {}\".format(grid.best_params_))\\nOut[31]:\\nBest parameters: {\\'polynomialfeatures__degree\\': 2, \\'ridge__alpha\\': 10}\\nWhich lead to the following score:\\nIn[32]:\\nprint(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test)))\\nOut[32]:\\nTest-set score: 0.77\\nLet’s run a grid search without polynomial features for comparison:\\nIn[33]:\\nparam_grid = {\\'ridge__alpha\\': [0.001, 0.01, 0.1, 1, 10, 100]}\\npipe = make_pipeline(StandardScaler(), Ridge())\\ngrid = GridSearchCV(pipe, param_grid, cv=5)\\ngrid.fit(X_train, y_train)\\nprint(\"Score without poly features: {:.2f}\".format(grid.score(X_test, y_test)))\\n318 | Chapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 332, 'page_label': '319'}, page_content=\"Out[33]:\\nScore without poly features: 0.63\\nAs we would expect looking at the grid search results visualized in Figure 6-4, using\\nno polynomial features leads to decidedly worse results.\\nSearching over preprocessing parameters together with model parameters is a very\\npowerful strategy. However, keep in mind that GridSearchCV tries all possible combi‐\\nnations of the specified parameters. Therefore, adding more parameters to your grid\\nexponentially increases the number of models that need to be built.\\nGrid-Searching Which Model To Use\\nY ou can even go further in combining GridSearchCV and Pipeline: it is also possible\\nto search over the actual steps being performed in the pipeline (say whether to use\\nStandardScaler or MinMaxScaler). This leads to an even bigger search space and\\nshould be considered carefully. Trying all possible solutions is usually not a viable\\nmachine learning strategy. However, here is an example comparing a RandomForest\\nClassifier and an SVC on the iris dataset. We know that the SVC might need the\\ndata to be scaled, so we also search over whether to use StandardScaler or no pre‐\\nprocessing. For the RandomForestClassifier, we know that no preprocessing is nec‐\\nessary. We start by defining the pipeline. Here, we explicitly name the steps. We want\\ntwo steps, one for the preprocessing and then a classifier. We can instantiate this\\nusing SVC and StandardScaler:\\nIn[34]:\\npipe = Pipeline([('preprocessing', StandardScaler()), ('classifier', SVC())])\\nNow we can define the parameter_grid to search over. We want the classifier to\\nbe either RandomForestClassifier or SVC. Because they have different parameters to\\ntune, and need different preprocessing, we can make use of the list of search grids we\\ndiscussed in “Search over spaces that are not grids” on page 271. To assign an estima‐\\ntor to a step, we use the name of the step as the parameter name. When we wanted to\\nskip a step in the pipeline (for example, because we don’t need preprocessing for the\\nRandomForest), we can set that step to None:\\nIn[35]:\\nfrom sklearn.ensemble import RandomForestClassifier\\nparam_grid = [\\n    {'classifier': [SVC()], 'preprocessing': [StandardScaler(), None],\\n     'classifier__gamma': [0.001, 0.01, 0.1, 1, 10, 100],\\n     'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]},\\n    {'classifier': [RandomForestClassifier(n_estimators=100)],\\n     'preprocessing': [None], 'classifier__max_features': [1, 2, 3]}]\\nGrid-Searching Which Model To Use | 319\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 333, 'page_label': '320'}, page_content='Now we can instantiate and run the grid search as usual, here on the cancer dataset:\\nIn[36]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, random_state=0)\\ngrid = GridSearchCV(pipe, param_grid, cv=5)\\ngrid.fit(X_train, y_train)\\nprint(\"Best params:\\\\n{}\\\\n\".format(grid.best_params_))\\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\\nprint(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test)))\\nOut[36]:\\nBest params:\\n{\\'classifier\\':\\n SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\\n     decision_function_shape=None, degree=3, gamma=0.01, kernel=\\'rbf\\',\\n     max_iter=-1, probability=False, random_state=None, shrinking=True,\\n     tol=0.001, verbose=False),\\n \\'preprocessing\\':\\n StandardScaler(copy=True, with_mean=True, with_std=True),\\n \\'classifier__C\\': 10, \\'classifier__gamma\\': 0.01}\\nBest cross-validation score: 0.99\\nTest-set score: 0.98\\nThe outcome of the grid search is that SVC with StandardScaler preprocessing, C=10,\\nand gamma=0.01 gave the best result.\\nSummary and Outlook\\nIn this chapter we introduced the Pipeline class, a general-purpose tool to chain\\ntogether multiple processing steps in a machine learning workflow. Real-world appli‐\\ncations of machine learning rarely involve an isolated use of a model, and instead are\\na sequence of processing steps. Using pipelines allows us to encapsulate multiple steps\\ninto a single Python object that adheres to the familiar scikit-learn interface of fit,\\npredict, and transform. In particular when doing model evaluation using cross-\\nvalidation and parameter selection using grid search, using the Pipeline class to cap‐\\nture all the processing steps is essential for proper evaluation. The Pipeline class also\\nallows writing more succinct code, and reduces the likelihood of mistakes that can\\nhappen when building processing chains without the pipeline class (like forgetting\\nto apply all transformers on the test set, or not applying them in the right order).\\nChoosing the right combination of feature extraction, preprocessing, and models is\\nsomewhat of an art, and often requires some trial and error. However, using pipe‐\\nlines, this “trying out” of many different processing steps is quite simple. When\\n320 | Chapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 334, 'page_label': '321'}, page_content='experimenting, be careful not to overcomplicate your processes, and make sure to\\nevaluate whether every component you are including in your model is necessary.\\nWith this chapter, we have completed our survey of general-purpose tools and algo‐\\nrithms provided by scikit-learn. Y ou now possess all the required skills and know\\nthe necessary mechanisms to apply machine learning in practice. In the next chapter,\\nwe will dive in more detail into one particular type of data that is commonly seen in\\npractice, and that requires some special expertise to handle correctly: text data.\\nSummary and Outlook | 321'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 335, 'page_label': '322'}, page_content=''),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 336, 'page_label': '323'}, page_content='CHAPTER 7\\nWorking with Text Data\\nIn Chapter 4, we talked about two kinds of features that can represent properties of\\nthe data: continuous features that describe a quantity, and categorical features that are\\nitems from a fixed list. There is a third kind of feature that can be found in many\\napplications, which is text. For example, if we want to classify an email message as\\neither a legitimate email or spam, the content of the email will certainly contain\\nimportant information for this classification task. Or maybe we want to learn about\\nthe opinion of a politician on the topic of immigration. Here, that individual’s\\nspeeches or tweets might provide useful information. In customer service, we often\\nwant to find out if a message is a complaint or an inquiry. We can use the subject line\\nand content of a message to automatically determine the customer’s intent, which\\nallows us to send the message to the appropriate department, or even send a fully\\nautomatic reply.\\nText data is usually represented as strings, made up of characters. In any of the exam‐\\nples just given, the length of the text data will vary. This feature is clearly very differ‐\\nent from the numeric features that we’ve discussed so far, and we will need to process\\nthe data before we can apply our machine learning algorithms to it.\\nTypes of Data Represented as Strings\\nBefore we dive into the processing steps that go into representing text data for\\nmachine learning, we want to briefly discuss different kinds of text data that you\\nmight encounter. Text is usually just a string in your dataset, but not all string features\\nshould be treated as text. A string feature can sometimes represent categorical vari‐\\nables, as we discussed in Chapter 5. There is no way to know how to treat a string\\nfeature before looking at the data.\\n323'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 337, 'page_label': '324'}, page_content='There are four kinds of string data you might see:\\n• Categorical data\\n• Free strings that can be semantically mapped to categories\\n• Structured string data\\n• Text data\\nCategorical data is data that comes from a fixed list. Say you collect data via a survey\\nwhere you ask people their favorite color, with a drop-down menu that allows them\\nto select from “red, ” “green, ” “blue, ” “yellow, ” “black, ” “white, ” “purple, ” and “pink. ”\\nThis will result in a dataset with exactly eight different possible values, which clearly\\nencode a categorical variable. Y ou can check whether this is the case for your data by\\neyeballing it (if you see very many different strings it is unlikely that this is a categori‐\\ncal variable) and confirm it by computing the unique values over the dataset, and\\npossibly a histogram over how often each appears. Y ou also might want to check\\nwhether each variable actually corresponds to a category that makes sense for your\\napplication. Maybe halfway through the existence of your survey, someone found that\\n“black” was misspelled as “blak” and subsequently fixed the survey. As a result, your\\ndataset contains both “blak” and “black, ” which correspond to the same semantic\\nmeaning and should be consolidated.\\nNow imagine instead of providing a drop-down menu, you provide a text field for the\\nusers to provide their own favorite colors. Many people might respond with a color\\nname like “black” or “blue. ” Others might make typographical errors, use different\\nspellings like “gray” and “grey, ” or use more evocative and specific names like “mid‐\\nnight blue. ” Y ou will also have some very strange entries. Some good examples come\\nfrom the xkcd Color Survey , where people had to name colors and came up with\\nnames like “velociraptor cloaka” and “my dentist’s office orange. I still remember his\\ndandruff slowly wafting into my gaping yaw, ” which are hard to map to colors auto‐\\nmatically (or at all). The responses you can obtain from a text field belong to the sec‐\\nond category in the list, free strings that can be semantically mapped to categories . It\\nwill probably be best to encode this data as a categorical variable, where you can\\nselect the categories either by using the most common entries, or by defining cate‐\\ngories that will capture responses in a way that makes sense for your application. Y ou\\nmight then have some categories for standard colors, maybe a category “multicol‐\\nored” for people that gave answers like “green and red stripes, ” and an “other” cate‐\\ngory for things that cannot be encoded otherwise. This kind of preprocessing of\\nstrings can take a lot of manual effort and is not easily automated. If you are in a posi‐\\ntion where you can influence data collection, we highly recommend avoiding man‐\\nually entered values for concepts that are better captured using categorical variables.\\nOften, manually entered values do not correspond to fixed categories, but still have\\nsome underlying structure, like addresses, names of places or people, dates, telephone\\n324 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 338, 'page_label': '325'}, page_content='1 Arguably, the content of websites linked to in tweets contains more information than the text of the tweets\\nthemselves.\\n2 Most of what we will talk about in the rest of the chapter also applies to other languages that use the Roman\\nalphabet, and partially to other languages with word boundary delimiters. Chinese, for example, does not\\ndelimit word boundaries, and has other challenges that make applying the techniques in this chapter difficult.\\n3 The dataset is available at http://ai.stanford.edu/~amaas/data/sentiment/.\\nnumbers, or other identifiers. These kinds of strings are often very hard to parse, and\\ntheir treatment is highly dependent on context and domain. A systematic treatment\\nof these cases is beyond the scope of this book.\\nThe final category of string data is freeform text data that consists of phrases or sen‐\\ntences. Examples include tweets, chat logs, and hotel reviews, as well as the collected\\nworks of Shakespeare, the content of Wikipedia, or the Project Gutenberg collection\\nof 50,000 ebooks. All of these collections contain information mostly as sentences\\ncomposed of words. 1 For simplicity’s sake, let’s assume all our documents are in one\\nlanguage, English. 2 In the context of text analysis, the dataset is often called the cor‐\\npus, and each data point, represented as a single text, is called a document. These\\nterms come from the information retrieval (IR) and natural language processing (NLP)\\ncommunity, which both deal mostly in text data.\\nExample Application: Sentiment Analysis of Movie\\nReviews\\nAs a running example in this chapter, we will use a dataset of movie reviews from the\\nIMDb (Internet Movie Database) website collected by Stanford researcher Andrew\\nMaas.3 This dataset contains the text of the reviews, together with a label that indi‐\\ncates whether a review is “positive” or “negative. ” The IMDb website itself contains\\nratings from 1 to 10. To simplify the modeling, this annotation is summarized as a\\ntwo-class classification dataset where reviews with a score of 6 or higher are labeled as\\npositive, and the rest as negative. We will leave the question of whether this is a good\\nrepresentation of the data open, and simply use the data as provided by Andrew\\nMaas.\\nAfter unpacking the data, the dataset is provided as text files in two separate folders,\\none for the training data and one for the test data. Each of these in turn has two sub‐\\nfolders, one called pos and one called neg:\\nExample Application: Sentiment Analysis of Movie Reviews | 325'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 339, 'page_label': '326'}, page_content='In[2]:\\n!tree -L 2 data/aclImdb\\nOut[2]:\\ndata/aclImdb\\n├── test\\n│   ├── neg\\n│   └── pos\\n└── train\\n    ├── neg\\n    └── pos\\n6 directories, 0 files\\nThe pos folder contains all the positive reviews, each as a separate text file, and simi‐\\nlarly for the neg folder. There is a helper function in scikit-learn to load files stored\\nin such a folder structure, where each subfolder corresponds to a label, called\\nload_files. We apply the load_files function first to the training data:\\nIn[3]:\\nfrom sklearn.datasets import load_files\\nreviews_train = load_files(\"data/aclImdb/train/\")\\n# load_files returns a bunch, containing training texts and training labels\\ntext_train, y_train = reviews_train.data, reviews_train.target\\nprint(\"type of text_train: {}\".format(type(text_train)))\\nprint(\"length of text_train: {}\".format(len(text_train)))\\nprint(\"text_train[1]:\\\\n{}\".format(text_train[1]))\\nOut[3]:\\ntype of text_train:  <class \\'list\\'>\\nlength of text_train:  25000\\ntext_train[1]:\\nb\\'Words can\\\\\\'t describe how bad this movie is. I can\\\\\\'t explain it by writing\\n  only. You have too see it for yourself to get at grip of how horrible a movie\\n  really can be. Not that I recommend you to do that. There are so many\\n  clich\\\\xc3\\\\xa9s, mistakes (and all other negative things you can imagine) here\\n  that will just make you cry. To start with the technical first, there are a\\n  LOT of mistakes regarding the airplane. I won\\\\\\'t list them here, but just\\n  mention the coloring of the plane. They didn\\\\\\'t even manage to show an\\n  airliner in the colors of a fictional airline, but instead used a 747\\n  painted in the original Boeing livery. Very bad. The plot is stupid and has\\n  been done many times before, only much, much better. There are so many\\n  ridiculous moments here that i lost count of it really early. Also, I was on\\n  the bad guys\\\\\\' side all the time in the movie, because the good guys were so\\n  stupid. \"Executive Decision\" should without a doubt be you\\\\\\'re choice over\\n  this one, even the \"Turbulence\"-movies are better. In fact, every other\\n  movie in the world is better than this one.\\'\\nY ou can see that text_train is a list of length 25,000, where each entry is a string\\ncontaining a review. We printed the review with index 1. Y ou can also see that the\\nreview contains some HTML line breaks (<br />). While these are unlikely to have a\\n326 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 340, 'page_label': '327'}, page_content='large impact on our machine learning models, it is better to clean the data and\\nremove this formatting before we proceed:\\nIn[4]:\\ntext_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]\\nThe type of the entries of text_train will depend on your Python version. In Python\\n3, they will be of type bytes which represents a binary encoding of the string data. In\\nPython 2, text_train contains strings. We won’t go into the details of the different\\nstring types in Python here, but we recommend that you read the Python 2  and/or\\nPython 3 documentation regarding strings and Unicode.\\nThe dataset was collected such that the positive class and the negative class balanced,\\nso that there are as many positive as negative strings:\\nIn[5]:\\nprint(\"Samples per class (training): {}\".format(np.bincount(y_train)))\\nOut[5]:\\nSamples per class (training): [12500 12500]\\nWe load the test dataset in the same manner:\\nIn[6]:\\nreviews_test = load_files(\"data/aclImdb/test/\")\\ntext_test, y_test = reviews_test.data, reviews_test.target\\nprint(\"Number of documents in test data: {}\".format(len(text_test)))\\nprint(\"Samples per class (test): {}\".format(np.bincount(y_test)))\\ntext_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\\nOut[6]:\\nNumber of documents in test data: 25000\\nSamples per class (test): [12500 12500]\\nThe task we want to solve is as follows: given a review, we want to assign the label\\n“positive” or “negative” based on the text content of the review. This is a standard\\nbinary classification task. However, the text data is not in a format that a machine\\nlearning model can handle. We need to convert the string representation of the text\\ninto a numeric representation that we can apply our machine learning algorithms to.\\nRepresenting Text Data as a Bag of Words\\nOne of the most simple but effective and commonly used ways to represent text for\\nmachine learning is using the bag-of-words representation. When using this represen‐\\ntation, we discard most of the structure of the input text, like chapters, paragraphs,\\nsentences, and formatting, and only count how often each word appears in each text in\\nRepresenting Text Data as a Bag of Words | 327'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 341, 'page_label': '328'}, page_content='the corpus. Discarding the structure and counting only word occurrences leads to the\\nmental image of representing text as a “bag. ”\\nComputing the bag-of-words representation for a corpus of documents consists of\\nthe following three steps:\\n1. Tokenization. Split each document into the words that appear in it (called tokens),\\nfor example by splitting them on whitespace and punctuation.\\n2. Vocabulary building. Collect a vocabulary of all words that appear in any of the\\ndocuments, and number them (say, in alphabetical order).\\n3. Encoding. For each document, count how often each of the words in the vocabu‐\\nlary appear in this document.\\nThere are some subtleties involved in step 1 and step 2, which we will discuss in more\\ndetail later in this chapter. For now, let’s look at how we can apply the bag-of-words\\nprocessing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\\nis how you get ants.\" . The output is one vector of word counts for each docu‐\\nment. For each word in the vocabulary, we have a count of how often it appears in\\neach document. That means our numeric representation has one feature for each\\nunique word in the whole dataset. Note how the order of the words in the original\\nstring is completely irrelevant to the bag-of-words feature representation.\\nFigure 7-1. Bag-of-words processing\\n328 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 342, 'page_label': '329'}, page_content='Applying Bag-of-Words to a Toy Dataset\\nThe bag-of-words representation is implemented in CountVectorizer, which is a\\ntransformer. Let’s first apply it to a toy dataset, consisting of two samples, to see it\\nworking:\\nIn[7]:\\nbards_words =[\"The fool doth think he is wise,\",\\n              \"but the wise man knows himself to be a fool\"]\\nWe import and instantiate the CountVectorizer and fit it to our toy data as follows:\\nIn[8]:\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nvect = CountVectorizer()\\nvect.fit(bards_words)\\nFitting the CountVectorizer consists of the tokenization of the training data and\\nbuilding of the vocabulary, which we can access as the vocabulary_ attribute:\\nIn[9]:\\nprint(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\\nprint(\"Vocabulary content:\\\\n {}\".format(vect.vocabulary_))\\nOut[9]:\\nVocabulary size: 13\\nVocabulary content:\\n {\\'the\\': 9, \\'himself\\': 5, \\'wise\\': 12, \\'he\\': 4, \\'doth\\': 2, \\'to\\': 11, \\'knows\\': 7,\\n  \\'man\\': 8, \\'fool\\': 3, \\'is\\': 6, \\'be\\': 0, \\'think\\': 10, \\'but\\': 1}\\nThe vocabulary consists of 13 words, from \"be\" to \"wise\".\\nTo create the bag-of-words representation for the training data, we call the transform\\nmethod:\\nIn[10]:\\nbag_of_words = vect.transform(bards_words)\\nprint(\"bag_of_words: {}\".format(repr(bag_of_words)))\\nOut[10]:\\nbag_of_words: <2x13 sparse matrix of type \\'<class \\'numpy.int64\\'>\\'\\n    with 16 stored elements in Compressed Sparse Row format>\\nThe bag-of-words representation is stored in a SciPy sparse matrix that only stores\\nthe entries that are nonzero (see Chapter 1). The matrix is of shape 2×13, with one\\nrow for each of the two data points and one feature for each of the words in the\\nvocabulary. A sparse matrix is used as most documents only contain a small subset of\\nthe words in the vocabulary, meaning most entries in the feature array are 0. Think\\nRepresenting Text Data as a Bag of Words | 329'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 343, 'page_label': '330'}, page_content='4 This is possible because we are using a small toy dataset that contains only 13 words. For any real dataset, this\\nwould result in a MemoryError.\\nabout how many different words might appear in a movie review compared to all the\\nwords in the English language (which is what the vocabulary models). Storing all\\nthose zeros would be prohibitive, and a waste of memory. To look at the actual con‐\\ntent of the sparse matrix, we can convert it to a “dense” NumPy array (that also stores\\nall the 0 entries) using the toarray method:4\\nIn[11]:\\nprint(\"Dense representation of bag_of_words:\\\\n{}\".format(\\n    bag_of_words.toarray()))\\nOut[11]:\\nDense representation of bag_of_words:\\n[[0 0 1 1 1 0 1 0 0 1 1 0 1]\\n [1 1 0 1 0 1 0 1 1 1 0 1 1]]\\nWe can see that the word counts for each word are either 0 or 1; neither of the two\\nstrings in bards_words contains a word twice. Let’s take a look at how to read these\\nfeature vectors. The first string ( \"The fool doth think he is wise,\" ) is repre‐\\nsented as the first row in, and it contains the first word in the vocabulary, \"be\", zero\\ntimes. It also contains the second word in the vocabulary, \"but\", zero times. It con‐\\ntains the third word, \"doth\", once, and so on. Looking at both rows, we can see that\\nthe fourth word, \"fool\", the tenth word, \"the\", and the thirteenth word, \"wise\",\\nappear in both strings.\\nBag-of-Words for Movie Reviews\\nNow that we’ve gone through the bag-of-words process in detail, let’s apply it to our\\ntask of sentiment analysis for movie reviews. Earlier, we loaded our training and test\\ndata from the IMDb reviews into lists of strings ( text_train and text_test), which\\nwe will now process:\\nIn[12]:\\nvect = CountVectorizer().fit(text_train)\\nX_train = vect.transform(text_train)\\nprint(\"X_train:\\\\n{}\".format(repr(X_train)))\\nOut[12]:\\nX_train:\\n<25000x74849 sparse matrix of type \\'<class \\'numpy.int64\\'>\\'\\n    with 3431196 stored elements in Compressed Sparse Row format>\\n330 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 344, 'page_label': '331'}, page_content='5 A quick analysis of the data confirms that this is indeed the case. Try confirming it yourself.\\nThe shape of X_train, the bag-of-words representation of the training data, is\\n25,000×74,849, indicating that the vocabulary contains 74,849 entries. Again, the data\\nis stored as a SciPy sparse matrix. Let’s look at the vocabulary in a bit more detail.\\nAnother way to access the vocabulary is using the get_feature_name method of the\\nvectorizer, which returns a convenient list where each entry corresponds to one fea‐\\nture:\\nIn[13]:\\nfeature_names = vect.get_feature_names()\\nprint(\"Number of features: {}\".format(len(feature_names)))\\nprint(\"First 20 features:\\\\n{}\".format(feature_names[:20]))\\nprint(\"Features 20010 to 20030:\\\\n{}\".format(feature_names[20010:20030]))\\nprint(\"Every 2000th feature:\\\\n{}\".format(feature_names[::2000]))\\nOut[13]:\\nNumber of features: 74849\\nFirst 20 features:\\n[\\'00\\', \\'000\\', \\'0000000000001\\', \\'00001\\', \\'00015\\', \\'000s\\', \\'001\\', \\'003830\\',\\n \\'006\\', \\'007\\', \\'0079\\', \\'0080\\', \\'0083\\', \\'0093638\\', \\'00am\\', \\'00pm\\', \\'00s\\',\\n \\'01\\', \\'01pm\\', \\'02\\']\\nFeatures 20010 to 20030:\\n[\\'dratted\\', \\'draub\\', \\'draught\\', \\'draughts\\', \\'draughtswoman\\', \\'draw\\', \\'drawback\\',\\n \\'drawbacks\\', \\'drawer\\', \\'drawers\\', \\'drawing\\', \\'drawings\\', \\'drawl\\',\\n \\'drawled\\', \\'drawling\\', \\'drawn\\', \\'draws\\', \\'draza\\', \\'dre\\', \\'drea\\']\\nEvery 2000th feature:\\n[\\'00\\', \\'aesir\\', \\'aquarian\\', \\'barking\\', \\'blustering\\', \\'bête\\', \\'chicanery\\',\\n \\'condensing\\', \\'cunning\\', \\'detox\\', \\'draper\\', \\'enshrined\\', \\'favorit\\', \\'freezer\\',\\n \\'goldman\\', \\'hasan\\', \\'huitieme\\', \\'intelligible\\', \\'kantrowitz\\', \\'lawful\\',\\n \\'maars\\', \\'megalunged\\', \\'mostey\\', \\'norrland\\', \\'padilla\\', \\'pincher\\',\\n \\'promisingly\\', \\'receptionist\\', \\'rivals\\', \\'schnaas\\', \\'shunning\\', \\'sparse\\',\\n \\'subset\\', \\'temptations\\', \\'treatises\\', \\'unproven\\', \\'walkman\\', \\'xylophonist\\']\\nAs you can see, possibly a bit surprisingly, the first 10 entries in the vocabulary are all\\nnumbers. All these numbers appear somewhere in the reviews, and are therefore\\nextracted as words. Most of these numbers don’t have any immediate semantic mean‐\\ning—apart from \"007\", which in the particular context of movies is likely to refer to\\nthe James Bond character. 5 Weeding out the meaningful from the nonmeaningful\\n“words” is sometimes tricky. Looking further along in the vocabulary, we find a col‐\\nlection of English words starting with “dra” . Y ou might notice that for \"draught\",\\n\"drawback\", and \"drawer\" both the singular and plural forms are contained in the\\nvocabulary as distinct words. These words have very closely related semantic mean‐\\nings, and counting them as different words, corresponding to different features,\\nmight not be ideal.\\nRepresenting Text Data as a Bag of Words | 331'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 345, 'page_label': '332'}, page_content='6 The attentive reader might notice that we violate our lesson from Chapter 6 on cross-validation with prepro‐\\ncessing here. Using the default settings of CountVectorizer, it actually does not collect any statistics, so our\\nresults are valid. Using Pipeline from the start would be a better choice for applications, but we defer it for\\nease of exposure.\\nBefore we try to improve our feature extraction, let’s obtain a quantitative measure of\\nperformance by actually building a classifier. We have the training labels stored in\\ny_train and the bag-of-words representation of the training data in X_train, so we\\ncan train a classifier on this data. For high-dimensional, sparse data like this, linear\\nmodels like LogisticRegression often work best.\\nLet’s start by evaluating LogisticRegresssion using cross-validation:6\\nIn[14]:\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.linear_model import LogisticRegression\\nscores = cross_val_score(LogisticRegression(), X_train, y_train, cv=5)\\nprint(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))\\nOut[14]:\\nMean cross-validation accuracy: 0.88\\nWe obtain a mean cross-validation score of 88%, which indicates reasonable perfor‐\\nmance for a balanced binary classification task. We know that LogisticRegression\\nhas a regularization parameter, C, which we can tune via cross-validation:\\nIn[15]:\\nfrom sklearn.model_selection import GridSearchCV\\nparam_grid = {\\'C\\': [0.001, 0.01, 0.1, 1, 10]}\\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\\ngrid.fit(X_train, y_train)\\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\\nprint(\"Best parameters: \", grid.best_params_)\\nOut[15]:\\nBest cross-validation score: 0.89\\nBest parameters:  {\\'C\\': 0.1}\\nWe obtain a cross-validation score of 89% using C=0.1. We can now assess the gener‐\\nalization performance of this parameter setting on the test set:\\nIn[16]:\\nX_test = vect.transform(text_test)\\nprint(\"{:.2f}\".format(grid.score(X_test, y_test)))\\nOut[16]:\\n0.88\\n332 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 346, 'page_label': '333'}, page_content='Now, let’s see if we can improve the extraction of words. The CountVectorizer\\nextracts tokens using a regular expression. By default, the regular expression that is\\nused is \"\\\\b\\\\w\\\\w+\\\\b\". If you are not familiar with regular expressions, this means it\\nfinds all sequences of characters that consist of at least two letters or numbers ( \\\\w)\\nand that are separated by word boundaries ( \\\\b). It does not find single-letter words,\\nand it splits up contractions like “doesn’t” or “bit.ly” , but it matches “h8ter” as a single\\nword. The CountVectorizer then converts all words to lowercase characters, so that\\n“soon” , “Soon” , and “sOon” all correspond to the same token (and therefore feature).\\nThis simple mechanism works quite well in practice, but as we saw earlier, we get\\nmany uninformative features (like the numbers). One way to cut back on these is to\\nonly use tokens that appear in at least two documents (or at least five documents, and\\nso on). A token that appears only in a single document is unlikely to appear in the test\\nset and is therefore not helpful. We can set the minimum number of documents a\\ntoken needs to appear in with the min_df parameter:\\nIn[17]:\\nvect = CountVectorizer(min_df=5).fit(text_train)\\nX_train = vect.transform(text_train)\\nprint(\"X_train with min_df: {}\".format(repr(X_train)))\\nOut[17]:\\nX_train with min_df: <25000x27271 sparse matrix of type \\'<class \\'numpy.int64\\'>\\'\\n    with 3354014 stored elements in Compressed Sparse Row format>\\nBy requiring at least five appearances of each token, we can bring down the number\\nof features to 27,271, as seen in the preceding output—only about a third of the origi‐\\nnal features. Let’s look at some tokens again:\\nIn[18]:\\nfeature_names = vect.get_feature_names()\\nprint(\"First 50 features:\\\\n{}\".format(feature_names[:50]))\\nprint(\"Features 20010 to 20030:\\\\n{}\".format(feature_names[20010:20030]))\\nprint(\"Every 700th feature:\\\\n{}\".format(feature_names[::700]))\\nOut[18]:\\nFirst 50 features:\\n[\\'00\\', \\'000\\', \\'007\\', \\'00s\\', \\'01\\', \\'02\\', \\'03\\', \\'04\\', \\'05\\', \\'06\\', \\'07\\', \\'08\\',\\n \\'09\\', \\'10\\', \\'100\\', \\'1000\\', \\'100th\\', \\'101\\', \\'102\\', \\'103\\', \\'104\\', \\'105\\', \\'107\\',\\n \\'108\\', \\'10s\\', \\'10th\\', \\'11\\', \\'110\\', \\'112\\', \\'116\\', \\'117\\', \\'11th\\', \\'12\\', \\'120\\',\\n \\'12th\\', \\'13\\', \\'135\\', \\'13th\\', \\'14\\', \\'140\\', \\'14th\\', \\'15\\', \\'150\\', \\'15th\\', \\'16\\',\\n \\'160\\', \\'1600\\', \\'16mm\\', \\'16s\\', \\'16th\\']\\nFeatures 20010 to 20030:\\n[\\'repentance\\', \\'repercussions\\', \\'repertoire\\', \\'repetition\\', \\'repetitions\\',\\n \\'repetitious\\', \\'repetitive\\', \\'rephrase\\', \\'replace\\', \\'replaced\\', \\'replacement\\',\\n \\'replaces\\', \\'replacing\\', \\'replay\\', \\'replayable\\', \\'replayed\\', \\'replaying\\',\\n \\'replays\\', \\'replete\\', \\'replica\\']\\nRepresenting Text Data as a Bag of Words | 333'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 347, 'page_label': '334'}, page_content='Every 700th feature:\\n[\\'00\\', \\'affections\\', \\'appropriately\\', \\'barbra\\', \\'blurbs\\', \\'butchered\\',\\n \\'cheese\\', \\'commitment\\', \\'courts\\', \\'deconstructed\\', \\'disgraceful\\', \\'dvds\\',\\n \\'eschews\\', \\'fell\\', \\'freezer\\', \\'goriest\\', \\'hauser\\', \\'hungary\\', \\'insinuate\\',\\n \\'juggle\\', \\'leering\\', \\'maelstrom\\', \\'messiah\\', \\'music\\', \\'occasional\\', \\'parking\\',\\n \\'pleasantville\\', \\'pronunciation\\', \\'recipient\\', \\'reviews\\', \\'sas\\', \\'shea\\',\\n \\'sneers\\', \\'steiger\\', \\'swastika\\', \\'thrusting\\', \\'tvs\\', \\'vampyre\\', \\'westerns\\']\\nThere are clearly many fewer numbers, and some of the more obscure words or mis‐\\nspellings seem to have vanished. Let’s see how well our model performs by doing a\\ngrid search again:\\nIn[19]:\\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\\ngrid.fit(X_train, y_train)\\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\\nOut[19]:\\nBest cross-validation score: 0.89\\nThe best validation accuracy of the grid search is still 89%, unchanged from before.\\nWe didn’t improve our model, but having fewer features to deal with speeds up pro‐\\ncessing and throwing away useless features might make the model more interpretable.\\nIf the transform method of CountVectorizer is called on a docu‐\\nment that contains words that were not contained in the training\\ndata, these words will be ignored as they are not part of the dictio‐\\nnary. This is not really an issue for classification, as it’s not possible\\nto learn anything about words that are not in the training data. For\\nsome applications, like spam detection, it might be helpful to add a\\nfeature that encodes how many so-called “out of vocabulary” words\\nthere are in a particular document, though. For this to work, you\\nneed to set min_df; otherwise, this feature will never be active dur‐\\ning training.\\nStopwords\\nAnother way that we can get rid of uninformative words is by discarding words that\\nare too frequent to be informative. There are two main approaches: using a language-\\nspecific list of stopwords, or discarding words that appear too frequently. scikit-\\nlearn has a built-in list of English stopwords in the feature_extraction.text\\nmodule:\\n334 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 348, 'page_label': '335'}, page_content='In[20]:\\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\\nprint(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS)))\\nprint(\"Every 10th stopword:\\\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))\\nOut[20]:\\nNumber of stop words: 318\\nEvery 10th stopword:\\n[\\'above\\', \\'elsewhere\\', \\'into\\', \\'well\\', \\'rather\\', \\'fifteen\\', \\'had\\', \\'enough\\',\\n \\'herein\\', \\'should\\', \\'third\\', \\'although\\', \\'more\\', \\'this\\', \\'none\\', \\'seemed\\',\\n \\'nobody\\', \\'seems\\', \\'he\\', \\'also\\', \\'fill\\', \\'anyone\\', \\'anything\\', \\'me\\', \\'the\\',\\n \\'yet\\', \\'go\\', \\'seeming\\', \\'front\\', \\'beforehand\\', \\'forty\\', \\'i\\']\\nClearly, removing the stopwords in the list can only decrease the number of features\\nby the length of the list—here, 318—but it might lead to an improvement in perfor‐\\nmance. Let’s give it a try:\\nIn[21]:\\n# Specifying stop_words=\"english\" uses the built-in list.\\n# We could also augment it and pass our own.\\nvect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\\nX_train = vect.transform(text_train)\\nprint(\"X_train with stop words:\\\\n{}\".format(repr(X_train)))\\nOut[21]:\\nX_train with stop words:\\n<25000x26966 sparse matrix of type \\'<class \\'numpy.int64\\'>\\'\\n    with 2149958 stored elements in Compressed Sparse Row format>\\nThere are now 305 (27,271–26,966) fewer features in the dataset, which means that\\nmost, but not all, of the stopwords appeared. Let’s run the grid search again:\\nIn[22]:\\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\\ngrid.fit(X_train, y_train)\\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\\nOut[22]:\\nBest cross-validation score: 0.88\\nThe grid search performance decreased slightly using the stopwords—not enough to\\nworry about, but given that excluding 305 features out of over 27,000 is unlikely to\\nchange performance or interpretability a lot, it doesn’t seem worth using this list.\\nFixed lists are mostly helpful for small datasets, which might not contain enough\\ninformation for the model to determine which words are stopwords from the data\\nitself. As an exercise, you can try out the other approach, discarding frequently\\nStopwords | 335'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 349, 'page_label': '336'}, page_content='7 We provide this formula here mostly for completeness; you don’t need to remember it to use the tf–idf\\nencoding.\\nappearing words, by setting the max_df option of CountVectorizer and see how it\\ninfluences the number of features and the performance.\\nRescaling the Data with tf–idf\\nInstead of dropping features that are deemed unimportant, another approach is to\\nrescale features by how informative we expect them to be. One of the most common\\nways to do this is using the term frequency–inverse document frequency  (tf–idf)\\nmethod. The intuition of this method is to give high weight to any term that appears\\noften in a particular document, but not in many documents in the corpus. If a word\\nappears often in a particular document, but not in very many documents, it is likely\\nto be very descriptive of the content of that document. scikit-learn implements the\\ntf–idf method in two classes: TfidfTransformer, which takes in the sparse matrix\\noutput produced by CountVectorizer and transforms it, and TfidfVectorizer,\\nwhich takes in the text data and does both the bag-of-words feature extraction and\\nthe tf–idf transformation. There are several variants of the tf–idf rescaling scheme,\\nwhich you can read about on Wikipedia . The tf–idf score for word w in document d\\nas implemented in both the TfidfTransformer and TfidfVectorizer classes is given\\nby:7\\ntfidf w, d = tf log N + 1\\nNw + 1 + 1\\nwhere N is the number of documents in the training set, Nw is the number of docu‐\\nments in the training set that the word w appears in, and tf (the term frequency) is the\\nnumber of times that the word w appears in the query document d (the document\\nyou want to transform or encode). Both classes also apply L2 normalization after\\ncomputing the tf–idf representation; in other words, they rescale the representation\\nof each document to have Euclidean norm 1. Rescaling in this way means that the\\nlength of a document (the number of words) does not change the vectorized repre‐\\nsentation.\\nBecause tf–idf actually makes use of the statistical properties of the training data, we\\nwill use a pipeline, as described in Chapter 6, to ensure the results of our grid search\\nare valid. This leads to the following code:\\n336 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 350, 'page_label': '337'}, page_content='In[23]:\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.pipeline import make_pipeline\\npipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None),\\n                     LogisticRegression())\\nparam_grid = {\\'logisticregression__C\\': [0.001, 0.01, 0.1, 1, 10]}\\ngrid = GridSearchCV(pipe, param_grid, cv=5)\\ngrid.fit(text_train, y_train)\\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\\nOut[23]:\\nBest cross-validation score: 0.89\\nAs you can see, there is some improvement when using tf–idf instead of just word\\ncounts. We can also inspect which words tf–idf found most important. Keep in mind\\nthat the tf–idf scaling is meant to find words that distinguish documents, but it is a\\npurely unsupervised technique. So, “important” here does not necessarily relate to the\\n“positive review” and “negative review” labels we are interested in. First, we extract\\nthe TfidfVectorizer from the pipeline:\\nIn[24]:\\nvectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\\n# transform the training dataset\\nX_train = vectorizer.transform(text_train)\\n# find maximum value for each of the features over the dataset\\nmax_value = X_train.max(axis=0).toarray().ravel()\\nsorted_by_tfidf = max_value.argsort()\\n# get feature names\\nfeature_names = np.array(vectorizer.get_feature_names())\\nprint(\"Features with lowest tfidf:\\\\n{}\".format(\\n    feature_names[sorted_by_tfidf[:20]]))\\nprint(\"Features with highest tfidf: \\\\n{}\".format(\\n    feature_names[sorted_by_tfidf[-20:]]))\\nOut[24]:\\nFeatures with lowest tfidf:\\n[\\'poignant\\' \\'disagree\\' \\'instantly\\' \\'importantly\\' \\'lacked\\' \\'occurred\\'\\n \\'currently\\' \\'altogether\\' \\'nearby\\' \\'undoubtedly\\' \\'directs\\' \\'fond\\' \\'stinker\\'\\n \\'avoided\\' \\'emphasis\\' \\'commented\\' \\'disappoint\\' \\'realizing\\' \\'downhill\\'\\n \\'inane\\']\\nFeatures with highest tfidf:\\n[\\'coop\\' \\'homer\\' \\'dillinger\\' \\'hackenstein\\' \\'gadget\\' \\'taker\\' \\'macarthur\\'\\n \\'vargas\\' \\'jesse\\' \\'basket\\' \\'dominick\\' \\'the\\' \\'victor\\' \\'bridget\\' \\'victoria\\'\\n \\'khouri\\' \\'zizek\\' \\'rob\\' \\'timon\\' \\'titanic\\']\\nRescaling the Data with tf–idf | 337'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 351, 'page_label': '338'}, page_content='Features with low tf–idf are those that either are very commonly used across docu‐\\nments or are only used sparingly, and only in very long documents. Interestingly,\\nmany of the high-tf–idf features actually identify certain shows or movies. These\\nterms only appear in reviews for this particular show or franchise, but tend to appear\\nvery often in these particular reviews. This is very clear, for example, for \"pokemon\",\\n\"smallville\", and \"doodlebops\", but \"scanners\" here actually also refers to a\\nmovie title. These words are unlikely to help us in our sentiment classification task\\n(unless maybe some franchises are universally reviewed positively or negatively) but\\ncertainly contain a lot of specific information about the reviews.\\nWe can also find the words that have low inverse document frequency—that is, those\\nthat appear frequently and are therefore deemed less important. The inverse docu‐\\nment frequency values found on the training set are stored in the idf_ attribute:\\nIn[25]:\\nsorted_by_idf = np.argsort(vectorizer.idf_)\\nprint(\"Features with lowest idf:\\\\n{}\".format(\\n    feature_names[sorted_by_idf[:100]]))\\nOut[25]:\\nFeatures with lowest idf:\\n[\\'the\\' \\'and\\' \\'of\\' \\'to\\' \\'this\\' \\'is\\' \\'it\\' \\'in\\' \\'that\\' \\'but\\' \\'for\\' \\'with\\'\\n \\'was\\' \\'as\\' \\'on\\' \\'movie\\' \\'not\\' \\'have\\' \\'one\\' \\'be\\' \\'film\\' \\'are\\' \\'you\\' \\'all\\'\\n \\'at\\' \\'an\\' \\'by\\' \\'so\\' \\'from\\' \\'like\\' \\'who\\' \\'they\\' \\'there\\' \\'if\\' \\'his\\' \\'out\\'\\n \\'just\\' \\'about\\' \\'he\\' \\'or\\' \\'has\\' \\'what\\' \\'some\\' \\'good\\' \\'can\\' \\'more\\' \\'when\\'\\n \\'time\\' \\'up\\' \\'very\\' \\'even\\' \\'only\\' \\'no\\' \\'would\\' \\'my\\' \\'see\\' \\'really\\' \\'story\\'\\n \\'which\\' \\'well\\' \\'had\\' \\'me\\' \\'than\\' \\'much\\' \\'their\\' \\'get\\' \\'were\\' \\'other\\'\\n \\'been\\' \\'do\\' \\'most\\' \\'don\\' \\'her\\' \\'also\\' \\'into\\' \\'first\\' \\'made\\' \\'how\\' \\'great\\'\\n \\'because\\' \\'will\\' \\'people\\' \\'make\\' \\'way\\' \\'could\\' \\'we\\' \\'bad\\' \\'after\\' \\'any\\'\\n \\'too\\' \\'then\\' \\'them\\' \\'she\\' \\'watch\\' \\'think\\' \\'acting\\' \\'movies\\' \\'seen\\' \\'its\\'\\n \\'him\\']\\nAs expected, these are mostly English stopwords like \"the\" and \"no\". But some are\\nclearly domain-specific to the movie reviews, like \"movie\", \"film\", \"time\", \"story\",\\nand so on. Interestingly, \"good\", \"great\", and \"bad\" are also among the most fre‐\\nquent and therefore “least relevant” words according to the tf–idf measure, even\\nthough we might expect these to be very important for our sentiment analysis task.\\nInvestigating Model \\nCoefficients\\nFinally, let’s look in a bit more detail into what our logistic regression model actually\\nlearned from the data. Because there are so many features—27,271 after removing the\\ninfrequent ones—we clearly cannot look at all of the coefficients at the same time.\\nHowever, we can look at the largest coefficients, and see which words these corre‐\\nspond to. We will use the last model that we trained, based on the tf–idf features.\\nThe following bar chart ( Figure 7-2) shows the 25 largest and 25 smallest coefficients\\nof the logistic regression model, with the bars showing the size of each coefficient:\\n338 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 352, 'page_label': '339'}, page_content='In[26]:\\nmglearn.tools.visualize_coefficients(\\n    grid.best_estimator_.named_steps[\"logisticregression\"].coef_,\\n    feature_names, n_top_features=40)\\nFigure 7-2. Largest and smallest coefficients of logistic regression trained on tf-idf fea‐\\ntures\\nThe negative coefficients on the left belong to words that according to the model are\\nindicative of negative reviews, while the positive coefficients on the right belong to\\nwords that according to the model indicate positive reviews. Most of the terms are\\nquite intuitive, like \"worst\", \"waste\", \"disappointment\", and \"laughable\" indicat‐\\ning bad movie reviews, while \"excellent\", \"wonderful\", \"enjoyable\", and\\n\"refreshing\" indicate positive movie reviews. Some words are slightly less clear, like\\n\"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “best\\ntoday. ”\\nBag-of-Words with More Than One Word (n-Grams)\\nOne of the main disadvantages of using a bag-of-words representation is that word\\norder is completely discarded. Therefore, the two strings “it’s bad, not good at all” and\\n“it’s good, not bad at all” have exactly the same representation, even though the mean‐\\nings are inverted. Putting “not” in front of a word is only one example (if an extreme\\none) of how context matters. Fortunately, there is a way of capturing context when\\nusing a bag-of-words representation, by not only considering the counts of single\\ntokens, but also the counts of pairs or triplets of tokens that appear next to each other.\\nPairs of tokens are known as bigrams, triplets of tokens are known as trigrams, and\\nmore generally sequences of tokens are known as n-grams. We can change the range\\nof tokens that are considered as features by changing the ngram_range parameter of\\nCountVectorizer or TfidfVectorizer. The ngram_range parameter is a tuple, con‐\\nBag-of-Words with More Than One Word (n-Grams) | 339'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 353, 'page_label': '340'}, page_content='sisting of the minimum length and the maximum length of the sequences of tokens\\nthat are considered. Here is an example on the toy data we used earlier:\\nIn[27]:\\nprint(\"bards_words:\\\\n{}\".format(bards_words))\\nOut[27]:\\nbards_words:\\n[\\'The fool doth think he is wise,\\',\\n \\'but the wise man knows himself to be a fool\\']\\nThe default is to create one feature per sequence of tokens that is at least one token\\nlong and at most one token long, or in other words exactly one token long (single\\ntokens are also called unigrams):\\nIn[28]:\\ncv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\\nprint(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\\nprint(\"Vocabulary:\\\\n{}\".format(cv.get_feature_names()))\\nOut[28]:\\nVocabulary size: 13\\nVocabulary:\\n[\\'be\\', \\'but\\', \\'doth\\', \\'fool\\', \\'he\\', \\'himself\\', \\'is\\', \\'knows\\', \\'man\\', \\'the\\',\\n \\'think\\', \\'to\\', \\'wise\\']\\nTo look only at bigrams—that is, only at sequences of two tokens following each\\nother—we can set ngram_range to (2, 2):\\nIn[29]:\\ncv = CountVectorizer(ngram_range=(2, 2)).fit(bards_words)\\nprint(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\\nprint(\"Vocabulary:\\\\n{}\".format(cv.get_feature_names()))\\nOut[29]:\\nVocabulary size: 14\\nVocabulary:\\n[\\'be fool\\', \\'but the\\', \\'doth think\\', \\'fool doth\\', \\'he is\\', \\'himself to\\',\\n \\'is wise\\', \\'knows himself\\', \\'man knows\\', \\'the fool\\', \\'the wise\\',\\n \\'think he\\', \\'to be\\', \\'wise man\\']\\nUsing longer sequences of tokens usually results in many more features, and in more\\nspecific features. There is no common bigram between the two phrases in\\nbard_words:\\n340 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 354, 'page_label': '341'}, page_content='In[30]:\\nprint(\"Transformed data (dense):\\\\n{}\".format(cv.transform(bards_words).toarray()))\\nOut[30]:\\nTransformed data (dense):\\n[[0 0 1 1 1 0 1 0 0 1 0 1 0 0]\\n [1 1 0 0 0 1 0 1 1 0 1 0 1 1]]\\nFor most applications, the minimum number of tokens should be one, as single\\nwords often capture a lot of meaning. Adding bigrams helps in most cases. Adding\\nlonger sequences—up to 5-grams—might help too, but this will lead to an explosion\\nof the number of features and might lead to overfitting, as there will be many very\\nspecific features. In principle, the number of bigrams could be the number of\\nunigrams squared and the number of trigrams could be the number of unigrams to\\nthe power of three, leading to very large feature spaces. In practice, the number of\\nhigher n-grams that actually appear in the data is much smaller, because of the struc‐\\nture of the (English) language, though it is still large.\\nHere is what using unigrams, bigrams, and trigrams on bards_words looks like:\\nIn[31]:\\ncv = CountVectorizer(ngram_range=(1, 3)).fit(bards_words)\\nprint(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\\nprint(\"Vocabulary:\\\\n{}\".format(cv.get_feature_names()))\\nOut[31]:\\nVocabulary size: 39\\nVocabulary:\\n[\\'be\\', \\'be fool\\', \\'but\\', \\'but the\\', \\'but the wise\\', \\'doth\\', \\'doth think\\',\\n \\'doth think he\\', \\'fool\\', \\'fool doth\\', \\'fool doth think\\', \\'he\\', \\'he is\\',\\n \\'he is wise\\', \\'himself\\', \\'himself to\\', \\'himself to be\\', \\'is\\', \\'is wise\\',\\n \\'knows\\', \\'knows himself\\', \\'knows himself to\\', \\'man\\', \\'man knows\\',\\n \\'man knows himself\\', \\'the\\', \\'the fool\\', \\'the fool doth\\', \\'the wise\\',\\n \\'the wise man\\', \\'think\\', \\'think he\\', \\'think he is\\', \\'to\\', \\'to be\\',\\n \\'to be fool\\', \\'wise\\', \\'wise man\\', \\'wise man knows\\']\\nLet’s try out the TfidfVectorizer on the IMDb movie review data and find the best\\nsetting of n-gram range using a grid search:\\nIn[32]:\\npipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())\\n# running the grid search takes a long time because of the\\n# relatively large grid and the inclusion of trigrams\\nparam_grid = {\"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10, 100],\\n              \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3)]}\\ngrid = GridSearchCV(pipe, param_grid, cv=5)\\ngrid.fit(text_train, y_train)\\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\\nprint(\"Best parameters:\\\\n{}\".format(grid.best_params_))\\nBag-of-Words with More Than One Word (n-Grams) | 341'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 355, 'page_label': '342'}, page_content='Out[32]:\\nBest cross-validation score: 0.91\\nBest parameters:\\n{\\'tfidfvectorizer__ngram_range\\': (1, 3), \\'logisticregression__C\\': 100}\\nAs you can see from the results, we improved performance by a bit more than a per‐\\ncent by adding bigram and trigram features. We can visualize the cross-validation\\naccuracy as a function of the ngram_range and C parameter as a heat map, as we did\\nin Chapter 5 (see Figure 7-3):\\nIn[33]:\\n# extract scores from grid_search\\nscores = grid.cv_results_[\\'mean_test_score\\'].reshape(-1, 3).T\\n# visualize heat map\\nheatmap = mglearn.tools.heatmap(\\n    scores, xlabel=\"C\", ylabel=\"ngram_range\", cmap=\"viridis\", fmt=\"%.3f\",\\n    xticklabels=param_grid[\\'logisticregression__C\\'],\\n    yticklabels=param_grid[\\'tfidfvectorizer__ngram_range\\'])\\nplt.colorbar(heatmap)\\nFigure 7-3. Heat map visualization of mean cross-validation accuracy as a function of\\nthe parameters ngram_range and C\\nFrom the heat map we can see that using bigrams increases performance quite a bit,\\nwhile adding trigrams only provides a very small benefit in terms of accuracy. To\\nunderstand better how the model improved, we can visualize the important coeffi‐\\n342 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 356, 'page_label': '343'}, page_content='cient for the best model, which includes unigrams, bigrams, and trigrams (see\\nFigure 7-4):\\nIn[34]:\\n# extract feature names and coefficients\\nvect = grid.best_estimator_.named_steps[\\'tfidfvectorizer\\']\\nfeature_names = np.array(vect.get_feature_names())\\ncoef = grid.best_estimator_.named_steps[\\'logisticregression\\'].coef_\\nmglearn.tools.visualize_coefficients(coef, feature_names, n_top_features=40)\\nFigure 7-4. Most important features when using unigrams, bigrams, and trigrams with\\ntf-idf rescaling\\nThere are particularly interesting features containing the word “worth” that were not\\npresent in the unigram model: \"not worth\" is indicative of a negative review, while\\n\"definitely worth\" and \"well worth\" are indicative of a positive review. This is a\\nprime example of context influencing the meaning of the word “worth. ”\\nNext, we’ll visualize only trigrams, to provide further insight into why these features\\nare helpful. Many of the useful bigrams and trigrams consist of common words that\\nwould not be informative on their own, as in the phrases \"none of the\", \"the only\\ngood\", \"on and on\" , \"this is one\" , \"of the most\" , and so on. However, the\\nimpact of these features is quite limited compared to the importance of the unigram\\nfeatures, as you can see in Figure 7-5:\\nIn[35]:\\n# find 3-gram features\\nmask = np.array([len(feature.split(\" \")) for feature in feature_names]) == 3\\n# visualize only 3-gram features\\nmglearn.tools.visualize_coefficients(coef.ravel()[mask],\\n                                     feature_names[mask], n_top_features=40)Bag-of-Words with More Than One Word (n-Grams) | 343'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 357, 'page_label': '344'}, page_content='Figure 7-5. Visualization of only the important trigram features of the model\\nAdvanced Tokenization, Stemming, and Lemmatization\\nAs mentioned previously, the feature extraction in the CountVectorizer and Tfidf\\nVectorizer is relatively simple, and much more elaborate methods are possible. One\\nparticular step that is often improved in more sophisticated text-processing applica‐\\ntions is the first step in the bag-of-words model: tokenization. This step defines what\\nconstitutes a word for the purpose of feature extraction.\\nWe saw earlier that the vocabulary often contains singular and plural versions of\\nsome words, as in \"drawback\" and \"drawbacks\", \"drawer\" and \"drawers\", and\\n\"drawing\" and \"drawings\". For the purposes of a bag-of-words model, the semantics\\nof \"drawback\" and \"drawbacks\" are so close that distinguishing them will only\\nincrease overfitting, and not allow the model to fully exploit the training data. Simi‐\\nlarly, we found the vocabulary includes words like \"replace\", \"replaced\", \"replace\\nment\", \"replaces\", and \"replacing\", which are different verb forms and a noun\\nrelating to the verb “to replace. ” Similarly to having singular and plural forms of a\\nnoun, treating different verb forms and related words as distinct tokens is disadvanta‐\\ngeous for building a model that generalizes well.\\nThis problem can be overcome by representing each word using its word stem, which\\ninvolves identifying (or \\nconflating) all the words that have the same word stem. If this\\nis done by using a rule-based heuristic, like dropping common suffixes, it is usually\\nreferred to as stemming. If instead a dictionary of known word forms is used (an\\nexplicit and human-verified system), and the role of the word in the sentence is taken\\ninto account, the process is referred to as lemmatization and the standardized form of\\nthe word is referred to as the lemma. Both processing methods, lemmatization and\\nstemming, are forms of normalization that try to extract some normal form of a\\nword. Another interesting case of normalization is spelling correction, which can be\\nhelpful in practice but is outside of the scope of this book.\\n344 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 358, 'page_label': '345'}, page_content='8 For details of the interface, consult the nltk and spacy documentation. We are more interested in the general\\nprinciples here.\\nTo get a better understanding of normalization, let’s compare a method for stemming\\n—the Porter stemmer, a widely used collection of heuristics (here imported from the\\nnltk package)—to lemmatization as implemented in the spacy package:8\\nIn[36]:\\nimport spacy\\nimport nltk\\n# load spacy\\'s English-language models\\nen_nlp = spacy.load(\\'en\\')\\n# instantiate nltk\\'s Porter stemmer\\nstemmer = nltk.stem.PorterStemmer()\\n# define function to compare lemmatization in spacy with stemming in nltk\\ndef compare_normalization(doc):\\n    # tokenize document in spacy\\n    doc_spacy = en_nlp(doc)\\n    # print lemmas found by spacy\\n    print(\"Lemmatization:\")\\n    print([token.lemma_ for token in doc_spacy])\\n    # print tokens found by Porter stemmer\\n    print(\"Stemming:\")\\n    print([stemmer.stem(token.norm_.lower()) for token in doc_spacy])\\nWe will compare lemmatization and the Porter stemmer on a sentence designed to\\nshow some of the differences:\\nIn[37]:\\ncompare_normalization(u\"Our meeting today was worse than yesterday, \"\\n                       \"I\\'m scared of meeting the clients tomorrow.\")\\nOut[37]:\\nLemmatization:\\n[\\'our\\', \\'meeting\\', \\'today\\', \\'be\\', \\'bad\\', \\'than\\', \\'yesterday\\', \\',\\', \\'i\\', \\'be\\',\\n \\'scared\\', \\'of\\', \\'meet\\', \\'the\\', \\'client\\', \\'tomorrow\\', \\'.\\']\\nStemming:\\n[\\'our\\', \\'meet\\', \\'today\\', \\'wa\\', \\'wors\\', \\'than\\', \\'yesterday\\', \\',\\', \\'i\\', \"\\'m\",\\n \\'scare\\', \\'of\\', \\'meet\\', \\'the\\', \\'client\\', \\'tomorrow\\', \\'.\\']\\nStemming is always restricted to trimming the word to a stem, so \"was\" becomes\\n\"wa\", while lemmatization can retrieve the correct base verb form, \"be\". Similarly,\\nlemmatization can normalize \"worse\" to \"bad\", while stemming produces \"wors\".\\nAnother major difference is that stemming reduces both occurrences of \"meeting\" to\\n\"meet\". Using lemmatization, the first occurrence of \"meeting\" is recognized as a\\nAdvanced Tokenization, Stemming, and Lemmatization | 345'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 359, 'page_label': '346'}, page_content='noun and left as is, while the second occurrence is recognized as a verb and reduced\\nto \"meet\". In general, lemmatization is a much more involved process than stem‐\\nming, but it usually produces better results than stemming when used for normaliz‐\\ning tokens for machine learning.\\nWhile scikit-learn implements neither form of normalization, CountVectorizer\\nallows specifying your own tokenizer to convert each document into a list of tokens\\nusing the tokenizer parameter. We can use the lemmatization from spacy to create a\\ncallable that will take a string and produce a list of lemmas:\\nIn[38]:\\n# Technicality: we want to use the regexp-based tokenizer\\n# that is used by CountVectorizer and only use the lemmatization\\n# from spacy. To this end, we replace en_nlp.tokenizer (the spacy tokenizer)\\n# with the regexp-based tokenization.\\nimport re\\n# regexp used in CountVectorizer\\nregexp = re.compile(\\'(?u)\\\\\\\\b\\\\\\\\w\\\\\\\\w+\\\\\\\\b\\')\\n# load spacy language model and save old tokenizer\\nen_nlp = spacy.load(\\'en\\')\\nold_tokenizer = en_nlp.tokenizer\\n# replace the tokenizer with the preceding regexp\\nen_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(\\n    regexp.findall(string))\\n# create a custom tokenizer using the spacy document processing pipeline\\n# (now using our own tokenizer)\\ndef custom_tokenizer(document):\\n    doc_spacy = en_nlp(document, entity=False, parse=False)\\n    return [token.lemma_ for token in doc_spacy]\\n# define a count vectorizer with the custom tokenizer\\nlemma_vect = CountVectorizer(tokenizer=custom_tokenizer, min_df=5)\\nLet’s transform the data and inspect the vocabulary size:\\nIn[39]:\\n# transform text_train using CountVectorizer with lemmatization\\nX_train_lemma = lemma_vect.fit_transform(text_train)\\nprint(\"X_train_lemma.shape: {}\".format(X_train_lemma.shape))\\n# standard CountVectorizer for reference\\nvect = CountVectorizer(min_df=5).fit(text_train)\\nX_train = vect.transform(text_train)\\nprint(\"X_train.shape: {}\".format(X_train.shape))\\n346 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 360, 'page_label': '347'}, page_content='Out[39]:\\nX_train_lemma.shape:  (25000, 21596)\\nX_train.shape:  (25000, 27271)\\nAs you can see from the output, lemmatization reduced the number of features from\\n27,271 (with the standard CountVectorizer processing) to 21,596. Lemmatization\\ncan be seen as a kind of regularization, as it conflates certain features. Therefore, we\\nexpect lemmatization to improve performance most when the dataset is small. To\\nillustrate how lemmatization can help, we will use StratifiedShuffleSplit for\\ncross-validation, using only 1% of the data as training data and the rest as test data:\\nIn[40]:\\n# build a grid search using only 1% of the data as the training set\\nfrom sklearn.model_selection import StratifiedShuffleSplit\\nparam_grid = {\\'C\\': [0.001, 0.01, 0.1, 1, 10]}\\ncv = StratifiedShuffleSplit(n_iter=5, test_size=0.99,\\n                            train_size=0.01, random_state=0)\\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv=cv)\\n# perform grid search with standard CountVectorizer\\ngrid.fit(X_train, y_train)\\nprint(\"Best cross-validation score \"\\n      \"(standard CountVectorizer): {:.3f}\".format(grid.best_score_))\\n# perform grid search with lemmatization\\ngrid.fit(X_train_lemma, y_train)\\nprint(\"Best cross-validation score \"\\n      \"(lemmatization): {:.3f}\".format(grid.best_score_))\\nOut[40]:\\nBest cross-validation score (standard CountVectorizer): 0.721\\nBest cross-validation score (lemmatization): 0.731\\nIn this case, lemmatization provided a modest improvement in performance. As with\\nmany of the different feature extraction techniques, the result varies depending on\\nthe dataset. Lemmatization and stemming can sometimes help in building better (or\\nat least more compact) models, so we suggest you give these techniques a try when\\ntrying to squeeze out the last bit of performance on a particular task.\\nTopic Modeling and Document Clustering\\nOne particular technique that is often applied to text data is topic modeling, which is\\nan umbrella term describing the task of assigning each document to one or multiple\\ntopics, usually without supervision. A good example for this is news data, which\\nmight be categorized into topics like “politics, ” “sports, ” “finance, ” and so on. If each\\ndocument is assigned a single topic, this is the task of clustering the documents, as\\ndiscussed in Chapter 3 . If each document can have more than one topic, the task\\nTopic Modeling and Document Clustering | 347'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 361, 'page_label': '348'}, page_content='9 There is another machine learning model that is also often abbreviated LDA: Linear Discriminant Analysis, a\\nlinear classification model. This leads to quite some confusion. In this book, LDA refers to Latent Dirichlet\\nAllocation.\\nrelates to the decomposition methods from Chapter 3. Each of the components we\\nlearn then corresponds to one topic, and the coefficients of the components in the\\nrepresentation of a document tell us how strongly related that document is to a par‐\\nticular topic. Often, when people talk about topic modeling, they refer to one particu‐\\nlar decomposition method called Latent Dirichlet Allocation (often LDA for short).9\\nLatent Dirichlet Allocation\\nIntuitively, the LDA model tries to find groups of words (the topics) that appear\\ntogether frequently. LDA also requires that each document can be understood as a\\n“mixture” of a subset of the topics. It is important to understand that for the machine\\nlearning model a “topic” might not be what we would normally call a topic in every‐\\nday speech, but that it resembles more the components extracted by PCA or NMF\\n(which we discussed in Chapter 3), which might or might not have a semantic mean‐\\ning. Even if there is a semantic meaning for an LDA “topic” , it might not be some‐\\nthing we’ d usually call a topic. Going back to the example of news articles, we might\\nhave a collection of articles about sports, politics, and finance, written by two specific\\nauthors. In a politics article, we might expect to see words like “governor, ” “vote, ”\\n“party, ” etc., while in a sports article we might expect words like “team, ” “score, ” and\\n“season. ” Words in each of these groups will likely appear together, while it’s less likely\\nthat, for example, “team” and “governor” will appear together. However, these are not\\nthe only groups of words we might expect to appear together. The two reporters\\nmight prefer different phrases or different choices of words. Maybe one of them likes\\nto use the word “demarcate” and one likes the word “polarize. ” Other “topics” would\\nthen be “words often used by reporter A ” and “words often used by reporter B, ”\\nthough these are not topics in the usual sense of the word.\\nLet’s apply LDA to our movie review dataset to see how it works in practice. For\\nunsupervised text document models, it is often good to remove very common words,\\nas they might otherwise dominate the analysis. We’ll remove words that appear in at\\nleast 20 percent of the documents, and we’ll limit the bag-of-words model to the\\n10,000 words that are most common after removing the top 20 percent:\\nIn[41]:\\nvect = CountVectorizer(max_features=10000, max_df=.15)\\nX = vect.fit_transform(text_train)\\n348 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 362, 'page_label': '349'}, page_content='10 In fact, NMF and LDA solve quite related problems, and we could also use NMF to extract topics.\\nWe will learn a topic model with 10 topics, which is few enough that we can look at all\\nof them. Similarly to the components in NMF , topics don’t have an inherent ordering,\\nand changing the number of topics will change all of the topics. 10 We’ll use the\\n\"batch\" learning method, which is somewhat slower than the default ( \"online\") but\\nusually provides better results, and increase \"max_iter\", which can also lead to better\\nmodels:\\nIn[42]:\\nfrom sklearn.decomposition import LatentDirichletAllocation\\nlda = LatentDirichletAllocation(n_topics=10, learning_method=\"batch\",\\n                                max_iter=25, random_state=0)\\n# We build the model and transform the data in one step\\n# Computing transform takes some time,\\n# and we can save time by doing both at once\\ndocument_topics = lda.fit_transform(X)\\nLike the decomposition methods we saw in Chapter 3, LatentDirichletAllocation\\nhas a components_ attribute that stores how important each word is for each topic.\\nThe size of components_ is (n_topics, n_words):\\nIn[43]:\\nlda.components_.shape\\nOut[43]:\\n(10, 10000)\\nTo understand better what the different topics mean, we will look at the most impor‐\\ntant words for each of the topics. The print_topics function provides a nice format‐\\nting for these features:\\nIn[44]:\\n# For each topic (a row in the components_), sort the features (ascending)\\n# Invert rows with [:, ::-1] to make sorting descending\\nsorting = np.argsort(lda.components_, axis=1)[:, ::-1]\\n# Get the feature names from the vectorizer\\nfeature_names = np.array(vect.get_feature_names())\\nIn[45]:\\n# Print out the 10 topics:\\nmglearn.tools.print_topics(topics=range(10), feature_names=feature_names,\\n                           sorting=sorting, topics_per_chunk=5, n_words=10)\\nTopic Modeling and Document Clustering | 349'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 363, 'page_label': '350'}, page_content='Out[45]:\\ntopic 0       topic 1       topic 2       topic 3       topic 4\\n--------      --------      --------      --------      --------\\nbetween       war           funny         show          didn\\nyoung         world         worst         series        saw\\nfamily        us            comedy        episode       am\\nreal          our           thing         tv            thought\\nperformance   american      guy           episodes      years\\nbeautiful     documentary   re            shows         book\\nwork          history       stupid        season        watched\\neach          new           actually      new           now\\nboth          own           nothing       television    dvd\\ndirector      point         want          years         got\\ntopic 5       topic 6       topic 7       topic 8       topic 9\\n--------      --------      --------      --------      --------\\nhorror        kids          cast          performance   house\\naction        action        role          role          woman\\neffects       animation     john          john          gets\\nbudget        game          version       actor         killer\\nnothing       fun           novel         oscar         girl\\noriginal      disney        both          cast          wife\\ndirector      children      director      plays         horror\\nminutes       10            played        jack          young\\npretty        kid           performance   joe           goes\\ndoesn         old           mr            performances  around\\nJudging from the important words, topic 1 seems to be about historical and war mov‐\\nies, topic 2 might be about bad comedies, topic 3 might be about TV series. Topic 4\\nseems to capture some very common words, while topic 6 appears to be about child‐\\nren’s movies and topic 8 seems to capture award-related reviews. Using only 10 topics,\\neach of the topics needs to be very broad, so that they can together cover all the dif‐\\nferent kinds of reviews in our dataset.\\nNext, we will learn another model, this time with 100 topics. Using more topics\\nmakes the analysis much harder, but makes it more likely that topics can specialize to\\ninteresting subsets of the data:\\nIn[46]:\\nlda100 = LatentDirichletAllocation(n_topics=100, learning_method=\"batch\",\\n                                   max_iter=25, random_state=0)\\ndocument_topics100 = lda100.fit_transform(X)\\nLooking at all 100 topics would be a bit overwhelming, so we selected some interest‐\\ning and representative topics:\\n350 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 364, 'page_label': '351'}, page_content='In[47]:\\ntopics = np.array([7, 16, 24, 25, 28, 36, 37, 45, 51, 53, 54, 63, 89, 97])\\nsorting = np.argsort(lda100.components_, axis=1)[:, ::-1]\\nfeature_names = np.array(vect.get_feature_names())\\nmglearn.tools.print_topics(topics=topics, feature_names=feature_names,\\n                           sorting=sorting, topics_per_chunk=7, n_words=20)\\nOut[48]:\\ntopic 7       topic 16      topic 24      topic 25      topic 28\\n--------      --------      --------      --------      --------\\nthriller      worst         german        car           beautiful\\nsuspense      awful         hitler        gets          young\\nhorror        boring        nazi          guy           old\\natmosphere    horrible      midnight      around        romantic\\nmystery       stupid        joe           down          between\\nhouse         thing         germany       kill          romance\\ndirector      terrible      years         goes          wonderful\\nquite         script        history       killed        heart\\nbit           nothing       new           going         feel\\nde            worse         modesty       house         year\\nperformances  waste         cowboy        away          each\\ndark          pretty        jewish        head          french\\ntwist         minutes       past          take          sweet\\nhitchcock     didn          kirk          another       boy\\ntension       actors        young         getting       loved\\ninteresting   actually      spanish       doesn         girl\\nmysterious    re            enterprise    now           relationship\\nmurder        supposed      von           night         saw\\nending        mean          nazis         right         both\\ncreepy        want          spock         woman         simple\\ntopic 36      topic 37      topic 41      topic 45      topic 51\\n--------      --------      --------      --------      --------\\nperformance   excellent     war           music         earth\\nrole          highly        american      song          space\\nactor         amazing       world         songs         planet\\ncast          wonderful     soldiers      rock          superman\\nplay          truly         military      band          alien\\nactors        superb        army          soundtrack    world\\nperformances  actors        tarzan        singing       evil\\nplayed        brilliant     soldier       voice         humans\\nsupporting    recommend     america       singer        aliens\\ndirector      quite         country       sing          human\\noscar         performance   americans     musical       creatures\\nroles         performances  during        roll          miike\\nactress       perfect       men           fan           monsters\\nexcellent     drama         us            metal         apes\\nscreen        without       government    concert       clark\\nplays         beautiful     jungle        playing       burton\\naward         human         vietnam       hear          tim\\nwork          moving        ii            fans          outer\\nplaying       world         political     prince        men\\ngives         recommended   against       especially    moon\\nTopic Modeling and Document Clustering | 351'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 365, 'page_label': '352'}, page_content='topic 53      topic 54      topic 63      topic 89      topic 97\\n--------      --------      --------      --------      --------\\nscott         money         funny         dead          didn\\ngary          budget        comedy        zombie        thought\\nstreisand     actors        laugh         gore          wasn\\nstar          low           jokes         zombies       ending\\nhart          worst         humor         blood         minutes\\nlundgren      waste         hilarious     horror        got\\ndolph         10            laughs        flesh         felt\\ncareer        give          fun           minutes       part\\nsabrina       want          re            body          going\\nrole          nothing       funniest      living        seemed\\ntemple        terrible      laughing      eating        bit\\nphantom       crap          joke          flick         found\\njudy          must          few           budget        though\\nmelissa       reviews       moments       head          nothing\\nzorro         imdb          guy           gory          lot\\ngets          director      unfunny       evil          saw\\nbarbra        thing         times         shot          long\\ncast          believe       laughed       low           interesting\\nshort         am            comedies      fulci         few\\nserial        actually      isn           re            half\\nThe topics we extracted this time seem to be more specific, though many are hard to\\ninterpret. Topic 7 seems to be about horror movies and thrillers; topics 16 and 54\\nseem to capture bad reviews, while topic 63 mostly seems to be capturing positive\\nreviews of comedies. If we want to make further inferences using the topics that were\\ndiscovered, we should confirm the intuition we gained from looking at the highest-\\nranking words for each topic by looking at the documents that are assigned to these\\ntopics. For example, topic 45 seems to be about music. Let’s check which kinds of\\nreviews are assigned to this topic:\\nIn[49]:\\n# sort by weight of \"music\" topic 45\\nmusic = np.argsort(document_topics100[:, 45])[::-1]\\n# print the five documents where the topic is most important\\nfor i in music[:10]:\\n    # pshow first two sentences\\n    print(b\".\".join(text_train[i].split(b\".\")[:2]) + b\".\\\\n\")\\nOut[49]:\\nb\\'I love this movie and never get tired of watching. The music in it is great.\\\\n\\'\\nb\"I enjoyed Still Crazy more than any film I have seen in years. A successful\\n  band from the 70\\'s decide to give it another try.\\\\n\"\\nb\\'Hollywood Hotel was the last movie musical that Busby Berkeley directed for\\n  Warner Bros. His directing style had changed or evolved to the point that\\n  this film does not contain his signature overhead shots or huge production\\n  numbers with thousands of extras.\\\\n\\'\\nb\"What happens to washed up rock-n-roll stars in the late 1990\\'s?\\n  They launch a comeback / reunion tour. At least, that\\'s what the members of\\n  Strange Fruit, a (fictional) 70\\'s stadium rock group do.\\\\n\"\\n352 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 366, 'page_label': '353'}, page_content='b\\'As a big-time Prince fan of the last three to four years, I really can\\\\\\'t\\n  believe I\\\\\\'ve only just got round to watching \"Purple Rain\". The brand new\\n  2-disc anniversary Special Edition led me to buy it.\\\\n\\'\\nb\"This film is worth seeing alone for Jared Harris\\' outstanding portrayal\\n  of John Lennon. It doesn\\'t matter that Harris doesn\\'t exactly resemble\\n  Lennon; his mannerisms, expressions, posture, accent and attitude are\\n  pure Lennon.\\\\n\"\\nb\"The funky, yet strictly second-tier British glam-rock band Strange Fruit\\n  breaks up at the end of the wild\\'n\\'wacky excess-ridden 70\\'s. The individual\\n  band members go their separate ways and uncomfortably settle into lackluster\\n  middle age in the dull and uneventful 90\\'s: morose keyboardist Stephen Rea\\n  winds up penniless and down on his luck, vain, neurotic, pretentious lead\\n  singer Bill Nighy tries (and fails) to pursue a floundering solo career,\\n  paranoid drummer Timothy Spall resides in obscurity on a remote farm so he\\n  can avoid paying a hefty back taxes debt, and surly bass player Jimmy Nail\\n  installs roofs for a living.\\\\n\"\\nb\"I just finished reading a book on Anita Loos\\' work and the photo in TCM\\n  Magazine of MacDonald in her angel costume looked great (impressive wings),\\n  so I thought I\\'d watch this movie. I\\'d never heard of the film before, so I\\n  had no preconceived notions about it whatsoever.\\\\n\"\\nb\\'I love this movie!!! Purple Rain came out the year I was born and it has had\\n  my heart since I can remember. Prince is so tight in this movie.\\\\n\\'\\nb\"This movie is sort of a Carrie meets Heavy Metal. It\\'s about a highschool\\n  guy who gets picked on alot and he totally gets revenge with the help of a\\n  Heavy Metal ghost.\\\\n\"\\nAs we can see, this topic covers a wide variety of music-centered reviews, from musi‐\\ncals, to biographical movies, to some hard-to-specify genre in the last review. Another\\ninteresting way to inspect the topics is to see how much weight each topic gets over‐\\nall, by summing the document_topics over all reviews. We name each topic by the\\ntwo most common words. Figure 7-6 shows the topic weights learned:\\nIn[50]:\\nfig, ax = plt.subplots(1, 2, figsize=(10, 10))\\ntopic_names = [\"{:>2} \".format(i) + \" \".join(words)\\n               for i, words in enumerate(feature_names[sorting[:, :2]])]\\n# two column bar chart:\\nfor col in [0, 1]:\\n    start = col * 50\\n    end = (col + 1) * 50\\n    ax[col].barh(np.arange(50), np.sum(document_topics100, axis=0)[start:end])\\n    ax[col].set_yticks(np.arange(50))\\n    ax[col].set_yticklabels(topic_names[start:end], ha=\"left\", va=\"top\")\\n    ax[col].invert_yaxis()\\n    ax[col].set_xlim(0, 2000)\\n    yax = ax[col].get_yaxis()\\n    yax.set_tick_params(pad=130)\\nplt.tight_layout()\\nTopic Modeling and Document Clustering | 353'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 367, 'page_label': '354'}, page_content='Figure 7-6. Topic weights learned by LDA\\nThe most important topics are 97, which seems to consist mostly of stopwords, possi‐\\nbly with a slight negative direction; topic 16, which is clearly about bad reviews; fol‐\\nlowed by some genre-specific topics and 36 and 37, both of which seem to contain\\nlaudatory words.\\nIt seems like LDA mostly discovered two kind of topics, genre-specific and rating-\\nspecific, in addition to several more unspecific topics. This is an interesting discovery,\\nas most reviews are made up of some movie-specific comments and some comments\\nthat justify or emphasize the rating.\\nTopic models like LDA are interesting methods to understand large text corpora in\\nthe absence of labels—or, as here, even if labels are available. The LDA algorithm is\\nrandomized, though, and changing the random_state parameter can lead to quite\\n354 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 368, 'page_label': '355'}, page_content='different outcomes. While identifying topics can be helpful, any conclusions you\\ndraw from an unsupervised model should be taken with a grain of salt, and we rec‐\\nommend verifying your intuition by looking at the documents in a specific topic. The\\ntopics produced by the LDA.transform method can also sometimes be used as a com‐\\npact representation for supervised learning. This is particularly helpful when few\\ntraining examples are available.\\nSummary and Outlook\\nIn this chapter we talked about the basics of processing text, also known as natural\\nlanguage processing (NLP), with an example application classifying movie reviews.\\nThe tools discussed here should serve as a great starting point when trying to process\\ntext data. In particular for text classification tasks such as spam and fraud detection\\nor sentiment analysis, bag-of-words representations provide a simple and powerful\\nsolution. As is often the case in machine learning, the representation of the data is key\\nin NLP applications, and inspecting the tokens and n-grams that are extracted can\\ngive powerful insights into the modeling process. In text-processing applications, it is\\noften possible to introspect models in a meaningful way, as we saw in this chapter, for\\nboth supervised and unsupervised tasks. Y ou should take full advantage of this ability\\nwhen using NLP-based methods in practice.\\nNatural language and text processing is a large research field, and discussing the\\ndetails of advanced methods is far beyond the scope of this book. If you want to learn\\nmore, we recommend the O’Reilly book Natural Language Processing with Python by\\nSteven Bird, Ewan Klein, and Edward Loper, which provides an overview of NLP\\ntogether with an introduction to the nltk Python package for NLP . Another great and\\nmore conceptual book is the standard reference Introduction to Information Retrieval\\nby Christopher Manning, Prabhakar Raghavan, and Hinrich Schütze, which describes\\nfundamental algorithms in information retrieval, NLP , and machine learning. Both\\nbooks have online versions that can be accessed free of charge. As we discussed ear‐\\nlier, the classes CountVectorizer and TfidfVectorizer only implement relatively\\nsimple text-processing methods. For more advanced text-processing methods, we\\nrecommend the Python packages spacy (a relatively new but very efficient and well-\\ndesigned package), nltk (a very well-established and complete but somewhat dated\\nlibrary), and gensim (an NLP package with an emphasis on topic modeling).\\nThere have been several very exciting new developments in text processing in recent\\nyears, which are outside of the scope of this book and relate to neural networks. The\\nfirst is the use of continuous vector representations, also known as word vectors or\\ndistributed word representations, as implemented in the word2vec library. The origi‐\\nnal paper “Distributed Representations of Words and Phrases and Their Composi‐\\ntionality” by Thomas Mikolov et al. is a great introduction to the subject. Both spacy\\nSummary and Outlook | 355'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 369, 'page_label': '356'}, page_content='and gensim provide functionality for the techniques discussed in this paper and its\\nfollow-ups.\\nAnother direction in NLP that has picked up momentum in recent years is the use of\\nrecurrent neural networks (RNNs) for text processing. RNNs are a particularly power‐\\nful type of neural network that can produce output that is again text, in contrast to\\nclassification models that can only assign class labels. The ability to produce text as\\noutput makes RNNs well suited for automatic translation and summarization. An\\nintroduction to the topic can be found in the relatively technical paper “Sequence to\\nSequence Learning with Neural Networks” by Ilya Suskever, Oriol Vinyals, and Quoc\\nLe. A more practical tutorial using the tensorflow framework can be found on the\\nTensorFlow website.\\n356 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 370, 'page_label': '357'}, page_content='CHAPTER 8\\nWrapping Up\\nY ou now know how to apply the important machine learning algorithms for super‐\\nvised and unsupervised learning, which allow you to solve a wide variety of machine\\nlearning problems. Before we leave you to explore all the possibilities that machine\\nlearning offers, we want to give you some final words of advice, point you toward\\nsome additional resources, and give you suggestions on how you can further improve\\nyour machine learning and data science skills.\\nApproaching a Machine Learning Problem\\nWith all the great methods that we introduced in this book now at your fingertips, it\\nmay be tempting to jump in and start solving your data-related problem by just run‐\\nning your favorite algorithm. However, this is not usually a good way to begin your\\nanalysis. The machine learning algorithm is usually only a small part of a larger data\\nanalysis and decision-making process. To make effective use of machine learning, we\\nneed to take a step back and consider the problem at large. First, you should think\\nabout what kind of question you want to answer. Do you want to do exploratory anal‐\\nysis and just see if you find something interesting in the data? Or do you already have\\na particular goal in mind? Often you will start with a goal, like detecting fraudulent\\nuser transactions, making movie recommendations, or finding unknown planets. If\\nyou have such a goal, before building a system to achieve it, you should first think\\nabout how to define and measure success, and what the impact of a successful solu‐\\ntion would be to your overall business or research goals. Let’s say your goal is fraud\\ndetection.\\n357'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 371, 'page_label': '358'}, page_content='Then the following questions open up:\\n• How do I measure if my fraud prediction is actually working?\\n• Do I have the right data to evaluate an algorithm?\\n• If I am successful, what will be the business impact of my solution?\\nAs we discussed in Chapter 5, it is best if you can measure the performance of your\\nalgorithm directly using a business metric, like increased profit or decreased losses.\\nThis is often hard to do, though. A question that can be easier to answer is “What if I\\nbuilt the perfect model?” If perfectly detecting any fraud will save your company $100\\na month, these possible savings will probably not be enough to warrant the effort of\\nyou even starting to develop an algorithm. On the other hand, if the model might\\nsave your company tens of thousands of dollars every month, the problem might be\\nworth exploring.\\nSay you’ve defined the problem to solve, you know a solution might have a significant\\nimpact for your project, and you’ve ensured that you have the right information to\\nevaluate success. The next steps are usually acquiring the data and building a working\\nprototype. In this book we have talked about many models you can employ, and how\\nto properly evaluate and tune these models. While trying out models, though, keep in\\nmind that this is only a small part of a larger data science workflow, and model build‐\\ning is often part of a feedback circle of collecting new data, cleaning data, building\\nmodels, and analyzing the models. Analyzing the mistakes a model makes can often\\nbe informative about what is missing in the data, what additional data could be col‐\\nlected, or how the task could be reformulated to make machine learning more effec‐\\ntive. Collecting more or different data or changing the task formulation slightly might\\nprovide a much higher payoff than running endless grid searches to tune parameters.\\nHumans in the Loop\\nY ou should also consider if and how you should have humans in the loop. Some pro‐\\ncesses (like pedestrian detection in a self-driving car) need to make immediate deci‐\\nsions. Others might not need immediate responses, and so it can be possible to have\\nhumans confirm uncertain decisions. Medical applications, for example, might need\\nvery high levels of precision that possibly cannot be achieved by a machine learning\\nalgorithm alone. But if an algorithm can make 90 percent, 50 percent, or maybe even\\njust 10 percent of decisions automatically, that might already increase response time\\nor reduce cost. Many applications are dominated by “simple cases, ” for which an algo‐\\nrithm can make a decision, with relatively few “complicated cases, ” which can be\\nrerouted to a human.\\n358 | Chapter 8: Wrapping Up'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 372, 'page_label': '359'}, page_content='From Prototype to Production\\nThe tools we’ve discussed in this book are great for many machine learning applica‐\\ntions, and allow very quick analysis and prototyping. Python and scikit-learn are\\nalso used in production systems in many organizations—even very large ones like\\ninternational banks and global social media companies. However, many companies\\nhave complex infrastructure, and it is not always easy to include Python in these sys‐\\ntems. That is not necessarily a problem. In many companies, the data analytics teams\\nwork with languages like Python and R that allow the quick testing of ideas, while\\nproduction teams work with languages like Go, Scala, C++, and Java to build robust,\\nscalable systems. Data analysis has different requirements from building live services,\\nand so using different languages for these tasks makes sense. A relatively common\\nsolution is to reimplement the solution that was found by the analytics team inside\\nthe larger framework, using a high-performance language. This can be easier than\\nembedding a whole library or programming language and converting from and to the\\ndifferent data formats.\\nRegardless of whether you can use scikit-learn in a production system or not, it is\\nimportant to keep in mind that production systems have different requirements from\\none-off analysis scripts. If an algorithm is deployed into a larger system, software\\nengineering aspects like reliability, predictability, runtime, and memory requirements\\ngain relevance. Simplicity is key in providing machine learning systems that perform\\nwell in these areas. Critically inspect each part of your data processing and prediction\\npipeline and ask yourself how much complexity each step creates, how robust each\\ncomponent is to changes in the data or compute infrastructure, and if the benefit of\\neach component warrants the complexity. If you are building involved machine learn‐\\ning systems, we highly recommend reading the paper “Machine Learning: The High\\nInterest Credit Card of Technical Debt” , published by researchers in Google’s\\nmachine learning team. The paper highlights the trade-off in creating and maintain‐\\ning machine learning software in production at a large scale. While the issue of tech‐\\nnical debt is particularly pressing in large-scale and long-term projects, the lessons\\nlearned can help us build better software even for short-lived and smaller systems.\\nTesting Production Systems\\nIn this book, we covered how to evaluate algorithmic predictions based on a test set\\nthat we collected beforehand. This is known as offline evaluation. If your machine\\nlearning system is user-facing, this is only the first step in evaluating an algorithm,\\nthough. The next step is usually online testing or live testing, where the consequences\\nof employing the algorithm in the overall system are evaluated. Changing the recom‐\\nmendations or search results users are shown by a website can drastically change\\ntheir behavior and lead to unexpected consequences. To protect against these sur‐\\nprises, most user-facing services employ A/B testing, a form of blind user study. In\\nFrom Prototype to Production | 359'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 373, 'page_label': '360'}, page_content='A/B testing, without their knowledge a selected portion of users will be provided with\\na website or service using algorithm A, while the rest of the users will be provided\\nwith algorithm B. For both groups, relevant success metrics will be recorded for a set\\nperiod of time. Then, the metrics of algorithm A and algorithm B will be compared,\\nand a selection between the two approaches will be made according to these metrics.\\nUsing A/B testing enables us to evaluate the algorithms “in the wild, ” which might\\nhelp us to discover unexpected consequences when users are interacting with our\\nmodel. Often A is a new model, while B is the established system. There are more\\nelaborate mechanisms for online testing that go beyond A/B testing, such as bandit\\nalgorithms. A great introduction to this subject can be found in the book Bandit Algo‐\\nrithms for Website Optimization by John Myles White (O’Reilly). \\nBuilding Your Own Estimator\\nThis book has covered a variety of tools and algorithms implemented in scikit-\\nlearn that can be used on a wide range of tasks. However, often there will be some\\nparticular processing you need to do for your data that is not implemented in\\nscikit-learn. It may be enough to just preprocess your data before passing it to your\\nscikit-learn model or pipeline. However, if your preprocessing is data dependent,\\nand you want to apply a grid search or cross-validation, things become trickier.\\nIn Chapter 6 we discussed the importance of putting all data-dependent processing\\ninside the cross-validation loop. So how can you use your own processing together\\nwith the scikit-learn tools? There is a simple solution: build your own estimator!\\nImplementing an estimator that is compatible with the scikit-learn interface, so\\nthat it can be used with Pipeline, GridSearchCV, and cross_val_score, is quite easy.\\nY ou can find detailed instructions in the scikit-learn documentation, but here is\\nthe gist. The simplest way to implement a transformer class is by inheriting from\\nBaseEstimator and TransformerMixin, and then implementing the __init__, fit,\\nand predict functions like this:\\n360 | Chapter 8: Wrapping Up'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 374, 'page_label': '361'}, page_content='In[1]:\\nfrom sklearn.base import BaseEstimator, TransformerMixin\\nclass MyTransformer(BaseEstimator, TransformerMixin):\\n    def __init__(self, first_parameter=1, second_parameter=2):\\n        # All parameters must be specified in the __init__ function\\n        self.first_parameter = 1\\n        self.second_parameter = 2\\n    def fit(self, X, y=None):\\n        # fit should only take X and y as parameters\\n        # Even if your model is unsupervised, you need to accept a y argument!\\n        # Model fitting code goes here\\n        print(\"fitting the model right here\")\\n        # fit returns self\\n        return self\\n    def transform(self, X):\\n        # transform takes as parameter only X\\n        # Apply some transformation to X\\n        X_transformed = X + 1\\n        return X_transformed\\nImplementing a classifier or regressor works similarly, only instead of Transformer\\nMixin you need to inherit from ClassifierMixin or RegressorMixin. Also, instead\\nof implementing transform, you would implement predict.\\nAs you can see from the example given here, implementing your own estimator\\nrequires very little code, and most scikit-learn users build up a collection of cus‐\\ntom models over time.\\nWhere to Go from Here\\nThis book provides an introduction to machine learning and will make you an effec‐\\ntive practitioner. However, if you want to further your machine learning skills, here\\nare some suggestions of books and more specialized resources to investigate to dive\\ndeeper.\\nTheory\\nIn this book, we tried to provide an intuition of how the most common machine\\nlearning algorithms work, without requiring a strong foundation in mathematics or\\ncomputer science. However, many of the models we discussed use principles from\\nprobability theory, linear algebra, and optimization. While it is not necessary to\\nunderstand all the details of how these algorithms are implemented, we think that\\nWhere to Go from Here | 361'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 375, 'page_label': '362'}, page_content='1 Andreas might not be entirely objective in this matter.\\nknowing some of the theory behind the algorithms will make you a better data scien‐\\ntist. There have been many good books written about the theory of machine learning,\\nand if we were able to excite you about the possibilities that machine learning opens\\nup, we suggest you pick up at least one of them and dig deeper. We already men‐\\ntioned Hastie, Tibshirani, and Friedman’s book The Elements of Statistical Learning in\\nthe Preface, but it is worth repeating this recommendation here. Another quite acces‐\\nsible book, with accompanying Python code, is Machine Learning: An Algorithmic\\nPerspective by Stephen Marsland (Chapman and Hall/CRC). Two other highly recom‐\\nmended classics are Pattern Recognition and Machine Learning by Christopher Bishop\\n(Springer), a book that emphasizes a probabilistic framework, and Machine Learning:\\nA Probabilistic Perspective  by Kevin Murphy (MIT Press), a comprehensive (read:\\n1,000+ pages) dissertation on machine learning methods featuring in-depth discus‐\\nsions of state-of-the-art approaches, far beyond what we could cover in this book.\\nOther Machine Learning Frameworks and Packages\\nWhile scikit-learn is our favorite package for machine learning 1 and Python is our\\nfavorite language for machine learning, there are many other options out there.\\nDepending on your needs, Python and scikit-learn might not be the best fit for\\nyour particular situation. Often using Python is great for trying out and evaluating\\nmodels, but larger web services and applications are more commonly written in Java\\nor C++, and integrating into these systems might be necessary for your model to be\\ndeployed. Another reason you might want to look beyond scikit-learn is if you are\\nmore interested in statistical modeling and inference than prediction. In this case,\\nyou should consider the statsmodel package for Python, which implements several\\nlinear models with a more statistically minded interface. If you are not married to\\nPython, you might also consider using R, another lingua franca of data scientists. R is\\na language designed specifically for statistical analysis and is famous for its excellent\\nvisualization capabilities and the availability of many (often highly specialized) statis‐\\ntical modeling packages.\\nAnother popular machine learning package is vowpal wabbit  (often called vw to\\navoid possible tongue twisting), a highly optimized machine learning package written\\nin C++ with a command-line interface. vw is particularly useful for large datasets and\\nfor streaming data. For running machine learning algorithms distributed on a cluster,\\none of the most popular solutions at the time of writing is mllib, a Scala library built\\non top of the spark distributed computing environment.\\n362 | Chapter 8: Wrapping Up'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 376, 'page_label': '363'}, page_content='Ranking, Recommender Systems, and Other Kinds of Learning\\nBecause this is an introductory book, we focused on the most common machine\\nlearning tasks: classification and regression in supervised learning, and clustering and\\nsignal decomposition in unsupervised learning. There are many more kinds of\\nmachine learning out there, with many important applications. There are two partic‐\\nularly important topics that we did not cover in this book. The first is ranking, in\\nwhich we want to retrieve answers to a particular query, ordered by their relevance.\\nY ou’ve probably already used a ranking system today; this is how search engines\\noperate. Y ou input a search query and obtain a sorted list of answers, ranked by how\\nrelevant they are. A great introduction to ranking is provided in Manning, Raghavan,\\nand Schütze’s book Introduction to Information Retrieval. The second topic is recom‐\\nmender systems , which provide suggestions to users based on their preferences.\\nY ou’ve probably encountered recommender systems under headings like “People Y ou\\nMay Know, ” “Customers Who Bought This Item Also Bought, ” or “Top Picks for\\nY ou. ” There is plenty of literature on the topic, and if you want to dive right in you\\nmight be interested in the now classic “Netflix prize challenge”, in which the Netflix\\nvideo streaming site released a large dataset of movie preferences and offered a prize\\nof $1 million to the team that could provide the best recommendations. Another\\ncommon application is prediction of time series (like stock prices), which also has a\\nwhole body of literature devoted to it. There are many more machine learning tasks\\nout there—much more than we can list here—and we encourage you to seek out\\ninformation from books, research papers, and online communities to find the para‐\\ndigms that best apply to your situation.\\nProbabilistic Modeling, Inference, and Probabilistic Programming\\nMost machine learning packages provide predefined machine learning models that\\napply one particular algorithm. However, many real-world problems have a particular\\nstructure that, when properly incorporated into the model, can yield much better-\\nperforming predictions. Often, the structure of a particular problem can be expressed\\nusing the language of probability theory. Such structure commonly arises from hav‐\\ning a mathematical model of the situation for which you want to predict. To under‐\\nstand what we mean by a structured problem, consider the following example.\\nLet’s say you want to build a mobile application that provides a very detailed position\\nestimate in an outdoor space, to help users navigate a historical site. A mobile phone\\nprovides many sensors to help you get precise location measurements, like the GPS,\\naccelerometer, and compass. Y ou also have an exact map of the area. This problem is\\nhighly structured. Y ou know where the paths and points of interest are from your\\nmap. Y ou also have rough positions from the GPS, and the accelerometer and com‐\\npass in the user’s device provide you with very precise relative measurements. But\\nthrowing these all together into a black-box machine learning system to predict posi‐\\ntions might not be the best idea. This would throw away all the information you\\nWhere to Go from Here | 363'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 377, 'page_label': '364'}, page_content='2 A preprint of Deep Learning can be viewed at http://www.deeplearningbook.org/.\\nalready know about how the real world works. If the compass and accelerometer tell\\nyou a user is going north, and the GPS is telling you the user is going south, you\\nprobably can’t trust the GPS. If your position estimate tells you the user just walked\\nthrough a wall, you should also be highly skeptical. It’s possible to express this situa‐\\ntion using a probabilistic model, and then use machine learning or probabilistic\\ninference to find out how much you should trust each measurement, and to reason\\nabout what the best guess for the location of a user is.\\nOnce you’ve expressed the situation and your model of how the different factors work\\ntogether in the right way, there are methods to compute the predictions using these\\ncustom models directly. The most general of these methods are called probabilistic\\nprogramming languages, and they provide a very elegant and compact way to express\\na learning problem. Examples of popular probabilistic programming languages are\\nPyMC (which can be used in Python) and Stan (a framework that can be used from\\nseveral languages, including Python). While these packages require some under‐\\nstanding of probability theory, they simplify the creation of new models significantly.\\nNeural Networks\\nWhile we touched on the subject of neural networks briefly in Chapters 2 and 7, this\\nis a rapidly evolving area of machine learning, with innovations and new applications\\nbeing announced on a weekly basis. Recent breakthroughs in machine learning and\\nartificial intelligence, such as the victory of the Alpha Go program against human\\nchampions in the game of Go, the constantly improving performance of speech\\nunderstanding, and the availability of near-instantaneous speech translation, have all\\nbeen driven by these advances. While the progress in this field is so fast-paced that\\nany current reference to the state of the art will soon be outdated, the recent book\\nDeep Learning by Ian Goodfellow, Y oshua Bengio, and Aaron Courville (MIT Press)\\nis a comprehensive introduction into the subject.2\\nScaling to Larger Datasets\\nIn this book, we always assumed that the data we were working with could be stored\\nin a NumPy array or SciPy sparse matrix in memory (RAM). Even though modern\\nservers often have hundreds of gigabytes (GB) of RAM, this is a fundamental restric‐\\ntion on the size of data you can work with. Not everybody can afford to buy such a\\nlarge machine, or even to rent one from a cloud provider. In most applications, the\\ndata that is used to build a machine learning system is relatively small, though, and\\nfew machine learning datasets consist of hundreds of gigabites of data or more. This\\nmakes expanding your RAM or renting a machine from a cloud provider a viable sol‐\\nution in many cases. If you need to work with terabytes of data, however, or you need\\n364 | Chapter 8: Wrapping Up'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 378, 'page_label': '365'}, page_content='to process large amounts of data on a budget, there are two basic strategies: out-of-\\ncore learning and parallelization over a cluster.\\nOut-of-core learning describes learning from data that cannot be stored in main\\nmemory, but where the learning takes place on a single computer (or even a single\\nprocessor within a computer). The data is read from a source like the hard disk or the\\nnetwork either one sample at a time or in chunks of multiple samples, so that each\\nchunk fits into RAM. This subset of the data is then processed and the model is upda‐\\nted to reflect what was learned from the data. Then, this chunk of the data is dis‐\\ncarded and the next bit of data is read. Out-of-core learning is implemented for some\\nof the models in scikit-learn, and you can find details on it in the online user\\nguide. Because out-of-core learning requires all of the data to be processed by a single\\ncomputer, this can lead to long runtimes on very large datasets. Also, not all machine\\nlearning algorithms can be implemented in this way.\\nThe other strategy for scaling is distributing the data over multiple machines in a\\ncompute cluster, and letting each computer process part of the data. This can be\\nmuch faster for some models, and the size of the data that can be processed is only\\nlimited by the size of the cluster. However, such computations often require relatively\\ncomplex infrastructure. One of the most popular distributed computing platforms at\\nthe moment is the spark platform built on top of Hadoop. spark includes some\\nmachine learning functionality within the MLLib package. If your data is already on a\\nHadoop filesystem, or you are already using spark to preprocess your data, this might\\nbe the easiest option. If you don’t already have such infrastructure in place, establish‐\\ning and integrating a spark cluster might be too large an effort, however. The vw\\npackage mentioned earlier provides some distributed features and might be a better\\nsolution in this case.\\nHoning Your Skills\\nAs with many things in life, only practice will allow you to become an expert in the\\ntopics we covered in this book. Feature extraction, preprocessing, visualization, and\\nmodel building can vary widely between different tasks and different datasets. Maybe\\nyou are lucky enough to already have access to a variety of datasets and tasks. If you\\ndon’t already have a task in mind, a good place to start is machine learning competi‐\\ntions, in which a dataset with a given task is published, and teams compete in creating\\nthe best possible predictions. Many companies, nonprofit organizations, and univer‐\\nsities host these competitions. One of the most popular places to find them is Kaggle,\\na website that regularly holds data science competitions, some of which have substan‐\\ntial prize money attached.\\nThe Kaggle forums are also a good source of information about the latest tools and\\ntricks in machine learning, and a wide range of datasets are available on the site. Even\\nmore datasets with associated tasks can be found on the OpenML platform , which\\nWhere to Go from Here | 365'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 379, 'page_label': '366'}, page_content='hosts over 20,000 datasets with over 50,000 associated machine learning tasks. Work‐\\ning with these datasets can provide a great opportunity to practice your machine\\nlearning skills. A disadvantage of competitions is that they already provide a particu‐\\nlar metric to optimize, and usually a fixed, preprocessed dataset. Keep in mind that\\ndefining the problem and collecting the data are also important aspects of real-world\\nproblems, and that representing the problem in the right way might be much more\\nimportant than squeezing the last percent of accuracy out of a classifier.\\nConclusion\\nWe hope we have convinced you of the usefulness of machine learning in a wide vari‐\\nety of applications, and how easily machine learning can be implemented in practice.\\nKeep digging into the data, and don’t lose sight of the larger picture.\\n366 | Chapter 8: Wrapping Up'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 380, 'page_label': '367'}, page_content='Index\\nA\\nA/B testing, 359\\naccuracy, 22, 282\\nacknowledgments, xi\\nadjusted rand index (ARI), 191\\nagglomerative clustering\\nevaluating and comparing, 191\\nexample of, 183\\nhierarchical clustering, 184\\nlinkage choices, 182\\nprinciple of, 182\\nalgorithm chains and pipelines, 305-321\\nbuilding pipelines, 308\\nbuilding pipelines with make_pipeline,\\n313-316\\ngrid search preprocessing steps, 317\\ngrid-searching for model selection, 319\\nimportance of, 305\\noverview of, 320\\nparameter selection with preprocessing, 306\\npipeline interface, 312\\nusing pipelines in grid searches, 309-311\\nalgorithm parameter, 118\\nalgorithms (see also models; problem solving)\\nevaluating, 28\\nminimal code to apply to algorithm, 24\\nsample datasets, 30-34\\nscaling\\nMinMaxScaler, 102, 135-139, 190, 230,\\n308, 319\\nNormalizer, 134\\nRobustScaler, 133\\nStandardScaler, 114, 133, 138, 144, 150,\\n190-195, 314-320\\nsupervised, classification\\ndecision trees, 70-83\\ngradient boosting, 88-91, 119, 124\\nk-nearest neighbors, 35-44\\nkernelized support vector machines,\\n92-104\\nlinear SVMs, 56\\nlogistic regression, 56\\nnaive Bayes, 68-70\\nneural networks, 104-119\\nrandom forests, 84-88\\nsupervised, regression\\ndecision trees, 70-83\\ngradient boosting, 88-91\\nk-nearest neighbors, 40\\nLasso, 53-55\\nlinear regression (OLS), 47, 220-229\\nneural networks, 104-119\\nrandom forests, 84-88\\nRidge, 49-55, 67, 112, 231, 234, 310,\\n317-319\\nunsupervised, clustering\\nagglomerative clustering, 182-187,\\n191-195, 203-207\\nDBSCAN, 187-190\\nk-means, 168-181\\nunsupervised, manifold learning\\nt-SNE, 163-168\\nunsupervised, signal decomposition\\nnon-negative matrix factorization,\\n156-163\\nprincipal component analysis, 140-155\\nalpha parameter in linear models, 50\\nAnaconda, 6\\n367'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 381, 'page_label': '368'}, page_content='analysis of variance (ANOV A), 236\\narea under the curve (AUC), 294-296\\nattributions, x\\naverage precision, 292\\nB\\nbag-of-words representation\\napplying to movie reviews, 330-334\\napplying to toy dataset, 329\\nmore than one word (n-grams), 339-344\\nsteps in computing, 327\\nBernoulliNB, 68\\nbigrams, 339\\nbinary classification, 25, 56, 276-296\\nbinning, 144, 220-224\\nbootstrap samples, 84\\nBoston Housing dataset, 34\\nboundary points, 188\\nBunch objects, 33\\nbusiness metric, 275, 358\\nC\\nC parameter in SVC, 99\\ncalibration, 288\\ncancer dataset, 32\\ncategorical features\\ncategorical data, defined, 324\\ndefined, 211\\nencoded as numbers, 218\\nexample of, 212\\nrepresentation in training and test sets, 217\\nrepresenting using one-hot-encoding, 213\\ncategorical variables (see categorical features)\\nchaining (see algorithm chains and pipelines)\\nclass labels, 25\\nclassification problems\\nbinary vs. multiclass, 25\\nexamples of, 26\\ngoals for, 25\\niris classification example, 14\\nk-nearest neighbors, 35\\nlinear models, 56\\nnaive Bayes classifiers, 68\\nvs. regression problems, 26\\nclassifiers\\nDecisionTreeClassifier, 75, 278\\nDecisionTreeRegressor, 75, 80\\nKNeighborsClassifier, 21-24, 37-43\\nKNeighborsRegressor, 42-47\\nLinearSVC, 56-59, 65, 67, 68\\nLogisticRegression, 56-62, 67, 209, 253, 279,\\n315, 332-347\\nMLPClassifier, 107-119\\nnaive Bayes, 68-70\\nSVC, 56, 100, 134, 139, 260, 269-272, 273,\\n305-309, 313-320\\nuncertainty estimates from, 119-127\\ncluster centers, 168\\nclustering algorithms\\nagglomerative clustering, 182-187\\napplications for, 131\\ncomparing on faces dataset, 195-207\\nDBSCAN, 187-190\\nevaluating with ground truth, 191-193\\nevaluating without ground truth, 193-195\\ngoals of, 168\\nk-means clustering, 168-181\\nsummary of, 207\\ncode examples\\ndownloading, x\\npermission for use, x\\ncoef_ attribute, 47, 50\\ncomments and questions, xi\\ncompetitions, 365\\nconflation, 344\\nconfusion matrices, 279-286\\ncontext, 343\\ncontinuous features, 211, 218\\ncore samples/core points, 187\\ncorpus, 325\\ncos function, 232\\nCountVectorizer, 334\\ncross-validation\\nanalyzing results of, 267-271\\nbenefits of, 254\\ncross-validation splitters, 256\\ngrid search and, 263-275\\nin scikit-learn, 253\\nleave-one-out cross-validation, 257\\nnested, 272\\nparallelizing with grid search, 274\\nprinciple of, 252\\npurpose of, 254\\nshuffle-split cross-validation, 258\\nstratified k-fold, 254-256\\nwith groups, 259\\ncross_val_score function, 254, 307\\n368 | Index'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 382, 'page_label': '369'}, page_content='D\\ndata points, defined, 4\\ndata representation, 211-250 (see also feature\\nextraction/feature engineering; text data)\\nautomatic feature selection, 236-241\\nbinning and, 220-224\\ncategorical features, 212-220\\neffect on model performance, 211\\ninteger features, 218\\nmodel complexity vs. dataset size, 29\\noverview of, 250\\ntable analogy, 4\\nin training vs. test sets, 217\\nunderstanding your data, 4\\nunivariate nonlinear transformations,\\n232-236\\ndata transformations, 134\\n(see also preprocessing)\\ndata-driven research, 1\\nDBSCAN\\nevaluating and comparing, 191-207\\nparameters, 189\\nprinciple of, 187\\nreturned cluster assignments, 190\\nstrengths and weaknesses, 187\\ndecision boundaries, 37, 56\\ndecision function, 120\\ndecision trees\\nanalyzing, 76\\nbuilding, 71\\ncontrolling complexity of, 74\\ndata representation and, 220-224\\nfeature importance in, 77\\nif/else structure of, 70\\nparameters, 82\\nvs. random forests, 83\\nstrengths and weaknesses, 83\\ndecision_function, 286\\ndeep learning (see neural networks)\\ndendrograms, 184\\ndense regions, 187\\ndimensionality reduction, 141, 156\\ndiscrete features, 211\\ndiscretization, 220-224\\ndistributed computing, 362\\ndocument clustering, 347\\ndocuments, defined, 325\\ndual_coef_ attribute, 98\\nE\\neigenfaces, 147\\nembarrassingly parallel, 274\\nencoding, 328\\nensembles\\ndefined, 83\\ngradient boosted regression trees, 88-92\\nrandom forests, 83-88\\nEnthought Canopy, 6\\nestimators, 21, 360\\nestimator_ attribute of RFECV, 85\\nevaluation metrics and scoring\\nfor binary classification, 276-296\\nfor multiclass classification, 296-299\\nmetric selection, 275\\nmodel selection and, 300\\nregression metrics, 299\\ntesting production systems, 359\\nexp function, 232\\nexpert knowledge, 242-250\\nF\\nf(x)=y formula, 18\\nfacial recognition, 147, 157\\nfactor analysis (FA), 163\\nfalse positive rate (FPR), 292\\nfalse positive/false negative errors, 277\\nfeature extraction/feature engineering, 211-250\\n(see also data representation; text data)\\naugmenting data with, 211\\nautomatic feature selection, 236-241\\ncategorical features, 212-220\\ncontinuous vs. discrete features, 211\\ndefined, 4, 34, 211\\ninteraction features, 224-232\\nwith non-negative matrix factorization, 156\\noverview of, 250\\npolynomial features, 224-232\\nwith principal component analysis, 147\\nunivariate nonlinear transformations,\\n232-236\\nusing expert knowledge, 242-250\\nfeature importance, 77\\nfeatures, defined, 4\\nfeature_names attribute, 33\\nfeed-forward neural networks, 104\\nfit method, 21, 68, 119, 135\\nfit_transform method, 138\\nfloating-point numbers, 26\\nIndex | 369'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 383, 'page_label': '370'}, page_content='folds, 252\\nforge dataset, 30\\nframeworks, 362\\nfree string data, 324\\nfreeform text data, 325\\nG\\ngamma parameter, 100\\nGaussian kernels of SVC, 97, 100\\nGaussianNB, 68\\ngeneralization\\nbuilding models for, 26\\ndefined, 17\\nexamples of, 27\\nget_dummies function, 218\\nget_support method of feature selection, 237\\ngradient boosted regression trees\\nfor feature selection, 220-224\\nlearning_rate parameter, 89\\nparameters, 91\\nvs. random forests, 88\\nstrengths and weaknesses, 91\\ntraining set accuracy, 90\\ngraphviz module, 76\\ngrid search\\naccessing pipeline attributes, 315\\nalternate strategies for, 272\\navoiding overfitting, 261\\nmodel selection with, 319\\nnested cross-validation, 272\\nparallelizing with cross-validation, 274\\npipeline preprocessing, 317\\nsearching non-grid spaces, 271\\nsimple example of, 261\\ntuning parameters with, 260\\nusing pipelines in, 309-311\\nwith cross-validation, 263-275\\nGridSearchCV\\nbest_estimator_ attribute, 267\\nbest_params_ attribute, 266\\nbest_score_ attribute, 266\\nH\\nhandcoded rules, disadvantages of, 1\\nheat maps, 146\\nhidden layers, 106\\nhidden units, 105\\nhierarchical clustering, 184\\nhigh recall, 293\\nhigh-dimensional datasets, 32\\nhistograms, 144\\nhit rate, 283\\nhold-out sets, 17\\nhuman involvement/oversight, 358\\nI\\nimbalanced datasets, 277\\nindependent component analysis (ICA), 163\\ninference, 363\\ninformation leakage, 310\\ninformation retrieval (IR), 325\\ninteger features, 218\\n\"intelligent\" applications, 1\\ninteractions, 34, 224-232\\nintercept_ attribute, 47\\niris classification application\\ndata inspection, 19\\ndataset for, 14\\ngoals for, 13\\nk-nearest neighbors, 20\\nmaking predictions, 22\\nmodel evaluation, 22\\nmulticlass problem, 26\\noverview of, 23\\ntraining and testing data, 17\\niterative feature selection, 240\\nJ\\nJupyter Notebook, 7\\nK\\nk-fold cross-validation, 252\\nk-means clustering\\napplying with scikit-learn, 170\\nvs. classification, 171\\ncluster centers, 169\\ncomplex datasets, 179\\nevaluating and comparing, 191\\nexample of, 168\\nfailures of, 173\\nstrengths and weaknesses, 181\\nvector quantization with, 176\\nk-nearest neighbors (k-NN)\\nanalyzing KNeighborsClassifier, 37\\nanalyzing KNeighborsRegressor, 43\\nbuilding, 20\\nclassification, 35-37\\n370 | Index'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 384, 'page_label': '371'}, page_content='vs. linear models, 46\\nparameters, 44\\npredictions with, 35\\nregression, 40\\nstrengths and weaknesses, 44\\nKaggle, 365\\nkernelized support vector machines (SVMs)\\nkernel trick, 97\\nlinear models and nonlinear features, 92\\nvs. linear support vector machines, 92\\nmathematics of, 92\\nparameters, 104\\npredictions with, 98\\npreprocessing data for, 102\\nstrengths and weaknesses, 104\\ntuning SVM parameters, 99\\nunderstanding, 98\\nknn object, 21\\nL\\nL1 regularization, 53\\nL2 regularization, 49, 60, 67\\nLasso model, 53\\nLatent Dirichlet Allocation (LDA), 348-355\\nleafs, 71\\nleakage, 310\\nlearn from the past approach, 243\\nlearning_rate parameter, 89\\nleave-one-out cross-validation, 257\\nlemmatization, 344-347\\nlinear functions, 56\\nlinear models\\nclassification, 56\\ndata representation and, 220-224\\nvs. k-nearest neighbors, 46\\nLasso, 53\\nlinear SVMs, 56\\nlogistic regression, 56\\nmulticlass classification, 63\\nordinary least squares, 47\\nparameters, 67\\npredictions with, 45\\nregression, 45\\nridge regression, 49\\nstrengths and weaknesses, 67\\nlinear regression, 47, 224-232\\nlinear support vector machines (SVMs), 56\\nlinkage arrays, 185\\nlive testing, 359\\nlog function, 232\\nloss functions, 56\\nlow-dimensional datasets, 32\\nM\\nmachine learning\\nalgorithm chains and pipelines, 305-321\\napplications for, 1-5\\napproach to problem solving, 357-366\\nbenefits of Python for, 5\\nbuilding your own systems, vii\\ndata representation, 211-250\\nexamples of, 1, 13-23\\nmathematics of, vii\\nmodel evaluation and improvement,\\n251-303\\npreprocessing and scaling, 132-140\\nprerequisites to learning, vii\\nresources, ix, 361-366\\nscikit-learn and, 5-13\\nsupervised learning, 25-129\\nunderstanding your data, 4\\nunsupervised learning, 131-209\\nworking with text data, 323-356\\nmake_pipeline function\\naccessing step attributes, 314\\ndisplaying steps attribute, 314\\ngrid-searched pipelines and, 315\\nsyntax for, 313\\nmanifold learning algorithms\\napplications for, 164\\nexample of, 164\\nresults of, 168\\nvisualizations with, 163\\nmathematical functions for feature transforma‐\\ntions, 232\\nmatplotlib, 9\\nmax_features parameter, 84\\nmeta-estimators for trees and forests, 266\\nmethod chaining, 68\\nmetrics (see evaluation metrics and scoring)\\nmglearn, 11\\nmllib, 362\\nmodel-based feature selection, 238\\nmodels (see also algorithms)\\ncalibrated, 288\\ncapable of generalization, 26\\ncoefficients with text data, 338-347\\ncomplexity vs. dataset size, 29\\nIndex | 371'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 385, 'page_label': '372'}, page_content='cross-validation of, 252-260\\neffect of data representation choices on, 211\\nevaluation and improvement, 251-252\\nevaluation metrics and scoring, 275-302\\niris classification application, 13-23\\noverfitting vs. underfitting, 28\\npipeline preprocessing and, 317\\nselecting, 300\\nselecting with grid search, 319\\ntheory behind, 361\\ntuning parameters with grid search, 260-275\\nmovie reviews, 325\\nmulticlass classification\\nvs. binary classification, 25\\nevaluation metrics and scoring for, 296-299\\nlinear models for, 63\\nuncertainty estimates, 124\\nmultilayer perceptrons (MLPs), 104\\nMultinomialNB, 68\\nN\\nn-grams, 339\\nnaive Bayes classifiers\\nkinds in scikit-learn, 68\\nparameters, 70\\nstrengths and weaknesses, 70\\nnatural language processing (NLP), 325, 355\\nnegative class, 26\\nnested cross-validation, 272\\nNetflix prize challenge, 363\\nneural networks (deep learning)\\naccuracy of, 114\\nestimating complexity in, 118\\npredictions with, 104\\nrandomization in, 113\\nrecent breakthroughs in, 364\\nstrengths and weaknesses, 117\\ntuning, 108\\nnon-negative matrix factorization (NMF)\\napplications for, 156\\napplying to face images, 157\\napplying to synthetic data, 156\\nnormalization, 344\\nnormalized mutual information (NMI), 191\\nNumPy (Numeric Python) library, 7\\nO\\noffline evaluation, 359\\none-hot-encoding, 213-217\\none-out-of-N encoding, 213-217\\none-vs.-rest approach, 63\\nonline resources, ix\\nonline testing, 359\\nOpenML platform, 365\\noperating points, 289\\nordinary least squares (OLS), 47\\nout-of-core learning, 364\\noutlier detection, 197\\noverfitting, 28, 261\\nP\\npair plots, 19\\npandas\\nbenefits of, 10\\nchecking string-encoded data, 214\\ncolumn indexing in, 216\\nconverting data to one-hot-encoding, 214\\nget_dummies function, 218\\nparallelization over a cluster, 364\\npermissions, x\\npipelines (see algorithm chains and pipelines)\\npolynomial features, 224-232\\npolynomial kernels, 97\\npolynomial regression, 228\\npositive class, 26\\nPOSIX time, 244\\npre- and post-pruning, 74\\nprecision, 282, 358\\nprecision-recall curves, 289-292\\npredict for the future approach, 243\\npredict method, 22, 37, 68, 267\\npredict_proba function, 122, 286\\npreprocessing, 132-140\\ndata transformation application, 134\\neffect on supervised learning, 138\\nkinds of, 133\\nparameter selection with, 306\\npipelines and, 317\\npurpose of, 132\\nscaling training and test data, 136\\nprincipal component analysis (PCA)\\ndrawbacks of, 146\\nexample of, 140\\nfeature extraction with, 147\\nunsupervised nature of, 145\\nvisualizations with, 142\\nwhitening option, 150\\nprobabilistic modeling, 363\\n372 | Index'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 386, 'page_label': '373'}, page_content='probabilistic programming, 363\\nproblem solving\\nbuilding your own estimators, 360\\nbusiness metrics and, 358\\ninitial approach to, 357\\nresources, 361-366\\nsimple vs. complicated cases, 358\\nsteps of, 358\\ntesting your system, 359\\ntool choice, 359\\nproduction systems\\ntesting, 359\\ntool choice, 359\\npruning for decision trees, 74\\npseudorandom number generators, 18\\npure leafs, 73\\nPyMC language, 364\\nPython\\nbenefits of, 5\\nprepackaged distributions, 6\\nPython 2 vs. Python 3, 12\\nPython(x,y), 6\\nstatsmodel package, 362\\nR\\nR language, 362\\nradial basis function (RBF) kernel, 97\\nrandom forests\\nanalyzing, 85\\nbuilding, 84\\ndata representation and, 220-224\\nvs. decision trees, 83\\nvs. gradient boosted regression trees, 88\\nparameters, 88\\npredictions with, 84\\nrandomization in, 83\\nstrengths and weaknesses, 87\\nrandom_state parameter, 18\\nranking, 363\\nreal numbers, 26\\nrecall, 282\\nreceiver operating characteristics (ROC)\\ncurves, 292-296\\nrecommender systems, 363\\nrectified linear unit (relu), 106\\nrectifying nonlinearity, 106\\nrecurrent neural networks (RNNs), 356\\nrecursive feature elimination (RFE), 240\\nregression\\nf_regression, 236, 310\\nLinearRegression, 47-56, 81, 247\\nregression problems\\nBoston Housing dataset, 34\\nvs. classification problems, 26\\nevaluation metrics and scoring, 299\\nexamples of, 26\\ngoals for, 26\\nk-nearest neighbors, 40\\nLasso, 53\\nlinear models, 45\\nridge regression, 49\\nwave dataset illustration, 31\\nregularization\\nL1 regularization, 53\\nL2 regularization, 49, 60\\nrescaling\\nexample of, 132-140\\nkernel SVMs, 102\\nresources, ix\\nridge regression, 49\\nrobustness-based clustering, 194\\nroots, 72\\nS\\nSafari Books Online, x\\nsamples, defined, 4\\nscaling, 132-140\\ndata transformation application, 134\\neffect on supervised learning, 138\\ninto larger datasets, 364\\nkinds of, 133\\npurpose of, 132\\ntraining and test data, 136\\nscatter plots, 19\\nscikit-learn\\nalternate frameworks, 362\\nbenefits of, 5\\nBunch objects, 33\\ncancer dataset, 32\\ncore code for, 24\\ndata and labels in, 18\\ndocumentation, 6\\nfeature_names attribute, 33\\nfit method, 21, 68, 119, 135\\nfit_transform method, 138\\ninstalling, 6\\nknn object, 21\\nlibraries and tools, 7-11\\nIndex | 373'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 387, 'page_label': '374'}, page_content='predict method, 22, 37, 68\\nPython 2 vs. Python 3, 12\\nrandom_state parameter, 18\\nscaling mechanisms in, 139\\nscore method, 23, 37, 43\\ntransform method, 135\\nuser guide, 6\\nversions used, 12\\nscikit-learn classes and functions\\naccuracy_score, 193\\nadjusted_rand_score, 191\\nAgglomerativeClustering, 182, 191, 203-207\\naverage_precision_score, 292\\nBaseEstimator, 360\\nclassification_report, 284-288, 298\\nconfusion_matrix, 279-299\\nCountVectorizer, 329-355\\ncross_val_score, 253, 256, 300, 307, 360\\nDBSCAN, 187-190\\nDecisionTreeClassifier, 75, 278\\nDecisionTreeRegressor, 75, 80\\nDummyClassifier, 278\\nElasticNet class, 55\\nENGLISH_STOP_WORDS, 334\\nEstimator, 21\\nexport_graphviz, 76\\nf1_score, 284, 291\\nfetch_lfw_people, 147\\nf_regression, 236, 310\\nGradientBoostingClassifier, 88-91, 119, 124\\nGridSearchCV, 263-275, 300-301, 305-309,\\n315-320, 360\\nGroupKFold, 259\\nKFold, 256, 260\\nKMeans, 174-181\\nKNeighborsClassifier, 21-24, 37-43\\nKNeighborsRegressor, 42-47\\nLasso, 53-55\\nLatentDirichletAllocation, 348\\nLeaveOneOut, 257\\nLinearRegression, 47-56, 81, 247\\nLinearSVC, 56-59, 65, 67, 68\\nload_boston, 34, 230, 317\\nload_breast_cancer, 32, 38, 59, 75, 134, 144,\\n236, 305\\nload_digits, 164, 278\\nload_files, 326\\nload_iris, 14, 124, 253\\nLogisticRegression, 56-62, 67, 209, 253, 279,\\n315, 332-347\\nmake_blobs, 92, 119, 136, 173-183, 188, 286\\nmake_circles, 119\\nmake_moons, 85, 108, 175, 190-195\\nmake_pipeline, 313-319\\nMinMaxScaler, 102, 133, 135-139, 190, 230,\\n308, 309, 319\\nMLPClassifier, 107-119\\nNMF, 140, 159-163, 179-182, 348\\nNormalizer, 134\\nOneHotEncoder, 218, 247\\nParameterGrid, 274\\nPCA, 140-166, 179, 195-206, 313-314, 348\\nPipeline, 305-319, 320\\nPolynomialFeatures, 227-230, 248, 317\\nprecision_recall_curve, 289-292\\nRandomForestClassifier, 84-86, 238, 290,\\n319\\nRandomForestRegressor, 84, 231, 240\\nRFE, 240-241\\nRidge, 49, 67, 112, 231, 234, 310, 317-319\\nRobustScaler, 133\\nroc_auc_score, 294-301\\nroc_curve, 293-296\\nSCORERS, 301\\nSelectFromModel, 238\\nSelectPercentile, 236, 310\\nShuffleSplit, 258, 258\\nsilhouette_score, 193\\nStandardScaler, 114, 133, 138, 144, 150,\\n190-195, 314-320\\nStratifiedKFold, 260, 274\\nStratifiedShuffleSplit, 258, 347\\nSVC, 56, 100, 134, 139, 260-267, 269-272,\\n305-309, 313-320\\nSVR, 92, 229\\nTfidfVectorizer, 336-356\\ntrain_test_split, 17-19, 251, 286, 289\\nTransformerMixin, 360\\nTSNE, 166\\nSciPy, 8\\nscore method, 23, 37, 43, 267, 308\\nsensitivity, 283\\nsentiment analysis example, 325\\nshapes, defined, 16\\nshuffle-split cross-validation, 258\\nsin function, 232\\nsoft voting strategy, 84\\n374 | Index'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 388, 'page_label': '375'}, page_content='spark computing environment, 362\\nsparse coding (dictionary learning), 163\\nsparse datasets, 44\\nsplits, 252\\nStan language, 364\\nstatsmodel package, 362\\nstemming, 344-347\\nstopwords, 334\\nstratified k-fold cross-validation, 254-256\\nstring-encoded categorical data, 214\\nsupervised learning, 25-129 (see also classifica‐\\ntion problems; regression problems)\\nalgorithms for\\ndecision trees, 70-83\\nensembles of decision trees, 83-92\\nk-nearest neighbors, 35-44\\nkernelized support vector machines,\\n92-104\\nlinear models, 45-68\\nnaive Bayes classifiers, 68\\nneural networks (deep learning),\\n104-119\\noverview of, 2\\ndata representation, 4\\nexamples of, 3\\ngeneralization, 26\\ngoals for, 25\\nmodel complexity vs. dataset size, 29\\noverfitting vs. underfitting, 28\\noverview of, 127\\nsample datasets, 30-34\\nuncertainty estimates, 119-127\\nsupport vectors, 98\\nsynthetic datasets, 30\\nT\\nt-SNE algorithm (see manifold learning algo‐\\nrithms)\\ntangens hyperbolicus (tanh), 106\\nterm frequency–inverse document frequency\\n(tf–idf), 336-347\\nterminal nodes, 71\\ntest data/test sets\\nBoston Housing dataset, 34\\ndefined, 17\\nforge dataset, 30\\nwave dataset, 31\\nWisconsin Breast Cancer dataset, 32\\ntext data, 323-356\\nbag-of-words representation, 327-334\\nexamples of, 323\\nmodel coefficients, 338\\noverview of, 355\\nrescaling data with tf-idf, 336-338\\nsentiment analysis example, 325\\nstopwords, 334\\ntopic modeling and document clustering,\\n347-355\\ntypes of, 323-325\\ntime series predictions, 363\\ntokenization, 328, 344-347\\ntop nodes, 72\\ntopic modeling, with LDA, 347-355\\ntraining data, 17\\ntrain_test_split function, 254\\ntransform method, 135, 312, 334\\ntransformations\\nselecting, 235\\nunivariate nonlinear, 232-236\\nunsupervised, 131\\ntree module, 76\\ntrigrams, 339\\ntrue positive rate (TPR), 283, 292\\ntrue positives/true negatives, 281\\ntypographical conventions, ix\\nU\\nuncertainty estimates\\napplications for, 119\\ndecision function, 120\\nin binary classification evaluation, 286-288\\nmulticlass classification, 124\\npredicting probabilities, 122\\nunderfitting, 28\\nunigrams, 340\\nunivariate nonlinear transformations, 232-236\\nunivariate statistics, 236\\nunsupervised learning, 131-209\\nalgorithms for\\nagglomerative clustering, 182-187\\nclustering, 168-207\\nDBSCAN, 187-190\\nk-means clustering, 168-181\\nmanifold learning with t-SNE, 163-168\\nnon-negative matrix factorization,\\n156-163\\noverview of, 3\\nprincipal component analysis, 140-155\\nIndex | 375'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 389, 'page_label': '376'}, page_content='challenges of, 132\\ndata representation, 4\\nexamples of, 3\\noverview of, 208\\nscaling and preprocessing for, 132-140\\ntypes of, 131\\nunsupervised transformations, 131\\nV\\nvalue_counts function, 214\\nvector quantization, 176\\nvocabulary building, 328\\nvoting, 36\\nvowpal wabbit, 362\\nW\\nwave dataset, 31\\nweak learners, 88\\nweights, 47, 106\\nwhitening option, 150\\nWisconsin Breast Cancer dataset, 32\\nword stems, 344\\nX\\nxgboost package, 91\\nxkcd Color Survey, 324\\n376 | Index'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 390, 'page_label': '377'}, page_content='About the Authors\\nAndreas Müller  received his PhD in machine learning from the University of Bonn.\\nAfter working as a machine learning researcher on computer vision applications at\\nAmazon for a year, he joined the Center for Data Science at New Y ork University. For\\nthe last four years, he has been a maintainer of and one of the core contributors to\\nscikit-learn, a machine learning toolkit widely used in industry and academia, and\\nhas authored and contributed to several other widely used machine learning pack‐\\nages. His mission is to create open tools to lower the barrier of entry for machine\\nlearning applications, promote reproducible science, and democratize the access to\\nhigh-quality machine learning algorithms.\\nSarah Guido is a data scientist who has spent a lot of time working in start-ups. She\\nloves Python, machine learning, large quantities of data, and the tech world. An\\naccomplished conference speaker, Sarah attended the University of Michigan for grad\\nschool and currently resides in New Y ork City.\\nColophon\\nThe animal on the cover of Introduction to Machine Learning with Python  is a hell‐\\nbender salamander (Cryptobranchus alleganiensis), an amphibian native to the eastern\\nUnited States (ranging from New Y ork to Georgia). It has many colorful nicknames,\\nincluding “ Allegheny alligator, ” “snot otter, ” and “mud-devil. ” The origin of the name\\n“hellbender” is unclear: one theory is that early settlers found the salamander’s\\nappearance unsettling and supposed it to be a demonic creature trying to return to\\nhell.\\nThe hellbender salamander is a member of the giant salamander family, and can grow\\nas large as 29 inches long. This is the third-largest aquatic salamander species in the\\nworld. Their bodies are rather flat, with thick folds of skin along their sides. While\\nthey do have a single gill on each side of the neck, hellbenders largely rely on their\\nskin folds to breathe: gas flows in and out through capillaries near the surface of the\\nskin.\\nBecause of this, their ideal habitat is in clear, fast-moving, shallow streams, which\\nprovide plenty of oxygen. The hellbender shelters under rocks and hunts primarily by\\nsense of smell, though it is also able to detect vibrations in the water. Its diet is made\\nup of crayfish, small fish, and occasionally the eggs of its own species. The hellbender\\nis also a key member of its ecosystem as prey: predators include various fish, snakes,\\nand turtles.\\nHellbender salamander populations have decreased significantly in the last few deca‐\\ndes. Water quality is the largest issue, as their respiratory system makes them very\\nsensitive to polluted or murky water. An increase in agriculture and other human'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\Data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 391, 'page_label': '378'}, page_content='activity near their habitat means greater amounts of sediment and chemicals in the\\nwater. In an effort to save this endangered species, biologists have begun to raise the\\namphibians in captivity and release them when they reach a less vulnerable age.\\nMany of the animals on O’Reilly covers are endangered; all of them are important to\\nthe world. To learn more about how you can help, go to animals.oreilly.com.\\nThe cover image is from Wood’s Animate Creation. The cover fonts are URW Type‐\\nwriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font is\\nAdobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 0, 'page_label': '1'}, page_content=''),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 1, 'page_label': '2'}, page_content='About Pearson\\nPearson is the world’s learning company, with presence across\\n70 countries worldwide. Our unique insights and world-class\\nexpertise comes from a long history of working closely with\\nrenowned teachers, authors and thought leaders, as a result of\\nwhich, we have emerged as the preferred choice for millions of\\nteachers and learners across the world.\\nWe believe learning opens up opportunities, creates fulfilling\\ncareers and hence better lives. We hence collaborate with the\\nbest of minds to deliver you class-leading products, spread\\nacross the Higher Education and K12 spectrum.\\nSuperior learning experience and improved outcomes are at\\nthe heart of everything we do. This product is the result of one\\nsuch effort.\\nYour feedback plays a critical role in the evolution of our\\nproducts and you can contact us at - reachus@pearson.com.\\nWe look forward to it.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 2, 'page_label': '3'}, page_content='Machine Learning'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 3, 'page_label': '4'}, page_content='Machine Learning\\n \\n \\nSaikat Dutt \\nDirector \\nCognizant Technology Solutions\\nSubramanian Chandramouli \\nAssociate Director \\nCognizant Technology Solutions\\nAmit Kumar Das \\nAssistant Professor \\nInstitute of Engineering & Management'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 4, 'page_label': '5'}, page_content='This book is dedicated to the people without whom the dream\\ncould not have come true – my parents, Tarun Kumar Dutt and\\nSrilekha Dutt; constant inspiration and support from my wife,\\nAdity and unconditional love of my sons Deepro and Devarko.\\nSaikat Dutt\\nMy sincerest thanks and appreciation go to several people…\\nMy parents Subramanian and Lalitha\\nMy wife Ramya\\nMy son Shri Krishna\\nMy daughter Shri Siva Ranjani\\nAnd my colleagues and friends\\nS. Chandramouli\\nMy humble gratitude goes to -'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 5, 'page_label': '6'}, page_content='My parents Mrs Juthika Das and Dr Ranajit Kumar Das\\nMy wife Arpana and two lovely daughters Ashmita and Ankita\\nMy academic collaborator, mentor and brother — Dr Saptarsi\\nGoswami and Mr  Mrityunjoy Panday\\nAmit Kumar Das'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 6, 'page_label': '7'}, page_content='Contents\\nPreface\\nAcknowledgements\\nAbout the Authors\\nModel Syllabus for Machine Learning\\nLesson plan\\n1 Introduction to Machine Learning\\n1.1 Introduction\\n1.2 What is Human Learning?\\n1.3 Types of Human Learning\\n1.3.1 Learning under expert guidance\\n1.3.2 Learning guided by knowledge gained from experts\\n1.3.3 Learning by self\\n1.4 What is Machine Learning?\\n1.4.1 How do machines learn?\\n1.4.2 Well-posed learning problem\\n1.5 Types of Machine Learning\\n1.5.1 Supervised learning\\n1.5.2 Unsupervised learning\\n1.5.3 Reinforcement learning\\n1.5.4 Comparison – supervised, unsupervised, and reinforcement learning'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 7, 'page_label': '8'}, page_content='1.6 Problems Not To Be Solved Using Machine Learning\\n1.7 Applications of Machine Learning\\n1.7.1 Banking and finance\\n1.7.2 Insurance\\n1.7.3 Healthcare\\n1.8 State-of-The-Art Languages/Tools In Machine Learning\\n1.8.1 Python\\n1.8.2 R\\n1.8.3 Matlab\\n1.8.4 SAS\\n1.8.5 Other languages/tools\\n1.9 Issues in Machine Learning\\n1.10 Summary\\n2 Preparing to Model\\n2.1 Introduction\\n2.2 Machine Learning Activities\\n2.3 Basic Types of Data in Machine Learning\\n2.4 Exploring Structure of Data\\n2.4.1 Exploring numerical data\\n2.4.2 Plotting and exploring numerical data\\n2.4.3 Exploring categorical data\\n2.4.4 Exploring relationship between variables\\n2.5 Data Quality and Remediation\\n2.5.1 Data quality\\n2.5.2 Data remediation'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 8, 'page_label': '9'}, page_content='2.6 Data Pre-Processing\\n2.6.1 Dimensionality reduction\\n2.6.2 Feature subset selection\\n2.7 Summary\\n3 Modelling and Evaluation\\n3.1 Introduction\\n3.2 Selecting a Model\\n3.2.1 Predictive models\\n3.2.2 Descriptive models\\n3.3 Training a Model (for Supervised Learning)\\n3.3.1 Holdout method\\n3.3.2 K-fold Cross-validation method\\n3.3.3 Bootstrap sampling\\n3.3.4 Lazy vs. Eager learner\\n3.4 Model Representation and Interpretability\\n3.4.1 Underfitting\\n3.4.2 Overfitting\\n3.4.3 Bias – variance trade-off\\n3.5 Evaluating Performance of a Model\\n3.5.1 Supervised learning – classification\\n3.5.2 Supervised learning – regression\\n3.5.3 Unsupervised learning – clustering\\n3.6 Improving Performance of a Model\\n3.7 Summary\\n4 Basics of Feature Engineering'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 9, 'page_label': '10'}, page_content='4.1 Introduction\\n4.1.1 What is a feature?\\n4.1.2 What is feature engineering?\\n4.2 Feature Transformation\\n4.2.1 Feature construction\\n4.2.2 Feature extraction\\n4.3 Feature Subset Selection\\n4.3.1 Issues in high-dimensional data\\n4.3.2 Key drivers of feature selection – feature relevance and redundancy\\n4.3.3 Measures of feature relevance and redundancy\\n4.3.4 Overall feature selection process\\n4.3.5 Feature selection approaches\\n4.4 Summary\\n5 Brief Overview of Probability\\n5.1 Introduction\\n5.2 Importance of Statistical Tools in Machine Learning\\n5.3 Concept of Probability – Frequentist and Bayesian Interpretation\\n5.3.1 A brief review of probability theory\\n5.4 Random Variables\\n5.4.1 Discrete random variables\\n5.4.2 Continuous random variables\\n5.5 Some Common Discrete Distributions\\n5.5.1 Bernoulli distributions\\n5.5.2 Binomial distribution\\n5.5.3 The multinomial and multinoulli distributions'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 10, 'page_label': '11'}, page_content='5.5.4 Poisson distribution\\n5.6 Some Common Continuous Distributions\\n5.6.1 Uniform distribution\\n5.6.2 Gaussian (normal) distribution\\n5.6.3 The laplace distribution\\n5.7 Multiple Random Variables\\n5.7.1 Bivariate random variables\\n5.7.2 Joint distribution functions\\n5.7.3 Joint probability mass functions\\n5.7.4 Joint probability density functions\\n5.7.5 Conditional distributions\\n5.7.6 Covariance and correlation\\n5.8 Central Limit Theorem\\n5.9 Sampling Distributions\\n5.9.1 Sampling with replacement\\n5.9.2 Sampling without replacement\\n5.9.3 Mean and variance of sample\\n5.10 Hypothesis Testing\\n5.11 Monte Carlo Approximation\\n5.12 Summary\\n6 Bayesian Concept Learning\\n6.1 Introduction\\n6.2 Why Bayesian Methods are Important?\\n6.3 Bayes’ Theorem\\n6.3.1 Prior'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 11, 'page_label': '12'}, page_content='6.3.2 Posterior\\n6.3.3 Likelihood\\n6.4 Bayes’ Theorem and Concept Learning\\n6.4.1 Brute-force Bayesian algorithm\\n6.4.2 Concept of consistent learners\\n6.4.3 Bayes optimal classifier\\n6.4.4 Naïve Bayes classifier\\n6.4.5 Applications of Naïve Bayes classifier\\n6.4.6 Handling Continuous Numeric Features in Naïve Bayes Classifier\\n6.5 Bayesian Belief Network\\n6.5.1 Independence and conditional independence\\n6.5.2 Use of the Bayesian Belief network in machine learning\\n6.6 Summary\\n7 Supervised Learning : Classification\\n7.1 Introduction\\n7.2 Example of Supervised Learning\\n7.3 Classification Model\\n7.4 Classification Learning Steps\\n7.5 Common Classification Algorithms\\n7.5.1 k-Nearest Neighbour (kNN)\\n7.5.2 Decision tree\\n7.5.3 Random forest model\\n7.5.4 Support vector machines\\n7.6 Summary\\n8 Super vised Learning : Regression'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 12, 'page_label': '13'}, page_content='8.1 Introduction\\n8.2 Example of Regression\\n8.3 Common Regression Algorithms\\n8.3.1 Simple linear regression\\n8.3.2 Multiple linear regression\\n8.3.3 Assumptions in Regression Analysis\\n8.3.4 Main Problems in Regression Analysis\\n8.3.5 Improving Accuracy of the Linear Regression Model\\n8.3.6 Polynomial Regression Model\\n8.3.7 Logistic Regression\\n8.3.8 Maximum Likelihood Estimation\\n8.4 Summary\\n9 Unsupervised Learning\\n9.1 Introduction\\n9.2 Unsupervised vs Supervised Learning\\n9.3 Application of Unsupervised Learning\\n9.4 Clustering\\n9.4.1 Clustering as a machine learning task\\n9.4.2 Different types of clustering techniques\\n9.4.3 Partitioning methods\\n9.4.4 K-Medoids: a representative object-based technique\\n9.4.5 Hierarchical clustering\\n9.4.6 Density-based methods - DBSCAN\\n9.5 Finding Pattern using Association Rule\\n9.5.1 Definition of common terms'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 13, 'page_label': '14'}, page_content='9.5.2 Association rule\\n9.5.3 The apriori algorithm for association rule learning\\n9.5.4 Build the apriori principle rules\\n9.6 Summary\\n10 Basics of Neural Network\\n10.1 Introduction\\n10.2 Understanding the Biological Neuron\\n10.3 Exploring the Artificial Neuron\\n10.4 Types of Activation Functions\\n10.4.1 Identity function\\n10.4.2 Threshold/step function\\n10.4.3 ReLU (Rectified Linear Unit) function\\n10.4.4 Sigmoid function\\n10.4.5 Hyperbolic tangent function\\n10.5 Early Implementations of ANN\\n10.5.1 McCulloch–Pitts model of neuron\\n10.5.2 Rosenblatt’s perceptron\\n10.5.3 ADALINE network model\\n10.6 Architectures of Neural Network\\n10.6.1 Single-layer feed forward network\\n10.6.2 Multi-layer feed forward ANNs\\n10.6.3 Competitive network\\n10.6.4 Recurrent network\\n10.7 Learning Process in ANN\\n10.7.1 Number of layers'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 14, 'page_label': '15'}, page_content='10.7.2 Direction of signal flow\\n10.7.3 Number of nodes in layers\\n10.7.4 Weight of interconnection between neurons\\n10.8 Backpropagation\\n10.9 Deep Learning\\n10.10 Summary\\n11 Other Types of Learning\\n11.1 Introduction\\n11.2 Representation Learning\\n11.2.1 Supervised neural networks and multilayer perceptron\\n11.2.2 Independent component analysis (Unsupervised)\\n11.2.3 Autoencoders\\n11.2.4 Various forms of clustering\\n11.3 Active Learning\\n11.3.1 Heuristics for active learning\\n11.3.2 Active learning query strategies\\n11.4 Instance-Based Learning (Memory-based Learning)\\n11.4.1 Radial basis function\\n11.4.2 Pros and cons of instance-based learning method\\n11.5 Association Rule Learning Algorithm\\n11.5.1 Apriori algorithm\\n11.5.2 Eclat algorithm\\n11.6 Ensemble Learning Algorithm\\n11.6.1 Bootstrap aggregation (Bagging)\\n11.6.2 Boosting'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 15, 'page_label': '16'}, page_content='11.6.3 Gradient boosting machines (GBM)\\n11.7 Regularization Algorithm\\n11.8 Summary\\nAppendix A: Programming Machine Learning in R\\nAppendix B: Programming Machine Learning in Python\\nAppendix C: A Case Study on Machine Learning Application: Grouping\\nSimilar Service Requests and Classifying a New One\\nModel Question Paper-1\\nModel Question Paper-2\\nModel Question Paper-3\\nIndex'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 16, 'page_label': '17'}, page_content='Preface\\nRepeated requests from Computer Science and IT engineering\\nstudents who are the readers of our previous books encouraged\\nus to write this book on machine learning. The concept of\\nmachine learning and the huge potential of its application is\\nstill niche knowledge and not so well-spread among the\\nstudent community. So, we thought of writing this book\\nspecifically for techies, college students, and junior managers\\nso that they understood the machine learning concepts easily.\\nThey should not only use the machine learning software\\npackages, but understand the concepts behind those packages.\\nThe application of machine learning is getting boosted day by\\nday. From recommending products to the buyers, to predicting\\nthe future real estate market, to helping medical practitioners\\nin diagnosis, to optimizing energy consumption, thus helping\\nthe cause of Green Earth, machine learning is finding its utility\\nin every sphere of life.\\nDue care was taken to write this book in simple English and\\nto present the machine learning concepts in an easily\\nunderstandable way which can be used as a textbook for both\\ngraduate and advanced undergraduate classes in machine\\nlearning or as a reference text.\\nWhom Is This Book For?\\nReaders of this book will gain a thorough understanding of\\nmachine learning concepts. Not only students but also'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 17, 'page_label': '18'}, page_content='software professionals will find a variety of techniques with\\nsufficient discussions in this book that cater to the needs of the\\nprofessional environments. Technical managers will get an\\ninsight into weaving machine learning into the overall\\nsoftware engineering process. Students, developers, and\\ntechnical managers with a basic background in computer\\nscience will find the material in this book easily readable.\\nHow This Book Is Organized?\\nEach chapter starts with an introductory paragraph, which\\ngives overview of the chapter along with a chapter-coverage\\ntable listing out the topics covered in the chapter. Sample\\nquestions at the end of the chapter helps the students to\\nprepare for the examination. Discussion points and Points to\\nPonder given inside the chapters help to clarify and understand\\nthe chapters easily and also explore the thinking ability of the\\nstudents and professionals.\\nA recap summary is given at the end of each chapter for\\nquick review of the topics. Throughout this book, you will see\\nmany exercises and discussion questions. Please don’t skip\\nthese – they are important in order to understand the machine\\nlearning concepts fully.\\nThis book starts with an introduction to Machine Learning\\nwhich lays the theoretical foundation for the remaining\\nchapters. Modelling, Feature engineering, and basic\\nprobability are discussed as chapters before entering into the\\nworld of machine learning which helps to grip the machine\\nlearning concepts easily at a later point of time.\\nBonus topics of machine learning exercise with multiple\\nexamples are discussed in Machine learning language of R &'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 18, 'page_label': '19'}, page_content='Python. Appendix A discusses Programming Machine\\nLearning in R and Appendix B discusses Programming\\nMachine Learning in Python.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 19, 'page_label': '20'}, page_content='Acknowledgements\\nWe are grateful to Pearson Education, who came forward to\\npublish this book. Ms. Neha Goomer and Mr. Purushothaman\\nChandrasekaran of Pearson Education were always kind and\\nunderstanding. Mr. Purushothaman reviewed this book with\\nabundant patience and helped us say what we had wanted to,\\nimprovising each and every page of this book with care. Their\\npatience and guidance were invaluable. Thank you very much\\nNeha and Puru.\\n–All authors\\nThe journey through traditional Project Management, Agile\\nProject Management, Program and Portfolio Management\\nalong with use of artificial intelligence and machine learning\\nin the field, has been very rewarding, as it has given us the\\nopportunity to work for some of the best organizations in the\\nworld and learn from some of the best minds. Along the way,\\nseveral individuals have been instrumental in providing us\\nwith the guidance, opportunities and insights we needed to\\nexcel in this field. We wish to personally thank Mr. Rajesh\\nBalaji Ramachandran, Senior Vice-president, Cognizant\\nTechnology Solutions and Mr. Pradeep Shilige, Executive Vice\\nPresident, Cognizant Technology Solutions; Mr. Alexis\\nSamuel, Senior Vice-president, Cognizant Technology\\nSolutions and Mr.Hariharan Mathrubutham,Vice-president,\\nCognizant Technology Solutions and Mr. Krishna Prasad\\nYerramilli, Assistant Vice-president, Cognizant Technology'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 20, 'page_label': '21'}, page_content='Solutions for their inspiration and help in creating this book.\\nThey have immensely contributed to improve our skills.\\n–Saikat and Chandramouli\\nThis book wouldn’t have been possible without the constant\\ninspiration and support of my lovely wife Adity. My parents\\nhave always been enthusiastic about this project and provided\\nme continuous guidance at every necessary step. The\\nunconditional love and affection my sons – Deepro and\\nDevarko ushered on me constantly provided me the motivation\\nto work hard on this crucial project.\\nThis book is the culmination of all the learning that I gained\\nfrom many highly reputed professionals in the industry. I was\\nfortunate to work with them and gained knowledge from them\\nwhich helped me molding my professional career. Prof.\\nIndranil Bose and Prof. Bodhibrata Nag from IIM Calcutta\\nguided me enormously in different aspects of life and career.\\nMy heartfelt thanks go to all the wonderful people who\\ncontributed in many ways in conceptualizing this book and\\nwished success of this project.\\n–Saikat Dutt\\nThis book is the result of all the learning I have gained from\\nmany highly reputed professionals in the industry. I was\\nfortunate to work with them and in the process, acquire\\nknowledge that helped me in molding my professional career. I\\nthank Mr. Chandra Sekaran, Group Chief Executive,Tech and\\nOps, Cognizant, for his continuous encouragement and\\nunabated passion for strategic value creation, whose advice\\nwas invaluable for working on this book. I am obliged to Mr.\\nChandrasekar, Ex CIO, Standard Chartered Bank, for'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 21, 'page_label': '22'}, page_content='demonstrating how the lives, thoughts and feelings of others in\\nprofessional life are to be valued. He is a wonderful and\\ncheerful man who inspired me and gave me a lot of\\nencouragement when he launched my first book, Virtual\\nProject Management Office.\\nMs. Meena Karthikeyan, Vice-president, Cognizant\\nTechnology Solutions; Ms. Kalyani Sekhar, Assistant Vice-\\npresident, Cognizant Technology Solutions and Mr.\\nBalasubramanian Narayanan, Senior Director, Cognizant\\nTechnology Solutions guided me enormously in different\\naspects of professional life.\\nMy parents (Mr. Subramanian and Ms. Lalitha) have always\\nbeen enthusiastic about this project. Their unconditional love\\nand affection provided the much-needed moral support. My\\nson, Shri Krishna, and daughter, Shri Siva Ranjani, constantly\\nadded impetus to my motivation to work hard.\\nThis book would not have been possible without the\\nconstant inspiration and support of my wife, Ramya. She was\\nunfailingly supportive and encouraging during the long\\nmonths that I had spent glued to my laptop while writing this\\nbook. Last and not the least, I beg forgiveness of all those who\\nhave been with me over the course of the years and whose\\nnames I have failed to mention here.\\n–S. Chandramouli\\nFirst of all, I would like to thank the Almighty for everything.\\nIt is the constant support and encouragement from my family\\nwhich made it possible for me to put my heart and soul in my\\nfirst authoring venture. My parents have always been my role\\nmodel, my wife a source of constant strength and my'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 22, 'page_label': '23'}, page_content='daughters are my happiness. Without my family, I wouldn’t\\nhave got the luxury of time to spend on hours in writing the\\nbook. Any amount of thanks will fell short to express my\\ngratitude and pleasure of being part of the family.\\nI would also thank the duo who have been my academic\\ncollaborator, mentor and brother - Dr. Saptarsi Goswami and\\nMr. Mrityunjoy Panday - without them I would be so\\nincomplete. My deep respects for my research guides and\\nmentors Amlan sir and Basabi madam for the knowledge and\\nsupport that I’ve been privileged to receive from them.\\nMy sincere gratitude to my mentors in my past organization,\\nCognizant Technology Solutions, Mr. Rajarshi Chatterjee, Mr.\\nManoj Paul, Mr. Debapriya Dasgupta and Mr. Balaji\\nVenkatesan for the valuable learning that I received from them.\\nI would thank all my colleagues at Institute of Engineering &\\nManagement, who have given me relentless support and\\nencouragement.\\nLast, but not the least, my students who are always a source\\nof inspiration for me need special mention. I’m especially\\nindebted to Goutam Bose, Sayan Bachhar and Piyush Nandi\\nfor their extreme support in reviewing the chapters and\\nproviding invaluable feedback to make them more student-\\nfriendly. Also, thanks to Gulshan, Sagarika, Samyak, Sahil,\\nPriya, Nibhash, Attri, Arunima, Salman, Deepayan, Arnab,\\nSwapnil, Gitesh, Ranajit, Ankur, Sudipta and Debankan for\\ntheir great support.\\n–Amit Kumar Das'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 23, 'page_label': '24'}, page_content='About the Authors\\nSaikat Dutt, PMP, PMI-ACP, CSM is author of three books\\n‘PMI Agile Certified Practitioner-Excel with Ease’, ‘Software\\nEngineering’ and ‘Software Project Management’ published\\nby Pearson. Two of these books - ‘Software Project\\nManagement’ and ‘PMI Agile Certified Practitioner-Excel\\nwith Ease’ are text book in IIMC for PGDBA class.\\nSaikat is working on AI and Machine Learning projects\\nespecially focusing on application of those in managing\\nsoftware projects. He is a regular speaker on machine learning\\ntopics and involved in reviewing machine learning and AI\\nrelated papers. Saikat is also a ‘Project Management\\nProfessional (PMP)’ and ‘PMI Agile Certified Professional’\\ncertified by Project Management Institute (PMI) USA and a\\nCertified Scrum Master (CSM). He has more than Nineteen\\nyears of IT industry experience and has expertise in managing\\nlarge scale multi-location and mission critical projects. Saikat\\nholds a B.E. degree from Jadavpur University, Kolkata and at\\npresent he is working as Director in Cognizant Technology'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 24, 'page_label': '25'}, page_content='Solutions. He is a guest lecturer in IIMC since 2016 for\\nSoftware Project Management course in PGDBA class. Saikat\\nis also an active speaker on Agile, Project management,\\nMachine learning and other recent topics several forums. He is\\nactively working with IIMC to develop management case\\nstudies which are taught in global business schools.\\n \\nS. Chandramouli, PMP, PMI-ACP, is an alumnus of the\\nIndian Institute of Management Kozhikode (IIM-K). He is a\\nCertified Global Business Leader from Harvard Business\\nSchool. He is a prolific writer of business management articles\\ndealing with delivery management, competitiveness, IT,\\norganizational culture and leadership.\\nAuthor of books published by Pearson which has been\\nrecognized as a reference books in various universities, title\\nincludes ‘PMI Agile Certified Practitioner—Excel with Ease’,\\n‘Software Engineering’, ‘Software Project Management’. In\\naddition to this, he has edited two books of foreign authors to\\nsuit the needs of Indian universities: ‘Applying UML and\\npatterns’ by Craig Larman and ‘Design pattern by Gamma’\\n(Gang of four). He is certified ‘Green Belt’ in six sigma'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 25, 'page_label': '26'}, page_content='methodology. He is a certified master practitioner in Neuro\\nLinguistic Programming (NLP).\\nChandramouli has a good record of delivering large-scale,\\nmission-critical projects on time and within budget to the\\ncustomer’s satisfaction which includes AI and machine\\nlearning projects. He was an active member in PMI’s\\nOrganization Project Management Maturity Model (OPM3)\\nand Project Management Competency Development\\nFramework (PMCDF) assignments. He has been an invited\\nspeaker at various technology and management conferences\\nand has addressed more than 7,000 software professionals\\nworldwide on a variety of themes associated with AI, machine\\nlearning, delivery management, competitiveness and\\nleadership.\\n \\nAmit Kumar Das is a seasoned industry practitioner turned to\\na full-time academician. He is currently working as an\\nAssistant Professor at Institute of Engineering & Management\\n(IEM). He is also guest teacher in the Department of\\nRadiophysics and Electronics, University of Calcutta. Before\\njoining academics, he was a Director in the Analytics and\\nInformation Management practice in Cognizant Technology\\nSolutions. Amit has spent more than 18 years in the IT'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 26, 'page_label': '27'}, page_content='industry in diverse roles and working with stakeholders across\\nthe globe.\\nAmit has done his Bachelor in Engineering from Indian\\nInstitute of Engineering Science and Technology (IIEST),\\nShibpur and his Master in Technology from Birla Institute of\\nTechnology and Science (BITS), Pilani. Currently he is\\npursuing his research in the University of Calcutta. Amit’s area\\nof research includes machine learning and deep learning. He\\nhas many published research papers in the area of data\\nanalytics and machine learning published in referred\\ninternational journals and conferences. He has also been a\\nregular speaker in the area of software engineering, data\\nanalytics and machine learning.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 27, 'page_label': '28'}, page_content='Model Syllabus for Machine Learning\\nCredits: 5\\nContacts per week: 3 lectures + 1 tutorial\\nMODULE I\\nIntroduction to Machine Learning: Human learning and it’s\\ntypes; Machine learning and it’s types; well-posed learning\\nproblem; applications of machine learning; issues in machine\\nlearning\\nPreparing to model: Basic data types; exploring numerical\\ndata; exploring categorical data; exploring relationship\\nbetween variables; data issues and remediation; data pre-\\nprocessing\\nModelling and Evaluation: Selecting a model; training model\\n– holdout, k-fold cross-validation, bootstrap sampling; model\\nrepresentation and interpretability – under-fitting, over-fitting,\\nbias-variance tradeoff; model performance evaluation –\\nclassification, regression, clustering; performance\\nimprovement\\nFeature engineering: Feature construction; feature extraction;\\nfeature selection\\n[12 Lectures]'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 28, 'page_label': '29'}, page_content='MODULE II\\nBrief review of probability: Basic concept of probability,\\nrandom variables; discrete distributions – binomial, Poisson,\\nBernoulli, etc.; continuous distribution – uniform, normal,\\nLaplace; central theorem; Monte Carlo approximation\\nBayesian concept learning: Bayes theorem – prior and\\nposterior probability, likelihood; concept learning; Bayesian\\nBelief Network\\n[6 Lectures]\\nMODULE III\\nSupervised learning – Classification: Basics of supervised\\nlearning – classification; k-Nearestneighbour; decision tree;\\nrandom forest; support vector machine\\nSupervised learning – Regression: Simple linear regression;\\nother regression techniques\\nUnsupervised learning: Basics of unsupervised learning;\\nclustering techniques; association rules\\n[10 Lectures]\\nMODULE IV\\nBasics of Neural Network: Understanding biological neuron\\nand artificial neuron; types of activation functions; early\\nimplementations – McCulloch Pitt’s, Rosenblatt’s Perceptron,\\nADALINE; architectures of neural network; learning process\\nin ANN; backpropagation'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 29, 'page_label': '30'}, page_content='Other types of learning: Representation learning; active\\nlearning; instance-based learning; association rule learning;\\nensemble learning; regularization\\nMachine Learning Live Case Studies\\n[8 Lectures]'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 30, 'page_label': '31'}, page_content='Lesson Plan'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 31, 'page_label': '32'}, page_content=''),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 32, 'page_label': '33'}, page_content=''),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 33, 'page_label': '34'}, page_content=''),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 34, 'page_label': '35'}, page_content='Chapter 1\\nIntroduction to Machine Learning\\nOBJECTIVE OF THE CHAPTER :\\nThe objective of this chapter is to venture into the arena of\\nmachine learning. New comers struggle a lot to understand\\nthe philosophy of machine learning. Also, they do not\\nknow where to start from and which problem could be and\\nshould be solved using machine learning tools and\\ntechniques. This chapter intends to give the new comers a\\nstarting point to the journey in machine learning. It starts\\nfrom a historical journey in this field and takes it forward\\nto give a glimpse of the modern day applications.\\n1.1 INTRODUCTION\\nIt has been more than 20 years since a computer program\\ndefeated the reigning world champion in a game which is\\nconsidered to need a lot of intelligence to play. The computer\\nprogram was IBM’s Deep Blue and it defeated world chess\\nchampion, Gary Kasparov. That was the time, probably, when\\nthe most number of people gave serious attention to a fast-'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 35, 'page_label': '36'}, page_content='evolving field in computer science or more specifically\\nartificial intelligence – i.e. machine learning (ML).\\nAs of today, machine learning is a mature technology area\\nfinding its application in almost every sphere of life. It can\\nrecommend toys to toddlers much in the same way as it can\\nsuggest a technology book to a geek or a rich title in literature\\nto a writer. It predicts the future market to help amateur traders\\ncompete with seasoned stock traders. It helps an oncologist\\nfind whether a tumour is malignant or benign. It helps in\\noptimizing energy consumption thus helping the cause of\\nGreen Earth. Google has become one of the front-runners\\nfocusing a lot of its research on machine learning and artificial\\nintelligence – Google self-driving car and Google Brain being\\ntwo most ambitious projects of Google in its journey of\\ninnovation in the field of machine learning. In a nutshell,\\nmachine learning has become a way of life, no matter\\nwhichever sphere of life we closely look at. But where did it\\nall start from?\\nThe foundation of machine learning started in the 18th and\\n19th centuries. The first related work dates back to 1763. In\\nthat year, Thomas Bayes’s work ‘An Essay towards solving a\\nProblem in the Doctrine of Chances’ was published two years\\nafter his death. This is the work underlying Bayes Theorem, a\\nfundamental work on which a number of algorithms of\\nmachine learning is based upon. In 1812, the Bayes theorem\\nwas actually formalized by the French mathematician Pierre-\\nSimon Laplace. The method of least squares, which is the\\nfoundational concept to solve regression problems, was\\nformalized in 1805. In 1913, Andrey Markov came up with the\\nconcept of Markov chains.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 36, 'page_label': '37'}, page_content='However, the real start of focused work in the field of\\nmachine learning is considered to be Alan Turing’s seminal\\nwork in 1950. In his paper ‘Computing Machinery and\\nIntelligence’ (Mind, New Series, Vol. 59, No. 236, Oct., 1950,\\npp. 433–460), Turing posed the question ‘Can machines\\nthink?’ or in other words, ‘Do machines have intelligence?’.\\nHe was the first to propose that machines can ‘learn’ and\\nbecome artificially intelligent. In 1952, Arthur Samuel of IBM\\nlaboratory started working on machine learning programs, and\\nfirst developed programs that could play Checkers. In 1957,\\nFrank Rosenblatt designed the first neural network program\\nsimulating the human brain. From then on, for the next 50\\nyears, the journey of machine learning has been fascinating. A\\nnumber of machine learning algorithms were formulated by\\ndifferent researchers, e.g. the nearest neighbour algorithm in\\n1969, recurrent neural network in 1982, support vector\\nmachines and random forest algorithms in 1995. The latest\\nfeather in the cap of machine learning development has been\\nGoogle’s AlphaGo program, which has beaten professional\\nhuman Go player using machine learning techniques.\\nPoints to Ponder\\nWhile Deep Blue was searching some 200 million positions per second,\\nKasparov was searching not more than 5–10 positions probably, per\\nsecond. Yet he played almost at the same level. This clearly shows that\\nhumans have some trick up their sleeve that computers could not master\\nyet.\\nGo is a board game which can be played by two players. It was\\ninvented in China almost 2500 years ago and is considered to be the\\noldest board game. Though it has relatively simple rules, Go is a very\\ncomplex game (more complex than chess) because of its larger board\\nsize and more number of possible moves.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 37, 'page_label': '38'}, page_content='The evolution of machine learning from 1950 is depicted in\\nFigure 1.1.\\nThe rapid development in the area of machine learning has\\ntriggered a question in everyone’s mind – can machines learn\\nbetter than human? To find its answer, the first step would be\\nto understand what learning is from a human perspective.\\nThen, more light can be shed on what machine learning is. In\\nthe end, we need to know whether machine learning has\\nalready surpassed or has the potential to surpass human\\nlearning in every facet of life.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 38, 'page_label': '39'}, page_content='FIG. 1.1 Evolution of machine learning\\n1.2 WHAT IS HUMAN LEARNING?\\nIn cognitive science, learning is typically referred to as the\\nprocess of gaining information through observation. And why\\ndo we need to learn? In our daily life, we need to carry out\\nmultiple activities. It may be a task as simple as walking down\\nthe street or doing the homework. Or it may be some complex\\ntask like deciding the angle in which a rocket should be\\nlaunched so that it can have a particular trajectory. To do a task\\nin a proper way, we need to have prior information on one or\\nmore things related to the task. Also, as we keep learning more\\nor in other words acquiring more information, the efficiency in\\ndoing the tasks keep improving. For example, with more\\nknowledge, the ability to do homework with less number of\\nmistakes increases. In the same way, information from past\\nrocket launches helps in taking the right precautions and\\nmakes more successful rocket launch. Thus, with more\\nlearning, tasks can be performed more efficiently.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 39, 'page_label': '40'}, page_content='1.3 TYPES OF HUMAN LEARNING\\nThinking intuitively, human learning happens in one of the\\nthree ways – (1) either somebody who is an expert in the\\nsubject directly teaches us, (2) we build our own notion\\nindirectly based on what we have learnt from the expert in the\\npast, or (3) we do it ourselves, may be after multiple attempts,\\nsome being unsuccessful. The first type of learning, we may\\ncall, falls under the category of learning directly under expert\\nguidance, the second type falls under learning guided by\\nknowledge gained from experts and the third type is learning\\nby self or self-learning. Let’s look at each of these types\\ndeeply using real-life examples and try to understand what\\nthey mean.\\n1.3.1 Learning under expert guidance\\nAn infant may inculcate certain traits and characteristics,\\nlearning straight from its guardians. He calls his hand, a\\n‘hand’, because that is the information he gets from his\\nparents. The sky is ‘blue’ to him because that is what his\\nparents have taught him. We say that the baby ‘learns’ things\\nfrom his parents.\\nThe next phase of life is when the baby starts going to\\nschool. In school, he starts with basic familiarization of\\nalphabets and digits. Then the baby learns how to form words\\nfrom the alphabets and numbers from the digits. Slowly more\\ncomplex learning happens in the form of sentences,\\nparagraphs, complex mathematics, science, etc. The baby is\\nable to learn all these things from his teacher who already has\\nknowledge on these areas.\\nThen starts higher studies where the person learns about\\nmore complex, application-oriented skills. Engineering'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 40, 'page_label': '41'}, page_content='students get skilled in one of the disciplines like civil,\\ncomputer science, electrical, mechanical, etc. medical students\\nlearn about anatomy, physiology, pharmacology, etc. There are\\nsome experts, in general the teachers, in the respective field\\nwho have in-depth subject matter knowledge, who help the\\nstudents in learning these skills.\\nThen the person starts working as a professional in some\\nfield. Though he might have gone through enough theoretical\\nlearning in the respective field, he still needs to learn more\\nabout the hands-on application of the knowledge that he has\\nacquired. The professional mentors, by virtue of the\\nknowledge that they have gained through years of hands-on\\nexperience, help all new comers in the field to learn on-job.\\nIn all phases of life of a human being, there is an element of\\nguided learning. This learning is imparted by someone, purely\\nbecause of the fact that he/she has already gathered the\\nknowledge by virtue of his/her experience in that field. So\\nguided learning is the process of gaining information from a\\nperson having sufficient knowledge due to the past experience.\\n1.3.2 Learning guided by knowledge gained from experts\\nAn essential part of learning also happens with the knowledge\\nwhich has been imparted by teacher or mentor at some point of\\ntime in some other form/context. For example, a baby can\\ngroup together all objects of same colour even if his parents\\nhave not specifically taught him to do so. He is able to do so\\nbecause at some point of time or other his parents have told\\nhim which colour is blue, which is red, which is green, etc. A\\ngrown-up kid can select one odd word from a set of words\\nbecause it is a verb and other words being all nouns. He could\\ndo this because of his ability to label the words as verbs or'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 41, 'page_label': '42'}, page_content='nouns, taught by his English teacher long back. In a\\nprofessional role, a person is able to make out to which\\ncustomers he should market a campaign from the knowledge\\nabout preference that was given by his boss long back.\\nIn all these situations, there is no direct learning. It is some\\npast information shared on some different context, which is\\nused as a learning to make decisions.\\n1.3.3 Learning by self\\nIn many situations, humans are left to learn on their own. A\\nclassic example is a baby learning to walk through obstacles.\\nHe bumps on to obstacles and falls down multiple times till he\\nlearns that whenever there is an obstacle, he needs to cross\\nover it. He faces the same challenge while learning to ride a\\ncycle as a kid or drive a car as an adult. Not all things are\\ntaught by others. A lot of things need to be learnt only from\\nmistakes made in the past. We tend to form a check list on\\nthings that we should do, and things that we should not do,\\nbased on our experiences.\\n1.4 WHAT IS MACHINE LEARNING?\\nBefore answering the question ‘What is machine learning?’\\nmore fundamental questions that peep into one’s mind are\\nDo machines really learn?\\nIf so, how do they learn?\\nWhich problem can we consider as a well-posed learning problem? What are\\nthe important features that are required to well-define a learning problem?\\nAt the onset, it is important to formalize the definition of\\nmachine learning. This will itself address the first question, i.e.\\nif machines really learn. There are multiple ways to define\\nmachine learning. But the one which is perhaps most relevant,'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 42, 'page_label': '43'}, page_content='concise and accepted universally is the one stated by Tom M.\\nMitchell, Professor of Machine Learning Department, School\\nof Computer Science, Carnegie Mellon University. Tom M.\\nMitchell has defined machine learning as\\n‘A computer program is said to learn from experience E with respect to\\nsome class of tasks T and performance measure P, if its performance at\\ntasks in T, as measured by P, improves with experience E.’\\nWhat this essentially means is that a machine can be\\nconsidered to learn if it is able to gather experience by doing a\\ncertain task and improve its performance in doing the similar\\ntasks in the future. When we talk about past experience, it\\nmeans past data related to the task. This data is an input to the\\nmachine from some source.\\nIn the context of the learning to play checkers, E represents\\nthe experience of playing the game, T represents the task of\\nplaying checkers and P is the performance measure indicated\\nby the percentage of games won by the player. The same\\nmapping can be applied for any other machine learning\\nproblem, for example, image classification problem. In context\\nof image classification, E represents the past data with images\\nhaving labels or assigned classes (for example whether the\\nimage is of a class cat or a class dog or a class elephant etc.), T\\nis the task of assigning class to new, unlabelled images and P\\nis the performance measure indicated by the percentage of\\nimages correctly classified.\\nThe first step in any project is defining your problem. Even\\nif the most powerful algorithm is used, the results will be\\nmeaningless if the wrong problem is solved.\\n1.4.1 How do machines learn?'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 43, 'page_label': '44'}, page_content='The basic machine learning process can be divided into three\\nparts.\\n1. Data Input: Past data or information is utilized as a basis for future\\ndecision-making\\n2. Abstraction: The input data is represented in a broader way through the\\nunderlying algorithm\\n3. Generalization: The abstracted representation is generalized to form a\\nframework for making decisions\\nFigure 1.2 is a schematic representation of the machine\\nlearning process.\\nFIG. 1.2 Process of machine learning\\nLet’s put the things in perspective of the human learning\\nprocess and try to understand the machine learning process\\nmore clearly. Reason is, in some sense, machine learning\\nprocess tries to emulate the process in which humans learn to a\\nlarge extent.\\nLet’s consider the situation of typical process of learning\\nfrom classroom and books and preparing for the examination.\\nIt is a tendency of many students to try and memorize (we\\noften call it ‘learn by heart’) as many things as possible. This\\nmay work well when the scope of learning is not so vast. Also,\\nthe kinds of questions which are asked in the examination are\\npretty much simple and straightforward. The questions can be\\nanswered by simply writing the same things which have been\\nmemorized. However, as the scope gets broader and the'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 44, 'page_label': '45'}, page_content='questions asked in the examination gets more complex, the\\nstrategy of memorizing doesn’t work well. The number of\\ntopics may get too vast for a student to memorize. Also, the\\ncapability of memorizing varies from student to student.\\nTogether with that, since the questions get more complex, a\\ndirect reproduction of the things memorized may not help. The\\nsituation continues to get worse as the student graduates to\\nhigher classes.\\nSo, what we see in the case of human learning is that just by\\ngreat memorizing and perfect recall, i.e. just based on\\nknowledge input, students can do well in the examinations\\nonly till a certain stage. Beyond that, a better learning strategy\\nneeds to be adopted:\\n1. to be able to deal with the vastness of the subject matter and the related\\nissues in memorizing it\\n2. to be able to answer questions where a direct answer has not been learnt\\nA good option is to figure out the key points or ideas\\namongst a vast pool of knowledge. This helps in creating an\\noutline of topics and a conceptual mapping of those outlined\\ntopics with the entire knowledge pool. For example, a broad\\npool of knowledge may consist of all living animals and their\\ncharacteristics such as whether they live in land or water,\\nwhether they lay eggs, whether they have scales or fur or none,\\netc. It is a difficult task for any student to memorize the\\ncharacteristics of all living animals – no matter how much\\nphotographic memory he/she may possess. It is better to draw\\na notion about the basic groups that all living animals belong\\nto and the characteristics which define each of the basic\\ngroups. The basic groups of animals are invertebrates and\\nvertebrates. Vertebrates are further grouped as mammals,\\nreptiles, amphibians, fishes, and birds. Here, we have mapped\\nanimal groups and their salient characteristics.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 45, 'page_label': '46'}, page_content='1. Invertebrate: Do not have backbones and skeletons\\n2. Vertebrate\\n1. Fishes: Always live in water and lay eggs\\n2. Amphibians: Semi-aquatic i.e. may live in water or land; smooth\\nskin; lay eggs\\n3. Reptiles: Semi-aquatic like amphibians; scaly skin; lay eggs;\\ncold-blooded\\n4. Birds: Can fly; lay eggs; warm-blooded\\n5. Mammals: Have hair or fur; have milk to feed their young;\\nwarm-blooded\\nThis makes it easier to memorize as the scope now reduces\\nto know the animal groups that the animals belong to. Rest of\\nthe answers about the characteristics of the animals may be\\nderived from the concept of mapping animal groups and their\\ncharacteristics.\\nMoving to the machine learning paradigm, the vast pool of\\nknowledge is available from the data input. However, rather\\nthan using it in entirety, a concept map, much in line with the\\nanimal group to characteristic mapping explained above, is\\ndrawn from the input data. This is nothing but knowledge\\nabstraction as performed by the machine. In the end, the\\nabstracted mapping from the input data can be applied to make\\ncritical conclusions. For example, if the group of an animal is\\ngiven, understanding of the characteristics can be\\nautomatically made. Reversely, if the characteristic of an\\nunknown animal is given, a definite conclusion can be made\\nabout the animal group it belongs to. This is generalization in\\ncontext of machine learning.\\n1.4.1.1 Abstraction\\nDuring the machine learning process, knowledge is fed in the\\nform of input data. However, the data cannot be used in the\\noriginal shape and form. As we saw in the example above,\\nabstraction helps in deriving a conceptual map based on the'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 46, 'page_label': '47'}, page_content='input data. This map, or a model as it is known in the machine\\nlearning paradigm, is summarized knowledge representation of\\nthe raw data. The model may be in any one of the following\\nforms\\nComputational blocks like if/else rules\\nMathematical equations\\nSpecific data structures like trees or graphs\\nLogical groupings of similar observations\\nThe choice of the model used to solve a specific learning\\nproblem is a human task. The decision related to the choice of\\nmodel is taken based on multiple aspects, some of which are\\nlisted below:\\nThe type of problem to be solved: Whether the problem is related to forecast\\nor prediction, analysis of trend, understanding the different segments or\\ngroups of objects, etc.\\nNature of the input data: How exhaustive the input data is, whether the data\\nhas no values for many fields, the data types, etc.\\nDomain of the problem: If the problem is in a business critical domain with a\\nhigh rate of data input and need for immediate inference, e.g. fraud detection\\nproblem in banking domain.\\nOnce the model is chosen, the next task is to fit the model\\nbased on the input data. Let’s understand this with an example.\\nIn a case where the model is represented by a mathematical\\nequation, say ‘y = c  + c x’ (the model is known as simple\\nlinear regression which we will study in a later chapter), based\\non the input data, we have to find out the values of c  and c .\\nOtherwise, the equation (or the model) is of no use. So, fitting\\nthe model, in this case, means finding the values of the\\nunknown coefficients or constants of the equation or the\\nmodel. This process of fitting the model based on the input\\ndata is known as training. Also, the input data based on which\\nthe model is being finalized is known as training data.\\n1.4.1.2 Generalization\\n1 2\\n1 2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 47, 'page_label': '48'}, page_content='The first part of machine learning process is abstraction i.e.\\nabstract the knowledge which comes as input data in the form\\nof a model. However, this abstraction process, or more\\npopularly training the model, is just one part of machine\\nlearning. The other key part is to tune up the abstracted\\nknowledge to a form which can be used to take future\\ndecisions. This is achieved as a part of generalization. This\\npart is quite difficult to achieve. This is because the model is\\ntrained based on a finite set of data, which may possess a\\nlimited set of characteristics. But when we want to apply the\\nmodel to take decision on a set of unknown data, usually\\ntermed as test data, we may encounter two problems:\\n1. The trained model is aligned with the training data too much, hence\\nmay not portray the actual trend.\\n2. The test data possess certain characteristics apparently unknown to the\\ntraining data.\\nHence, a precise approach of decision-making will not\\nwork. An approximate or heuristic approach, much like gut-\\nfeeling-based decision-making in human beings, has to be\\nadopted. This approach has the risk of not making a correct\\ndecision – quite obviously because certain assumptions that\\nare made may not be true in reality. But just like machines,\\nsame mistakes can be made by humans too when a decision is\\nmade based on intuition or gut-feeling – in a situation where\\nexact reason-based decision-making is not possible.\\n1.4.2 Well-posed learning problem\\nFor defining a new problem, which can be solved using\\nmachine learning, a simple framework, highlighted below, can\\nbe used. This framework also helps in deciding whether the\\nproblem is a right candidate to be solved using machine\\nlearning. The framework involves answering three questions:'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 48, 'page_label': '49'}, page_content='1. What is the problem?\\n2. Why does the problem need to be solved?\\n3. How to solve the problem?\\nStep 1: What is the Problem?\\nA number of information should be collected to know what\\nis the problem.\\nInformal description of the problem, e.g. I need a\\nprogram that will prompt the next word as and when I type a\\nword.\\nFormalism\\nUse Tom Mitchell’s machine learning formalism stated above\\nto define the T, P, and E for the problem.\\nFor example:\\nTask (T): Prompt the next word when I type a word.\\nExperience (E): A corpus of commonly used English words and phrases.\\nPerformance (P): The number of correct words prompted considered as a\\npercentage (which in machine learning paradigm is known as learning\\naccuracy).\\nAssumptions - Create a list of assumptions about the\\nproblem.\\nSimilar problems\\nWhat other problems have you seen or can you think of that\\nare similar to the problem that you are trying to solve?\\nStep 2: Why does the problem need to be solved?\\nMotivation'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 49, 'page_label': '50'}, page_content='What is the motivation for solving the problem? What\\nrequirement will it fulfil?\\nFor example, does this problem solve any long-standing\\nbusiness issue like finding out potentially fraudulent\\ntransactions? Or the purpose is more trivial like trying to\\nsuggest some movies for upcoming weekend.\\nSolution benefits\\nConsider the benefits of solving the problem. What\\ncapabilities does it enable?\\nIt is important to clearly understand the benefits of solving\\nthe problem. These benefits can be articulated to sell the\\nproject.\\nSolution use\\nHow will the solution to the problem be used and the life time\\nof the solution is expected to have?\\nStep 3: How would I solve the problem?\\nTry to explore how to solve the problem manually.\\nDetail out step-by-step data collection, data preparation, and\\nprogram design to solve the problem. Collect all these details\\nand update the previous sections of the problem definition,\\nespecially the assumptions.\\nSummary\\nStep 1: What is the problem? Describe the problem informally and\\nformally and list assumptions and similar problems.\\nStep 2: Why does the problem need to be solved? List the motivation for\\nsolving the problem, the benefits that the solution will provide and how the'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 50, 'page_label': '51'}, page_content='solution will be used.\\nStep 3: How would I solve the problem? Describe how the problem would\\nbe solved manually to flush domain knowledge.\\nDid you know?\\nSony created a series of robotic pets called Aibo. It was\\nbuilt in 1998. Although most models sold were dog-like,\\nother inspirations included lion-cubs. It could express\\nemotions and could also recognize its owner. In 2006,\\nAibo was added to the Carnegie Mellon University’s\\n‘Robot Hall of Fame’. A new generation of Aibo was\\nlaunched in Japan in January 2018.\\n1.5 TYPES OF MACHINE LEARNING\\nAs highlighted in Figure 1.3, Machine learning can be\\nclassified into three broad categories:\\n1. Supervised learning – Also called predictive learning. A machine\\npredicts the class of unknown objects based on prior class-related\\ninformation of similar objects.\\n2. Unsupervised learning – Also called descriptive learning. A machine\\nfinds patterns in unknown objects by grouping similar objects together.\\n3. Reinforcement learning – A machine learns to act on its own to achieve\\nthe given goals.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 51, 'page_label': '52'}, page_content='FIG. 1.3 Types of machine learning\\nDid you know?\\nMany video games are based on artificial intelligence\\ntechnique called Expert System. This technique can imitate\\nareas of human behaviour, with a goal to mimic the human\\nability of senses, perception, and reasoning.\\n1.5.1 Supervised learning\\nThe major motivation of supervised learning is to learn from\\npast information. So what kind of past information does the\\nmachine need for supervised learning? It is the information\\nabout the task which the machine has to execute. In context of\\nthe definition of machine learning, this past information is the\\nexperience. Let’s try to understand it with an example.\\nSay a machine is getting images of different objects as input\\nand the task is to segregate the images by either shape or\\ncolour of the object. If it is by shape, the images which are of\\nround-shaped objects need to be separated from images of'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 52, 'page_label': '53'}, page_content='triangular-shaped objects, etc. If the segregation needs to\\nhappen based on colour, images of blue objects need to be\\nseparated from images of green objects. But how can the\\nmachine know what is round shape, or triangular shape? Same\\nway, how can the machine distinguish image of an object\\nbased on whether it is blue or green in colour? A machine is\\nvery much like a little child whose parents or adults need to\\nguide him with the basic information on shape and colour\\nbefore he can start doing the task. A machine needs the basic\\ninformation to be provided to it. This basic input, or the\\nexperience in the paradigm of machine learning, is given in the\\nform of training data . Training data is the past information\\non a specific task. In context of the image segregation\\nproblem, training data will have past data on different aspects\\nor features on a number of images, along with a tag on\\nwhether the image is round or triangular, or blue or green in\\ncolour. The tag is called ‘ label’ and we say that the training\\ndata is labelled in case of supervised learning.\\nFigure 1.4 is a simple depiction of the supervised learning\\nprocess. Labelled training data containing past information\\ncomes as an input. Based on the training data, the machine\\nbuilds a predictive model that can be used on test data to\\nassign a label for each record in the test data.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 53, 'page_label': '54'}, page_content='FIG. 1.4 Supervised learning\\nSome examples of supervised learning are\\nPredicting the results of a game\\nPredicting whether a tumour is malignant or benign\\nPredicting the price of domains like real estate, stocks, etc.\\nClassifying texts such as classifying a set of emails as spam or non-spam\\nNow, let’s consider two of the above examples, say\\n‘predicting whether a tumour is malignant or benign’ and\\n‘predicting price of domains such as real estate’. Are these two\\nproblems same in nature? The answer is ‘no’. Though both of\\nthem are prediction problems, in one case we are trying to\\npredict which category or class an unknown data belongs to\\nwhereas in the other case we are trying to predict an absolute\\nvalue and not a class. When we are trying to predict a\\ncategorical or nominal variable, the problem is known as a\\nclassification problem. Whereas when we are trying to predict\\na real-valued variable, the problem falls under the category of\\nregression.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 54, 'page_label': '55'}, page_content='Note:\\nSupervised machine learning is as good as the data used to\\ntrain it. If the training data is of poor quality, the prediction\\nwill also be far from being precise.\\nLet’s try to understand these two areas of supervised\\nlearning, i.e. classification and regression in more details.\\n1.5.1.1 Classification\\nLet’s discuss how to segregate the images of objects based on\\nthe shape . If the image is of a round object, it is put under one\\ncategory, while if the image is of a triangular object, it is put\\nunder another category. In which category the machine should\\nput an image of unknown category, also called a test data in\\nmachine learning parlance, depends on the information it gets\\nfrom the past data, which we have called as training data.\\nSince the training data has a label or category defined for each\\nand every image, the machine has to map a new image or test\\ndata to a set of images to which it is similar to and assign the\\nsame label or category to the test data.\\nSo we observe that the whole problem revolves around\\nassigning a label or category or class to a test data based on the\\nlabel or category or class information that is imparted by the\\ntraining data. Since the target objective is to assign a class\\nlabel, this type of problem as classification problem. Figure\\n1.5 depicts the typical process of classification.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 55, 'page_label': '56'}, page_content='FIG. 1.5 Classification\\nThere are number of popular machine learning algorithms\\nwhich help in solving classification problems. To name a few,\\nNaïve Bayes, Decision tree, and k-Nearest Neighbour\\nalgorithms are adopted by many machine learning\\npractitioners.\\nA critical classification problem in context of banking\\ndomain is identifying potential fraudulent transactions. Since\\nthere are millions of transactions which have to be scrutinized\\nand assured whether it might be a fraud transaction, it is not\\npossible for any human being to carry out this task. Machine\\nlearning is effectively leveraged to do this task and this is a\\nclassic case of classification. Based on the past transaction\\ndata, specifically the ones labelled as fraudulent, all new\\nincoming transactions are marked or labelled as normal or'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 56, 'page_label': '57'}, page_content='suspicious. The suspicious transactions are subsequently\\nsegregated for a closer review.\\nIn summary, classification is a type of supervised learning\\nwhere a target feature, which is of type categorical, is\\npredicted for test data based on the information imparted by\\ntraining data. The target categorical feature is known as class.\\nSome typical classification problems include:\\nImage classification\\nPrediction of disease\\nWin–loss prediction of games\\nPrediction of natural calamity like earthquake, flood, etc.\\nRecognition of handwriting\\nDid you know?\\nMachine learning saves life – ML can spot 52% of breast cancer cells, a\\nyear before patients are diagnosed.\\nUS Postal Service uses machine learning for handwriting recognition.\\nFacebook’s news feed uses machine learning to personalize each\\nmember’s feed.\\n1.5.1.2. Regression\\nIn linear regression, the objective is to predict numerical\\nfeatures like real estate or stock price, temperature, marks in\\nan examination, sales revenue, etc. The underlying predictor\\nvariable and the target variable are continuous in nature. In\\ncase of linear regression, a straight line relationship is ‘fitted’\\nbetween the predictor variables and the target variables, using\\nthe statistical concept of least squares method. As in the case\\nof least squares method, the sum of square of error between'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 57, 'page_label': '58'}, page_content='actual and predicted values of the target variable is tried to be\\nminimized. In case of simple linear regression, there is only\\none predictor variable whereas in case of multiple linear\\nregression, multiple predictor variables can be included in the\\nmodel.\\nLet’s take the example of yearly budgeting exercise of the\\nsales managers. They have to give sales prediction for the next\\nyear based on sales figure of previous years vis-à-vis\\ninvestment being put in. Obviously, the data related to past as\\nwell as the data to be predicted are continuous in nature. In a\\nbasic approach, a simple linear regression model can be\\napplied with investment as predictor variable and sales\\nrevenue as the target variable.\\nFigure 1.6 shows a typical simple regression model, where\\nregression line is fitted based on values of target variable with\\nrespect to different values of predictor variable. A typical\\nlinear regression model can be represented in the form –\\nwhere ‘x’ is the predictor variable and ‘y’ is the target variable.\\nThe input data come from a famous multivariate data set\\nnamed Iris introduced by the British statistician and biologist\\nRonald Fisher. The data set consists of 50 samples from each\\nof three species of Iris – Iris setosa, Iris virginica, and Iris\\nversicolor. Four features were measured for each sample –\\nsepal length, sepal width, petal length, and petal width. These\\nfeatures can uniquely discriminate the different species of the\\nflower.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 58, 'page_label': '59'}, page_content='The Iris data set is typically used as a training data for\\nsolving the classification problem of predicting the flower\\nspecies based on feature values. However, we can also\\ndemonstrate regression using this data set, by predicting the\\nvalue of one feature using another feature as predictor. In\\nFigure 1.6, petal length is a predictor variable which, when\\nfitted in the simple linear regression model, helps in predicting\\nthe value of the target variable sepal length.\\nFIG. 1.6 Regression\\nTypical applications of regression can be seen in\\nDemand forecasting in retails\\nSales prediction for managers\\nPrice prediction in real estate\\nWeather forecast\\nSkill demand forecast in job market\\n1.5.2 Unsupervised learning\\nUnlike supervised learning, in unsupervised learning, there is\\nno labelled training data to learn from and no prediction to be'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 59, 'page_label': '60'}, page_content='made. In unsupervised learning, the objective is to take a\\ndataset as input and try to find natural groupings or patterns\\nwithin the data elements or records. Therefore, unsupervised\\nlearning is often termed as descriptive model and the process\\nof unsupervised learning is referred as pattern discovery or\\nknowledge discovery. One critical application of\\nunsupervised learning is customer segmentation.\\nClustering is the main type of unsupervised learning. It\\nintends to group or organize similar objects together. For that\\nreason, objects belonging to the same cluster are quite similar\\nto each other while objects belonging to different clusters are\\nquite dissimilar. Hence, the objective of clustering to discover\\nthe intrinsic grouping of unlabelled data and form clusters, as\\ndepicted in Figure 1.7. Different measures of similarity can be\\napplied for clustering. One of the most commonly adopted\\nsimilarity measure is distance. Two data items are considered\\nas a part of the same cluster if the distance between them is\\nless. In the same way, if the distance between the data items is\\nhigh, the items do not generally belong to the same cluster.\\nThis is also known as distance-based clustering. Figure 1.8\\ndepicts the process of clustering at a high level.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 60, 'page_label': '61'}, page_content='FIG. 1.7 Distance-based clustering\\nOther than clustering of data and getting a summarized view\\nfrom it, one more variant of unsupervised learning is\\nassociation analysis. As a part of association analysis, the\\nassociation between data elements is identified. Let’s try to\\nunderstand the approach of association analysis in context of\\none of the most common examples, i.e. market basket analysis\\nas shown in Figure 1.9. From past transaction data in a grocery\\nstore, it may be observed that most of the customers who have\\nbought item A, have also bought item B and item C or at least\\none of them. This means that there is a strong association of\\nthe event ‘purchase of item A’ with the event ‘purchase of item\\nB’, or ‘purchase of item C’. Identifying these sorts of\\nassociations is the goal of association analysis. This helps in\\nboosting up sales pipeline, hence a critical input for the sales\\ngroup. Critical applications of association analysis include\\nmarket basket analysis and recommender systems.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 61, 'page_label': '62'}, page_content='FIG. 1.8 Unsupervised learning\\nFIG. 1.9 Market basket analysis\\n1.5.3 Reinforcement learning\\nWe have seen babies learn to walk without any prior\\nknowledge of how to do it. Often we wonder how they really'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 62, 'page_label': '63'}, page_content='do it. They do it in a relatively simple way.\\nFirst they notice somebody else walking around, for\\nexample parents or anyone living around. They understand\\nthat legs have to be used, one at a time, to take a step. While\\nwalking, sometimes they fall down hitting an obstacle,\\nwhereas other times they are able to walk smoothly avoiding\\nbumpy obstacles. When they are able to walk overcoming the\\nobstacle, their parents are elated and appreciate the baby with\\nloud claps / or may be a chocolates. When they fall down\\nwhile circumventing an obstacle, obviously their parents do\\nnot give claps or chocolates. Slowly a time comes when the\\nbabies learn from mistakes and are able to walk with much\\nease.\\nIn the same way, machines often learn to do tasks\\nautonomously. Let’s try to understand in context of the\\nexample of the child learning to walk. The action tried to be\\nachieved is walking, the child is the agent and the place with\\nhurdles on which the child is trying to walk resembles the\\nenvironment. It tries to improve its performance of doing the\\ntask. When a sub-task is accomplished successfully, a reward\\nis given. When a sub-task is not executed correctly, obviously\\nno reward is given. This continues till the machine is able to\\ncomplete execution of the whole task. This process of learning\\nis known as reinforcement learning. Figure 1.10 captures the\\nhigh-level process of reinforcement learning.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 63, 'page_label': '64'}, page_content='FIG. 1.10 Reinforcement learning\\nOne contemporary example of reinforcement learning is\\nself-driving cars. The critical information which it needs to\\ntake care of are speed and speed limit in different road\\nsegments, traffic conditions, road conditions, weather\\nconditions, etc. The tasks that have to be taken care of are\\nstart/stop, accelerate/decelerate, turn to left / right, etc.\\nFurther details on reinforcement learning have been kept out\\nof the scope of this book.\\nPoints to Ponder:'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 64, 'page_label': '65'}, page_content='Reinforcement learning is getting more and more attention from both\\nindustry and academia. Annual publications count in the area of\\nreinforcement learning in Google Scholar support this view.\\nWhile Deep Blue used brute force to defeat the human chess champion,\\nAlphaGo used RL to defeat the best human Go player.\\nRL is an effective tool for personalized online marketing. It considers\\nthe demographic details and browsing history of the user real-time to\\nshow most relevant advertisements.\\n1.5.4 Comparison – supervised, unsupervised, and\\nreinforcement learning'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 65, 'page_label': '66'}, page_content='1.6 PROBLES NOT TO BE SOLVED USING MACHINE LEARNING'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 66, 'page_label': '67'}, page_content='Machine learning should not be applied to tasks in which\\nhumans are very effective or frequent human intervention is\\nneeded. For example, air traffic control is a very complex task\\nneeding intense human involvement. At the same time, for\\nvery simple tasks which can be implemented using traditional\\nprogramming paradigms, there is no sense of using machine\\nlearning. For example, simple rule-driven or formula-based\\napplications like price calculator engine, dispute tracking\\napplication, etc. do not need machine learning techniques.\\nMachine learning should be used only when the business\\nprocess has some lapses. If the task is already optimized,\\nincorporating machine learning will not serve to justify the\\nreturn on investment.\\nFor situations where training data is not sufficient, machine\\nlearning cannot be used effectively. This is because, with small\\ntraining data sets, the impact of bad data is exponentially\\nworse. For the quality of prediction or recommendation to be\\ngood, the training data should be sizeable.\\n1.7 APPLICATIONS OF MACHINE LEARNING\\nWherever there is a substantial amount of past data, machine\\nlearning can be used to generate actionable insight from the\\ndata. Though machine learning is adopted in multiple forms in\\nevery business domain, we have covered below three major\\ndomains just to give some idea about what type of actions can\\nbe done using machine learning.\\n1.7.1 Banking and finance\\nIn the banking industry, fraudulent transactions, especially the\\nones related to credit cards, are extremely prevalent. Since the\\nvolumes as well as velocity of the transactions are extremely'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 67, 'page_label': '68'}, page_content='high, high performance machine learning solutions are\\nimplemented by almost all leading banks across the globe. The\\nmodels work on a real-time basis, i.e. the fraudulent\\ntransactions are spotted and prevented right at the time of\\noccurrence. This helps in avoiding a lot of operational hassles\\nin settling the disputes that customers will otherwise raise\\nagainst those fraudulent transactions.\\nCustomers of a bank are often offered lucrative proposals by\\nother competitor banks. Proposals like higher bank interest,\\nlower processing charge of loans, zero balance savings\\naccounts, no overdraft penalty, etc. are offered to customers,\\nwith the intent that the customer switches over to the\\ncompetitor bank. Also, sometimes customers get demotivated\\nby the poor quality of services of the banks and shift to\\ncompetitor banks. Machine learning helps in preventing or at\\nleast reducing the customer churn. Both descriptive and\\npredictive learning can be applied for reducing customer\\nchurn. Using descriptive learning, the specific pockets of\\nproblem, i.e. a specific bank or a specific zone or a specific\\ntype of offering like car loan, may be spotted where maximum\\nchurn is happening. Quite obviously, these are troubled areas\\nwhere further investigation needs to be done to find and fix the\\nroot cause. Using predictive learning, the set of vulnerable\\ncustomers who may leave the bank very soon, can be\\nidentified. Proper action can be taken to make sure that the\\ncustomers stay back.\\n1.7.2 Insurance\\nInsurance industry is extremely data intensive. For that reason,\\nmachine learning is extensively used in the insurance industry.\\nTwo major areas in the insurance industry where machine\\nlearning is used are risk prediction during new customer'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 68, 'page_label': '69'}, page_content='onboarding and claims management. During customer\\nonboarding, based on the past information the risk profile of a\\nnew customer needs to be predicted. Based on the quantum of\\nrisk predicted, the quote is generated for the prospective\\ncustomer. When a customer claim comes for settlement, past\\ninformation related to historic claims along with the adjustor\\nnotes are considered to predict whether there is any possibility\\nof the claim to be fraudulent. Other than the past information\\nrelated to the specific customer, information related to similar\\ncustomers, i.e. customer belonging to the same geographical\\nlocation, age group, ethnic group, etc., are also considered to\\nformulate the model.\\n1.7.3 Healthcare\\nWearable device data form a rich source for applying machine\\nlearning and predict the health conditions of the person real\\ntime. In case there is some health issue which is predicted by\\nthe learning model, immediately the person is alerted to take\\npreventive action. In case of some extreme problem, doctors or\\nhealthcare providers in the vicinity of the person can be\\nalerted. Suppose an elderly person goes for a morning walk in\\na park close to his house. Suddenly, while walking, his blood\\npressure shoots up beyond a certain limit, which is tracked by\\nthe wearable. The wearable data is sent to a remote server and\\na machine learning algorithm is constantly analyzing the\\nstreaming data. It also has the history of the elderly person and\\npersons of similar age group. The model predicts some fatality\\nunless immediate action is taken. Alert can be sent to the\\nperson to immediately stop walking and take rest. Also,\\ndoctors and healthcare providers can be alerted to be on\\nstandby.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 69, 'page_label': '70'}, page_content='Machine learning along with computer vision also plays a\\ncrucial role in disease diagnosis from medical imaging.\\n1.8 STATE-OF-THE-ART LANGUAGES/TOOLS IN MACHINE LEARNING\\nThe algorithms related to different machine learning tasks are\\nknown to all and can be implemented using any\\nlanguage/platform. It can be implemented using a Java\\nplatform or C / C++ language or in .NET. However, there are\\ncertain languages and tools which have been developed with a\\nfocus for implementing machine learning. Few of them, which\\nare most widely used, are covered below.\\n1.8.1 Python\\nPython is one of the most popular, open source programming\\nlanguage widely adopted by machine learning community. It\\nwas designed by Guido van Rossum and was first released in\\n1991. The reference implementation of Python, i.e. CPython,\\nis managed by Python Software Foundation, which is a non-\\nprofit organization.\\nPython has very strong libraries for advanced mathematical\\nfunctionalities (NumPy), algorithms and mathematical tools\\n(SciPy) and numerical plotting (matplotlib). Built on these\\nlibraries, there is a machine learning library named scikit-\\nlearn, which has various classification, regression, and\\nclustering algorithms embedded in it.\\n1.8.2 R\\nR is a language for statistical computing and data analysis. It is\\nan open source language, extremely popular in the academic\\ncommunity – especially among statisticians and data miners. R\\nis considered as a variant of S, a GNU project which was'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 70, 'page_label': '71'}, page_content='developed at Bell Laboratories. Currently, it is supported by\\nthe R Foundation for statistical computing.\\nR is a very simple programming language with a huge set of\\nlibraries available for different stages of machine learning.\\nSome of the libraries standing out in terms of popularity are\\nplyr/dplyr (for data transformation), caret (‘Classification and\\nRegression Training’ for classification), RJava (to facilitate\\nintegration with Java), tm (for text mining), ggplot2 (for data\\nvisualization). Other than the libraries, certain packages like\\nShiny and R Markdown have been developed around R to\\ndevelop interactive web applications, documents and\\ndashboards on R without much effort.\\n1.8.3 Matlab\\nMATLAB (matrix laboratory) is a licenced commercial\\nsoftware with a robust support for a wide range of numerical\\ncomputing. MATLAB has a huge user base across industry\\nand academia. MATLAB is developed by MathWorks, a\\ncompany founded in 1984. Being proprietary software,\\nMATLAB is developed much more professionally, tested\\nrigorously, and has comprehensive documentation.\\nMATLAB also provides extensive support of statistical\\nfunctions and has a huge number of machine learning\\nalgorithms in-built. It also has the ability to scale up for large\\ndatasets by parallel processing on clusters and cloud.\\n1.8.4 SAS\\nSAS (earlier known as ‘Statistical Analysis System’) is\\nanother licenced commercial software which provides strong'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 71, 'page_label': '72'}, page_content='support for machine learning functionalities. Developed in C\\nby SAS Institute, SAS had its first release in the year 1976.\\nSAS is a software suite comprising different components.\\nThe basic data management functionalities are embedded in\\nthe Base SAS component whereas the other components like\\nSAS/INSIGHT, Enterprise Miner, SAS/STAT, etc. help in\\nspecialized functions related to data mining and statistical\\nanalysis.\\n1.8.5 Other languages/tools\\nThere are a host of other languages and tools that also support\\nmachine learning functionalities. Owned by IBM, SPSS\\n(originally named as Statistical Package for the Social\\nSciences) is a popular package supporting specialized data\\nmining and statistical analysis. Originally popular for\\nstatistical analysis in social science (as the name reflects),\\nSPSS is now popular in other fields as well.\\nReleased in 2012, Julia is an open source, liberal licence\\nprogramming language for numerical analysis and\\ncomputational science. It has baked in all good things of\\nMATLAB, Python, R, and other programming languages used\\nfor machine learning for which it is gaining steady attention\\nfrom machine learning development community. Another big\\npoint in favour of Julia is its ability to implement high-\\nperformance machine learning algorithms.\\n1.9 ISSUES IN MACHINE LEARNING\\nMachine learning is a field which is relatively new and still\\nevolving. Also, the level of research and kind of use of\\nmachine learning tools and technologies varies drastically\\nfrom country to country. The laws and regulations, cultural'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 72, 'page_label': '73'}, page_content='background, emotional maturity of people differ drastically in\\ndifferent countries. All these factors make the use of machine\\nlearning and the issues originating out of machine learning\\nusage are quite different.\\nThe biggest fear and issue arising out of machine learning is\\nrelated to privacy and the breach of it. The primary focus of\\nlearning is on analyzing data, both past and current, and\\ncoming up with insight from the data. This insight may be\\nrelated to people and the facts revealed might be private\\nenough to be kept confidential. Also, different people have a\\ndifferent preference when it comes to sharing of information.\\nWhile some people may be open to sharing some level of\\ninformation publicly, some other people may not want to share\\nit even to all friends and keep it restricted just to family\\nmembers. Classic examples are a birth date (not the day, but\\nthe date as a whole), photographs of a dinner date with family,\\neducational background, etc. Some people share them with all\\nin the social platforms like Facebook while others do not, or if\\nthey do, they may restrict it to friends only. When machine\\nlearning algorithms are implemented using those information,\\ninadvertently people may get upset. For example, if there is a\\nlearning algorithm to do preference-based customer\\nsegmentation and the output of the analysis is used for sending\\ntargeted marketing campaigns, it will hurt the emotion of\\npeople and actually do more harm than good. In certain\\ncountries, such events may result in legal actions to be taken\\nby the people affected.\\nEven if there is no breach of privacy, there may be situations\\nwhere actions were taken based on machine learning may\\ncreate an adverse reaction. Let’s take the example of\\nknowledge discovery exercise done before starting an election\\ncampaign. If a specific area reveals an ethnic majority or'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 73, 'page_label': '74'}, page_content='skewness of a certain demographic factor, and the campaign\\npitch carries a message keeping that in mind, it might actually\\nupset the voters and cause an adverse result.\\nSo a very critical consideration before applying machine\\nlearning is that proper human judgement should be exercised\\nbefore using any outcome from machine learning. Only then\\nthe decision taken will be beneficial and also not result in any\\nadverse impact.\\n1.10 SUMMARY\\nMachine learning imbibes the philosophy of human learning, i.e. learning\\nfrom expert guidance and from experience.\\nThe basic machine learning process can be divided into three parts.\\nData Input: Past data or information is utilized as a basis for future\\ndecision-making.\\nAbstraction: The input data is represented in a summarized way\\nGeneralization: The abstracted representation is generalized to form a\\nframework for making decisions.\\nBefore starting to solve any problem using machine learning, it should be\\ndecided whether the problem is a right problem to be solved using machine\\nlearning.\\nMachine learning can be classified into three broad categories:\\nSupervised learning: Also called predictive learning. The objective of\\nthis learning is to predict class/value of unknown objects based on\\nprior information of similar objects. Examples: predicting whether a\\ntumour is malignant or benign, price prediction in domains such as\\nreal estate, stocks, etc.\\nUnsupervised learning: Also called descriptive learning, helps in\\nfinding groups or patterns in unknown objects by grouping similar\\nobjects together. Examples: customer segmentation, recommender\\nsystems, etc.\\nReinforcement learning: A machine learns to act on its own to achieve\\nthe given goals. Examples: self-driving cars, intelligent robots, etc.\\nMachine learning has been adopted by various industry domains such as\\nBanking and Financial Services, Insurance, Healthcare, Life Sciences, etc. to\\nsolve problems.\\nSome of the most adopted platforms to implement machine learning include\\nPython, R, MATLAB, SAS, SPPSS, etc.\\nTo avoid ethical issues, the critical consideration is required before applying\\nmachine learning and using any outcome from machine learning.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 74, 'page_label': '75'}, page_content='SAMPLE QUESTIONS\\nMULTIPLE-CHOICE QUESTIONS (1 MARK EACH):\\n1. Machine learning is ___ field.\\n1. Inter-disciplinary\\n2. Single\\n3. Multi-disciplinary\\n4. All of the above\\n2. A computer program is said to learn from __________ E with respect to\\nsome class of tasks T and performance measure P, if its performance at\\ntasks in T, as measured by P, improves with E.\\n1. Training\\n2. Experience\\n3. Database\\n4. Algorithm\\n3. __________ has been used to train vehicles to steer correctly and\\nautonomously on road.\\n1. Machine learning\\n2. Data mining\\n3. Neural networks\\n4. Robotics\\n4. Any hypothesis find an approximation of the target function over a\\nsufficiently large set of training examples will also approximate the\\ntarget function well over other unobserved examples. This is called\\n_____.\\n1. Hypothesis\\n2. Inductive hypothesis\\n3. Learning\\n4. Concept learning\\n5. Factors which affect performance of a learner system does not include\\n1. Representation scheme used\\n2. Training scenario\\n3. Type of feedback\\n4. Good data structures\\n6. Different learning methods does not include\\n1. Memorization\\n2. Analogy\\n3. Deduction\\n4. Introduction\\n7. A model of language consists of the categories which does not include\\n1. Language units\\n2. Role structure of units\\n3. System constraints'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 75, 'page_label': '76'}, page_content='4. Structural units\\n8. How many types are available in machine learning?\\n1. 1\\n2. 2\\n3. 3\\n4. 4\\n9. The k-means algorithm is a\\n1. Supervised learning algorithm\\n2. Unsupervised learning algorithm\\n3. Semi-supervised learning algorithm\\n4. Weakly supervised learning algorithm\\n10. The Q-learning algorithm is a\\n1. Supervised learning algorithm\\n2. Unsupervised learning algorithm\\n3. Semi-supervised learning algorithm\\n4. Reinforcement learning algorithm\\n11. This type of learning to be used when there is no idea about the class or\\nlabel of a particular data\\n1. Supervised learning algorithm\\n2. Unsupervised learning algorithm\\n3. Semi-supervised learning algorithm\\n4. Reinforcement learning algorithm\\n12. The model learns and updates itself through reward/punishment in case\\nof\\n1. Supervised learning algorithm\\n2. Unsupervised learning algorithm\\n3. Semi-supervised learning algorithm\\n4. Reinforcement learning algorithm\\nSHORT-ANSWER TYPE QUESTIONS (5 MARKS EACH):\\n1. What is human learning? Give any two examples.\\n2. What are the types of human learning? Are there equivalent forms of\\nmachine learning?\\n3. What is machine learning? What are key tasks of machine learning?\\n4. Explain the concept of penalty and reward in reinforcement learning.\\n5. What are concepts of learning as search?\\n6. What are different objectives of machine learning? How are these\\nrelated with human learning?\\n7. Define machine learning and explain the different elements with a real\\nexample.\\n8. Explain the process of abstraction with an example.\\n9. What is generalization? What role does it play in the process of machine\\nlearning?'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 76, 'page_label': '77'}, page_content='10. What is classification? Explain the key differences between\\nclassification and regression.\\n11. What is regression? Give example of some practical problems solved\\nusing regression.\\n12. Explain the process of clustering in details.\\n13. Write short notes on any two of the following:\\n1. Application of machine learning algorithms\\n2. Supervised learning\\n3. Unsupervised learning\\n4. Reinforcement learning\\nLONG-ANSWER TYPE QUESTIONS (10 MARKS QUESTIONS):\\n1. What is machine learning? Explain any two business applications of\\nmachine learning. What are the possible ethical issues of machine\\nlearning applications?\\n2. Explain how human learning happens:\\n1. Under direct guidance of experts\\n2. Under indirect guidance of experts\\n3. Self-learning\\n3. Explain the different forms of machine learning with a few examples.\\n4. Compare the different types of machine learning.\\n5. What do you mean by a well-posed learning problem? Explain\\nimportant features that are required to well-define a learning problem.\\n6. Can all problems be solved using machine learning? Explain your\\nresponse in detail.\\n7. What are different tools and technologies available for solving problems\\nin machine learning? Give details about any two of them.\\n8. What are the different types of supervised learning? Explain them with\\na sample application in each area.\\n9. What are the different types of unsupervised learning? Explain them\\ndifference with a sample application in each area.\\n10. Explain, in details, the process of machine learning.\\n1. Write short notes on any two:\\n1. MATLAB\\n2. Application of machine learning in Healthcare\\n3. Market basket analysis\\n4. Simple linear regression\\n2. Write the difference between (any two):\\n1. Abstraction and generalization\\n2. Supervised and unsupervised learning\\n3. Classification and regression'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 77, 'page_label': '78'}, page_content=''),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 78, 'page_label': '79'}, page_content='Chapter 2\\nPreparing to Model\\nOBJECTIVE OF THE CHAPTER:\\nThis chapter gives a detailed view of how to understand\\nthe incoming data and create basic understanding about the\\nnature and quality of the data. This information, in turn,\\nhelps to select and then how to apply the model. So, the\\nknowledge imparted in this chapter helps a beginner take\\nthe first step towards effective modelling and solving a\\nmachine learning problem.\\n2.1 INTRODUCTION\\nIn the last chapter, we got introduced to machine learning. In\\nthe beginning, we got a glimpse of the journey of machine\\nlearning as an evolving technology. It all started as a\\nproposition from the renowned computer scientist Alan Turing\\n– machines can ‘learn’ and become artificially intelligent.\\nGradually, through the next few decades path-breaking\\ninnovations came in from Arthur Samuel, Frank Rosenblatt,\\nJohn Hopfield, Christopher Watkins, Geoffrey Hinton and\\nmany other computer scientists. They shaped up concepts of'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 79, 'page_label': '80'}, page_content='Neural Networks, Recurrent Neural Network, Reinforcement\\nLearning, Deep Learning, etc. which took machine learning to\\nnew heights. In parallel, interesting applications of machine\\nlearning kept on happening, with organizations like IBM and\\nGoogle taking a lead. What started with IBM’s Deep Blue\\nbeating the world chess champion Gary Kasparov, continued\\nwith IBM’s Watson beating two human champions in a\\nJeopardy competition.Google also started with a series of\\ninnovations applying machine learning. The Google Brain,\\nSibyl, Waymo, AlphaGo programs – are all extremely\\nadvanced applications of machine learning which have taken\\nthe technology a few notches up. Now we can see an all-\\npervasive presence of machine learning technology in all\\nwalks of life.\\nWe have also seen the types of human learning and how\\nthat, in some ways, can be related to the types of machine\\nlearning – supervised, unsupervised, and reinforcement.\\nSupervised learning, as we saw, implies learning from past\\ndata, also called training data, which has got known values or\\nclasses. Machines can ‘learn’ or get ‘trained’ from the past\\ndata and assign classes or values to unknown data, termed as\\ntest data. This helps in solving problems related to prediction.\\nThis is much like human learning through expert guidance as\\nhappens for infants from parents or students through teachers.\\nSo, supervised learning in case of machines can be perceived\\nas guided learning from human inputs. Unsupervised machine\\nlearning doesn’t have labelled data to learn from. It tries to\\nfind patterns in unlabelled data. This is much like human\\nbeings trying to group together objects of similar shape. This\\nlearning is not guided by labelled inputs but uses the\\nknowledge gained from the labels themselves. Last but not the\\nleast is reinforcement learning in which machine tries to learn'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 80, 'page_label': '81'}, page_content='by itself through penalty/ reward mechanism – again pretty\\nmuch in the same way as human self-learning happens.\\nLastly, we saw some of the applications of machine learning\\nin different domains such as banking and finance, insurance,\\nand healthcare. Fraud detection is a critical business case\\nwhich is implemented in almost all banks across the world and\\nuses machine learning predominantly. Risk prediction for new\\ncustomers is a similar critical case in the insurance industry\\nwhich finds the application of machine learning. In the\\nhealthcare sector, disease prediction makes wide use of\\nmachine learning, especially in the developed countries.\\nWhile development in machine learning technology has\\nbeen extensive and its implementation has become\\nwidespread, to start as a practitioner, we need to gain some\\nbasic understanding. We need to understand how to apply the\\narray of tools and technologies available in the machine\\nlearning to solve a problem. In fact, that is going to be very\\nspecific to the kind of problem that we are trying to solve. If it\\nis a prediction problem, the kind of activities that will be\\ninvolved is going to be completely different vis-à-vis if it is a\\nproblem where we are trying to unfold a pattern in a data\\nwithout any past knowledge about the data. So how a machine\\nlearning project looks like or what are the salient activities that\\nform the core of a machine learning project will depend on\\nwhether it is in the area of supervised or unsupervised or\\nreinforcement learning area. However, irrespective of the\\nvariation, some foundational knowledge needs to be built\\nbefore we start with the core machine learning concepts and\\nkey algorithms. In this section, we will have a quick look at a\\nfew typical machine learning activities and focus on some of\\nthe foundational concepts that all practitioners need to gain as'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 81, 'page_label': '82'}, page_content='pre-requisites before starting their journey in the area of\\nmachine learning.\\nPoints to Ponder\\nNo man is perfect. The same is applicable for machines. To\\nincrease the level of accuracy of a machine, human\\nparticipation should be added to the machine learning\\nprocess. In short, incorporating human intervention is the\\nrecipe for the success of machine learning.\\n2.2 MACHINE LEARNING ACTIVITIES\\nThe first step in machine learning activity starts with data. In\\ncase of supervised learning, it is the labelled training data set\\nfollowed by test data which is not labelled. In case of\\nunsupervised learning, there is no question of labelled data but\\nthe task is to find patterns in the input data. A thorough review\\nand exploration of the data is needed to understand the type of\\nthe data, the quality of the data and relationship between the\\ndifferent data elements. Based on that, multiple pre-processing\\nactivities may need to be done on the input data before we can\\ngo ahead with core machine learning activities. Following are\\nthe typical preparation activities done once the input data\\ncomes into the machine learning system:\\nUnderstand the type of data in the given input data set.\\nExplore the data to understand the nature and quality.\\nExplore the relationships amongst the data elements, e.g. inter-feature\\nrelationship.\\nFind potential issues in data.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 82, 'page_label': '83'}, page_content='Do the necessary remediation, e.g. impute missing data values, etc., if\\nneeded.\\nApply pre-processing steps, as necessary.\\nOnce the data is prepared for modelling, then the learning tasks start off. As\\na part of it, do the following activities:\\nThe input data is first divided into parts – the training data and the test\\ndata (called holdout). This step is applicable for supervised learning\\nonly.\\nConsider different models or learning algorithms for selection.\\nTrain the model based on the training data for supervised learning\\nproblem and apply to unknown data. Directly apply the chosen\\nunsupervised model on the input data for unsupervised learning\\nproblem.\\nAfter the model is selected, trained (for supervised\\nlearning), and applied on input data, the performance of the\\nmodel is evaluated. Based on options available, specific\\nactions can be taken to improve the performance of the model,\\nif possible.\\nFigure 2.1 depicts the four-step process of machine learning.\\nFIG. 2.1 Detailed process of machine learning'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 83, 'page_label': '84'}, page_content='Table 2.1 contains a summary of steps and activities\\ninvolved:\\n \\nTable 2.1 Activities in Machine Learning\\nIn this chapter, we will cover the first part, i.e. preparing to\\nmodel. The remaining parts, i.e. learning, performance\\nevaluation, and performance improvement will be covered in\\nChapter 3.\\n2.3 BASIC TYPES OF DATA IN MACHINE LEARNING\\nBefore starting with types of data, let’s first understand what a\\ndata set is and what are the elements of a data set. A data set is\\na collection of related information or records. The information'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 84, 'page_label': '85'}, page_content='may be on some entity or some subject area. For example (Fig.\\n2.2), we may have a data set on students in which each record\\nconsists of information about a specific student. Again, we can\\nhave a data set on student performance which has records\\nproviding performance, i.e. marks on the individual subjects.\\nEach row of a data set is called a record. Each data set also\\nhas multiple attributes, each of which gives information on a\\nspecific characteristic. For example, in the data set on students,\\nthere are four attributes namely Roll Number, Name, Gender,\\nand Age, each of which understandably is a specific\\ncharacteristic about the student entity. Attributes can also be\\ntermed as feature, variable, dimension or field. Both the data\\nsets, Student and Student Performance, are having four\\nfeatures or dimensions; hence they are told to have four-\\ndimensional data space. A row or record represents a point in\\nthe four-dimensional data space as each row has specific\\nvalues for each of the four attributes or features. Value of an\\nattribute, quite understandably, may vary from record to\\nrecord. For example, if we refer to the first two records in the\\nStudent data set, the value of attributes Name, Gender, and\\nAge are different (Fig. 2.3).'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 85, 'page_label': '86'}, page_content='FIG. 2.2 Examples of data set\\nFIG. 2.3 Data set records and attributes\\nNow that a context of data sets is given, let’s try to\\nunderstand the different types of data that we generally come\\nacross in machine learning problems. Data can broadly be\\ndivided into following two types:\\n1. Qualitative data\\n2. Quantitative data\\nQualitative data provides information about the quality of\\nan object or information which cannot be measured. For'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 86, 'page_label': '87'}, page_content='example, if we consider the quality of performance of students\\nin terms of ‘Good’, ‘Average’, and ‘Poor’, it falls under the\\ncategory of qualitative data. Also, name or roll number of\\nstudents are information that cannot be measured using some\\nscale of measurement. So they would fall under qualitative\\ndata. Qualitative data is also called categorical data.\\nQualitative data can be further subdivided into two types as\\nfollows:\\n1. Nominal data\\n2. Ordinal data\\nNominal data is one which has no numeric value, but a\\nnamed value. It is used for assigning named values to\\nattributes. Nominal values cannot be quantified. Examples of\\nnominal data are\\n1. Blood group: A, B, O, AB, etc.\\n2. Nationality: Indian, American, British, etc.\\n3. Gender: Male, Female, Other\\nNote:\\nA special case of nominal data is when only two labels are\\npossible, e.g. pass/fail as a result of an examination. This\\nsub-type of nominal data is called ‘dichotomous’.\\nIt is obvious, mathematical operations such as addition,\\nsubtraction, multiplication, etc. cannot be performed on\\nnominal data. For that reason, statistical functions such as\\nmean, variance, etc. can also not be applied on nominal data.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 87, 'page_label': '88'}, page_content='However, a basic count is possible. So mode, i.e. most\\nfrequently occurring value, can be identified for nominal data.\\nOrdinal data, in addition to possessing the properties of\\nnominal data, can also be naturally ordered. This means\\nordinal data also assigns named values to attributes but unlike\\nnominal data, they can be arranged in a sequence of increasing\\nor decreasing value so that we can say whether a value is\\nbetter than or greater than another value. Examples of ordinal\\ndata are\\n1. Customer satisfaction: ‘Very Happy’, ‘Happy’, ‘Unhappy’, etc.\\n2. Grades: A, B, C, etc.\\n3. Hardness of Metal: ‘Very Hard’, ‘Hard’, ‘Soft’, etc.\\nLike nominal data, basic counting is possible for ordinal\\ndata. Hence, the mode can be identified. Since ordering is\\npossible in case of ordinal data, median, and quartiles can be\\nidentified in addition. Mean can still not be calculated.\\nQuantitative data relates to information about the quantity\\nof an object – hence it can be measured. For example, if we\\nconsider the attribute ‘marks’, it can be measured using a scale\\nof measurement. Quantitative data is also termed as numeric\\ndata. There are two types of quantitative data:\\n1. Interval data\\n2. Ratio data\\nInterval data is numeric data for which not only the order\\nis known, but the exact difference between values is also\\nknown. An ideal example of interval data is Celsius\\ntemperature. The difference between each value remains the\\nsame in Celsius temperature. For example, the difference\\nbetween 12°C and 18°C degrees is measurable and is 6°C as in'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 88, 'page_label': '89'}, page_content='the case of difference between 15.5°C and 21.5°C. Other\\nexamples include date, time, etc.\\nFor interval data, mathematical operations such as addition\\nand subtraction are possible. For that reason, for interval data,\\nthe central tendency can be measured by mean, median, or\\nmode. Standard deviation can also be calculated.\\nHowever, interval data do not have something called a ‘true\\nzero’ value. For example, there is nothing called ‘0\\ntemperature’ or ‘no temperature’. Hence, only addition and\\nsubtraction applies for interval data. The ratio cannot be\\napplied. This means, we can say a temperature of 40°C is\\nequal to the temperature of 20°C + temperature of 20°C.\\nHowever, we cannot say the temperature of 40°C means it is\\ntwice as hot as in temperature of 20°C.\\nRatio data represents numeric data for which exact value\\ncan be measured. Absolute zero is available for ratio data.\\nAlso, these variables can be added, subtracted, multiplied, or\\ndivided. The central tendency can be measured by mean,\\nmedian, or mode and methods of dispersion such as standard\\ndeviation. Examples of ratio data include height, weight, age,\\nsalary, etc.\\nFigure 2.4 gives a summarized view of different types of\\ndata that we may find in a typical machine learning problem.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 89, 'page_label': '90'}, page_content='FIG. 2.4 Types of data\\nApart from the approach detailed above, attributes can also\\nbe categorized into types based on a number of values that can\\nbe assigned. The attributes can be either discrete or continuous\\nbased on this factor.\\nDiscrete attributes can assume a finite or countably infinite\\nnumber of values. Nominal attributes such as roll number,\\nstreet number, pin code, etc. can have a finite number of\\nvalues whereas numeric attributes such as count, rank of\\nstudents, etc. can have countably infinite values. A special\\ntype of discrete attribute which can assume two values only is\\ncalled binary attribute. Examples of binary attribute include\\nmale/ female, positive/negative, yes/no, etc.\\nContinuous attributes can assume any possible value which\\nis a real number. Examples of continuous attribute include\\nlength, height, weight, price, etc.\\nNote:\\nIn general, nominal and ordinal attributes are discrete. On\\nthe other hand, interval and ratio attributes are continuous,'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 90, 'page_label': '91'}, page_content='barring a few exceptions, e.g. ‘count’ attribute.\\n2.4 EXPLORING STRUCTURE OF DATA\\nBy now, we understand that in machine learning, we come\\nacross two basic data types – numeric and categorical. With\\nthis context in mind, we can delve deeper into understanding a\\ndata set. We need to understand that in a data set, which of the\\nattributes are numeric and which are categorical in nature. This\\nis because, the approach of exploring numeric data is different\\nthan the approach of exploring categorical data. In case of a\\nstandard data set, we may have the data dictionary available\\nfor reference. Data dictionary is a metadata repository, i.e. the\\nrepository of all information related to the structure of each\\ndata element contained in the data set. The data dictionary\\ngives detailed information on each of the attributes – the\\ndescription as well as the data type and other relevant details.\\nIn case the data dictionary is not available, we need to use\\nstandard library function of the machine learning tool that we\\nare using and get the details. For the time being, let us move\\nahead with a standard data set from UCI machine learning\\nrepository.\\nDid you know?\\nUniversity of California, Irvine (UCI) Machine Learning\\nRepository (http:// archive.ics.uci.edu/ml/index.php) is a\\ncollection of 400+ data sets which serve as benchmarks for\\nresearchers and practitioners in the machine learning\\ncommunity.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 91, 'page_label': '92'}, page_content='The data set that we take as a reference is the Auto MPG\\ndata set available in the UCI repository. Figure 2.5 is a\\nsnapshot of the first few rows of the data set.\\nFIG. 2.5 Auto MPG data set\\nAs is quite evident from the data, the attributes such as\\n‘mpg’, ‘cylinders’, ‘displacement’, ‘horsepower’, ‘weight’,\\n‘acceleration’, ‘model year’, and ‘origin’ are all numeric. Out\\nof these attributes, ‘cylinders’, ‘model year’, and ‘origin’ are'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 92, 'page_label': '93'}, page_content='discrete in nature as the only finite number of values can be\\nassumed by these attributes. The remaining of the numeric\\nattributes, i.e. ‘mpg’, ‘displacement’, ‘horsepower’, ‘weight’,\\nand ‘acceleration’ can assume any real value.\\nNote:\\nSince the attributes ‘cylinders’ or ‘origin’ have a small\\nnumber of possible values, one may prefer to treat it as a\\ncategorical or qualitative attribute and explore in that way.\\nAnyways, we will treat these attributes as numeric or\\nquantitative as we are trying to show data exploration and\\nrelated nuances in this section.\\nHence, these attributes are continuous in nature. The only\\nremaining attribute ‘car name’ is of type categorical, or more\\nspecifically nominal. This data set is regarding prediction of\\nfuel consumption in miles per gallon, i.e. the numeric attribute\\n‘mpg’ is the target attribute.\\nWith this understanding of the data set attributes, we can\\nstart exploring the numeric and categorical attributes\\nseparately.\\n2.4.1 Exploring numerical data\\nThere are two most effective mathematical plots to explore\\nnumerical data – box plot and histogram. We will explore all\\nthese plots one by one, starting with the most critical one,\\nwhich is the box plot.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 93, 'page_label': '94'}, page_content='2.4.1.1 Understanding central tendency\\nTo understand the nature of numeric variables, we can apply\\nthe measures of central tendency of data, i.e. mean and\\nmedian. In statistics, measures of central tendency help us\\nunderstand the central point of a set of data. Mean, by\\ndefinition, is a sum of all data values divided by the count of\\ndata elements. For example, mean of a set of observations –\\n21, 89, 34, 67, and 96 is calculated as below.\\nIf the above set of numbers represents marks of 5 students\\nin a class, the mean marks, or the falling in the middle of the\\nrange is 61.4.\\nMedian, on contrary, is the value of the element appearing\\nin the middle of an ordered list of data elements. If we\\nconsider the above 5 data elements, the ordered list would be –\\n21, 34, 67, 89, and 96. Since there are 5 data elements, the 3rd\\nelement in the ordered list is considered as the median. Hence,\\nthe median value of this set of data is 67.\\nThere might be a natural curiosity to understand why two\\nmeasures of central tendency are reviewed. The reason is mean\\nand median are impacted differently by data values appearing\\nat the beginning or at the end of the range. Mean being\\ncalculated from the cumulative sum of data values, is impacted\\nif too many data elements are having values closer to the far\\nend of the range, i.e. close to the maximum or minimum\\nvalues. It is especially sensitive to outliers, i.e. the values\\nwhich are unusually high or low, compared to the other values.\\nMean is likely to get shifted drastically even due to the'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 94, 'page_label': '95'}, page_content='presence of a small number of outliers. If we observe that for\\ncertain attributes the deviation between values of mean and\\nmedian are quite high, we should investigate those attributes\\nfurther and try to find out the root cause along with the need\\nfor remediation.\\nSo, in the context of the Auto MPG data set, let’s try to find\\nout for each of the numeric attributes the values of mean and\\nmedian. We can also find out if the deviation between these\\nvalues is large. In Figure 2.6, the comparison between mean\\nand median for all the attributes has been shown. We can see\\nthat for the attributes such as ‘mpg’, ‘weight’, ‘acceleration’,\\nand ‘model.year’ the deviation between mean and median is\\nnot significant which means the chance of these attributes\\nhaving too many outlier values is less. However, the deviation\\nis significant for the attributes ‘cylinders’, ‘displacement’ and\\n‘origin’. So, we need to further drill down and look at some\\nmore statistics for these attributes. Also, there is some problem\\nin the values of the attribute ‘horsepower’ because of which\\nthe mean and median calculation is not possible.\\nFIG. 2.6 Mean vs. Median for Auto MPG\\nWith a bit of investigation, we can find out that the problem\\nis occurring because of the 6 data elements, as shown in Figure\\n2.7, do not have value for the attribute ‘horsepower’.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 95, 'page_label': '96'}, page_content='FIG. 2.7 Missing values of attribute ‘horsepower’ in Auto MPG\\nFor that reason, the attribute ‘horsepower’ is not treated as a\\nnumeric. That’s why the operations applicable on numeric\\nvariables, like mean or median, are failing. So we have to first\\nremediate the missing values of the attribute ‘horsepower’\\nbefore being able to do any kind of exploration. However, we\\nwill cover the approach of remediation of missing values a\\nlittle later.\\n2.4.1.2 Understanding data spread\\nNow that we have explored the central tendency of the\\ndifferent numeric attributes, we have a clear idea of which\\nattributes have a large deviation between mean and median.\\nLet’s look closely at those attributes. To drill down more, we\\nneed to look at the entire range of values of the attributes,\\nthough not at the level of data elements as that may be too vast\\nto review manually. So we will take a granular view of the\\ndata spread in the form of\\n1. Dispersion of data\\n2. Position of the different data values\\n2.4.1.2.1 Measuring data dispersion'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 96, 'page_label': '97'}, page_content='Consider the data values of two attributes\\n1. Attribute 1 values : 44, 46, 48, 45, and 47\\n2. Attribute 2 values : 34, 46, 59, 39, and 52\\nBoth the set of values have a mean and median of 46.\\nHowever, the first set of values that is of attribute 1 is more\\nconcentrated or clustered around the mean/median value\\nwhereas the second set of values of attribute 2 is quite spread\\nout or dispersed. To measure the extent of dispersion of a data,\\nor to find out how much the different values of a data are\\nspread out, the variance of the data is measured. The variance\\nof a data is measured using the formula given below:\\nVariance \\n , where x is the variable or\\nattribute whose variance is to be measured and n is the number\\nof observations or values of variable x.\\nStandard deviation of a data is measured as follows:\\nLarger value of variance or standard deviation indicates\\nmore dispersion in the data and vice versa. In the above\\nexample, let’s calculate the variance of attribute 1 and that of\\nattribute 2. For attribute 1,'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 97, 'page_label': '98'}, page_content='For attribute 2,\\nSo it is quite clear from the measure that attribute 1 values\\nare quite concentrated around the mean while attribute 2\\nvalues are extremely spread out. Since this data was small, a\\nvisual inspection and understanding were possible and that\\nmatches with the measured value.\\n2.4.1.2.2 Measuring data value position\\nWhen the data values of an attribute are arranged in an\\nincreasing order, we have seen earlier that median gives the\\ncentral data value, which divides the entire data set into two\\nhalves. Similarly, if the first half of the data is divided into two\\nhalves so that each half consists of one-quarter of the data set,'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 98, 'page_label': '99'}, page_content='then that median of the first half is known as first quartile or\\nQ. In the same way, if the second half of the data is divided\\ninto two halves, then that median of the second half is known\\nas third quartile or Q. The overall median is also known as\\nsecond quartile or Q. So, any data set has five values -\\nminimum, first quartile (Q1), median (Q2), third quartile (Q3),\\nand maximum.\\nLet’s review these values for the attributes ‘cylinders’,\\n‘displacement’, and ‘origin’. Figure 2.8 captures a summary of\\nthe range of statistics for the attributes. If we take the example\\nof the attribute ‘displacement’, we can see that the difference\\nbetween minimum value and Q1 is 36.2 and the difference\\nbetween Q1 and median is 44.3. On the contrary, the\\ndifference between median and Q3 is 113.5 and Q3 and the\\nmaximum value is 193. In other words, the larger values are\\nmore spread out than the smaller ones. This helps in\\nunderstanding why the value of mean is much higher than that\\nof the median for the attribute ‘displacement’. Similarly, in\\ncase of attribute ‘cylinders’, we can observe that the difference\\nbetween minimum value and median is 1 whereas the\\ndifference between median and the maximum value is 4. For\\nthe attribute ‘origin’, the difference between minimum value\\nand median is 0 whereas the difference between median and\\nthe maximum value is 2.\\nFIG. 2.8 Attribute value drill-down for Auto MPG\\n1\\n3\\n2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 99, 'page_label': '100'}, page_content='Note:\\nQuantiles refer to specific points in a data set which divide\\nthe data set into equal parts or equally sized quantities.\\nThere are specific variants of quantile, the one dividing\\ndata set into four parts being termed as quartile. Another\\nsuch popular variant is percentile, which divides the data\\nset into 100 parts.\\nHowever, we still cannot ascertain whether there is any\\noutlier present in the data. For that, we can better adopt some\\nmeans to visualize the data. Box plot is an excellent\\nvisualization medium for numeric data.\\n2.4.2 Plotting and exploring numerical data\\n2.4.2.1 Box plots\\nNow that we have a fairly clear understanding of the data set\\nattributes in terms of spread and central tendency, let’s try to\\nmake an attempt to visualize the whole thing as a box-plot. A\\nbox plot is an extremely effective mechanism to get a one-shot\\nview and understand the nature of the data. But before we get\\nto review the box plot for different attributes of Auto MPG\\ndata set, let’s first try to understand a box plot in general and\\nthe interpretation of different aspects in a box plot. As we can\\nsee in Figure 2.9, the box plot (also called box and whisker\\nplot) gives a standard visualization of the five-number\\nsummary statistics of a data, namely minimum, first quartile\\n(Q1), median (Q2), third quartile (Q3), and maximum. Below\\nis a detailed interpretation of a box plot.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 100, 'page_label': '101'}, page_content='The central rectangle or the box spans from first to third quartile (i.e. Q1 to\\nQ3), thus giving the inter-quartile range (IQR).\\nMedian is given by the line or band within the box.\\nThe lower whisker extends up to 1.5 times of the inter-quartile range (or\\nIQR) from the bottom of the box, i.e. the first quartile or Q1. However, the\\nactual length of the lower whisker depends on the lowest data value that falls\\nwithin (Q1 − 1.5 times of IQR). Let’s try to understand this with an example.\\nSay for a specific set of data, Q1 = 73, median = 76 and Q3 = 79. Hence,\\nIQR will be 6 (i.e. Q3 – Q1).So, lower whisker can extend maximum till (Q1\\n– 1.5 × IQR) = 73 – 1.5 × 6 = 64. However, say there are lower range data\\nvalues such as 70, 63, and 60. So, the lower whisker will come at 70 as this\\nis the lowest data value larger than 64.\\nThe upper whisker extends up to 1.5 as times of the inter-quartile range (or\\nIQR) from the top of the box, i.e. the third quartile or Q3. Similar to lower\\nwhisker, the actual length of the upper whisker will also depend on the\\nhighest data value that falls within (Q3 + 1.5 times of IQR). Let’s try to\\nunderstand this with an example. For the same set of data mentioned in the\\nabove point, upper whisker can extend maximum till (Q3 + 1.5 × IQR) = 79\\n+ 1.5 × 6 = 88. If there is higher range of data values like 82, 84, and 89. So,\\nthe upper whisker will come at 84 as this is the highest data value lower than\\n88.\\nThe data values coming beyond the lower or upper whiskers are the ones\\nwhich are of unusually low or high values respectively. These are the\\noutliers, which may deserve special consideration.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 101, 'page_label': '102'}, page_content='FIG. 2.9 Box plot\\nNote:\\nThere are different variants of box plots. The one covered\\nabove is the Tukey box plot. Famous mathematician John\\nW. Tukey introduced this type of box plot in 1969.\\nLet’s visualize the box plot for the three attributes -\\n‘cylinders’, ‘displacement’, and ‘origin’. We will also review\\nthe box plot of another attribute in which the deviation\\nbetween mean and median is very little and see what the basic'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 102, 'page_label': '103'}, page_content='difference in the respective box plots is. Figure 2.10 presents\\nthe respective box plots.\\nFIG. 2.10 Box plot of Auto MPG attributes\\n2.4.2.1.1 Analysing box plot for ‘cylinders’\\nThe box plot for attribute ‘cylinders’ looks pretty weird in\\nshape. The upper whisker is missing, the band for median falls\\nat the bottom of the box, even the lower whisker is pretty\\nsmall compared to the length of the box! Is everything right?\\nThe answer is a big YES, and you can figure it out if you\\ndelve a little deeper into the actual data values of the attribute.\\nThe attribute ‘cylinders’ is discrete in nature having values\\nfrom 3 to 8. Table 2.2 captures the frequency and cumulative\\nfrequency of it.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 103, 'page_label': '104'}, page_content='Table 2.2 Frequency of “Cylinders” Attribute\\nAs can be observed in the table, the frequency is extremely\\nhigh for data value 4. Two other data values where the\\nfrequency is quite high are 6 and 8. So now if we try to find\\nthe quartiles, since the total frequency is 398, the first quartile\\n(Q1), median (Q2), and third quartile (Q3) will be at a\\ncumulative frequency 99.5 (i.e. average of 99th and 100th\\nobservation), 199 and 298.5 (i.e. average of 298th and 299th\\nobservation), respectively. This way Q1 = 4, median = 4 and\\nQ3 = 8. Since there is no data value beyond 8, there is no\\nupper whisker. Also, since both Q1 and median are 4, the band\\nfor median falls on the bottom of the box. Same way, though\\nthe lower whisker could have extended till -2 (Q1 – 1.5 × IQR\\n= 4 – 1.5 × 4 = –2), in reality, there is no data value lower than\\n3. Hence, the lower whisker is also short. In any case, a value\\nof cylinders less than 1 is not possible.\\n2.4.2.1.2 Analysing box plot for ‘origin’\\nLike the box plot for attribute ‘cylinders’, the box plot for\\nattribute ‘cylinders’ also looks pretty weird in shape. Here the\\nlower whisker is missing and the band for median falls at the\\nbottom of the box! Let’s verify if everything right?\\nJust like the attribute ‘cylinders’, attribute ‘origin’ is\\ndiscrete in nature having values from 1 to 3. Table 2.3 captures'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 104, 'page_label': '105'}, page_content='the frequency and cumulative frequency (i.e. a summation of\\nfrequencies of all previous intervals) of it.\\n \\nTable 2.3 Frequency of “Origin” Attribute\\nAs can be observed in the table, the frequency is extremely\\nhigh for data value 1. Since the total frequency is 398, the first\\nquartile (Q1), median (Q2), and third quartile (Q3) will be at a\\ncumulative frequency 99.5 (i.e. average of 99th and 100th\\nobservation), 199 and 298.5 (i.e. average of 298th and 299th\\nobservation), respectively. This way Q1 = 1, median = 1, and\\nQ3 = 2. Since Q1 and median are same in value, the band for\\nmedian falls on the bottom of the box. There is no data value\\nlower than Q1. Hence, the lower whisker is missing.\\n2.4.2.1.3 Analysing box plot for ‘displacement’\\nThe box plot for the attribute ‘displacement’ looks better than\\nthe previous box plots. However, still, there are few small\\nabnormalities, the cause of which needs to be reviewed.\\nFirstly, the lower whisker is much smaller than an upper\\nwhisker. Also, the band for median is closer to the bottom of\\nthe box.\\nLet’s take a closer look at the summary data of the attribute\\n‘displacement’. The value of first quartile, Q1 = 104.2, median\\n= 148.5, and third quartile, Q3 = 262. Since (median – Q1) ='),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 105, 'page_label': '106'}, page_content='44.3 is greater than (Q3 – median) = 113.5, the band for the\\nmedian is closer to the bottom of the box (which represents\\nQ1). The value of IQR, in this case, is 157.8. So the lower\\nwhisker can be 1.5 times 157.8 less than Q1. But minimum\\ndata value for the attribute ‘displacement’ is 68. So, the lower\\nwhisker at 15% [(Q1 – minimum)/1.5 × IQR = (104.2 – 68) /\\n(1.5 × 157.8) = 15%] of the permissible length. On the other\\nhand, the maximum data value is 455. So the upper whisker is\\n81% [(maximum – Q3)/1.5 × IQR = (455 – 262) / (1.5 ×\\n157.8) = 81%] of the permissible length. This is why the upper\\nwhisker is much longer than the lower whisker.\\n2.4.2.1.4 Analysing box plot for ‘model Year’\\nThe box plot for the attribute ‘model. year’ looks perfect. Let’s\\nvalidate is it really what expected to be.\\nFor the attribute ‘model.year’:\\n   First quartile, Q1 = 73\\n   Median, Q2 = 76\\n   Third quartile, Q3 = 79\\nSo, the difference between median and Q1 is exactly equal\\nto Q3 and median (both are 3). That is why the band for the\\nmedian is exactly equidistant from the bottom and top of the\\nbox.\\n \\nI QR = Q3 9 Q1 = 7 9 - 7 3 = 6'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 106, 'page_label': '107'}, page_content='Difference between Q1 and minimum data value (i.e. 70) is\\nalso same as maximum data value (i.e. 82) and Q3 (both are\\n3). So both lower and upper whiskers are expected to be of the\\nsame size which is 33% [3 / (1.5 × 6)] of the permissible\\nlength.\\n2.4.2.2 Histogram\\nHistogram is another plot which helps in effective\\nvisualization of numeric attributes. It helps in understanding\\nthe distribution of a numeric data into series of intervals, also\\ntermed as ‘bins’. The important difference between histogram\\nand box plot is\\nThe focus of histogram is to plot ranges of data values (acting as ‘bins’), the\\nnumber of data elements in each range will depend on the data distribution.\\nBased on that, the size of each bar corresponding to the different ranges\\nwill vary.\\nThe focus of box plot is to divide the data elements in a data set into four\\nequal  portions, such that each portion contains an equal number of data\\nelements.\\nHistograms might be of different shapes depending on the\\nnature of the data, e.g. skewness. Figure 2.11 provides a\\ndepiction of different shapes of the histogram that are\\ngenerally created. These patterns give us a quick\\nunderstanding of the data and thus act as a great data\\nexploration tool.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 107, 'page_label': '108'}, page_content='FIG. 2.11 General Histogram shapes\\nLet’s now examine the histograms for the different attributes\\nof Auto MPG data set presented in Figure 2.12. The\\nhistograms for ‘mpg’ and ‘weight’ are right-skewed. The\\nhistogram for ‘acceleration’ is symmetric and unimodal,\\nwhereas the one for ‘model.year’ is symmetric and uniform.\\nFor the remaining attributes, histograms are multimodal in\\nnature.\\nNow let’s dig deep into one of the histograms, say the one\\nfor the attribute ‘acceleration’. The histogram is composed of a\\nnumber of bars, one bar appearing for each of the ‘bins’. The\\nheight of the bar reflects the total count of data elements\\nwhose value falls within the specific bin value, or the\\nfrequency. Talking in context of the histogram for acceleration,\\neach ‘bin’ represents an acceleration value interval of 2 units.\\nSo the second bin, e.g., reflects acceleration value of 10 to 12\\nunits. The corresponding bar chart height reflects the count of'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 108, 'page_label': '109'}, page_content='all data elements whose value lies between 10 and 12 units.\\nAlso, it is evident from the histogram that it spans over the\\nacceleration value of 8 to 26 units. The frequency of data\\nelements corresponding to the bins first keep on increasing, till\\nit reaches the bin of range 14 to 16 units. At this range, the bar\\nis tallest in size. So we can conclude that a maximum number\\nof data elements fall within this range. After this range, the bar\\nsize starts decreasing till the end of the whole range at the\\nacceleration value of 26 units.\\nPlease note that when the histogram is uniform, as in the\\ncase of attribute ‘model. year’, it gives a hint that all values are\\nequally likely to occur.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 109, 'page_label': '110'}, page_content='FIG. 2.12 Histogram Auto MPG attributes\\n2.4.3 Exploring categorical data\\nWe have seen there are multiple ways to explore numeric data.\\nHowever, there are not many options for exploring categorical\\ndata. In the Auto MPG data set, attribute ‘car.name’ is\\ncategorical in nature. Also, as we discussed earlier, we may\\nconsider ‘cylinders’ as a categorical variable instead of a\\nnumeric variable.\\nThe first summary which we may be interested in noting is\\nhow many unique names are there for the attribute ‘car name’'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 110, 'page_label': '111'}, page_content='or how many unique values are there for ‘cylinders’ attribute.\\nWe can get this as follows:\\nFor attribute ‘car name’\\n1. Chevrolet chevelle malibu\\n2. Buick skylark 320\\n3. Plymouth satellite\\n4. Amc rebel sst\\n5. Ford torino\\n6. Ford galaxie 500\\n7. Chevrolet impala\\n8. Plymouth fury iii\\n9. Pontiac catalina\\n10. Amc ambassador dpl\\nFor attribute ‘cylinders’\\n8 4 6 3 5\\nWe may also look for a little more details and want to get a\\ntable consisting the categories of the attribute and count of the\\ndata elements falling into that category. Tables 2.4 and 2.5\\ncontain these details.\\nFor attribute ‘car name’\\n \\nTable 2.4 Count of Categories for ‘car name’ Attribute'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 111, 'page_label': '112'}, page_content='For attribute “cylinders”\\n \\nTable 2.5 Count of Categories for ‘Cylinders’ Attribute\\nIn the same way, we may also be interested to know the\\nproportion (or percentage) of count of data elements belonging\\nto a category. Say, e.g., for the attributes ‘cylinders’, the\\nproportion of data elements belonging to the category 4 is 204\\n÷ 398 = 0.513, i.e. 51.3%. Tables 2.6 and 2.7 contain the\\nsummarization of the categorical attributes by proportion of\\ndata elements.\\nFor attribute ‘car name’\\n \\nTable 2.6 Proportion of Categories for ‘“Cylinders’ Attribute\\nFor attribute ‘cylinders’'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 112, 'page_label': '113'}, page_content='Table 2.7 Proportion of Categories for “Cylinders” Attribute\\nLast but not the least, as we have read in the earlier section\\non types of data, statistical measure “mode” is applicable on\\ncategorical attributes. As we know, like mean and median,\\nmode is also a statistical measure for central tendency of a\\ndata. Mode of a data is the data value which appears most\\noften. In context of categorical attribute, it is the category\\nwhich has highest number of data values. Since mean and\\nmedian cannot be applied for categorical variables, mode is the\\nsole measure of central tendency.\\nLet’s try to find out the mode for the attributes ‘car name’\\nand ‘cylinders’. For cylinders, since the number of categories\\nis less and we have the entire table listed above, we can see\\nthat the mode is 4, as that is the data value for which frequency\\nis highest. More than 50% of data elements belong to the\\ncategory 4. However, it is not so evident for the attribute ‘car\\nname’ from the information given above. When we probe and\\ntry to find the mode, it is found to be category ‘ford pinto’ for\\nwhich frequency is of highest value 6.\\nAn attribute may have one or more modes. Frequency\\ndistribution of an attribute having single mode is called\\n‘unimodal’, two modes are called ‘bimodal’ and multiple\\nmodes are called ‘multimodal’.\\n2.4.4 Exploring relationship between variables'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 113, 'page_label': '114'}, page_content='Till now we have been exploring single attributes in isolation.\\nOne more important angle of data exploration is to explore\\nrelationship between attributes. There are multiple plots to\\nenable us explore the relationship between variables. The basic\\nand most commonly used plot is scatter plot.\\n2.4.4.1 Scatter plot\\nA scatter plot helps in visualizing bivariate relationships, i.e.\\nrelationship between two variables. It is a two-dimensional\\nplot in which points or dots are drawn on coordinates provided\\nby values of the attributes. For example, in a data set there are\\ntwo attributes – attr_1 and attr_2. We want to understand the\\nrelationship between two attributes, i.e. with a change in value\\nof one attribute, say attr_1, how does the value of the other\\nattribute, say attr_2, changes. We can draw a scatter plot, with\\nattr_1 mapped to x-axis and attr_2 mapped in y-axis. So, every\\npoint in the plot will have value of attr_1 in the x-coordinate\\nand value of attr_2 in the y-coordinate. As in a two-\\ndimensional plot, attr_1 is said to be the independent variable\\nand attr_2 as the dependent variable.\\nLet’s take a real example in this context. In the data set Auto\\nMPG, there is expected to be some relation between the\\nattributes ‘displacement’ and ‘mpg’. Let’s try to verify our\\nintuition using the scatter plot of ‘displacement’ and ‘mpg’.\\nLet’s map ‘displacement’ as the x-coordinate and ‘mpg’ as the\\ny-coordinate. The scatter plot comes as in Figure 2.13.\\nAs is evident in the scatter plot, there is a definite relation\\nbetween the two variables. The value of ‘mpg’ seems to\\nsteadily decrease with the increase in the value of\\n‘displacement’. It may come in our mind that what is the\\nextent of relationship? Well, it can be reviewed by calculating'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 114, 'page_label': '115'}, page_content='the correlation between the variables. Refer to chapter 5 if you\\nwant to find more about correlation and how to calculate it.\\nOne more interesting fact to notice is that there are certain data\\nvalues which stand-out of the others. For example, there is one\\ndata element which has a mpg of 37 for a displacement of 250.\\nThis record is completely different from other data elements\\nhaving similar displacement value but mpg value in the range\\nof 15 to 25. This gives an indication that of presence of outlier\\ndata values.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 115, 'page_label': '116'}, page_content='FIG. 2.13 Scatter plot of ‘displacement’ and ‘mpg’\\nIn Figure 2.14, the pair wise relationship among the features\\n– ‘mpg’, ‘displacement’, ‘horsepower’, ‘weight’, and\\n‘acceleration’ have been captured. As you can see, in most of\\nthe cases, there is a significant relationship between the\\nattribute pairs. However, in some cases, e.g. between attributes\\n‘weight’ and ‘acceleration’, the relationship doesn’t seem to be\\nvery strong.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 116, 'page_label': '117'}, page_content='FIG. 2.14 Pair wise scatter plot between different attributes of Auto MPG\\n2.4.4.2 Two-way cross-tabulations\\nTwo-way cross-tabulations (also called cross-tab or\\ncontingency table) are used to understand the relationship of\\ntwo categorical attributes in a concise way. It has a matrix\\nformat that presents a summarized view of the bivariate\\nfrequency distribution. A cross-tab, very much like a scatter\\nplot, helps to understand how much the data values of one\\nattribute changes with the change in data values of another'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 117, 'page_label': '118'}, page_content='attribute. Let’s try to see with examples, in context of the Auto\\nMPG data set.\\nLet’s assume the attributes ‘cylinders’, ‘model.year’, and\\n‘origin’ as categorical and try to examine the variation of one\\nwith respect to the other. As we understand, attribute\\n‘cylinders’ reflects the number of cylinders in a car and\\nassumes values 3, 4, 5, 6, and 8. Attribute ‘model.year’\\ncaptures the model year of each of the car and ‘origin’ gives\\nthe region of the car, the values for origin 1, 2, and 3\\ncorresponding to North America, Europe, and Asia. Below are\\nthe cross-tabs. Let’s try to understand what information they\\nactually provide.\\nThe first cross-tab, i.e. the one showing relationship\\nbetween attributes ‘model. year’ and ‘origin’ help us\\nunderstand the number of vehicles per year in each of the\\nregions North America, Europe, and Asia. Looking at it in\\nanother way, we can get the count of vehicles per region over\\nthe different years. All these are in the context of the sample\\ndata given in the Auto MPG data set.\\nMoving to the second cross-tab, it gives the number of 3, 4,\\n5, 6, or 8 cylinder cars in every region present in the sample\\ndata set. The last cross-tab presents the number of 3, 4, 5, 6, or\\n8 cylinder cars every year.\\nWe may also want to create cross-tabs with a more\\nsummarized view like have a cross-tab giving a number of cars\\nhaving 4 or less cylinders and more than 4 cylinders in each\\nregion or by the years. This can be done by rolling up data\\nvalues by the attribute ‘cylinder’. Tables 2.8–2.10 present\\ncross-tabs for different attribute combinations.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 118, 'page_label': '119'}, page_content='‘Model year’ vs. ‘origin’\\n \\nTable 2.8 Cross-tab for ‘Model year’ vs. ‘Origin’\\n‘Cylinders’ vs. ‘Origin’\\n \\nTable 2.9 Cross-tab for ‘Cylinders’ vs. ‘Origin’\\n‘Cylinders’ vs. ‘Model year’'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 119, 'page_label': '120'}, page_content='Table 2.10 Cross-tab for ‘Cylinders’ vs. ‘Model year’\\n2.5 DATA QUALITY AND REMEDIATION\\n2.5.1 Data quality\\nSuccess of machine learning depends largely on the quality of\\ndata. A data which has the right quality helps to achieve better\\nprediction accuracy, in case of supervised learning. However,\\nit is not realistic to expect that the data will be flawless. We\\nhave already come across at least two types of problems:\\n1. Certain data elements without a value or data with a missing value.\\n2. Data elements having value surprisingly different from the other\\nelements, which we term as outliers.\\nThere are multiple factors which lead to these data quality\\nissues. Following are some of them:\\nIncorrect sample set selection: The data may not reflect normal or regular\\nquality due to incorrect selection of sample set. For example, if we are\\nselecting a sample set of sales transactions from a festive period and trying to\\nuse that data to predict sales in future. In this case, the prediction will be far\\napart from the actual scenario, just because the sample set has been selected\\nin a wrong time. Similarly, if we are trying to predict poll results using a\\ntraining data which doesn’t comprise of a right mix of voters from different\\nsegments such as age, sex, ethnic diversities, etc., the prediction is bound to\\nbe a failure. It may also happen due to incorrect sample size. For example, a\\nsample of small size may not be able to capture all aspects or information\\nneeded for right learning of the model.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 120, 'page_label': '121'}, page_content='Errors in data collection: resulting in outliers and missing values\\nIn many cases, a person or group of persons are responsible for the\\ncollection of data to be used in a learning activity. In this manual\\nprocess, there is the possibility of wrongly recording data either in\\nterms of value (say 20.67 is wrongly recorded as 206.7 or 2.067) or in\\nterms of a unit of measurement (say cm. is wrongly recorded as m. or\\nmm.). This may result in data elements which have abnormally high\\nor low value from other elements. Such records are termed as outliers.\\nIt may also happen that the data is not recorded at all. In case of a\\nsurvey conducted to collect data, it is all the more possible as survey\\nresponders may choose not to respond to a certain question. So the\\ndata value for that data element in that responder’s record is missing.\\n2.5.2 Data remediation\\nThe issues in data quality, as mentioned above, need to be\\nremediated, if the right amount of efficiency has to be\\nachieved in the learning activity. Out of the two major areas\\nmentioned above, the first one can be remedied by proper\\nsampling technique. This is a completely different area –\\ncovered as a specialized subject area in statistics. We will not\\ncover that in this book. However, human errors are bound to\\nhappen, no matter whatever checks and balances we put in.\\nHence, proper remedial steps need to be taken for the second\\narea mentioned above. We will discuss how to handle outliers\\nand missing values.\\n2.5.2.1 Handling outliers\\nOutliers are data elements with an abnormally high value\\nwhich may impact prediction accuracy, especially in\\nregression models. Once the outliers are identified and the\\ndecision has been taken to amend those values, you may\\nconsider one of the following approaches. However, if the\\noutliers are natural, i.e. the value of the data element is\\nsurprisingly high or low because of a valid reason, then we\\nshould not amend it.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 121, 'page_label': '122'}, page_content='Remove outliers: If the number of records which are outliers is not many, a\\nsimple approach may be to remove them.\\nImputation: One other way is to impute the value with mean or median or\\nmode. The value of the most similar data element may also be used for\\nimputation.\\nCapping: For values that lie outside the 1.5|×| IQR limits, we can cap them\\nby replacing those observations below the lower limit with the value of 5th\\npercentile and those that lie above the upper limit, with the value of 95th\\npercentile.\\nIf there is a significant number of outliers, they should be\\ntreated separately in the statistical model. In that case, the\\ngroups should be treated as two different groups, the model\\nshould be built for both groups and then the output can be\\ncombined.\\n2.5.2.2 Handling missing values\\nIn a data set, one or more data elements may have missing\\nvalues in multiple records. As discussed above, it can be\\ncaused by omission on part of the surveyor or a person who is\\ncollecting sample data or by the responder, primarily due to\\nhis/her unwillingness to respond or lack of understanding\\nneeded to provide a response. It may happen that a specific\\nquestion (based on which the value of a data element\\noriginates) is not applicable to a person or object with respect\\nto which data is collected. There are multiple strategies to\\nhandle missing value of data elements. Some of those\\nstrategies have been discussed below.\\n2.5.2.2.1 Eliminate records having a missing value of data elements\\nIn case the proportion of data elements having missing values\\nis within a tolerable limit, a simple but effective approach is to\\nremove the records having such data elements. This is possible\\nif the quantum of data left after removing the data elements\\nhaving missing values is sizeable.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 122, 'page_label': '123'}, page_content='In the case of Auto MPG data set, only in 6 out of 398\\nrecords, the value of attribute ‘horsepower’ is missing. If we\\nget rid of those 6 records, we will still have 392 records, which\\nis definitely a substantial number. So, we can very well\\neliminate the records and keep working with the remaining\\ndata set.\\nHowever, this will not be possible if the proportion of\\nrecords having data elements with missing value is really high\\nas that will reduce the power of model because of reduction in\\nthe training data size.\\n2.5.2.2.2 Imputing missing values\\nImputation is a method to assign a value to the data elements\\nhaving missing values. Mean/mode/median is most frequently\\nassigned value. For quantitative attributes, all missing values\\nare imputed with the mean, median, or mode of the remaining\\nvalues under the same attribute. For qualitative attributes, all\\nmissing values are imputed by the mode of all remaining\\nvalues of the same attribute. However, another strategy may be\\nidentify the similar types of observations whose values are\\nknown and use the mean/median/mode of those known values.\\nFor example, in context of the attribute ‘horsepower’ of the\\nAuto MPG data set, since the attribute is quantitative, we take\\na mean or median of the remaining data element values and\\nassign that to all data elements having a missing value. So, we\\nmay assign the mean, which is 104.47 and assign it to all the\\nsix data elements. The other approach is that we can take a\\nsimilarity based mean or median. If we refer to the six\\nobservations with missing values for attribute ‘horsepower’ as\\ndepicted in Table 2.11, ‘cylinders’ is the attribute which is\\nlogically most connected to ‘horsepower’ because with the\\nincrease in number of cylinders of a car, the horsepower of the'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 123, 'page_label': '124'}, page_content='car is expected to increase. So, for five observations, we can\\nuse the mean of data elements of the ‘horsepower’ attribute\\nhaving cylinders = 4; i.e. 78.28 and for one observation which\\nhas cylinders = 6, we can use a similar mean of data elements\\nwith cylinders = 6, i.e. 101.5, to impute value to the missing\\ndata elements.\\n \\nTable 2.11 Missing Values for ‘Horsepower’ Attribute\\n2.5.2.2.3 Estimate missing values\\nIf there are data points similar to the ones with missing\\nattribute values, then the attribute values from those similar\\ndata points can be planted in place of the missing value. For\\nfinding similar data points or observations, distance function\\ncan be used.\\nFor example, let’s assume that the weight of a Russian\\nstudent having age 12 years and height 5 ft. is missing. Then\\nthe weight of any other Russian student having age close to 12\\nyears and height close to 5 ft. can be assigned.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 124, 'page_label': '125'}, page_content='2.6 DATA PRE-PROCESSING\\n2.6.1 Dimensionality reduction\\nTill the end of the 1990s, very few domains were explored\\nwhich included data sets with a high number of attributes or\\nfeatures. In general, the data sets used in machine learning\\nused to be in few 10s. However, in the last two decades, there\\nhas been a rapid advent of computational biology like genome\\nprojects. These projects have produced extremely high-\\ndimensional data sets with 20,000 or more features being very\\ncommon. Also, there has been a wide-spread adoption of\\nsocial networking leading to a need for text classification for\\ncustomer behaviour analysis.\\nHigh-dimensional data sets need a high amount of\\ncomputational space and time. At the same time, not all\\nfeatures are useful – they degrade the performance of machine\\nlearning algorithms. Most of the machine learning algorithms\\nperform better if the dimensionality of data set, i.e. the number\\nof features in the data set, is reduced. Dimensionality\\nreduction helps in reducing irrelevance and redundancy in\\nfeatures. Also, it is easier to understand a model if the number\\nof features involved in the learning activity is less.\\nDimensionality reduction refers to the techniques of\\nreducing the dimensionality of a data set by creating new\\nattributes by combining the original attributes. The most\\ncommon approach for dimensionality reduction is known as\\nPrincipal Component Analysis (PCA). PCA is a statistical\\ntechnique to convert a set of correlated variables into a set of\\ntransformed, uncorrelated variables called principal\\ncomponents. The principal components are a linear\\ncombination of the original variables. They are orthogonal to\\neach other. Since principal components are uncorrelated, they'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 125, 'page_label': '126'}, page_content='capture the maximum amount of variability in the data.\\nHowever, the only challenge is that the original attributes are\\nlost due to the transformation.\\nAnother commonly used technique which is used for\\ndimensionality reduction is Singular Value Decomposition\\n(SVD).\\nMore about these concepts have been discussed in Chapter\\n4.\\n2.6.2 Feature subset selection\\nFeature subset selection or simply called feature selection,\\nboth for supervised as well as unsupervised learning, try to\\nfind out the optimal subset of the entire feature set which\\nsignificantly reduces computational cost without any major\\nimpact on the learning accuracy. It may seem that a feature\\nsubset may lead to loss of useful information as certain\\nfeatures are going to be excluded from the final set of features\\nused for learning. However, for elimination only features\\nwhich are not relevant or redundant are selected.\\nA feature is considered as irrelevant if it plays an\\ninsignificant role (or contributes almost no information) in\\nclassifying or grouping together a set of data instances. All\\nirrelevant features are eliminated while selecting the final\\nfeature subset. A feature is potentially redundant when the\\ninformation contributed by the feature is more or less same as\\none or more other features. Among a group of potentially\\nredundant features, a small number of features can be selected\\nas a part of the final feature subset without causing any\\nnegative impact to learn model accuracy.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 126, 'page_label': '127'}, page_content='There are different ways to select a feature subset. In\\nChapter 4, we will be discussing feature selection in details.\\n2.7 SUMMARY\\nA data set is a collection of related information or records.\\nData can be broadly divided into following two types\\nQualitative data\\nQuantitative data\\nQualitative data provides information about the quality of an object or\\ninformation which cannot be measured. Qualitative data can be further\\nsubdivided into two types as follows:\\nNominal data: has named value\\nOrdinal data: has named value which can be naturally ordered\\nQuantitative data relates to information about the quantity of an object –\\nhence it can be measured. There are two types of quantitative data:\\nInterval data: numeric data for which the exact difference between\\nvalues is known. However, such data do not have something called a\\n‘true zero’ value.\\nRatio data: numeric data for which exact value can be measured and\\nabsolute zero is available.\\nMeasures of central tendency help to understand the central point of a set of\\ndata. Standard measures of central tendency of data are mean, median, and\\nmode.\\nDetailed view of the data spread is available in the form of\\nDispersion of data:extent of dispersion of a data is measured by\\nvariance\\nRelated to the position of the different data values there are five\\nvalues:minimum, first quartile (Q1), median (Q2), third quartile (Q3),\\nand maximum\\nExploration of numerical data can be best done using box plots and\\nhistograms.\\nOptions for exploration of categorical data are very limited.\\nFor exploring relations between variables, scatter-plots and two-way cross-\\ntabulations can be effectively used.\\nSuccess of machine learning depends largely on the quality of data. Two\\ncommon types of data issue are:\\nData with a missing value\\nData values which are surprisingly different termed as outliers\\nHigh-dimensional data sets need a high amount of computational space and\\ntime. Most of the machine learning algorithms perform better if the\\ndimensionality of data set is reduced.\\nSome popular dimensionality reduction techniques are PCA, SVD, and\\nfeature selection.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 127, 'page_label': '128'}, page_content='SAMPLE QUESTIONS\\nMULTIPLE-CHOICE QUESTIONS (1 MARK QUESTIONS) :\\n1. Temperature is a\\n1. Interval data\\n2. Ratio data\\n3. Discrete data\\n4. None of the above\\n2. Principal component is a technique for\\n1. Feature selection\\n2. Dimensionality reduction\\n3. Exploration\\n4. None of the above\\n3. For bi-variate data exploration, _____ is an effective tool.\\n1. Box plot\\n2. Two-way cross-tab\\n3. Histogram\\n4. None of the above\\n4. For box plot, the upper and lower whisker length depends on\\n1. Median\\n2. Mean\\n3. IQR\\n4. All of the above\\n5. Feature selection tries to eliminate features which are\\n1. Rich\\n2. Redundant\\n3. Irrelevant\\n4. Relevant\\n6. When the number of features increase\\n1. Computation time increases\\n2. Model becomes complex\\n3. Learning accuracy decreases\\n4. All of the above\\n7. For categorical data, ____ cannot be used as a measure of central\\ntendency.\\n1. Median\\n2. Mean\\n3. Quartile\\n4. None of the above\\n8. For understanding relationship between two variables, ____ can be\\nused.\\n1. Box plot\\n2. Scatter plot'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 128, 'page_label': '129'}, page_content='3. Histogram\\n4. None of the above\\n9. Two common types of data issue are\\n1. Outlier\\n2. Missing value\\n3. Boundary value\\n4. None of the above\\n10. Exploration of numerical data can be best done using\\n1. Boxplots\\n2. Histograms\\n3. Scatter plot\\n4. None of the above\\n11. Data can broadly divided into following two types\\n1. Qualitative\\n2. Speculative\\n3. Quantitative\\n4. None of the above\\n12. Ordinal data can be naturally ____.\\n1. Measured\\n2. Ordered\\n3. Divided\\n4. None of the above\\nSHORT-ANSWER TYPE QUESTIONS (5 MARKS QUESTIONS):\\n1. What are the main activities involved when you are preparing to start\\nwith modelling in machine learning?\\n2. What are the basic data types in machine learning? Give an example of\\neach one of them.\\n3. Differentiate:\\n1. Categorical vs. Numeric attribute\\n2. Dimensionality reduction vs. Feature selection\\n4. Write short notes on any two:\\n1. Histogram\\n2. Scatter plot\\n3. PCA\\n5. Why do we need to explore data? Is there a difference in the way of\\nexploring qualitative data vis-a-vis quantitative data?\\n6. What are different shapes of histogram? What are ‘bins’?\\n7. How can we take care of outliers in data?\\n8. What are the different measures of central tendency? Why do mean, in\\ncertain data sets, differ widely from median?\\n9. Explain how bivariate relationships can be explored using scatter plot.\\nCan outliers be detected using scatter plot?'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 129, 'page_label': '130'}, page_content='10. Explain how cross-tabs can be used to understand relationship between\\ntwo variables.\\nLONG-ANSWER TYPE QUESTIONS (10 MARKS QUESTIONS) :\\n1. What are the main activities involved in machine learning? What is\\nmeant by data pre-processing?\\n2. Explain qualitative and quantitative data in details. Differentiate\\nbetween the two.\\n3. Prepare a simple data set along with some sample records in it. Have at\\nleast one attribute of the different data types used in machine learning.\\n4. What are the different causes of data issues in machine learning? What\\nare the fallouts?\\n5. Explain, with proper example, different ways of exploring categorical\\ndata.\\n6. When there are variables with certain values missing, will that impact\\nthe learning activity? If so, how can that be addressed?\\n7. Explain, in details, the different strategies of addressing missing data\\nvalues.\\n8. What are the different techniques for data pre-processing? Explain, in\\nbrief, dimensionality reduction and feature selection.\\n9. 1. What is IQR? How is it measured?\\n2. Explain, in details, the different components of a box plot? When\\nwill the lower whisker be longer than the upper whisker? How\\ncan outliers be detected using box plot?\\n10. 1. Write short notes on any two:\\n1. Interval data\\n2. Inter-quartile range\\n3. Cross-tab\\n2. Write the difference between (any two):\\n1. Nominal and ordinal data\\n2. Box plot and histogram\\n3. Mean and median'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 130, 'page_label': '131'}, page_content='Chapter 3\\nModelling and Evaluation\\nOBJECTIVE OF THE CHAPTER :\\nThe previous chapter gives a comprehensive understanding\\nof the basic data types in the context of machine learning.\\nIt also enables a beginner in the field of machine learning\\nto acquire an understanding about the nature and quality of\\nthe data by effective exploration of the data set. In this\\nchapter, the objective is to introduce the basic concepts of\\nlearning. In this regard, the information shared concerns\\nthe aspects of model selection and application. It also\\nimparts knowledge regarding how to judge the\\neffectiveness of the model in doing a specific learning task,\\nsupervised or unsupervised, and how to boost the model\\nperformance using different tuning parameters.\\n3.1 INTRODUCTION\\nThe learning process of machines may seem quite magical to\\nsomebody who is new to machine learning. The thought that a\\nmachine is able to think and take intelligent action may be\\nmesmerizing – much like a science fiction or a fantasy story.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 131, 'page_label': '132'}, page_content='However, delving a bit deeper helps them realize that it is not\\nas magical as it may seem to be. In fact, it tries to emulate\\nhuman learning by applying mathematical and statistical\\nformulations. In that sense, both human and machine learning\\nstrives to build formulations or mapping based on a limited\\nnumber of observations. As introduced in Chapter 1, the basic\\nlearning process, irrespective of the fact that the learner is a\\nhuman or a machine, can be divided into three parts:\\n1. Data Input\\n2. Abstraction\\n3. Generalization\\nThough in Chapter 1 we have understood these aspects in\\ndetails, let’s quickly refresh our memory with an example. It’s\\na fictitious situation. The detective department of New City\\nPolice has got a tip that in a campaign gathering for the\\nupcoming election, a criminal is going to launch an attack on\\nthe main candidate. However, it is not known who the person\\nis and quite obviously the person might use some disguise.\\nThe only thing that is for sure is the person is a history-sheeter\\nor a criminal having a long record of serious crime. From the\\ncriminal database, a list of such criminals along with their\\nphotographs has been collected. Also, the photos taken by\\nsecurity cameras positioned at different places near the\\ngathering are available with the detective department. They\\nhave to match the photos from the criminal database with the\\nfaces in the gathering to spot the potential attacker. So the\\nmain problem here is to spot the face of the criminal based on\\nthe match with the photos in the criminal database.\\nThis can be done using human learning where a person from\\nthe detective department can scan through each shortlisted\\nphoto and try to match that photo with the faces in the\\ngathering. A person having a strong memory can take a glance'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 132, 'page_label': '133'}, page_content='at the photos of all criminals in one shot and then try to find a\\nface in the gathering which closely resembles one of the\\ncriminal photos that she has viewed. Easy, isn’t it? But that is\\nnot possible in reality. The number of criminals in the database\\nand hence the count of photos runs in hundreds, if not\\nthousands. So taking a look at all the photos and memorizing\\nthem is not possible. Also, an exact match is out of the\\nquestion as the criminal, in most probability, will come in\\ndisguise. The strategy to be taken here is to match the photos\\nin smaller counts and also based on certain salient physical\\nfeatures like the shape of the jaw, the slope of the forehead, the\\nsize of the eyes, the structure of the ear, etc. So, the photos\\nfrom the criminal database form the input data. Based on it,\\nkey features can be abstracted. Since human matching for each\\nand every photo may soon lead to a visual as well as mental\\nfatigue, a generalization of abstracted feature-based data is a\\ngood way to detect potential criminal faces in the gathering.\\nFor example, from the abstracted feature-based data, say it is\\nobserved that most of the criminals have a shorter distance\\nbetween the inner corners of the eyes, a smaller angle between\\nthe nose and the corners of the mouth, a higher curvature to\\nthe upper lip, etc. Hence, a face in the gathering may be\\nclassified as ‘potentially criminal’ based on whether they\\nmatch with these generalized observations. Thus, using the\\ninput data, feature-based abstraction could be built and by\\napplying generalization of the abstracted data, human learning\\ncould classify the faces as potentially criminal ultimately\\nleading to spotting of the criminal.\\nThe same thing can be done using machine learning too.\\nUnlike human detection, a machine has no subjective baggage,\\nno emotion, no bias due to past experience, and above all no\\nmental fatigue. The machine can also use the same input data,\\ni.e. criminal database photos, apply computational techniques'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 133, 'page_label': '134'}, page_content='to abstract feature-based concept map from the input data and\\ngeneralize the same in the form of a classification algorithm to\\ndecide whether a face in the gathering is potentially criminal\\nor not.\\nWhen we talk about the learning process, abstraction is a\\nsignificant step as it represents raw input data in a summarized\\nand structured format, such that a meaningful insight is\\nobtained from the data. This structured representation of raw\\ninput data to the meaningful pattern is called a model. The\\nmodel might have different forms. It might be a mathematical\\nequation, it might be a graph or tree structure, it might be a\\ncomputational block, etc. The decision regarding which model\\nis to be selected for a specific data set is taken by the learning\\ntask, based on the problem to be solved and the type of data.\\nFor example, when the problem is related to prediction and the\\ntarget field is numeric and continuous, the regression model is\\nassigned. The process of assigning a model, and fitting a\\nspecific model to a data set is called model training. Once the\\nmodel is trained, the raw input data is summarized into an\\nabstracted form.\\nHowever, with abstraction, the learner is able to only\\nsummarize the knowledge. This knowledge might be still very\\nbroad-based – consisting of a huge number of feature-based\\ndata and inter-relations. To generate actionable insight from\\nsuch broad-based knowledge is very difficult. This is where\\ngeneralization comes into play. Generalization searches\\nthrough the huge set of abstracted knowledge to come up with\\na small and manageable set of key findings. It is not possible\\nto do an exhaustive search by reviewing each of the abstracted\\nfindings one-by-one. A  heuristic search is employed, an\\napproach which is also used for human learning (often termed\\nas ‘gut-feel’). It is quite obvious that the heuristics sometimes'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 134, 'page_label': '135'}, page_content='result in erroneous result. If the outcome is systematically\\nincorrect, the learning is said to have a bias.\\nPoints to Ponder:\\nA machine learning algorithm creates its cognitive capability by\\nbuilding a mathematical formulation or function, known as target\\nfunction, based on the features in the input data set.\\nJust like a child learning things for the first time needs her parents\\nguidance to decide whether she is right or wrong, in machine learning\\nsomeone has to provide some non-learnable parameters, also called\\nhyper-parameters. Without these human inputs, machine learning\\nalgorithms cannot be successful.\\n3.2 SELECTING A MODEL\\nNow that you are familiar with the basic learning process and\\nhave understood model abstraction and generalization in that\\ncontext, let’s try to formalize it in context of a motivating\\nexample. Continuing the thread of the potential attack during\\nthe election campaign, New City Police department has\\nsucceeded in foiling the bid to attack the electoral candidate.\\nHowever, this was a wake-up call for them and they want to\\ntake a proactive action to eliminate all criminal activities in the\\nregion. They want to find the pattern of criminal activities in\\nthe recent past, i.e. they want to see whether the number of\\ncriminal incidents per month has any relation with an average\\nincome of the local population, weapon sales, the inflow of\\nimmigrants, and other such factors. Therefore, an association\\nbetween potential causes of disturbance and criminal incidents\\nhas to be determined. In other words, the goal or target is to\\ndevelop a model to infer how the criminal incidents change\\nbased on the potential influencing factors mentioned above.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 135, 'page_label': '136'}, page_content='In machine learning paradigm, the potential causes of\\ndisturbance, e.g. average income of the local population,\\nweapon sales, the inflow of immigrants, etc. are input\\nvariables. They are also called predictors, attributes, features,\\nindependent variables, or simply variables. The number of\\ncriminal incidents is an output variable (also called response or\\ndependent variable). Input variables can be denoted by X,\\nwhile individual input variables are represented as X , X , X ,\\n…, X  and output variable by symbol Y. The relationship\\nbetween X and Y is represented in the general form: Y = f (X) +\\ne, where ‘f ’ is the target function and ‘e’ is a random error\\nterm.\\nNote:\\nJust like a target function with respect to a machine\\nlearning model, some other functions which are frequently\\ntracked are\\nA cost function (also called error function) helps to measure the\\nextent to which the model is going wrong in estimating the relationship\\nbetween X and Y. In that sense, cost function can tell how bad the model\\nis performing. For example, R-squared (to be discussed later in this\\nchapter) is a cost function of regression model.\\nLoss function is almost synonymous to cost function – only difference\\nbeing loss function is usually a function defined on a data point, while\\ncost function is for the entire training data set.\\nMachine learning is an optimization problem. We try to define a model\\nand tune the parameters to find the most suitable solution to a problem.\\nHowever, we need to have a way to evaluate the quality or optimality of\\na solution. This is done using objective function. Objective means\\ngoal.\\nObjective function takes in data and model (along with parameters) as\\ninput and returns a value. Target is to find values of model parameter to\\nmaximize or minimize the return value. When the objective is to\\nminimize the value, it becomes synonymous to cost function. Examples:\\n1 2 3\\nn'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 136, 'page_label': '137'}, page_content='maximize the reward function in reinforcement learning, maximize the\\nposterior probability in Naive Bayes, minimize squared error in\\nregression.\\nBut the problem that we just talked about is one specific\\ntype of problem in machine learning. We have seen in Chapter\\n1 that there are three broad categories of machine learning\\napproaches used for resolving different types of problems.\\nQuickly recapitulating, they are\\n1. Supervised\\n1. Classification\\n2. Regression\\n2. Unsupervised\\n1. Clustering\\n2. Association analysis\\n3. Reinforcement\\nFor each of the cases, the model that has to be\\ncreated/trained is different. Multiple factors play a role when\\nwe try to select the model for solving a machine learning\\nproblem. The most important factors are (i) the kind of\\nproblem we want to solve using machine learning and (ii) the\\nnature of the underlying data. The problem may be related to\\nthe prediction of a class value like whether a tumour is\\nmalignant or benign, whether the next day will be snowy or\\nrainy, etc. It may be related to prediction – but of some\\nnumerical value like what the price of a house should be in the\\nnext quarter, what is the expected growth of a certain IT stock\\nin the next 7 days, etc. Certain problems are related to\\ngrouping of data like finding customer segments that are using\\na certain product, movie genres which have got more box\\noffice success in the last one year, etc. So, it is very difficult to\\ngive a generic guidance related to which machine learning has\\nto be selected. In other words, there is no one model that'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 137, 'page_label': '138'}, page_content='works best for every machine learning problem. This is what\\n‘No Free Lunch’ theorem also states.\\nAny learning model tries to simulate some real-world\\naspect. However, it is simplified to a large extent removing all\\nintricate details. These simplifications are based on certain\\nassumptions – which are quite dependent on situations. Based\\non the exact situation, i.e. the problem in hand and the data\\ncharacteristics, assumptions may or may not hold. So the same\\nmodel may yield remarkable results in a certain situation while\\nit may completely fail in a different situation. That’s why,\\nwhile doing the data exploration, which we covered in the\\nprevious chapter, we need to understand the data\\ncharacteristics, combine this understanding with the problem\\nwe are trying to solve and then decide which model to be\\nselected for solving the problem.\\nLet’s try to understand the philosophy of model selection in\\na structured way. Machine learning algorithms are broadly of\\ntwo types: models for supervised learning, which primarily\\nfocus on solving predictive problems and models for\\nunsupervised learning, which solve descriptive problems.\\n3.2.1 Predictive models\\nModels for supervised learning or predictive models, as is\\nunderstandable from the name itself, try to predict certain\\nvalue using the values in an input data set. The learning model\\nattempts to establish a relation between the target feature, i.e.\\nthe feature being predicted, and the predictor features. The\\npredictive models have a clear focus on what they want to\\nlearn and how they want to learn.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 138, 'page_label': '139'}, page_content='Predictive models, in turn, may need to predict the value of\\na category or class to which a data instance belongs to. Below\\nare some examples:\\n1. Predicting win/loss in a cricket match\\n2. Predicting whether a transaction is fraud\\n3. Predicting whether a customer may move to another product\\nThe models which are used for prediction of target features\\nof categorical value are known as classification models. The\\ntarget feature is known as a class and the categories to which\\nclasses are divided into are called levels. Some of the popular\\nclassification models include k-Nearest Neighbor (kNN),\\nNaïve Bayes, and Decision Tree.\\nPredictive models may also be used to predict numerical\\nvalues of the target feature based on the predictor features.\\nBelow are some examples:\\n1. Prediction of revenue growth in the succeeding year\\n2. Prediction of rainfall amount in the coming monsoon\\n3. Prediction of potential flu patients and demand for flu shots next winter\\nThe models which are used for prediction of the numerical\\nvalue of the target feature of a data instance are known as\\nregression models. Linear Regression and Logistic Regression\\nmodels are popular regression models.\\nPoints to Ponder:\\nCategorical values can be converted to numerical values and vice versa.\\nFor example, for stock price growth prediction, any growth percentage\\nlying between certain ranges may be represented by a categorical value,\\ne.g. 0%–5% as ‘low’, 5%–10% as ‘moderate’, 10%–20% as ‘high’ and\\n> 20% as ‘booming’. In a similar way, a categorical value can be\\nconverted to numerical value, e.g. in the tumor malignancy detection'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 139, 'page_label': '140'}, page_content='problem, replace ‘benign’ as 0 and ‘malignant’ as 1. This way, the\\nmodels can be used interchangeably, though it may not work always.\\nThere are multiple factors to be considered while selecting a model. For\\nexample, while selecting the model for prediction, the training data size\\nis an important factor to be considered. If the training data set is small,\\nlow variance models like Naïve Bayes are supposed to perform better\\nbecause model overfitting needs to be avoided in this situation.\\nSimilarly, when the training data is large, low bias models like logistic\\nregression should be preferred because they can represent complex\\nrelationships in a more effective way.\\nFew models like Support Vector Machines and Neural\\nNetwork can be used for both classifications as well as for\\nregression.\\n3.2.2 Descriptive models\\nModels for unsupervised learning or descriptive models are\\nused to describe a data set or gain insight from a data set.\\nThere is no target feature or single feature of interest in case of\\nunsupervised learning. Based on the value of all features,\\ninteresting patterns or insights are derived about the data set.\\nDescriptive models which group together similar data\\ninstances, i.e. data instances having a similar value of the\\ndifferent features are called clustering models. Examples of\\nclustering include\\n1. Customer grouping or segmentation based on social, demographic, ethnic,\\netc. factors\\n2. Grouping of music based on different aspects like genre, language, time-\\nperiod, etc.\\n3. Grouping of commodities in an inventory\\nThe most popular model for clustering is k-Means.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 140, 'page_label': '141'}, page_content='Descriptive models related to pattern discovery is used for\\nmarket basket analysis of transactional data. In market basket\\nanalysis, based on the purchase pattern available in the\\ntransactional data, the possibility of purchasing one product\\nbased on the purchase of another product is determined. For\\nexample, transactional data may reveal a pattern that generally\\na customer who purchases milk also purchases biscuit at the\\nsame time. This can be useful for targeted promotions or in-\\nstore set up. Promotions related to biscuits can be sent to\\ncustomers of milk products or vice versa. Also, in the store\\nproducts related to milk can be placed close to biscuits.\\n3.3 TRAINING A MODEL (FOR SUPERVISED LEARNING)\\n3.3.1 Holdout method\\nIn case of supervised learning, a model is trained using the\\nlabelled input data. However, how can we understand the\\nperformance of the model? The test data may not be available\\nimmediately. Also, the label value of the test data is not\\nknown. That is the reason why a part of the input data is held\\nback (that is how the name holdout originates) for evaluation\\nof the model. This subset of the input data is used as the test\\ndata for evaluating the performance of a trained model. In\\ngeneral 70%–80% of the input data (which is obviously\\nlabelled) is used for model training. The remaining 20%–30%\\nis used as test data for validation of the performance of the\\nmodel. However, a different proportion of dividing the input\\ndata into training and test data is also acceptable. To make sure\\nthat the data in both the buckets are similar in nature, the\\ndivision is done randomly. Random numbers are used to assign\\ndata items to the partitions. This method of partitioning the\\ninput data into two parts – training and test data (depicted in'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 141, 'page_label': '142'}, page_content='Figure 3.1), which is by holding back a part of the input data\\nfor validating the trained model is known as holdout method.\\nFIG. 3.1 Holdout method\\nOnce the model is trained using the training data, the labels\\nof the test data are predicted using the model’s target function.\\nThen the predicted value is compared with the actual value of\\nthe label. This is possible because the test data is a part of the\\ninput data with known labels. The performance of the model is\\nin general measured by the accuracy of prediction of the label\\nvalue.\\nIn certain cases, the input data is partitioned into three\\nportions – a training and a test data, and a third validation data.\\nThe validation data is used in place of test data, for measuring\\nthe model performance. It is used in iterations and to refine the\\nmodel in each iteration. The test data is used only for once,\\nafter the model is refined and finalized, to measure and report\\nthe final performance of the model as a reference for future\\nlearning efforts.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 142, 'page_label': '143'}, page_content='An obvious problem in this method is that the division of\\ndata of different classes into the training and test data may not\\nbe proportionate. This situation is worse if the overall\\npercentage of data related to certain classes is much less\\ncompared to other classes. This may happen despite the fact\\nthat random sampling is employed for test data selection. This\\nproblem can be addressed to some extent by applying stratified\\nrandom sampling in place of sampling. In case of stratified\\nrandom sampling, the whole data is broken into several\\nhomogenous groups or strata and a random sample is selected\\nfrom each such stratum. This ensures that the generated\\nrandom partitions have equal proportions of each class.\\n3.3.2 K-fold Cross-validation method\\nHoldout method employing stratified random sampling\\napproach still heads into issues in certain specific situations.\\nEspecially, the smaller data sets may have the challenge to\\ndivide the data of some of the classes proportionally amongst\\ntraining and test data sets. A special variant of holdout\\nmethod, called repeated holdout, is sometimes employed to\\nensure the randomness of the composed data sets. In repeated\\nholdout, several random holdouts are used to measure the\\nmodel performance. In the end, the average of all\\nperformances is taken. As multiple holdouts have been drawn,\\nthe training and test data (and also validation data, in case it is\\ndrawn) are more likely to contain representative data from all\\nclasses and resemble the original input data closely. This\\nprocess of repeated holdout is the basis of k-fold cross-\\nvalidation technique. In k-fold cross-validation, the data set is\\ndivided into k-completely distinct or non-overlapping random\\npartitions called folds. Figure 3.2 depicts an overall approach\\nfor k-fold cross-validation.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 143, 'page_label': '144'}, page_content='The value of ‘k’ in k-fold cross-validation can be set to any\\nnumber. However, there are two approaches which are\\nextremely popular:\\n1. 10-fold cross-validation (10-fold CV)\\n2. Leave-one-out cross-validation (LOOCV)\\n10-fold cross-validation is by far the most popular approach.\\nIn this approach, for each of the 10-folds, each comprising of\\napproximately 10% of the data, one of the folds is used as the\\ntest data for validating model performance trained based on\\nthe remaining 9 folds (or 90% of the data). This is repeated 10\\ntimes, once for each of the 10 folds being used as the test data\\nand the remaining folds as the training data.The average\\nperformance across all folds is being reported. Figure 3.3\\ndepicts the detailed approach of selecting the ‘k’ folds in k-fold\\ncross-validation. As can be observed in the figure, each of the\\ncircles resembles a record in the input data set whereas the\\ndifferent colors indicate the different classes that the records\\nbelong to.The entire data set is broken into ‘k’ folds – out of\\nwhich one fold is selected in each iteration as the test data set.\\nThe fold selected as test data set in each of the ‘k’ iterations is\\ndifferent. Also, note that though in figure 3.3 the circles\\nresemble the records in the input data set, the contiguous\\ncircles represented as folds do not mean that they are\\nsubsequent records in the data set. This is more a virtual\\nrepresentation and not a physical representation. As already\\nmentioned, the records in a fold are drawn by using random\\nsampling technique.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 144, 'page_label': '145'}, page_content='FIG. 3.2 Overall approach for K-fold cross-validation\\nFIG. 3.3 Detailed approach for fold selection'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 145, 'page_label': '146'}, page_content='Leave-one-out cross-validation (LOOCV) is an extreme\\ncase of k-fold cross-validation using one record or data\\ninstance at a time as a test data. This is done to maximize the\\ncount of data used to train the model. It is obvious that the\\nnumber of iterations for which it has to be run is equal to the\\ntotal number of data in the input data set. Hence, obviously, it\\nis computationally very expensive and not used much in\\npractice.\\n3.3.3 Bootstrap sampling\\nBootstrap sampling or simply bootstrapping is a popular way\\nto identify training and test data sets from the input data set. It\\nuses the technique of Simple Random Sampling with\\nReplacement (SRSWR), which is a well-known technique in\\nsampling theory for drawing random samples. We have seen\\nearlier that k-fold cross-validation divides the data into\\nseparate partitions – say 10 partitions in case of 10-fold cross-\\nvalidation. Then it uses data instances from partition as test\\ndata and the remaining partitions as training data. Unlike this\\napproach adopted in case of k-fold cross- validation,\\nbootstrapping randomly picks data instances from the input\\ndata set, with the possibility of the same data instance to be\\npicked multiple times. This essentially means that from the\\ninput data set having ‘n’ data instances, bootstrapping can\\ncreate one or more training data sets having ‘n’ data instances,\\nsome of the data instances being repeated multiple times.\\nFigure 3.4 briefly presents the approach followed in bootstrap\\nsampling.\\nThis technique is particularly useful in case of input data\\nsets of small size, i.e.  having very less number of data\\ninstances.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 146, 'page_label': '147'}, page_content='FIG. 3.4 Bootstrap sampling\\n3.3.4 Lazy vs. Eager learner\\nEager learning follows the general principles of machine\\nlearning – it tries to construct a generalized, input-independent\\ntarget function during the model training phase. It follows the\\ntypical steps of machine learning, i.e. abstraction and\\ngeneralization and comes up with a trained model at the end of\\nthe learning phase. Hence, when the test data comes in for\\nclassification, the eager learner is ready with the model and'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 147, 'page_label': '148'}, page_content='doesn’t need to refer back to the training data. Eager learners\\ntake more time in the learning phase than the lazy learners.\\nSome of the algorithms which adopt eager learning approach\\ninclude Decision Tree, Support Vector Machine, Neural\\nNetwork, etc.\\nLazy learning, on the other hand, completely skips the\\nabstraction and generalization processes, as explained in\\ncontext of a typical machine learning process. In that respect,\\nstrictly speaking, lazy learner doesn’t ‘learn’ anything. It uses\\nthe training data in exact, and uses the knowledge to classify\\nthe unlabelled test data. Since lazy learning uses training data\\nas-is, it is also known as rote learning (i.e. memorization\\ntechnique based on repetition). Due to its heavy dependency\\non the given training data instance, it is also known as instance\\nlearning. They are also called non-parametric learning. Lazy\\nlearners take very little time in training because not much of\\ntraining actually happens. However, it takes quite some time in\\nclassification as for each tuple of test data, a comparison-based\\nassignment of label happens. One of the most popular\\nalgorithm for lazy learning is k-nearest neighbor.\\nNote:\\nParametric learning models have finite number of\\nparameters. In case of non-parametric models, quite\\ncontradicting to its name, the number of parameters is\\npotentially infinite.\\nModels such as Linear Regression and Support Vector\\nMachine, since the coefficients form the learning\\nparameters, they are fixed in size. Hence, these models are'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 148, 'page_label': '149'}, page_content='clubbed as parametric.On the other hand, in case of models\\nsuch as k-Nearest Neighbor (kNN) and decision tree,\\nnumber of parameters grows with the size of the training\\ndata. Hence, they are considered as non-parametric\\nlearning models.\\n3.4 MODEL REPRESENTATION AND INTERPRETABILITY\\nWe have already seen that the goal of supervised machine\\nlearning is to learn or derive a target function which can best\\ndetermine the target variable from the set of input variables. A\\nkey consideration in learning the target function from the\\ntraining data is the extent of generalization. This is because the\\ninput data is just a limited, specific view and the new,\\nunknown data in the test data set may be differing quite a bit\\nfrom the training data.\\nFitness of a target function approximated by a learning\\nalgorithm determines how correctly it is able to classify a set\\nof data it has never seen.\\n3.4.1 Underfitting\\nIf the target function is kept too simple, it may not be able to\\ncapture the essential nuances and represent the underlying data\\nwell. A typical case of underfitting may occur when trying to\\nrepresent a non-linear data with a linear model as\\ndemonstrated by both cases of underfitting shown in figure\\n3.5. Many times underfitting happens due to unavailability of\\nsufficient training data. Underfitting results in both poor\\nperformance with training data as well as poor generalization\\nto test data. Underfitting can be avoided by'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 149, 'page_label': '150'}, page_content='1. using more training data\\n2. reducing features by effective feature selection\\nFIG. 3.5 Underfitting and Overfitting of models\\n3.4.2 Overfitting\\nOverfitting refers to a situation where the model has been\\ndesigned in such a way that it emulates the training data too\\nclosely. In such a case, any specific deviation in the training\\ndata, like noise or outliers, gets embedded in the model. It\\nadversely impacts the performance of the model on the test\\ndata. Overfitting, in many cases, occur as a result of trying to\\nfit an excessively complex model to closely match the training\\ndata. This is represented with a sample data set in figure 3.5 .\\nThe target function, in these cases, tries to make sure all\\ntraining data points are correctly partitioned by the decision\\nboundary. However, more often than not, this exact nature is\\nnot replicated in the unknown test data set. Hence, the target\\nfunction results in wrong classification in the test data set.\\nOverfitting results in good performance with training data set,'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 150, 'page_label': '151'}, page_content='but poor generalization and hence poor performance with test\\ndata set. Overfitting can be avoided by\\n1. using re-sampling techniques like k-fold cross validation\\n2. hold back of a validation data set\\n3. remove the nodes which have little or no predictive power for the given\\nmachine learning problem.\\nBoth underfitting and overfitting result in poor classification\\nquality which is reflected by low classification accuracy.\\n3.4.3 Bias – variance trade-off\\nIn supervised learning, the class value assigned by the learning\\nmodel built based on the training data may differ from the\\nactual class value. This error in learning can be of two types –\\nerrors due to ‘bias’ and error due to ‘variance’. Let’s try to\\nunderstand each of them in details.\\n3.4.3.1 Errors due to ‘Bias’\\nErrors due to bias arise from simplifying assumptions made by\\nthe model to make the target function less complex or easier to\\nlearn. In short, it is due to underfitting of the model.\\nParametric models generally have high bias making them\\neasier to understand/interpret and faster to learn. These\\nalgorithms have a poor performance on data sets, which are\\ncomplex in nature and do not align with the simplifying\\nassumptions made by the algorithm. Underfitting results in\\nhigh bias.\\n3.4.3.2 Errors due to ‘Variance’\\nErrors due to variance occur from difference in training data\\nsets used to train the model. Different training data sets\\n(randomly sampled from the input data set) are used to train'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 151, 'page_label': '152'}, page_content='the model. Ideally the difference in the data sets should not be\\nsignificant and the model trained using different training data\\nsets should not be too different. However, in case of\\noverfitting, since the model closely matches the  training data,\\neven a small difference in training data gets magnified in\\nthe model.\\nFIG. 3.6 Bias-variance trade-off\\nSo, the problems in training a model can either happen\\nbecause either (a) the model is too simple and hence fails to\\ninterpret the data grossly or (b) the model is extremely'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 152, 'page_label': '153'}, page_content='complex and magnifies even small differences in the training\\ndata.\\nAs is quite understandable:\\nIncreasing the bias will decrease the variance, and\\nIncreasing the variance will decrease the bias\\nOn one hand, parametric algorithms are generally seen to\\ndemonstrate high bias but low variance. On the other hand,\\nnon-parametric algorithms demonstrate low bias and high\\nvariance.\\nAs can be observed in Figure 3.6 , the best solution is to\\nhave a model with low bias as well as low variance. However,\\nthat may not be possible in reality. Hence, the goal of\\nsupervised machine learning is to achieve a balance between\\nbias and variance. The learning algorithm chosen and the user\\nparameters which can be configured helps in striking a trade-\\noff between bias and variance. For example, in a popular\\nsupervised algorithm k-Nearest Neighbors or kNN, the user\\nconfigurable parameter ‘k’ can be used to do a trade-off\\nbetween bias and variance. In one hand, when the value of ‘k’\\nis decreased, the model becomes simpler to fit and bias\\nincreases. On the other hand, when the value of ‘k’ is\\nincreased, the variance increases.\\n3.5 EVALUATING PERFORMANCE OF A MODEL\\n3.5.1 Supervised learning - classification\\nIn supervised learning, one major task is classification. The\\nresponsibility of the classification model is to assign class\\nlabel to the target feature based on the value of the predictor\\nfeatures. For example, in the problem of predicting the'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 153, 'page_label': '154'}, page_content='win/loss in a cricket match, the classifier will assign a class\\nvalue win/loss to target feature based on the values of other\\nfeatures like whether the team won the toss, number of\\nspinners in the team, number of wins the team had in the\\ntournament, etc. To evaluate the performance of the model, the\\nnumber of correct classifications or predictions made by the\\nmodel has to be recorded. A classification is said to be correct\\nif, say for example in the given problem, it has been predicted\\nby the model that the team will win and it has actually won.\\nBased on the number of correct and incorrect classifications\\nor predictions made by a model, the accuracy of the model is\\ncalculated. If 99 out of 100 times the model has classified\\ncorrectly, e.g. if in 99 out of 100 games what the model has\\npredicted is same as what the outcome has been, then the\\nmodel accuracy is said to be 99%. However, it is quite relative\\nto say whether a model has performed well just by looking at\\nthe accuracy value. For example, 99% accuracy in case of a\\nsports win predictor model may be reasonably good but the\\nsame number may not be acceptable as a good threshold when\\nthe learning problem deals with predicting a critical illness. In\\nthis case, even the 1% incorrect prediction may lead to loss of\\nmany lives. So the model performance needs to be evaluated\\nin light of the learning problem in question. Also, in certain\\ncases, erring on the side of caution may be preferred at the cost\\nof overall accuracy. For that reason, we need to look more\\nclosely at the model accuracy and also at the same time look at\\nother measures of performance of a model like sensitivity,\\nspecificity, precision, etc. So, let’s start with looking at model\\naccuracy more closely. And let’s try to understand it with an\\nexample.\\nThere are four possibilities with regards to the cricket match\\nwin/loss prediction:'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 154, 'page_label': '155'}, page_content='1. the model predicted win and the team won\\n2. the model predicted win and the team lost\\n3. the model predicted loss and the team won\\n4. the model predicted loss and the team lost\\nIn this problem, the obvious class of interest is ‘win’.\\nThe first case, i.e. the model predicted win and the team\\nwon is a case where the model has correctly classified data\\ninstances as the class of interest. These cases are referred as\\nTrue Positive (TP) cases.\\nThe second case, i.e. the model predicted win and the team\\nlost is a case where the model incorrectly classified data\\ninstances as the class of interest. These cases are referred as\\nFalse Positive (FP) cases.\\nThe third case, i.e. the model predicted loss and the team\\nwon is a case where the model has incorrectly classified as not\\nthe class of interest. These cases are referred as False Negative\\n(FN) cases.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 155, 'page_label': '156'}, page_content='FIG. 3.7 Details of model classification\\nThe fourth case, i.e. the model predicted loss and the team\\nlost is a case where the model has correctly classified as not\\nthe class of interest. These cases are referred as True Negative\\n(TN) cases. All these four cases are depicted in Figure 3.7 .\\nFor any classification model, model accuracy is given by\\ntotal number of correct classifications (either as the class of\\ninterest, i.e. True Positive or as not the class of interest, i.e.\\nTrue Negative) divided by total number of classifications\\ndone.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 156, 'page_label': '157'}, page_content='A matrix containing correct and incorrect predictions in the\\nform of TPs, FPs, FNs and TNs is known as confusion\\nmatrix. The win/loss prediction of cricket match has two\\nclasses of interest – win and loss. For that reason it will\\ngenerate a 2 × 2 confusion matrix. For a classification problem\\ninvolving three classes, the confusion matrix would be 3 × 3,\\netc.\\nLet’s assume the confusion matrix of the win/loss prediction\\nof cricket match problem to be as below:\\nIn context of the above confusion matrix, total count of TPs\\n= 85, count of FPs = 4, count of FNs = 2 and count of TNs = 9.\\nThe percentage of misclassifications is indicated using\\nerror rate which is measured as\\nIn context of the above confusion matrix,'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 157, 'page_label': '158'}, page_content='Sometimes, correct prediction, both TPs as well as TNs,\\nmay happen by mere coincidence. Since these occurrences\\nboost model accuracy, ideally it should not happen. Kappa\\nvalue of a model indicates the adjusted the model accuracy. It\\nis calculated using the formula below:\\nIn context of the above confusion matrix, total count of TPs\\n= 85, count of FPs = 4, count of FNs = 2 and count of TNs = 9.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 158, 'page_label': '159'}, page_content='Note:\\nKappa value can be 1 at the maximum, which represents\\nperfect agreement between model’s prediction and actual\\nvalues.\\nAs discussed earlier, in certain learning problems it is\\ncritical to have extremely low number of FN cases, if needed,\\nat the cost of a conservative classification model. Though it is\\na clear case of misclassification and will impact model\\naccuracy adversely, it is still required as missing each class of\\ninterest may have serious consequence. This happens more in\\nproblems from medical domains like disease prediction\\nproblem. For example, if a tumor is malignant but wrongly\\nclassified as benign by the classifier, then the repercussion of\\nsuch misclassification is fatal. It does not matter if higher\\nnumber of tumours which are benign are wrongly classified as\\nmalignant. In these problems there are some measures of\\nmodel performance which are more important than accuracy.\\nTwo such critical measurements are sensitivity and specificity\\nof the model.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 159, 'page_label': '160'}, page_content='The sensitivity of a model measures the proportion of TP\\nexamples or positive cases which were correctly classified. It\\nis measured as\\nIn the context of the above confusion matrix for the cricket\\nmatch win prediction problem,\\nSo, again taking the example of the malignancy prediction\\nof tumours, class of interest is ‘malignant’. Sensitivity\\nmeasure gives the proportion of tumours which are actually\\nmalignant and have been predicted as malignant. It is quite\\nobvious that for such problems the most critical measure of the\\nperformance of a good model is sensitivity. A high value of\\nsensitivity is more desirable than a high value of accuracy.\\nSpecificity is also another good measure to indicate a good\\nbalance of a model being excessively conservative or\\nexcessively aggressive. Specificity of a model measures the\\nproportion of negative examples which have been correctly\\nclassified. In the context, of malignancy prediction of tumours,\\nspecificity gives the proportion of benign tumours which have\\nbeen correctly classified. In the context of the above confusion\\nmatrix for the cricket match win prediction problem,'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 160, 'page_label': '161'}, page_content='A higher value of specificity will indicate a better model\\nperformance. However, it is quite understandable that a\\nconservative approach to reduce False Negatives might\\nactually push up the number of FPs. Reason for this is that the\\nmodel, in order to reduce FNs, is going to classify more\\ntumours as malignant. So  the chance that benign tumours will\\nbe classified as malignant or FPs will increase.\\nThere are two other performance measures of a supervised\\nlearning model which are similar to sensitivity and specificity.\\nThese are precision and recall. While precision gives the\\nproportion of positive predictions which are truly positive,\\nrecall gives the proportion of TP cases over all actually\\npositive cases.\\nPrecision indicates the reliability of a model in predicting a\\nclass of interest. When the model is related to win / loss\\nprediction of cricket, precision indicates how often it predicts\\nthe win correctly. In context of the above confusion matrix for\\nthe cricket match win prediction problem,\\nIt is quite understandable that a model with higher precision\\nis perceived to be more reliable.\\nRecall indicates the proportion of correct prediction of\\npositives to the total number of positives. In case of win/loss'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 161, 'page_label': '162'}, page_content='prediction of cricket, recall resembles what proportion of the\\ntotal wins were predicted correctly.\\nIn the context of the above confusion matrix for the cricket\\nmatch win prediction problem,\\n3.5.1.1 F-measure\\nF-measure is another measure of model performance which\\ncombines the precision and recall. It takes the harmonic mean\\nof precision and recall as calculated as\\nIn context of the above confusion matrix for the cricket\\nmatch win prediction problem,\\nAs a combination of multiple measures into one, F-score\\ngives the right measure using which performance of different\\nmodels can be compared. However, one assumption the\\ncalculation is based on is that precision and recall have equal\\nweight, which may not always be true in reality. In certain\\nproblems, the disease prediction problems, e.g., precision may'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 162, 'page_label': '163'}, page_content='be given far more weightage. In that case, different weightages\\nmay be assigned to precision and recall. However, there may\\nbe a serious dilemma regarding what value to be adopted for\\neach and what is the basis for the specific value adopted.\\n3.5.1.1.1 Receiver operating characteristic (ROC) curves\\nAs we have seen till now, though accuracy is the most popular\\nmeasure, there are quite a number of other measures to\\nevaluate the performance of a supervised learning model.\\nHowever, visualization is an easier and more effective way to\\nunderstand the model performance. It also helps in comparing\\nthe efficiency of two models.\\nReceiver Operating Characteristic (ROC) curve helps in\\nvisualizing the performance of a classification model. It shows\\nthe efficiency of a model in the detection of true positives\\nwhile avoiding the occurrence of false positives. To refresh our\\nmemory, true positives are those cases where the model has\\ncorrectly classified data instances as the class of interest. For\\nexample, the model has correctly classified the tumours as\\nmalignant, in case of a tumour malignancy prediction problem.\\nOn the other hand, FPs are those cases where the model\\nincorrectly classified data instances as the class of interest.\\nUsing the same example, in this case, the model has\\nincorrectly classified the tumours as malignant, i.e. tumours\\nwhich are actually benign have been classified as malignant.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 163, 'page_label': '164'}, page_content='In the ROC curve, the FP rate is plotted (in the horizontal\\naxis) against true positive rate (in the vertical axis) at different\\nclassification thresholds. If we assume a lower value of\\nclassification threshold, the model classifies more items as\\npositive. Hence, the values of both False Positives and True\\nPositives increase. The area under curve (AUC) value, as\\nshown in figure 3.8a , is the area of the two-dimensional space\\nunder the curve extending from (0, 0) to (1, 1), where each\\npoint on the curve gives a set of true and false positive values\\nat a specific classification threshold. This curve gives an\\nindication of the predictive quality of a model. AUC value\\nranges from 0 to 1, with an AUC of less than 0.5 indicating\\nthat the classifier has no predictive ability. Figure 3.8b shows\\nthe curves of two classifiers – classifier 1 and classifier 2.\\nQuite obviously, the AUC of classifier 1 is more than the AUC\\nof classifier 2. So, we can draw the inference that classifier 1 is\\nbetter than classifier 2.\\nFIG. 3.8 ROC curve'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 164, 'page_label': '165'}, page_content='A quick indicative interpretation of the predictive values\\nfrom 0.5 to 1.0 is given below:\\n0.5 – 0.6 ➔  Almost no predictive ability\\n0.6 – 0.7 ➔  Weak predictive ability\\n0.7 – 0.8 ➔  Fair predictive ability\\n0.8 – 0.9 ➔  Good predictive ability\\n0.9 – 1.0 ➔  Excellent predictive ability\\n3.5.2 Supervised learning – regression\\nA well-fitted regression model churns out predicted values\\nclose to actual values. Hence, a regression model which\\nensures that the difference between predicted and actual values\\nis low can be considered as a good model. Figure 3.9\\nrepresents a very simple problem of real estate value\\nprediction solved using linear regression model. If ‘area’ is the\\npredictor variable (say x) and ‘value’ is the target variable (say\\ny), the linear regression model can be represented in the form:'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 165, 'page_label': '166'}, page_content='FIG. 3.9 Error – Predicted vs. actual value\\nFor a certain value of x, say x̂, the value of y is predicted as\\nŷ whereas the actual value of y is Y (say). The distance\\nbetween the actual value and the fitted or predicted value, i.e. ŷ\\nis known as residual. The regression model can be considered\\nto be fitted well if the difference between actual and predicted\\nvalue, i.e. the residual value is less.\\nR-squared is a good measure to evaluate the model fitness.\\nIt is also known as the coefficient of determination, or for\\nmultiple regression, the coefficient of multiple determination.\\nThe R-squared value lies between 0 to 1 (0%–100%) with a\\nlarger value representing a better fit. It is calculated as:'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 166, 'page_label': '167'}, page_content='Sum of Squares Total (SST) = squared differences of each\\nobservation from the overall mean =\\n  where y̅  is the\\nmean.\\nSum of Squared Errors (SSE) (of prediction) = sum of the\\nsquared residuals = \\n  where \\n  is the predicted value\\nof y and Y is the actual value of y\\n3.5.3 Unsupervised learning - clustering\\nClustering algorithms try to reveal natural groupings amongst\\nthe data sets. However, it is quite tricky to evaluate the\\nperformance of a clustering algorithm. Clustering, by nature, is\\nvery subjective and whether the cluster is good or bad is open\\nfor interpretations. It was noted, ‘clustering is in the eye of the\\nbeholder’. This stems from the two inherent challenges which\\nlie in the process of clustering:\\n1. It is generally not known how many clusters can be formulated from a\\nparticular data set. It is completely open-ended in most cases and\\nprovided as a user input to a clustering algorithm.\\n2. Even if the number of clusters is given, the same number of clusters can\\nbe formed with different groups of data instances.\\nIn a more objective way, it can be said that a clustering\\nalgorithm is successful if the clusters identified using the\\nalgorithm is able to achieve the right results in the overall\\nproblem domain. For example, if clustering is applied for\\ni i i.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 167, 'page_label': '168'}, page_content='identifying customer segments for a marketing campaign of a\\nnew product launch, the clustering can be considered\\nsuccessful only if the marketing campaign ends with a\\nsuccess,i.e. it is able to create the right brand recognition\\nresulting in steady revenue from new product sales. However,\\nthere are couple of popular approaches which are adopted for\\ncluster quality evaluation.\\n1. Internal evaluation\\nIn this approach, the cluster is assessed based on the underlying data that was\\nclustered. The internal evaluation methods generally measure cluster quality\\nbased on homogeneity of data belonging to the same cluster and\\nheterogeneity of data belonging to different clusters. The\\nhomogeneity/heterogeneity is decided by some similarity measure. For\\nexample, silhouette coefficient, which is one of the most popular internal\\nevaluation methods, uses distance (Euclidean or Manhattan distances most\\ncommonly used) between data elements as a similarity measure. The value of\\nsilhouette width ranges between –1 and +1, with a high value indicating high\\nintra-cluster homogeneity and inter-cluster heterogeneity.\\nFor a data set clustered into ‘k’ clusters, silhouette width is calculated as:\\na(i) is the average distance between the i th data instance and all other data\\ninstances belonging to the same cluster and b(i) is the lowest average\\ndistance between the i-the data instance and data instances of all other\\nclusters.\\nLet’s try to understand this in context of the example depicted in figure 3.10.\\nThere are four clusters namely cluster 1, 2, 3, and 4. Let’s consider an\\narbitrary data element ‘i’ in cluster 1, resembled by the asterisk. a(i) is the\\naverage of the distances a , a , …, a  of the different data elements from\\nthe i th data element in cluster 1, assuming there are n  data elements in\\ncluster 1. Mathematically,\\ni1 i2 in1\\n1'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 168, 'page_label': '169'}, page_content='FIG. 3.10 Silhouette width calculation\\nIn the same way, let’s calculate the distance of an arbitrary data element ‘i’ in\\ncluster 1 with the different data elements from another cluster, say cluster 4\\nand take an average of all those distances. Hence,\\nwhere n  is the total number of elements in cluster 4. In the same way, we\\ncan calculate the values of b  (average) and b  (average). b (i) is the\\nminimum of all these values. Hence, we can say that,\\n \\nb(i) = minimum [b (average), b (average), b (average)]\\n \\n2. External evaluation\\nIn this approach, class label is known for the data set subjected to clustering.\\nHowever, quite obviously, the known class labels are not a part of the data\\nused in clustering. The cluster algorithm is assessed based on how close the\\n4\\n12 13\\n12 13 14'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 169, 'page_label': '170'}, page_content='results are compared to those known class labels. For example, purity is one\\nof the most popular measures of cluster algorithms – evaluates the extent to\\nwhich clusters contain a single class.\\nFor a data set having ‘n’ data instances and ‘c’ known class labels which\\ngenerates ‘k’ clusters, purity is measured as:\\n3.6 IMPROVING PERFORMANCE OF A MODEL\\nNow we have almost reached the end of the journey of\\nbuilding learning models. We have got some idea about what\\nmodelling is, how to approach about it to solve a learning\\nproblem and how to measure the success of our model. Now\\ncomes a million dollar question. Can we improve the\\nperformance of our model? If so, then what are the levers for\\nimproving the performance? In fact, even before that comes\\nthe question of model selection – which model should be\\nselected for which machine learning task? We have already\\ndiscussed earlier that the model selection is done one several\\naspects:\\n1. Type of learning the task in hand, i.e. supervised or unsupervised\\n2. Type of the data, i.e. categorical or numeric\\n3. Sometimes on the problem domain\\n4. Above all, experience in working with different models to solve\\nproblems of diverse domains\\nSo, assuming that the model selection is done, what are the\\ndifferent avenues to improve the performance of models?\\nOne effective way to improve model performance is by\\ntuning model parameter. Model parameter tuning is the\\nprocess of adjusting the model fitting options. For example, in\\nthe popular classification model k-Nearest Neighbour (kNN),\\nusing different values of ‘k’ or the number of nearest'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 170, 'page_label': '171'}, page_content='neighbours to be considered, the model can be tuned. In the\\nsame way, a number of hidden layers can be adjusted to tune\\nthe performance in neural networks model. Most machine\\nlearning models have at least one parameter which can be\\ntuned.\\nAs an alternate approach of increasing the performance of\\none model, several models may be combined together. The\\nmodels in such combination are complimentary to each other,\\ni.e. one model may learn one type data sets well while struggle\\nwith another type of data set. Another model may perform\\nwell with the data set which the first one struggled with. This\\napproach of combining different models with diverse strengths\\nis known as ensemble (depicted in Figure 3.11 ). Ensemble\\nhelps in averaging out biases of the different underlying\\nmodels and also reducing the variance. Ensemble methods\\ncombine weaker learners to create stronger ones. A\\nperformance boost can be expected even if models are built as\\nusual and then ensembled. Following are the typical steps in\\nensemble process:\\nBuild a number of models based on the training data\\nFor diversifying the models generated, the training data subset can be varied\\nusing the allocation function. Sampling techniques like bootstrapping may be\\nused to generate unique training data sets.\\nAlternatively, the same training data may be used but the models combined\\nare quite varying, e.g, SVM, neural network, kNN, etc.\\nThe outputs from the different models are combined using a combination\\nfunction. A very simple strategy of combining, say in case of a prediction\\ntask using ensemble, can be majority voting of the different models\\ncombined. For example, 3 out of 5 classes predict ‘win’ and 2 predict ‘loss’ –\\nthen the final outcome of the ensemble using majority vote would be a ‘win’.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 171, 'page_label': '172'}, page_content='FIG. 3.11 Ensemble\\nOne of the earliest and most popular ensemble models is\\nbootstrap aggregating or bagging. Bagging uses bootstrap\\nsampling method (refer section 3.3.3) to generate multiple\\ntraining data sets. These training data sets are used to generate\\n(or train) a set of models using the same learning algorithm.\\nThen the outcomes of the models are combined by majority\\nvoting (classification) or by average (regression). Bagging is a\\nvery simple ensemble technique which can perform really well\\nfor unstable learners like a decision tree, in which a slight\\nchange in data can impact the outcome of a model\\nsignificantly.\\nJust like bagging, boosting is another key ensemble-based\\ntechnique. In this type of ensemble, weaker learning models\\nare trained on resampled data and the outcomes are combined\\nusing a weighted voting approach based on the performance of\\ndifferent models. Adaptive boosting or AdaBoost is a special'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 172, 'page_label': '173'}, page_content='variant of boosting algorithm. It is based on the idea of\\ngenerating weak learners and slowly learning\\nRandom forest is another ensemble-based technique. It is\\nan ensemble of decision trees – hence the name random forest\\nto indicate a forest of decision trees. It has been discussed in\\nmore details in chapter 7.\\n \\nIn this chapter, you have been introduced to the crux of\\nmachine learning, i.e. modelling. Thorough understanding of\\nthe technical aspects elaborated in this chapter is extremely\\ncrucial for the success of any machine learning project.\\nFor example, the first dilemma comes about which model to\\nselect. Again, in case of supervised learning, how can we deal\\nwith the unavailability of sufficient training data. In the same\\nway, once the model is trained in case of supervised learning\\nor the grouping is done in case of clustering, how we can\\nunderstand whether the model training (for supervised) or\\ngrouping done (for unsupervised) is good or bad. All these and\\nmore have been addressed as a part of this chapter.\\n3.7 SUMMARY\\nStructured representation of raw input data to the meaningful pattern is\\ncalled a model.\\nThe process of fitting a specific model to a data set is called model training.\\nModels for supervised learning or predictive models try to predict certain\\nvalue using the input data set.\\nModels for unsupervised learning or descriptive models are used to describe\\na data set or gain insight from a data set.\\nThe method of partitioning the input data into two parts – training and test\\ndata, which is holding back a part of the input data for validating the trained\\nmodel is known as holdout method.\\nIn k-fold cross-validation technique, the data set is divided into k- completely\\nseparate random partitions called folds. It is basically repeated holdout into'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 173, 'page_label': '174'}, page_content='‘k’ folds. The value of ‘k’ in k-fold cross-validation can be set to any\\nnumber. Two extremely popular approaches are:\\n10-fold cross-validation (10-fold CV)\\nLeave-one-out cross-validation (LOOCV)\\nBootstrap sampling or simply bootstrapping is a popular way to identify\\ntraining and test data sets from the input data set. It uses the technique of\\nSimple Random Sampling with Replacement (SRSWR). Bootstrapping\\nrandomly picks data instances from the input data set, with the possibility of\\nthe same data instance to be picked multiple times.\\nTarget function of a model is the function defining the relationship between\\nthe input (also called predictor or independent) variables and the output (also\\ncalled response or dependent or target) variable. It is represented in the\\ngeneral form: Y = f (X) + e, where Y is the output variable, X represents the\\ninput variables and ‘e’ is a random error term.\\nFitness of a target function approximated by a learning algorithm determines\\nhow correctly it is able to predict the value or class for a set of data it has\\nnever seen.\\nIf the target function is kept too simple, it may not be able to capture the\\nessential nuances and represent the underlying data well. This known as\\nunderfitting.\\nOverfitting refers to a situation where the model has been designed in such a\\nway that it emulates the training data too closely. In such a case, any specific\\nnuance in the training data, like noise or outliers, gets embedded in the\\nmodel. It adversely impacts the performance of the model on the test data.\\nIn supervised learning, the value predicted by the learning model built based\\non the training data may differ from the actual class value. This error in\\nlearning can be of two types – errors due to ‘bias’ and error due to\\n‘variance’. Errors due to bias arise from simplifying assumptions made by\\nthe model whereas errors due to variance occur from over-aligning the model\\nwith the training data sets.\\nFor any classification model, model accuracy is the primary indicator of the\\ngoodness of the model. It is given by a total number of correct classifications\\n(either as the class of interest, or as not the class of interest) divided by total\\nnumber of classifications done. There are other indicators like error rate,\\nsensitivity, specificity, precision and recall.\\nFor unsupervised learning (clustering), silhouette coefficient (or width) is\\none of the most popular internal evaluation methods. A high value of\\nsilhouette width indicates high intra-cluster homogeneity and inter-cluster\\nheterogeneity. In case, class label is known for the data set, purity is another\\npopular measure which evaluates the extent to which clusters contain a\\nsingle class.\\nModel parameter tuning is the process of adjusting the model fitting options.\\nFor example, in the popular classification model k-Nearest Neighbour\\n(kNN), using different values of ‘k’ or the number of nearest neighbours to\\nbe considered, the model can be tuned.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 174, 'page_label': '175'}, page_content='The approach of combining different models with diverse strengths is known\\nas ensemble. Ensemble methods combine weaker learners to create stronger\\nones.\\nOne of the earliest and most popular ensemble models is bootstrap\\naggregating or bagging. Bagging uses bootstrapping to generate multiple\\ntraining data sets. These training data sets are used to generate a set of\\nmodels using the same learning algorithm.\\nJust like bagging, boosting is another key ensemble-based technique. In\\nboosting, weaker learning models are trained on resampled data and the\\noutcomes are combined using a weighted voting approach based on the\\nperformance of different models.\\nAdaptive boosting or AdaBoost is a special variant of boosting algorithm.\\nSAMPLE QUESTIONS\\nMULTIPLE-CHOICE QUESTIONS (1 MARK QUESTIONS):\\n1. Structured representation of raw input data to meaningful ___ is called a\\nmodel.\\n1. pattern\\n2. data\\n3. object\\n4. none of the above\\n2. For supervised learning we have ____ model.\\n1. interactive\\n2. predictive\\n3. descriptive\\n4. prescriptive\\n3. For unsupervised learning we have ____ model.\\n1. interactive\\n2. predictive\\n3. descriptive\\n4. prescriptive\\n4. Which of the following measure is not used for a classification model?\\n1. Accuracy\\n2. Recall\\n3. Purity\\n4. Error rate\\n5. Which of the following is a performance measure for regression?\\n1. Accuracy\\n2. Recall\\n3. RMSE\\n4. Error rate\\n6. Which of the following is the measure of cluster quality?'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 175, 'page_label': '176'}, page_content='1. Purity\\n2. Distance\\n3. Accuracy\\n4. all of the above\\n7. Out of 200 emails, a classification model correctly predicted 150 spam\\nemails and 30 ham emails. What is the accuracy of the model?\\n1. 10%\\n2. 90%\\n3. 80%\\n4. none of the above\\n8. Out of 200 emails, a classification model correctly predicted 150 spam\\nemails and 30 ham emails. What is the error rate of the model?\\n1. 10%\\n2. 90%\\n3. 80%\\n4. none of the above\\n9. There is no one model that works best for every machine learning\\nproblem. This is stated as\\n1. Fit gap model theorem\\n2. One model theorem\\n3. Free lunch theorem\\n4. No free lunch theorem\\n10. LOOCV in machine learning stands for\\n1. Love one-out cross validation\\n2. Leave-one-out cross-validation\\n3. Leave-object oriented cross-validation\\n4. Leave-one-out class-validation\\nSHORT-ANSWER TYPE QUESTIONS (5 MARKS QUESTIONS):\\n1. What is a model in context of machine learning? How can you train a\\nmodel?\\n2. Explain “No Free Lunch” theorem in context of machine learning.\\n3. Explain, in details, the process of K-fold cross-validation.\\n4. Explain the bootstrap sampling. Why is it needed?\\n5. Why do we need to calculate Kappa value for a classification model?\\nShow, with a sample set of data, how to calculate Kappa value of a\\nclassification model.\\n6. Explain the process of ensemble of models. What role does in play in\\nmachine learning?\\n7. What is the main purpose of a descriptive model? State some real-world\\nproblems solved using descriptive models.\\n8. Explain the process of evaluating a linear regression model.\\n9. Differentiate (any two):'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 176, 'page_label': '177'}, page_content='1. Predictive vs. descriptive models\\n2. Model underfitting vs. overfitting\\n3. Cross-validation vs. bootstrapping\\n10. Write short notes on any two:\\n1. LOOCV\\n2. F-measure\\n3. Silhouette width\\n4. ROC curve\\nLONG-ANSWER TYPE QUESTIONS (10 MARKS QUESTIONS):\\n1. What is a target function? Express target function in context of a real-\\nlife example. How is the fitness of a target function measured?\\n2. What are predictive models? What are descriptive models? Give\\nexamples of both types of models. Explain the difference between these\\ntypes of models.\\n3. Explain, in details, the process of evaluating the performance of a\\nclassification model. Explain the different parameters of measurement.\\n4. 1. What is underfitting in context of machine learning models?\\nWhat is the major cause of underfitting?\\n2. What is overfitting? When does it happen?\\n3. Explain bias-variance trade-off in context of model fitting.\\n5. Can the performance of a learning model be improved? If yes, explain\\nhow.\\n6. How would you evaluate the success of an unsupervised learning\\nmodel? What are the most popular measures of performance for an\\nunsupervised learning model?\\n7. Is there a way to use a classification model for a numerical data or a\\nregression model on a categorical data? Explain your answer.\\n8. Describe the process of predictive modelling for numerical values. How\\nis it different from predictive modelling for categorical values?\\n9. While predicting malignancy of tumour of a set of patients using a\\nclassification model, following are the data recorded:\\n1. Correct predictions – 15 malignant, 75 benign\\n2. Incorrect predictions – 3 malignant, 7 benign\\nCalculate the error rate, Kappa value, sensitivity, precision, and\\nF-measure of the model.\\n10. 1. Write short notes on any two:\\n1. Holdout method\\n2. 10-fold cross-validation\\n3. Parameter tuning\\n2. Write the difference between (any two):\\n1. Purity vs. Silhouette width\\n2. Bagging vs. Boosting'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 177, 'page_label': '178'}, page_content='3. Lazy vs. Eager learner'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 178, 'page_label': '179'}, page_content='Chapter 4\\nBasics of Feature Engineering\\nOBJECTIVE OF THE CHAPTER\\nIn the last three chapters, you have been introduced to the\\nbasic concepts of machine learning. Also, the process to\\nstart modelling a problem has also been discussed in\\ndetails. With this context in mind, in this chapter, we will\\nintroduce you to another very important aspect of machine\\nlearning, that is feature engineering. Though not a core\\npart of the machine learning processes, feature engineering\\nis a critical allied task that we need to perform to make\\nlearning more effective. It has three key components –\\nfeature construction, feature selection, and feature\\ntransformation, each of which will be covered in details in\\nthis chapter.\\n4.1 INTRODUCTION\\nIn the last three chapters, we had a jumpstart to the machine\\nlearning process. We first started with what human learning is\\nand how the different types of machine learning emulate the\\naspects of human learning. We had a detailed view of the'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 179, 'page_label': '180'}, page_content='different types of problem that can be solved using machine\\nlearning techniques. Before applying machine learning to\\nsolve the problems, there are certain preparatory steps. These\\npreparatory steps have been covered in details. After that, we\\nhave done a step-by-step navigation of the different activities\\nof modelling a problem using machine learning. Modelling\\nalone doesn’t help us to realize the effectiveness of\\nmachine learning as a problem- solving tool. So we also learnt\\nhow to measure the effectiveness of machine learning models\\nin solving problems. In case a specific model is not effective,\\nwe can use different levers to boost the effectiveness. Those\\nlevers of boosting the model performance were also covered.\\nNow that we are ready (well almost ready!) to start solving\\nproblems using machine learning, we need to touch upon\\nanother key aspect which plays a critical role in solving any\\nmachine learning problem – feature engineering. Though\\nfeature engineering is a part of the preparatory activities which\\nhave already been covered in Chapter 2, the criticality and\\nvastness of the area call for treating it separately. This area\\ndeals with features of the data set, which form an important\\ninput of any machine learning problem – be supervised or\\nunsupervised learning. Feature engineering is a critical\\npreparatory process in machine learning. It is responsible for\\ntaking raw input data and converting that to well-aligned\\nfeatures which are ready to be used by the machine learning\\nmodels.\\nBut before we start discussing feature engineering, let’s try\\nto understand more clearly what feature is.\\nDid you know?'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 180, 'page_label': '181'}, page_content='Unstructured data is raw, unorganized data which doesn’t\\nfollow a specific format or hierarchy. Typical examples of\\nunstructured data include text data from social networks,\\ne.g. Twitter, Facebook, etc. or data from server logs, etc.\\n4.1.1 What is a feature?\\nA feature is an attribute of a data set that is used in a machine\\nlearning process. There is a view amongst certain machine\\nlearning practitioners that only those attributes which are\\nmeaningful to a machine learning problem are to be called as\\nfeatures, but this view has to be taken with a pinch of salt. In\\nfact, selection of the subset of features which are meaningful\\nfor machine learning is a sub-area of feature engineering\\nwhich draws a lot of research interest. The features in a data\\nset are also called its dimensions. So a data set having ‘n’\\nfeatures is called an n-dimensional data set.\\nLet’s take the example of a famous machine learning data\\nset, Iris, introduced by the British statistician and biologist\\nRonald Fisher, partly shown in Figure 4.1. It has five attributes\\nor features namely Sepal.Length, Sepal.Width, Petal.Length,\\nPetal. Width and Species. Out of these, the feature ‘Species’\\nrepresent the class variable and the remaining features are the\\npredictor variables. It is a five-dimensional data set.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 181, 'page_label': '182'}, page_content='FIG. 4.1 Data set features\\n4.1.2 What is feature engineering?\\nFeature engineering refers to the process of translating a data\\nset into features such that these features are able to represent\\nthe data set more effectively and result in a better learning\\nperformance.\\nAs we know already, feature engineering is an important\\npre-processing step for machine learning. It has two major\\nelements:\\n1. feature transformation\\n2. feature subset selection\\nFeature transformation transforms the data – structured or\\nunstructured, into a new set of features which can represent the\\nunderlying problem which machine learning is trying to solve.\\nThere are two variants of feature transformation:\\n1. feature construction\\n2. feature extraction\\nBoth are sometimes known as feature discovery.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 182, 'page_label': '183'}, page_content='Feature construction process discovers missing information\\nabout the relationships between features and augments the\\nfeature space by creating additional features. Hence, if there\\nare ‘n’ features or dimensions in a data set, after feature\\nconstruction ‘m’ more features or dimensions may get added.\\nSo at the end, the data set will become ‘n + m’ dimensional.\\nFeature extraction is the process of extracting or creating a\\nnew set of features from the original set of features using some\\nfunctional mapping.\\n \\nUnlike feature transformation, in case of feature subset\\nselection (or simply feature selection) no new feature is\\ngenerated. The objective of feature selection is to derive a\\nsubset of features from the full feature set which is most\\nmeaningful in the context of a specific machine learning\\nproblem. So, essentially the job of feature selection is to derive\\na subset F (F , F , …, F ) of F (F , F , …, F ), where m < n,\\nsuch that F is most meaningful and gets the best result for a\\nmachine learning problem. We will discuss these concepts in\\ndetail in the next section.\\nPoints to Ponder\\nData scientists and machine learning practitioners spend\\nsignificant amount of time in different feature engineering\\nactivities. Selecting the right features has a critical role to\\nplay in the success of a machine learning model.\\nj 1 2 m i 1 2 n\\nj'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 183, 'page_label': '184'}, page_content='It is quite evident that feature construction expands the\\nfeature space, while feature extraction and feature selection\\nreduces the feature space.\\n4.2 FEATURE TRANSFORMATION\\nEngineering a good feature space is a crucial prerequisite for\\nthe success of any machine learning model. However, often it\\nis not clear which feature is more important. For that reason,\\nall available attributes of the data set are used as features and\\nthe problem of identifying the important features is left to the\\nlearning model. This is definitely not a feasible approach,\\nparticularly for certain domains e.g. medical image\\nclassification, text categorization, etc. In case a model has to\\nbe trained to classify a document as spam or non-spam, we can\\nrepresent a document as a bag of words. Then the feature\\nspace will contain all unique words occurring across all\\ndocuments. This will easily be a feature space of a few\\nhundred thousand features. If we start including bigrams or\\ntrigrams along with words, the count of features will run in\\nmillions. To deal with this problem, feature transformation\\ncomes into play. Feature transformation is used as an effective\\ntool for dimensionality reduction and hence for boosting\\nlearning model performance. Broadly, there are two distinct\\ngoals of feature transformation:\\nAchieving best reconstruction of the original features in the data set\\nAchieving highest efficiency in the learning task\\nDid you know?'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 184, 'page_label': '185'}, page_content='In the field of natural language processing, ‘n-gram’ is a\\ncontiguous set of n items for example words in a text block\\nor document. Using numerical prefixes, n-gram of size 1 is\\ncalled unigram (i.e. a single word), size 2 is called bigram\\n(i.e. a two-word phrase), size 3 is called trigram (i.e. a\\nthree-word phrase) etc.\\n4.2.1 Feature construction\\nFeature construction involves transforming a given set of input\\nfeatures to generate a new set of more powerful features. To\\nunderstand more clearly, let’s take the example of a real estate\\ndata set having details of all apartments sold in a specific\\nregion.\\nThe data set has three features – apartment length,\\napartment breadth, and price of the apartment. If it is used as\\nan input to a regression problem, such data can be training data\\nfor the regression model. So given the training data, the model\\nshould be able to predict the price of an apartment whose price\\nis not known or which has just come up for sale. However,\\ninstead of using length and breadth of the apartment as a\\npredictor, it is much convenient and makes more sense to use\\nthe area of the apartment, which is not an existing feature of\\nthe data set. So such a feature, namely apartment area, can be\\nadded to the data set. In other words, we transform the three-\\ndimensional data set to a four-dimensional data set, with the\\nnewly ‘discovered’ feature apartment area being added to the\\noriginal data set. This is depicted in Figure 4.2.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 185, 'page_label': '186'}, page_content='FIG. 4.2 Feature construction (example 1)\\nNote: Though for the sake of simplicity the features\\napartment length and apartment breadth have been retained in\\nFigure 4.2, in reality, it makes more sense to exclude these\\nfeatures when building the model.\\nThere are certain situations where feature construction is an\\nessential activity before we can start with the machine learning\\ntask. These situations are\\nwhen features have categorical value and machine learning needs numeric\\nvalue inputs\\nwhen features having numeric (continuous) values and need to be converted\\nto ordinal values\\nwhen text-specific feature construction needs to be done\\n4.2.1.1 Encoding categorical (nominal) variables\\nLet’s take the example of another data set on athletes, as\\npresented in Figure 4.3a. Say the data set has features age, city\\nof origin, parents athlete (i.e. indicate whether any one of the\\nparents was an athlete) and Chance of Win. The feature chance\\nof a win is a class variable while the others are predictor\\nvariables. We know that any machine learning algorithm,\\nwhether it’s a classification algorithm (like kNN) or a\\nregression algorithm, requires numerical figures to learn from.\\nSo there are three features – City of origin, Parents athlete, and'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 186, 'page_label': '187'}, page_content='Chance of win, which are categorical in nature and cannot be\\nused by any machine learning task.\\nIn this case, feature construction can be used to create new\\ndummy features which are usable by machine learning\\nalgorithms. Since the feature ‘City of origin’ has three unique\\nvalues namely City A, City B, and City C, three dummy\\nfeatures namely origin_ city_A, origin_city_B, and\\norigin_city_C is created. In the same way, dummy features\\nparents_athlete_Y and parents_athlete_N are created for\\nfeature ‘Parents athlete’ and win_chance_Y and\\nwin_chance_N are created for feature ‘Chance of win’. The\\ndummy features have value 0 or 1 based on the categorical\\nvalue for the original feature in that row. For example, the\\nsecond row had a categorical value ‘City B’ for the feature\\n‘City of origin’. So, the newly created features in place of\\n‘City of origin’, i.e. origin_city_A, origin_city_B and\\norigin_city_C will have values 0, 1 and 0, respectively. In the\\nsame way, parents_athlete_Y and parents_athlete_N will have\\nvalues 0 and 1, respectively in row 2 as the original feature\\n‘Parents athlete’ had a categorical value ‘No’ in row 2. The\\nentire set of transformation for athletes’ data set is shown in\\nFigure 4.3b.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 187, 'page_label': '188'}, page_content='FIG. 4.3 Feature construction (encoding nominal variables)\\nHowever, examining closely, we see that the features\\n‘Parents athlete’ and ‘Chance of win’ in the original data set\\ncan have two values only. So creating two features from them\\nis a kind of duplication, since the value of one feature can be\\ndecided from the value of the other. To avoid this duplication,\\nwe can just leave one feature and eliminate the other, as shown\\nin Figure 4.3c.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 188, 'page_label': '189'}, page_content='4.2.1.2 Encoding categorical (ordinal) variables\\nLet’s take an example of a student data set. Let’s assume that\\nthere are three variable – science marks, maths marks and\\ngrade as shown in Figure 4.4a. As we can see, the grade is an\\nordinal variable with values A, B, C, and D. To transform this\\nvariable to a numeric variable, we can create a feature\\nnum_grade mapping a numeric value against each ordinal\\nvalue. In the context of the current example, grades A, B, C,\\nand D in Figure 4.4a is mapped to values 1, 2, 3, and 4 in the\\ntransformed variable shown in Figure 4.4b.\\nFIG. 4.4 Feature construction (encoding ordinal variables)\\n4.2.1.3 Transforming numeric (continuous) features to\\ncategorical features\\nSometimes there is a need of transforming a continuous\\nnumerical variable into a categorical variable. For example, we\\nmay want to treat the real estate price prediction problem,\\nwhich is a regression problem, as a real estate price category\\nprediction, which is a classification problem. In that case, we\\ncan ‘bin’ the numerical data into multiple categories based on\\nthe data range. In the context of the real estate price prediction\\nexample, the original data set has a numerical feature\\napartment_price as shown in Figure 4.5a. It can be'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 189, 'page_label': '190'}, page_content='transformed to a categorical variable price-grade either as\\nshown in Figure 4.5b or as shown in Figure 4.5c.\\n4.2.1.4 Text-specific feature construction\\nIn the current world, text is arguably the most predominant\\nmedium of communication. Whether we think about social\\nnetworks like Facebook or micro-blogging channels like\\nTwitter or emails or short messaging services such as\\nWhatsapp, text plays a major role in the flow of information.\\nHence, text mining is an important area of research – not only\\nfor technology practitioners but also for industry practitioners.\\nHowever, making sense of text data, due to the inherent\\nunstructured nature of the data, is not so straightforward. In the\\nfirst place, the text data chunks that we can think about do not\\nhave readily available features, like structured data sets, on\\nwhich machine learning tasks can be executed. All machine\\nlearning models need numerical data as input. So the text data\\nin the data sets need to be transformed into numerical features.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 190, 'page_label': '191'}, page_content='FIG. 4.5 Feature construction (numeric to categorical)\\nText data, or corpus which is the more popular keyword, is\\nconverted to a numerical representation following a process is\\nknown as vectorization. In this process, word occurrences in\\nall documents belonging to the corpus are consolidated in the\\nform of bag-of-words. There are three major steps that are\\nfollowed:\\n1. tokenize\\n2. count\\n3. normalize\\nIn order to tokenize a corpus, the blank spaces and\\npunctuations are used as delimiters to separate out the words,\\nor tokens. Then the number of occurrences of each token is\\ncounted, for each document. Lastly, tokens are weighted with'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 191, 'page_label': '192'}, page_content='reducing importance when they occur in the majority of the\\ndocuments. A matrix is then formed with each token\\nrepresenting a column and a specific document of the corpus\\nrepresenting each row. Each cell contains the count of\\noccurrence of the token in a specific document. This matrix is\\nknown as a document-term matrix (also known as a term-\\ndocument matrix). Figure 4.6 represents a typical document-\\nterm matrix which forms an input to a machine learning\\nmodel.\\nFIG. 4.6 Feature construction (text-specific)\\n4.2.2 Feature extraction\\nIn feature extraction, new features are created from a\\ncombination of original features. Some of the commonly used\\noperators for combining the original features include\\n1. For Boolean features: Conjunctions, Disjunctions, Negation, etc.\\n2. For nominal features: Cartesian product, M of N, etc.\\n3. For numerical features: Min, Max, Addition, Subtraction,\\nMultiplication, Division, Average, Equivalence, Inequality, etc.\\nLet’s take an example and try to understand. Say, we have a\\ndata set with a feature set F (F , F , …, F ). After feature\\nextraction using a mapping function f (F , F , …, F ) say, we\\ni 1 2 n\\n1 2 n'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 192, 'page_label': '193'}, page_content='will have a set of features \\n  such that \\nand m  <  n. For  example, \\n . This is depicted in\\nFigure 4.7.\\nFIG. 4.7 Feature extraction\\nLet’s discuss the most popular feature extraction algorithms\\nused in machine learning:\\n4.2.2.1 Principal Component Analysis\\nEvery data set, as we have seen, has multiple attributes or\\ndimensions – many of which might have similarity with each\\nother. For example, the height and weight of a person, in\\ngeneral, are quite related. If the height is more, generally\\nweight is more and vice versa. So if a data set has height and\\nweight as two of the attributes, obviously they are expected to\\nbe having quite a bit of similarity. In general, any machine\\nlearning algorithm performs better as the number of related\\nattributes or features reduced. In other words, a key to the'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 193, 'page_label': '194'}, page_content='success of machine learning lies in the fact that the features are\\nless in number as well as the similarity between each other is\\nvery less. This is the main guiding philosophy of principal\\ncomponent analysis (PCA) technique of feature extraction.\\nIn PCA, a new set of features are extracted from the original\\nfeatures which are quite dissimilar in nature. So an n-\\ndimensional feature space gets transformed to an m-\\ndimensional feature space, where the dimensions are\\northogonal to each other, i.e. completely independent of each\\nother. To understand the concept of orthogonality, we have to\\nstep back and do a bit of dip dive into vector space concept in\\nlinear algebra.\\nWe all know that a vector is a quantity having both\\nmagnitude and direction and hence can determine the position\\nof a point relative to another point in the Euclidean space (i.e.\\na two or three or ‘n’ dimensional space). A vector space is a\\nset of vectors. Vector spaces have a property that they can be\\nrepresented as a linear combination of a smaller set of vectors,\\ncalled basis vectors. So, any vector ‘v’ in a vector space can be\\nrepresented as\\nwhere, a represents ‘n’ scalars and u represents the basis\\nvectors. Basis vectors are orthogonal to each other.\\nOrthogonality of vectors in n-dimensional vector space can be\\nthought of an extension of the vectors being perpendicular in a\\ntwo-dimensional vector space. Two orthogonal vectors are\\ncompletely unrelated or independent of each other. So the\\ntransformation of a set of vectors to the corresponding set of\\ni i'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 194, 'page_label': '195'}, page_content='basis vectors such that each vector in the original set can be\\nexpressed as a linear combination of basis vectors helps in\\ndecomposing the vectors to a number of independent\\ncomponents.\\nNow, let’s extend this notion to the feature space of a data\\nset. The feature vector can be transformed to a vector space of\\nthe basis vectors which are termed as principal components.\\nThese principal components, just like the basis vectors, are\\northogonal to each other. So a set of feature vectors which may\\nhave similarity with each other is transformed to a set of\\nprincipal components which are completely unrelated.\\nHowever, the principal components capture the variability of\\nthe original feature space. Also, the number of principal\\ncomponent derived, much like the basis vectors, is much\\nsmaller than the original set of features.\\nThe objective of PCA is to make the transformation in such\\na way that\\n1. The new features are distinct, i.e. the covariance between the new\\nfeatures, i.e. the principal components is 0.\\n2. The principal components are generated in order of the variability in the\\ndata that it captures. Hence, the first principal component should\\ncapture the maximum variability, the second principal component\\nshould capture the next highest variability etc.\\n3. The sum of variance of the new features or the principal components\\nshould be equal to the sum of variance of the original features.\\nPCA works based on a process called eigenvalue\\ndecomposition of a covariance matrix of a data set. Below are\\nthe steps to be followed:\\n1. First, calculate the covariance matrix of a data set.\\n2. Then, calculate the eigenvalues of the covariance matrix.\\n3. The eigenvector having highest eigenvalue represents the direction in\\nwhich there is the highest variance. So this will help in identifying the'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 195, 'page_label': '196'}, page_content='first principal component.\\n4. The eigenvector having the next highest eigenvalue represents the\\ndirection in which data has the highest remaining variance and also\\northogonal to the first direction. So this helps in identifying the second\\nprincipal component.\\n5. Like this, identify the top ‘k’ eigenvectors having top ‘k’ eigenvalues so\\nas to get the ‘k’ principal components.\\n4.2.2.2 Singular value decomposition\\nSingular value decomposition (SVD) is a matrix factorization\\ntechnique commonly used in linear algebra. SVD of a matrix\\nA (m × n) is a factorization of the form:\\nwhere, U and V are orthonormal matrices, U is an m × m\\nunitary matrix, V is an n × n unitary matrix and ∑ is an m × n\\nrectangular diagonal matrix. The diagonal entries of ∑ are\\nknown as singular values of matrix A. The columns of U and\\nV are called the left-singular and right-singular vectors of\\nmatrix A, respectively.\\nSVD is generally used in PCA, once the mean of each\\nvariable has been removed. Since it is not always advisable to\\nremove the mean of a data attribute, especially when the data\\nset is sparse (as in case of text data), SVD is a good choice for\\ndimensionality reduction in those situations.\\nSVD of a data matrix is expected to have the properties\\nhighlighted below:\\n1. Patterns in the attributes are captured by the right-singular vectors, i.e.\\nthe columns of V.\\n2. Patterns among the instances are captured by the left-singular, i.e. the\\ncolumns of U.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 196, 'page_label': '197'}, page_content='3. Larger a singular value, larger is the part of the matrix A that it accounts\\nfor and its associated vectors.\\n4. New data matrix with ‘k’ attributes is obtained using the equation\\n \\nD = D × [v , v , … , v ]\\nThus, the dimensionality gets reduced to k\\nSVD is often used in the context of text data.\\n4.2.2.3 Linear Discriminant Analysis\\nLinear discriminant analysis (LDA) is another commonly used\\nfeature extraction technique like PCA or SVD. The objective\\nof LDA is similar to the sense that it intends to transform a\\ndata set into a lower dimensional feature space. However,\\nunlike PCA, the focus of LDA is not to capture the data set\\nvariability. Instead, LDA focuses on class separability, i.e.\\nseparating the features based on class separability so as to\\navoid over-fitting of the machine learning model.\\nUnlike PCA that calculates eigenvalues of the covariance\\nmatrix of the data set, LDA calculates eigenvalues and\\neigenvectors within a class and inter-class scatter matrices.\\nBelow are the steps to be followed:\\n1. Calculate the mean vectors for the individual classes.\\n2. Calculate intra-class and inter-class scatter matrices.\\n3. Calculate eigenvalues and eigenvectors for S  and S , where S  is\\nthe intra-class scatter matrix and S  is the inter-class scatter matrix \\nwhere, m is the mean vector of the i-th class\\n1 2 k\\nW B W\\nB\\ni\\n′\\n-1'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 197, 'page_label': '198'}, page_content='where, mi is the sample mean for each class, m is the overall mean of\\nthedata set, Ni is the sample size of each class\\n4. Identify the top ‘k’ eigenvectors having top ‘k’ eigenvalues\\n4.3 FEATURE SUBSET SELECTION\\nFeature selection is arguably the most critical pre-processing\\nactivity in any machine learning project. It intends to select a\\nsubset of system attributes or features which makes a most\\nmeaningful contribution in a machine learning activity. Let’s\\nquickly discuss a practical example to understand the\\nphilosophy behind feature selection. Say we are trying to\\npredict the weight of students based on past information about\\nsimilar students, which is captured in a ‘student weight’ data\\nset. The student weight data set has features such as Roll\\nNumber, Age, Height, and Weight. We can well understand\\nthat roll number can have no bearing, whatsoever, in\\npredicting student weight. So we can eliminate the feature roll\\nnumber and build a feature subset to be considered in this\\nmachine learning problem. The subset of features is expected\\nto give better results than the full set. The same has been\\ndepicted in Figure 4.8.\\nFIG. 4.8 Feature selection\\nBut before we go forward with more detailed discussion on\\nfeature selection, let’s try to understand the issues which have'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 198, 'page_label': '199'}, page_content='made feature selection such a relevant problem to be solved.\\n4.3.1 Issues in high-dimensional data\\nWith the rapid innovations in the digital space, the volume of\\ndata generated has increased to an unbelievable extent. At the\\nsame time, breakthroughs in the storage technology area have\\nmade storage of large quantity of data quite cheap. This has\\nfurther motivated the storage and mining of very large and\\nhigh-dimensionality data sets.\\nPoints to Ponder\\n‘High-dimensional’ refers to the high number of variables\\nor attributes or features present in certain data sets, more\\nso in the domains like DNA analysis, geographic\\ninformation systems (GIS), social networking, etc. The\\nhigh-dimensional spaces often have hundreds or thousands\\nof dimensions or attributes, e.g. DNA microarray data can\\nhave up to 450,000 variables (gene probes).\\nAlongside, two new application domains have seen drastic\\ndevelopment. One is that of biomedical research, which\\nincludes gene selection from microarray data. The other one is\\ntext categorization which deals with huge volumes of text data\\nfrom social networking sites, emails, etc. The first domain, i.e.\\nbiomedical research generates data sets having a number of\\nfeatures in the range of a few tens of thousands. The text data\\ngenerated from different sources also have extremely high\\ndimensions. In a large document corpus having few thousand\\ndocuments embedded, the number of unique word tokens'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 199, 'page_label': '200'}, page_content='which represent the feature of the text data set, can also be in\\nthe range of a few tens of thousands. To get insight from such\\nhigh-dimensional data may be a big challenge for any machine\\nlearning algorithm. On one hand, very high quantity of\\ncomputational resources and high amount of time will be\\nrequired. On the other hand the performance of the model –\\nboth for supervised and unsupervised machine learning task,\\nalso degrades sharply due to unnecessary noise in the data.\\nAlso, a model built on an extremely high number of features\\nmay be very difficult to understand. For this reason, it is\\nnecessary to take a subset of the features instead of the full set.\\nThe objective of feature selection is three-fold:\\nHaving faster and more cost-effective (i.e. less need for computational\\nresources) learning model\\nImproving the efficiency of the learning model\\nHaving a better understanding of the underlying model that generated the\\ndata\\n4.3.2 Key drivers of feature selection – feature relevance\\nand redundancy\\n4.3.2.1 Feature relevance\\nIn supervised learning, the input data set which is the training\\ndata set, has a class label attached. A model is inducted based\\non the training data set – so that the inducted model can assign\\nclass labels to new, unlabelled data. Each of the predictor\\nvariables, is expected to contribute information to decide the\\nvalue of the class label. In case a variable is not contributing\\nany information, it is said to be irrelevant. In case the\\ninformation contribution for prediction is very little, the\\nvariable is said to be weakly relevant. Remaining variables,\\nwhich make a significant contribution to the prediction task\\nare said to be strongly relevant variables.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 200, 'page_label': '201'}, page_content='In unsupervised learning, there is no training data set or\\nlabelled data. Grouping of similar data instances are done and\\nsimilarity of data instances are evaluated based on the value of\\ndifferent variables. Certain variables do not contribute any\\nuseful information for deciding the similarity of dissimilarity\\nof data instances. Hence, those variables make no significant\\ninformation contribution in the grouping process. These\\nvariables are marked as irrelevant variables in the context of\\nthe unsupervised machine learning task.\\nTo get a perspective, we can think of the simple example of\\nthe student data set that we discussed at the beginning of this\\nsection. Roll number of a student doesn’t contribute any\\nsignificant information in predicting what the Weight of a\\nstudent would be. Similarly, if we are trying to group together\\nstudents with similar academic capabilities, Roll number can\\nreally not contribute any information whatsoever. So, in\\ncontext of the supervised task of predicting student Weight or\\nthe unsupervised task of grouping students with similar\\nacademic merit, the variable Roll number is quite irrelevant.\\nAny feature which is irrelevant in the context of a machine\\nlearning task is a candidate for rejection when we are selecting\\na subset of features. We can consider whether the weakly\\nrelevant features are to be rejected or not on a case-to-case\\nbasis.\\n4.3.2.2 Feature redundancy\\nA feature may contribute information which is similar to the\\ninformation contributed by one or more other features. For\\nexample, in the weight prediction problem referred earlier in\\nthe section, both the features Age and Height contribute\\nsimilar information. This is because with an increase in Age,'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 201, 'page_label': '202'}, page_content='Weight is expected to increase. Similarly, with the increase of\\nHeight also Weight is expected to increase. Also, Age and\\nHeight increase with each other. So, in context of the Weight\\nprediction problem, Age and Height contribute similar\\ninformation. In other words, irrespective of whether the feature\\nheight is present as a part of the feature subset, the learning\\nmodel will give almost same results. In the same way, without\\nage being part of the predictor variables, the outcome of the\\nlearning model will be more or less same. In this kind of a\\nsituation when one feature is similar to another feature, the\\nfeature is said to be potentially redundant in the context of the\\nlearning problem.\\nAll features having potential redundancy are candidates for\\nrejection in the final feature subset. Only a small number of\\nrepresentative features out of a set of potentially redundant\\nfeatures are considered for being a part of the final feature\\nsubset.\\nSo, in a nutshell, the main objective of feature selection is to\\nremove all features which are irrelevant and take a\\nrepresentative subset of the features which are potentially\\nredundant. This leads to a meaningful feature subset in context\\nof a specific learning task.\\nNow, the question is how to find out which of the features\\nare irrelevant or which features have potential redundancy. For\\nthat multiple measures are being used, some of which have\\nbeen covered in the next sub-section.\\n4.3.3 Measures of feature relevance and redundancy\\n4.3.3.1 Measures of feature relevance'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 202, 'page_label': '203'}, page_content='As mentioned earlier, feature relevance is to be gauged by the\\namount of information contributed by a feature. For supervised\\nlearning, mutual information is considered as a good measure\\nof information contribution of a feature to decide the value of\\nthe class label. That’s why it is a good indicator of the\\nrelevance of a feature with respect to the class variable. Higher\\nthe value of mutual information of a feature, more relevant is\\nthat feature. Mutual information can be calculated as follows:\\n \\nMI(C, f ) = H(C) + H( f ) - H(C, f )\\n \\nwhere, marginal entropy of the class, H(C) = \\nmarginal entropy of the feature ‘x’, H( f ) = \\n \\nand K = number of classes, C = class variable, f = feature set\\nthat take discrete values.\\nIn case of unsupervised learning, there is no class variable.\\nHence, feature-to-class mutual information cannot be used to\\nmeasure the information contribution of the features. In case of\\nunsupervised learning, the entropy of the set of features\\nwithout one feature at a time is calculated for all the features.\\nThen, the features are ranked in a descending order of'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 203, 'page_label': '204'}, page_content='information gain from a feature and top ‘β’ percentage (value\\nof ‘β’ is a design parameter of the algorithm) of features are\\nselected as relevant features. The entropy of a feature f is\\ncalculated using Shannon’s formula below:\\n is used only for features that take discrete values. For\\ncontinuous features, it should be replaced by discretization\\nperformed first to estimate probabilities p(f = x).\\n4.3.3.2 Measures of Feature redundancy\\nFeature redundancy, as we have already discussed, is based on\\nsimilar information contribution by multiple features. There\\nare multiple measures of similarity of information\\ncontribution, salient ones being\\n1. Correlation-based measures\\n2. Distance-based measures, and\\n3. Other coefficient-based measure\\n1. Correlation-based similarity measure\\nCorrelation is a measure of linear dependency between two\\nrandom variables. Pearson’s product moment correlation\\ncoefficient is one of the most popular and accepted measures\\nof correlation between two random variables. For two random\\nfeature variables F  and F , Pearson correlation coefficient is\\ndefined as:\\n1 2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 204, 'page_label': '205'}, page_content='Correlation values range between +1 and –1. A correlation\\nof 1 (+ / –) indicates perfect correlation, i.e. the two features\\nhaving a perfect linear relationship. In case the correlation is 0,\\nthen the features seem to have no linear relationship.\\nGenerally, for all feature selection problems, a threshold value\\nis adopted to decide whether two features have adequate\\nsimilarity or not.\\n2. Distance-based similarity measure\\nThe most common distance measure is the Euclidean\\ndistance, which, between two features F  and F  are\\ncalculated as:\\nwhere F  and F  are features of an n-dimensional data set.\\nRefer to the Figure 4.9. The data set has two features, aptitude\\n(F ) and communication (F ) under consideration. The\\nEuclidean distance between the features has been calculated\\nusing the formula provided above.\\n1 2\\n1 2\\n1 2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 205, 'page_label': '206'}, page_content='FIG. 4.9 Distance calculation between features\\nA more generalized form of the Euclidean distance is the\\nMinkowski distance, measured as\\nMinkowski distance takes the form of Euclidean distance\\n(also called L norm) when r = 2.\\nAt r = 1, it takes the form of Manhattan distance (also\\ncalled L norm), as shown below:\\nA specific example of Manhattan distance, used more\\nfrequently to calculate the distance between binary vectors is\\nthe Hamming distance. For example, the Hamming distance\\n2\\n1'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 206, 'page_label': '207'}, page_content='between two vectors 01101011 and 11001001 is 3, as\\nillustrated in Figure 4.10a.\\n3. Other similarity measures\\nJaccard index/coefficient is used as a measure of similarity\\nbetween two features. The Jaccard distance, a measure of\\ndissimilarity between two features, is complementary of\\nJaccard index.\\nFIG. 4.10 Distance measures between features\\nFor two features having binary values, Jaccard index is\\nmeasured as'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 207, 'page_label': '208'}, page_content='where, n  = number of cases where both the features have\\nvalue 1\\nn  = number of cases where the feature 1 has value 0 and\\nfeature 2 has value 1\\nn  = number of cases where the feature 1 has value 1 and\\nfeature 2 has value 0\\nJaccard distance, d = 1 - J\\nLet’s consider two features F  and F  having values (0, 1, 1,\\n0, 1, 0, 1, 0) and (1, 1, 0, 0, 1, 0, 0, 0). Figure 4.10b shows the\\nidentification of the values of n , n  and n . As shown, the\\ncases where both the values are 0 have been left out without\\nborder – as an indication of the fact that they will be excluded\\nin the calculation of Jaccard coefficient.\\nJaccard coefficient of F  and F , J = \\n∴  Jaccard distance between F  and F , d = 1 – J = \\n  or 0.6.\\nSimple matching coefficient (SMC) is almost same as\\nJaccard coeficient except the fact that it includes a number of\\ncases where both the features have a value of 0.\\n11\\n01\\n10\\nJ\\n1 2\\n11 01 10\\n1 2\\n1 2 J'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 208, 'page_label': '209'}, page_content='where, n  = number of cases where both the features have\\nvalue 1\\nn  = number of cases where the feature 1 has value 0 and\\nfeature 2 has value 1\\nn  = number of cases where the feature 1 has value 1 and\\nfeature 2 has value 0\\nn  = number of cases where both the features have value 0\\nQuite understandably, the total count of rows, n = n + n\\n+ n + n . As shown in Figure 4.10c, all values have been\\nincluded in the calculation of SMC.\\nOne more measure of similarity using similarity coefficient\\ncalculation is Cosine Similarity. Let’s take the example of a\\ntypical text classification problem. The text corpus needs to be\\nfirst transformed into features with a word token being a\\nfeature and the number of times the word occurs in a\\ndocument comes as a value in each row. There are thousands\\nof features in such a text data set. However, the data set is\\nsparse in nature as only a few words do appear in a document,\\nand hence in a row of the data set. So each row has very few\\nnon-zero values. However, the non-zero values can be\\nanything integer value as the same word may occur any\\nnumber of times. Also, considering the sparsity of the data set,\\nthe 0-0 matches (which obviously is going to be pretty high)\\nneed to be ignored. Cosine similarity which is one of the most\\npopular measures in text classification is calculated as:\\n11\\n01\\n10\\n00\\n00 01\\n10 11'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 209, 'page_label': '210'}, page_content='where, x.y = vector dot product of x and y = \\nLet’s calculate the cosine similarity of x and y, where x = (2,\\n4, 0, 0, 2, 1, 3, 0, 0) and y = (2, 1, 0, 0, 3, 2, 1, 0, 1).\\nIn this case, x.y = 2*2 + 4*1 + 0*0 + 0*0 + 2*3 + 1*2 + 3*1 +\\n0*0 + 0*1 = 19\\nCosine similarity actually measures the angle (refer to Fig.\\n4.11) between x and y vectors. Hence, if cosine similarity has a\\nvalue 1, the angle between x and y is 0° which means x and y\\nare same except for the magnitude. If cosine similarity is 0, the\\nangle between x and y is 90°. Hence, they do not share any\\nsimilarity (in case of text data, no term/word is common). In\\nthe above example, the angle comes to be 43.2°.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 210, 'page_label': '211'}, page_content='FIG. 4.11 Cosine similarity\\n4.3.4 Overall feature selection process\\nFeature selection is the process of selecting a subset of features\\nin a data set. As depicted in Figure 4.12, a typical feature\\nselection process consists of four steps:\\n1. generation of possible subsets\\n2. subset evaluation\\n3. stop searching based on some stopping criterion\\n4. validation of the result\\nFIG. 4.12 Feature selection process\\nSubset generation, which is the first step of any feature\\nselection algorithm, is a search procedure which ideally should\\nproduce all possible candidate subsets. However, for an n-\\ndimensional data set, 2  subsets can be generated. So, as the\\nvalue of ‘n’ becomes high, finding an optimal subset from all\\nthe 2  candidate subsets becomes intractable. For that reason,\\nn\\nn'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 211, 'page_label': '212'}, page_content='different approximate search strategies are employed to find\\ncandidate subsets for evaluation. On one hand, the search may\\nstart with an empty set and keep adding features. This search\\nstrategy is termed as a sequential forward selection. On the\\nother hand, a search may start with a full set and successively\\nremove features. This strategy is termed as sequential\\nbackward elimination. In certain cases, search start with both\\nends and add and remove features simultaneously. This\\nstrategy is termed as a bi-directional selection.\\nEach candidate subset is then evaluated and compared with\\nthe previous best performing subset based on certain\\nevaluation criterion. If the new subset performs better, it\\nreplaces the previous one.\\nThis cycle of subset generation and evaluation continues till\\na pre-defined stopping criterion is fulfilled. Some commonly\\nused stopping criteria are\\n1. the search completes\\n2. some given bound (e.g. a specified number of iterations) is reached\\n3. subsequent addition (or deletion) of the feature is not producing a better\\nsubset\\n4. a sufficiently good subset (e.g. a subset having better classification\\naccuracy than the existing benchmark) is selected\\nThen the selected best subset is validated either against\\nprior benchmarks or by experiments using real-life or synthetic\\nbut authentic data sets. In case of supervised learning, the\\naccuracy of the learning model may be the performance\\nparameter considered for validation. The accuracy of the\\nmodel using the subset derived is compared against the model\\naccuracy of the subset derived using some other benchmark\\nalgorithm. In case of unsupervised, the cluster quality may be\\nthe parameter for validation.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 212, 'page_label': '213'}, page_content='4.3.5 Feature selection approaches\\nThere are four types of approach for feature selection:\\n1. Filter approach\\n2. Wrapper approach\\n3. Hybrid approach\\n4. Embedded approach\\nIn the filter approach (as depicted in Fig. 4.13), the feature\\nsubset is selected based on statistical measures done to assess\\nthe merits of the features from the data perspective. No\\nlearning algorithm is employed to evaluate the goodness of the\\nfeature selected. Some of the common statistical tests\\nconducted on features as a part of filter approach are –\\nPearson’s correlation, information gain, Fisher score, analysis\\nof variance (ANOVA), Chi-Square, etc.\\nFIG. 4.13 Filter approach\\nIn the wrapper approach (as depicted in Fig. 4.14),\\nidentification of best feature subset is done using the induction\\nalgorithm as a black box. The feature selection algorithm\\nsearches for a good feature subset using the induction\\nalgorithm itself as a part of the evaluation function. Since for\\nevery candidate subset, the learning model is trained and the\\nresult is evaluated by running the learning algorithm, wrapper\\napproach is computationally very expensive. However, the\\nperformance is generally superior compared to filter approach.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 213, 'page_label': '214'}, page_content='FIG. 4.14 Wrapper approach\\nHybrid approach takes the advantage of both filter and\\nwrapper approaches. A typical hybrid algorithm makes use of\\nboth the statistical tests as used in filter approach to decide the\\nbest subsets for a given cardinality and a learning algorithm to\\nselect the final best subset among the best subsets across\\ndifferent cardinalities.\\nEmbedded approach (as depicted in Fig. 4.15) is quite\\nsimilar to wrapper approach as it also uses and inductive\\nalgorithm to evaluate the generated feature subsets. However,\\nthe difference is it performs feature selection and classification\\nsimultaneously.\\nFIG. 4.15 Embedded approach'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 214, 'page_label': '215'}, page_content='4.4 SUMMARY\\nA feature is an attribute of a data set that is used in a machine learning\\nprocess.\\nFeature engineering is an important pre-processing step for machine\\nlearning, having two major elements:\\n1. feature transformation\\n2. feature subset selection\\nFeature transformation transforms data into a new set of features which can\\nrepresent the underlying machine learning problem\\nThere are two variants of feature transformation:\\n1. feature construction\\n2. feature extraction\\nFeature construction process discovers missing information about the\\nrelationships between features and augments the feature space by creating\\nadditional features.\\nFeature extraction is the process of extracting or creating a new set of\\nfeatures from the original set of features using some functional mapping.\\nSome popular feature extraction algorithms used in machine learning:\\n1. Principal Component Analysis (PCA)\\n2. Singular Value Decomposition (SVD)\\n3. Linear Discriminant Analysis (LDA)\\nFeature subset selection is intended to derive a subset of features from the\\nfull feature set. No new feature is generated.\\nThe objective of feature selection is three-fold:\\n1. Having faster and more cost-effective (i.e. less need for\\ncomputational resources) learning model\\n2. Improving the efficiency of the learning model\\n3. Having a better understanding of the underlying model that\\ngenerated the data\\nFeature selection intends to remove all features which are\\nirrelevant and take a representative subset of the features which\\nare potentially redundant. This leads to a meaningful feature\\nsubset in context of a specific learning task.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 215, 'page_label': '216'}, page_content='Feature relevance is indicated by the information gain from a feature\\nmeasured in terms of relative entropy.\\nFeature redundancy is based on similar information contributed by multiple\\nfeatures measured by feature-to-feature:\\n1. Correlation\\n2. Distance (Minkowski distances, e.g. Manhattan, Euclidean, etc.\\nused as most popular measures)\\n3. Other coefficient-based (Jaccard, SMC, Cosine similarity, etc.)\\nMain approaches for feature selection are\\n1. Filter\\n2. Wrapper\\n3. Hybrid\\n4. Embedded\\nSAMPLE QUESTIONS\\nMULTIPLE-CHOICE QUESTIONS (1 MARK QUESTIONS)\\n1. Engineering a good feature space is a crucial ___ for the success of any\\nmachine learning model.\\n1. Pre-requisite\\n2. Process\\n3. Objective\\n4. None of the above\\n2. n-gram of size 1 is called\\n1. Bigram\\n2. Unigram\\n3. Trigram\\n4. None of the above\\n3. Feature ___ involves transforming a given set of input features to\\ngenerate a new set of more powerful features.\\n1. Selection\\n2. Engineering\\n3. Transformation\\n4. Re-engineering\\n4. Conversion of a text corpus to a numerical representation is done using\\n___ process.\\n1. Tokenization\\n2. Normalization\\n3. Vectorization'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 216, 'page_label': '217'}, page_content='4. None of the above\\n5. ___ approach uses induction algorithm for subset validation.\\n1. Filter\\n2. Hybrid\\n3. Wrapper\\n4. Embedded\\n6. In feature extraction, some of the commonly used ___ are used for\\ncombining the original features.\\n1. Operators\\n2. Delimiters\\n3. Words\\n4. All of the above\\n7. Hamming distance between binary vectors 1001 and 0101 is\\n1. 1\\n2. 2\\n3. 3\\n4. 4\\n8. PCA is a technique for\\n1. Feature extraction\\n2. Feature construction\\n3. Feature selection\\n4. None of the above\\n9. The new features created in PCA are known as\\n1. Principal components\\n2. Eigenvectors\\n3. Secondary components\\n4. None of the above\\n10. In LDA, intra-class and inter-class ___ matrices are calculated.\\n1. Scatter\\n2. Adjacency\\n3. Similarity\\n4. None of the above\\n11. Cosine similarity is most popularly used in\\n1. Text classification\\n2. Image classification\\n3. Feature selection\\n4. None of the above\\n12. This approach is quite similar to wrapper approach as it also uses and\\ninductive algorithm to evaluate the generated feature subsets.\\n1. Embedded approach\\n2. Filter approach\\n3. Pro Wrapper approach\\n4. Hybrid approach\\n13. In ___ approach, identification of best feature subset is done using the\\ninduction algorithm as a black box.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 217, 'page_label': '218'}, page_content='1. Embedded\\n2. Filter\\n3. Wrapper\\n4. Hybrid\\nSHORT-ANSWER TYPE QUESTIONS (5 MARKS QUESTIONS)\\n1. What is a feature? Explain with an example.\\n2. What are the different situations which necessitate feature construction?\\n3. Explain the process of encoding nominal variables.\\n4. Explain the process of transforming numeric features to categorical\\nfeatures.\\n5. Explain the wrapper approach of feature selection. What are the merits\\nand de-merits of this approach?\\n6. When can a feature be termed as irrelevant? How can it be measured?\\n7. When can a feature be termed as redundant? What are the measures to\\ndetermine the potentially redundant features?\\n8. What are the different distance measures that can be used to determine\\nsimilarity of features?\\n9. Compare Euclidean distance with Manhattan distance?\\n10. Differentiate feature transformation with feature selection\\n11. Write short notes on any two:\\n1. SVD\\n2. Hybrid method of feature selection\\n3. Silhouette width\\n4. ROC curve\\nLONG-ANSWER TYPE QUESTIONS (10 MARKS QUESTIONS)\\n1. What is feature engineering? Explain, in details, the different aspects of\\nfeature engineering?\\n2. What is feature selection? Why is it needed? What are the different\\napproaches of feature selection?\\n3. Explain the filter and wrapper approaches of feature selection. What are\\nthe merits and demerits of these approaches?\\n4. 1. Explain the overall process of feature selection\\n2. Explain, with an example, the main underlying concept of feature\\nextraction. What are the most popular algorithms for feature\\nextraction?\\n5. Explain the process of feature engineering in context of a text\\ncategorization problem.\\n6. Why is cosine similarity a suitable measure in context of text\\ncategorization? Two rows in a document-term matrix have values - (2,'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 218, 'page_label': '219'}, page_content='3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the cosine\\nsimilarity.\\n7. 1. How can we calculate Hamming distance? Find the Hamming\\ndistance between 10001011 and 11001111.\\n2. Compare the Jaccard index and similarity matching coefficient of\\ntwo features having values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 0, 0, 1, 1,\\n0, 0, 1).\\n8. What do you understand by a high-dimensional data set? Give a few\\npractical examples? What is the challenge while applying machine\\nlearning technique on a high-dimensional data set? How can that be\\naddressed?\\n9. 1. Write short notes on any two:\\n1. PCA\\n2. Vectorization\\n3. Embedded method\\n2. Write the difference between (any two):\\n1. Sequential forward selection vs. sequential backward\\nelimination\\n2. Filter vs. wrapper method of feature selection\\n3. Jaccard coefficient vs. SMC'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 219, 'page_label': '220'}, page_content='Chapter 5\\nBrief Overview of Probability\\nOBJECTIVE OF THE CHAPTER\\nThe principles of machine learning are largely dependent\\non effectively handling the uncertainty in data and\\npredicting the outcome based on data in hand. All the\\nlearning processes we will be discussing in later chapters\\nof this book expects the readers to understand the\\nfoundational rules of probability, the concept of random\\nand continuous variables, distribution, and sampling\\nprinciples and few basic principles such as central limit\\ntheorem, hypothesis testing, and Monte Carlo\\napproximations. As these rules, theorems, and principles\\nform the basis of learning principles, we will be discussing\\nthose in this chapter with examples and illustrations.\\n5.1 INTRODUCTION\\nAs we discussed in previous chapters, machine learning\\nprovides us a set of methods that can automatically detect\\npatterns in data, and then can be used to uncover patterns to\\npredict future data, or to perform other kinds of decision'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 220, 'page_label': '221'}, page_content='making under uncertainty. The best way to perform such\\nactivities on top of huge data set known as big data is to use\\nthe tools of probability theory because probability theory can\\nbe applied to any situation involving uncertainty. In machine\\nlearning there may be uncertainties in different forms like\\narriving at the best prediction of future given the past data,\\narriving at the best model based on certain data, arriving at the\\nconfidence level while predicting the future outcome based on\\npast data, etc. The probabilistic approach used in machine\\nlearning is closely related to the field of statistics, but the\\nemphasis is in a different direction as we see in this chapter. In\\nthis chapter we will discuss the tools, equations, and models of\\nprobability that are useful for machine learning domain.\\n5.2 IMPORTANCE OF STATISTICAL TOOLS IN MACHINE LEARNING\\nIn machine learning, we train the system by using a limited\\ndata set called ‘training data’ and based on the confidence\\nlevel of the training data we expect the machine learning\\nalgorithm to depict the behaviour of the larger set of actual\\ndata. If we have observation on a subset of events, called\\n‘sample’, then there will be some uncertainty in attributing the\\nsample results to the whole set or population. So, the question\\nwas how a limited knowledge of a sample set can be used to\\npredict the behaviour of a real set with some confidence. It\\nwas realized by mathematicians that even if some knowledge\\nis based on a sample, if we know the amount of uncertainty\\nrelated to it, then it can be used in an optimum way without\\ncausing loss of knowledge. Refer Figure 5.1\\nFIG. 5.1 Knowledge and uncertainty'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 221, 'page_label': '222'}, page_content='Probability theory provides a mathematical foundation for\\nquantifying this uncertainty of the knowledge. As the\\nknowledge about the training data comes in the form of\\ninterdependent feature sets, the conditional probability\\ntheories (especially the Bayes theorem), discussed later in this\\nchapter form the basis for deriving required confidence level\\nof the training data.\\nDifferent distribution principles discussed in this chapter\\ncreate the view of how the data set that we will be dealing with\\nin machine learning can behave, in terms of their feature\\ndistributions. It is important to understand the mathematical\\nfunction behind each of these distributions so that we can\\nunderstand how the data is spread out from its average value –\\ndenoted by the mean and the variance. While choosing the\\nsamples from these distribution sets we should be able to\\ncalculate to what extent the sample is representing the actual\\nbehaviour of the full data set. These along with the test of\\nhypothesis principles build the basis for finding out the\\nuncertainty in the training data set to represent the actual data\\nset which is the fundamental principle of machine learning.\\n5.3 CONCEPT OF PROBABILITY – FREQUENTIST AND BAYESIAN\\nINTERPRETATION\\nThe concept of probability is not new. In our day to day life,\\nwe use the concept of probability in many places. For\\nexample, when we talk about probabilities of getting the heads\\nand the tails when a coin is flipped are equal, we actually\\nintend to say that if a coin is flipped many times, the coin will\\nland heads a same number of times as it lands tails. This is the\\nfrequentist interpretation of probability. This interpretation\\nrepresents the long run frequencies of events. Another\\nimportant interpretation of probability tries to quantify the\\nuncertainty of some event and thus focuses on information'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 222, 'page_label': '223'}, page_content='rather than repeated trials. This is called the Bayesian\\ninterpretation of probability. Taking the same example of coin\\nflipping is interpreted as – the coin is equally likely to land\\nheads or tails when we flip the coin next.\\nThe reason the Bayesian interpretation can be used to model\\nthe uncertainty of events is that it does not expect the long run\\nfrequencies of the events to happen. For example, if we have\\nto compute the probability of Brazil winning 2018 football\\nworld cup final, that event can happen only once and can’t be\\nrepeated over and over again to calculate its probability. But\\nstill, we should be able to quantify the uncertainty about the\\nevent and which is only possible if we interpret probability the\\nBayesian way. To give some more machine learning oriented\\nexamples, we are starting a new software implementation\\nproject for a large customer and want to compute the\\nprobability of this project getting into customer escalation\\nbased on data from similar projects in the past, or we want to\\ncompute the probability of a tumor to be malignant or not\\nbased on the probability distribution of such cases among the\\npatients of similar profile. In all these cases it is not possible to\\ndo a repeated trial of the event and the Bayesian concept is\\nvalid for computing the uncertainty. So, in this book, we will\\nfocus on the Bayesian interpretation to develop our machine\\nlearning models. The basic formulae of probability remain the\\nsame anyway irrespective of whatever interpretation we adopt.\\n5.3.1 A brief review of probability theory\\nWe will briefly touch upon the basic probability theory in this\\nsection just as a refresher so that we can move to the building\\nblocks of machine learning way of use of probability. As\\ndiscussed in previous chapters, the basic concept of machine\\nlearning is that we want to have a limited set of ‘Training’ data'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 223, 'page_label': '224'}, page_content='that we use as a representative of a large set of Actual data and\\nthrough probability distribution we try to find out how an\\nevent which is matching with the training data can represent\\nthe outcome with some confidence.\\n5.3.1.1 Foundation rules\\nWe will review the basic rules of probability in this section.\\nLet us introduce few notations that will be used throughout\\nthis book.\\np(A) denotes the probability that the event A is true. For\\nexample, A might be the logical statement ‘Brazil is going to\\nwin the next football world cup final’. The expression 0 ≤ p(A)\\n≤ 1 denotes that the probability of this event happening lies\\nbetween 0 and 1, where p(A) = 0 means the event will\\ndefinitely not happen, and p(A) = 1 means the event will\\ndefinitely happen. The notation p(A̅) denotes the probability of\\nthe event not A; this is defined as p(A̅) = 1 − p(A). It is also\\ncommon practice to write A = 1 to mean the event A is true,\\nand A = 0 to mean the event A is false. So, this is a binary\\nevent where the event is either true or false but can’t be\\nsomething indefinite.\\nThe probability of selecting an event A, from a sample size\\nof X is defined as\\np(A) = \\n , where n is the number of times the instance of\\nevent A is present in the sample of size X.\\n5.3.1.2 Probability of a union of two events'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 224, 'page_label': '225'}, page_content='Two events A and B are called mutually exclusive if they can’t\\nhappen together. For example, England winning the Football\\nWorld Cup 2018 and Brazil winning the Football World Cup\\n2018 are two mutually exclusive events and can’t happen\\ntogether. For any two events, A and B, we define the\\nprobability of A or B as\\n5.3.1.3 Joint probabilities\\nThe probability of the joint event A and B is defined as the\\nproduct rule:\\nwhere p(A|B) is defined as the conditional probability of\\nevent A happening if event B happens. Based on this joint\\ndistribution on two events p(A, B),we can define the\\nmarginal distribution as follows:\\nsumming up the all probable states of B gives the total\\nprobability formulae, which is also called sum rule or the\\nrule of total probability.\\nSame way, p(B) can be defined as'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 225, 'page_label': '226'}, page_content='This formula can be extended for the countably infinite\\nnumber of events in the set and chain rule of probability can\\nbe derived if the product rule is applied multiple times as\\n5.3.1.4 Conditional probability\\nWe define the conditional probability of event A, given that\\nevent B is true, as follows:\\nwhere, p(A, B) is the joint probability of A and B and can\\nalso be denoted as p(A ∩ B)\\nSimilarly,\\nIllustration. In a toy-making shop, the automated machine\\nproduces few defective pieces. It is observed that in a lot of\\n1,000 toy parts, 25 are defective. If two random samples are\\nselected for testing without replacement (meaning that the first\\nsample is not put back to the lot and thus the second sample is\\nselected from the lot size of 999) from the lot, calculate the\\nprobability that both the samples are defective.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 226, 'page_label': '227'}, page_content='Solution: Let A denote the probability of first part being\\ndefective and B denote the second part being defective. Here,\\nwe have to employ the conditional probability of the second\\npart being found defective when the first part is already found\\ndefective.\\nBy law of probability, p(A) = \\nAs we are selecting the second sample without replacing the\\nfirst sample into the lot and the first one is already found\\ndefective, there are now 24 defective pieces out of 999 pieces\\nleft in the lot.\\nwhich is the probability of both the parts being found\\ndefective.\\n5.3.1.5 Bayes rule\\nThe Bayes rule, also known as Bayes Theorem, can be\\nderived by combining the definition of conditional probability\\nwith the product and sum rules, as below:'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 227, 'page_label': '228'}, page_content='Illustration: Flagging an email as spam based on the\\nsender name of the email\\nLet’s take an example to identify the probability of an email to\\nbe really spam based on the name of the sender. We often\\nreceive email from mail id containing junk characters or words\\nsuch as bulk, mass etc. which turn out to be a spam mail. So,\\nwe want our machine learning agent to flag the spam emails\\nfor us so that we can easily delete them.\\nLet’s also assume that we have knowledge about the\\nreliability of the assumption that emails with sender names\\n‘mass’ and ‘bulk’ are spam email is 80% meaning that if some\\nemail has sender name with ‘mass’ or ‘bulk’ then the email\\nwill be spam with probability 0.8. The probability of false\\nalarm is 10% meaning that the agent will show the positive\\nresult as spam even if the email is not a spam with a\\nprobability 0.1. Also, we have a prior knowledge that only\\n0.4% of the total emails received are spam.\\nSolution:'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 228, 'page_label': '229'}, page_content='Let x be the event of the flag being set as spam because the\\nsender name has the words ‘mass’ or ‘bulk’ and y be the event\\nof some mail really being spam.\\nSo, p(x = 1|y = 1) = 0.8, which is the probability of the flag\\nbeing positive if the email is spam.\\nMany times this probability is wrongly interpreted as the\\nprobability of an email being spam if the flag is positive. But\\nthat is not true, because we will then ignore the prior\\nknowledge that only 0.4% of the emails are actually spam and\\nalso there can be a false alarm in 10% of the cases.\\nSo, p(y = 1) = 0.004 and p(y = 0) = 1 − p(y = 1) = 0.996\\n \\np(x = 1 | y = 0 ) = 0 .1\\nCombining these terms using the Bayes rule, we can compute\\nthe correct answer as follows:\\nThis means that if we combine our prior knowledge with the\\nactual test results then there is only about a 3% chance of\\nemails actually being spam if the sender name contains the\\nterms ‘bulk’ or ‘mass’. This is significantly lower than the\\nearlier assumption of 80% chance and thus calls for additional'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 229, 'page_label': '230'}, page_content='checks before someone starts deleting such emails based on\\nonly this flag.\\nDetails of the practice application of Bayes theorem in\\nmachine learning is discussed in Chapter 6.\\n5.4 RANDOM VARIABLES\\nLet’s take an example of tossing a coin once. We can associate\\nrandom variables X and Y as\\nX(H) = 1, X(T) = 0, which means this variable is associated\\nwith the outcome of the coin facing head.\\nY(H) = 0, Y(T) = 1, which means this variable is associated\\nwith the outcome of the coin facing tails.\\nHere in the sample space S which is the outcome related to\\ntossing the coin once, random variables represent the single-\\nvalued real function [X(ζ)] that assigns a real number, called\\nits value to each sample point of S. A random variable is not a\\nvariable but is a function. The sample space S is called the\\ndomain of random variable X and the collection of all the\\nnumbers, i.e. values of X(ζ), is termed the range of the random\\nvariable.\\nWe can define the event (X = x) where x is a fixed real\\nnumber as\\n \\n(X = x) = { ζ:X(ζ) = x }\\nAccordingly, we can define the following events for fixed\\nnumbers x, x , and x :1 2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 230, 'page_label': '231'}, page_content='(X ≤ x) = { ζ:X(ζ) ≤ x }\\n(X > x) = { ζ:X(ζ) > x }\\n(x < X ≤ x ) = { ζ:x < X(ζ) ≤ x  }\\nThe probabilities of these events are denoted by\\n \\nP(X = x) = P{ ζ:X(ζ) = x }\\nP(X ≤ x) = P{ ζ:X(ζ) ≤ x }\\nP(X > x) = P{ ζ:X(ζ) > x }\\nP(x < X ≤ x ) = P{ ζ:x < X(ζ) ≤ x  }\\nWe will use the term cdf in this book to denote the\\ndistribution function or cumulative distribution function,\\nwhich takes the form of random variable X as\\n \\nF(x ) = P(X ≤ x ) - ∞ < x < ∞\\nSome of the important properties of F(x) are\\n5.4.1 Discrete random variables\\nLet us extend the concept of binary events by defining a\\ndiscrete random variable X. Let X be a random variable with\\ncdf F(x) and it changes values only in jumps (a countable\\nnumber of them) and remains constant between the jumps then\\n1 2 1 2\\n1 2 1 2\\nx\\nx\\nx'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 231, 'page_label': '232'}, page_content='it is called a discrete random variable. So, the definition of a\\ndiscrete random variable is that its range or the set X contains a\\nfinite or countably infinite number of points.\\nLet us consider that the jumps of F(x) of the discrete\\nrandom variable X is occurring at points x x x  where this\\nsequence represents a finite or countably infinite set and x <x\\nif i < j. See Figure 5.2.\\nWe can denote this through notation as\\nThe probability of the event that X = x is denoted as p(X =\\nx), or just p(x) for short. p is called a probability mass\\nfunction (pmf).\\nx\\n1, 2, 3….,\\ni j'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 232, 'page_label': '233'}, page_content='FIG. 5.2 A discrete random variable X with cdf F(x) = p(X ≤ x) for x = 0, 1, 2, 3,\\n4, and F(x) has jumps at x = 0, 1, 2, 3, 4\\nThis satisfies the properties 0 ≤ p(x) ≤ 1 and Ʃ  p(x) = 1.\\nThe cumulative distribution function (cdf) F(x) of a\\ndiscrete random variable X can be denoted as\\nRefer to Figure 5.3, the pmf is defined on a finite state\\nspace X = {1, 2, 3, 4} that shows a uniform distribution with\\np(x ) = \\n . This distribution means that X is always equal to the\\nvalue \\n , in other words, it is a constant.\\nx\\nx\\nx∈X\\nx'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 233, 'page_label': '234'}, page_content='Example. Suppose X is a discrete random variable with\\nR {0, 1, 2, …}. we can prove \\n .\\nFrom the equation above,\\n \\nP(X > 0) = P (1) + P (2) + P (3) + P (4) + …,\\nP(X > 1) = P (2) + P (3) + P (4) + …,\\nP(X > 2) = P (3) + P (4) + P (5) + ….\\nThus,\\nX∈\\nX X X X\\nX X X\\nX X X'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 234, 'page_label': '235'}, page_content='FIG. 5.3 A uniform distribution within the set {1, 2, 3, 4} where p(x = k) = \\nDISCUSSION POINTS\\nDiscuss few examples of discrete random variables in\\npractical life. How do you think the behaviour is different\\nfrom the continuous random variables?\\n5.4.2 Continuous random variables'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 235, 'page_label': '236'}, page_content='We have discussed the probabilities of uncertain discrete\\nquantities. But most of the real-life events are continuous in\\nnature. For example, if we have to measure the actual time\\ntaken to finish an activity then there can be an infinite number\\nof possible ways to complete the activity and thus the\\nmeasurement is continuous and not discrete as it is not similar\\nto the discrete event of rolling a dice or flipping a coin. Here,\\nthe function F(x) describing the event is continuous and also\\nhas a derivative \\n  that exists and is piecewise continuous.\\nThe probability density function (pdf) of the continuous\\nrandom variable x is defined as\\nWe will now see how to extend probability to reason about\\nuncertain continuous quantities. The probability that x lies in\\nany interval a ≤ x≤ b can be computed as follows.\\nWe can define the events A = (x≤ a), B = (x≤ b), and W = (a\\n< x≤ b). We have that B = A∪W, and since A and W are\\nmutually exclusive, according to the sum rules:\\nand hence,\\nx'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 236, 'page_label': '237'}, page_content='Define the function F(q) = p(X ≤ q). The cumulative\\ndistribution function (cdf) of random variable X can be\\nobtained by\\nwhich is a monotonically increasing function. Using this\\nnotation, we have\\nusing the pdf of x, we can compute the probability of the\\ncontinuous variable being in a finite interval as follows:\\nAs the size of the interval gets smaller, it is possible to write\\n5.4.2.1 Mean and variance\\nThe mean in statistical terms represents the weighted average\\n(often called as an expected value) of the all the possible\\nvalues of random variable X and each value is weighted by its\\nprobability. It is denoted by µ or E(X) and defined asx'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 237, 'page_label': '238'}, page_content='mean is one of the important parameters for a probability\\ndistribution.\\nVariance of a random variable X measures the spread or\\ndispersion of X. If E(X) is the mean of the random variable X,\\nthen the variance is given by\\nPoints to Ponder:\\nThe difference between Probability Density Function (pdf)\\nand the Probability Mass Function (pmf) is that the latter is\\nassociated with the continuous random variable and the\\nformer is associated with the discrete random variable.\\nWhile pmf represents the probability that a discrete\\nrandom variable is exactly equal to some value, the pdf\\ndoes not represent a probability by itself. The integration\\nof pdf over a continuous interval yields the probability.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 238, 'page_label': '239'}, page_content='5.5 SOME COMMON DISCRETE DISTRIBUTIONS\\nIn this section, we will discuss some commonly used\\nparametric distributions defined on discrete state spaces, both\\nfinite and countably infinite.\\n5.5.1 Bernoulli distributions\\nWhen we have a situation where the outcome of a trial is\\nwithered ‘success’ or ‘failure’, then the behaviour of the\\nrandom variable X can be represented by Bernoulli\\ndistribution. Such trials or experiments are termed as Bernoulli\\ntrials.\\nSo we can derive that, a random variable X is called\\nBernoulli random variable with parameter p when its pmf\\ntakes the form of\\n \\nPx (k) = P(X = k) = p (1 9 p )\\nWhere 0 ≤ p ≤ 1. So, using the cdf F(x) of Bernoulli\\nrandom variable is expressed as\\nThe mean and variance of Bernoulli random variable X are\\nx\\nk 1 -k'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 239, 'page_label': '240'}, page_content='here, the probability of success is p and probability of\\nfailure is 1 – p. This is obviously just a special case of a\\nBinomial distribution with n = 1 as we will discuss below.\\n5.5.2 Binomial distribution\\nIf n independent Bernoulli trials are performed and X\\nrepresents the number of success in those n trials, then X is\\ncalled a binomial random variable. That’s the reason a\\nBernoulli random variable is a special case of binomial\\nrandom variable with parameters (1, p).\\nThe pmf of X with parameters (n, p) is given by\\nWhere, 0 ≤ p ≤ 1 and\\n is also called the binomial coefficient\\nwhich is the number of ways to choose k items from n.\\nFor example, if we toss a coin n times. Let X ∈  {0, …, n}\\nbe the number of heads. If the probability of heads is p, then\\nwe say X has a binomial distribution, written as X ~Bin(n, p).\\nThe corresponding cdf of x is'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 240, 'page_label': '241'}, page_content='This distribution has the following mean and variance:\\nFigure 5.4 shows the binomial distribution of n = 6 and p =\\n0.6\\nFIG. 5.4 A binomial distribution of n = 6 and p = 0.6\\n5.5.3 The multinomial and multinoulli distributions'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 241, 'page_label': '242'}, page_content='The binomial distribution can be used to model the outcomes\\nof coin tosses, or for experiments where the outcome can be\\neither success or failure. But to model the outcomes of tossing\\na K-sided die, or for experiments where the outcome can be\\nmultiple, we can use the multinomial distribution. This is\\ndefined as: let x = (x , …, x ) be a random vector, where x is\\nthe number of times side j of the die occurs. Then x has the\\nfollowing pmf:\\nwhere, x  + x  +….. x  = n and\\nthe multinomial coefficient is defined as\\nand the summation is over the set of all non-negative\\nintegers x , x , … . x  whose sum is n.\\nNow consider a special case of n = 1, which is like rolling a\\nK-sided dice once, so x will be a vector of 0s and 1s (a bit\\nvector), in which only one bit can be turned on. This means, if\\nthe dice shows up face k, then the k’th bit will be on. We can\\nconsider x as being a scalar categorical random variable with K\\nstates or values, and x is its dummy encoding with x = [Π(x =\\n1), …,Π(x = K)].\\nFor example, if K = 3, this states that 1, 2, and 3 can be\\nencoded as (1, 0, 0), (0, 1, 0), and (0, 0, 1). This is also called a\\none-hot encoding, as we interpret that only one of the K\\n1 K j\\n1 2 K\\n1 2 k'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 242, 'page_label': '243'}, page_content='‘wires’ is ‘hot’ or on. This very common special case is known\\nas a categorical or discrete distribution and because of the\\nanalogy with the Binomial/ Bernoulli distinction, Gustavo\\nLacerda suggested that this is called the multinoulli\\ndistribution.\\n5.5.4 Poisson distribution\\nPoisson random variable has a wide range of application as it\\nmay be used as an approximation for binomial with parameter\\n(n,p) when n is large and p is small and thus np is of moderate\\nsize. An example application area is, if a fax machine has a\\nfaulty transmission line, then the probability of receiving an\\nerroneous digit within a certain page transmitted can be\\ncalculated using a Poisson random variable.\\nSo, a random variable X is called a Poisson random variable\\nwith parameter λ (>0) when the pmf looks like\\nthe cdf is given as:\\nFigure 5.5 shows the Poisson distribution with λ = 2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 243, 'page_label': '244'}, page_content='FIG. 5.5 A Poisson distribution with λ = 2\\n5.6 SOME COMMON CONTINUOUS DISTRIBUTIONS\\nIn this section we present some commonly used univariate\\n(one-dimensional) continuous probability distributions.\\n5.6.1 Uniform distribution\\nThe pdf of a uniform random variable is given by:\\nAnd the cdf of X is:'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 244, 'page_label': '245'}, page_content='See Figure 5.6 that represents a uniform distribution with a\\n= 3, b = 8 with increment 0.5 and repetition 20.\\nFIG. 5.6 A uniform distribution with a = 3, b = 8 with increment 0.5 and repetition\\n20\\nExample. If X is a continuous random variable with\\nX~Uniform(a,b). Find E(X). We know as per equation 5.33'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 245, 'page_label': '246'}, page_content='This is also corroborated by the fact that because X is\\nuniformly distributed over the interval [a, b] we can expect the\\nmean to be at the middle point, E(X) = \\nThe mean and variance of a uniform random variable X are\\nThis is a useful distribution when we don’t have any prior\\nknowledge of the actual pdf and all continuous values in the\\nsame range seems to be equally likely.\\n5.6.2 Gaussian (normal) distribution\\nThe most widely used distribution in statistics and machine\\nlearning is the Gaussian or normal distribution. Its pdf is given'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 246, 'page_label': '247'}, page_content='by\\nand the corresponding cdf looks like\\nFor easier reference a function Φ(z) is defined as\\nWhich will help us evaluate the value of F(x) which can be\\nwritten as\\nIt can be derived that\\nFigure 5.7 shows the normal distribution graph with mean =\\n4, s.d. = 12.\\nx'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 247, 'page_label': '248'}, page_content='FIG. 5.7 A normal distribution graph\\nFor a normal random variable, mean and variance are\\nthe notation N(μ; σ ) is used to denote that p(X = x) = N(x|μ;\\nσ )\\nA standard normal random variable is defined as the one\\nwhose mean is 0 and variance is 1 which means Z = N(0;1).\\nThe plot for this is sometimes called the bell curve, (see Fig\\n5.8)\\n2\\n2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 248, 'page_label': '249'}, page_content='The Gaussian or Normal distribution is the most widely\\nused distribution in the study of random phenomena in nature\\nstatistics due to few reasons:\\n1. It has two parameters that are easy to interpret, and which capture some\\nof the most basic properties of a distribution, namely its mean and\\nvariance.\\n2. The central limit theorem ( Section 5.8) provides the result that sums of\\nindependent random variables have an approximately Gaussian\\ndistribution, which makes it a good choice for modelling residual errors\\nor ‘noise’.\\n3. The Gaussian distribution makes the least number of assumptions,\\nsubject to the constraint of having a specified mean and variance and\\nthus is a good default choice in many cases.\\n4. Its simple mathematical form is easy to implement, but often highly\\neffective.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 249, 'page_label': '250'}, page_content='FIG. 5.8 A standard normal distribution graph'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 250, 'page_label': '251'}, page_content='5.6.3 The laplace distribution\\nAnother distribution with heavy tails is the Laplace\\ndistribution, which is also known as the double-sided\\nexponential distribution. This has the following pdf:\\nHere μ is a location parameter and b >0 is a scale parameter.\\nFigure 5.9 represents the distribution of Laplace \\nFIG. 5.9 The distribution graph of Laplace \\nThis puts more probability density at 0 than the Gaussian.\\nThis property is a useful way to encourage sparsity in a model.\\n5.7 MULTIPLE RANDOM VARIABLES'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 251, 'page_label': '252'}, page_content='Till now we were dealing with one random variable in a\\nsample space, but in most of the practical purposes there are\\ntwo or more random variables on the same sample space. We\\nwill discuss their associated distribution and interdependence.\\n5.7.1 Bivariate random variables\\nLet us consider two random variables X and Y in the sample\\nspace of S of a random experiment. Then the pair (X, Y) is\\ncalled s bivariate random variable or two-dimensional random\\nvector where each of X and Y are associated with a real\\nnumber for every element of S. The range space of bivariate\\nrandom variable (X, Y) is denoted by R  and (X, Y) can be\\nconsidered as a function that to each point ζ in S assigns a\\npoint (x, y) in the plane.\\n(X, Y) is called a discrete bivariate random variable if the\\nrandom variables X and Y both by themselves are discrete.\\nSimilarly, (X, Y) is called a continuous bivariate random\\nvariable if the random variables X and Y both are continuous\\nand is called a mixed bivariate random variable if one of X and\\nY is discrete and the other is continuous.\\n5.7.2 Joint distribution functions\\nThe joint cumulative distribution function (or joint cdf) of X\\nand Y is defined as:\\nFor the event (X ≤ x, Y ≤ y), we can define\\nxy'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 252, 'page_label': '253'}, page_content='A = {ζ ∈S; X(ζ) ≤ x} and B = {ζ ∈S; Y( ζ ) ≤ y}\\nAnd P(A) = F (x) and P(B) = F (y)\\nThen, F  (x, y) = P (A ∩ B).\\nFor certain values of x and y, if A and B are independent\\nevents of S, then\\nFew important properties of joint cdf of two random\\nvariables which are similar to that of the cdf of single random\\nvariable are\\n1. 0 ≤ F  (x, y) ≤ 1\\nIf x ≤ x  and y ≤ y , then\\nF  (x , y ) ≤ F  (x , y ) ≤ F  (x , y )\\n2. F  (x , y ) ≤ F  (x , y) ≤ F  (x , y )\\n3. \\n4. \\n5. \\n6. P (x < X ≤ x , Y ≤ y) = F (x , y)-F (x , y)\\nP (X < x, y < Y ≤ y ) = F (x, y )-F (x, y )\\n5.7.3 Joint probability mass functions\\nFor the discrete bivariate random variable (X, Y) if it takes the\\nvalues (x, y) for certain allowable integers I and j, then the\\njoint probability mass function (joint pmf) of (X, Y) is given by\\nX Y\\nXY\\nXY\\n1 2 1 2\\nXY 1 1 XY 2 1 XY 2 2\\nXY 1 1 XY 1 2 XY 2 2\\n1 2 XY 2 XY 1\\n1 2 XY 2 XY 1\\ni j'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 253, 'page_label': '254'}, page_content='Few important properties of p  (x, y) are\\n1. 0 ≤ p  (x, y) ≤ 1\\n2. \\n3. P[(X, Y) ∈  A] = Ʃ  (x, y) ∈  R Ʃ  p (x, y), where the summation is\\ndone over the points (x, y ) in the range space R .\\nThe joint cdf of a discrete bivariate random variable (X, Y)\\nis given by\\n5.7.4 Joint probability density functions\\nIn case (X, Y) is a continuous bivariate random variable with\\ncdf F  (x, y) and then the function,\\nis called the joint probability density function (joint pdf) of\\n(X, Y). Thus, integrating, we get\\nFew important properties of f (x, y) are\\nXY i j\\nXY i j\\ni j A XY i j\\ni j A\\nXY\\nxy'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 254, 'page_label': '255'}, page_content='1. f (x, y) ≥ 0\\n2. \\n3. \\n5.7.5 Conditional distributions\\nWhile working with a discrete bivariate random variable, it is\\nimportant to deduce the conditional probability function as X\\nand Y are related in the finite space. Based on the joint pmf of\\n(X, Y) the conditional pmf of Y when X = x is defined as\\nNote few important properties of P (y|x):\\n1. 0 ≤ P (y|x) ≤ 1\\n2. \\nIn the same way, when (X,Y) is a continuous bivariate\\nrandom variable and has joint pdf f  (x, y), then the\\nconditional cdf of Y in case X = x is defined as\\nxy\\ni\\nY|X j i\\nY|X j i\\nXY'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 255, 'page_label': '256'}, page_content='Note few important properties of f (y|x):\\n1. f (y|x) ≥ 0\\n2. \\n5.7.6 Covariance and correlation\\nThe covariance between two random variables X and Y\\nmeasure the degree to which X and Y are (linearly) related,\\nwhich means how X varies with Y and vice versa.\\nSo, if the variance is the measure of how a random variable\\nvaries with itself, then the covariance is the measure of how\\ntwo random variables vary with each other.\\nCovariance can be between 0 and infinity. Sometimes, it is\\nmore convenient to work with a normalized measure, because\\ncovariance alone may not have enough information about the\\nrelationship among the random variables. For example, let’s\\ndefine 3 different random variables based on flipping of a\\ncoin:\\nY|X\\nY|X'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 256, 'page_label': '257'}, page_content='Just by looking into these random variables we can\\nunderstand that they are essentially the same just a constant\\nmultiplied at their output. But the covariance of them will be\\nvery different when calculating with the equation 5.55:\\n \\nCov(X, Y) = 2.5, Cov(X, Z) = 25, Cov (Y, Z) = 250\\n \\nTo solve this problem, it is necessary to add a normalizing\\nterm that provides this intelligence: \\nIf Cov (X,Y) = 0, then we can say X and Y are uncorrelated.\\nThen from equation 5.55, X and Y are uncorrelated if\\n \\nE(X, Y) = E(X)E(Y)'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 257, 'page_label': '258'}, page_content='If X and Y are independent then it can be shown that they\\nare uncorrelated, but note that the converse is not true in\\ngeneral.\\nSo, we should remember\\nThe outcomes of a Random Variable weighted by their probability is\\nExpectation, E(X).\\nThe difference between Expectation of a squared Random Variable and the\\nExpectation of that Random Variable squared is Variance: E(X) − E(X).\\nCovariance, E(XY) − E(X)E(Y) is the same as Variance, but here two\\nRandom Variables are compared, rather than a single Random Variable\\nagainst itself.\\nCorrelation, Corr (X, Y) = \\n  is the Covariance\\nnormalized.\\nFew important properties of correlation are\\n1. −1 ≤ corr [X, Y] ≤ 1. Hence, in a correlation matrix, each entry on the\\ndiagonal is 1, and the other entries are between -1 and 1.\\n2. corr [X, Y] = 1 if and only if Y = aX + b for some parameters a and b,\\ni.e., if there is a linear relationship between X and Y.\\n3. From property 2 it may seem that the correlation coefficient is related to\\nthe slope of the regression line, i.e., the coefficient a in the expression Y\\n= aX + b. However, the regression coefficient is in fact given by a =\\ncov[X, Y] /var [X]. A better way to interpret the correlation coefficient is\\nas a degree of linearity.\\n5.8 CENTRAL LIMIT THEOREM\\nThis is one of the most important theorems in probability\\ntheory. It states that if X,… X is a sequence of independent\\nidentically distributed random variables and each having mean\\nμ and variance σ  and \\n1 n\\n2 2\\n2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 258, 'page_label': '259'}, page_content='As n → ∞ (meaning for a very large but finite set) then Z\\ntends to the standard normal.\\nOr\\nwhere, Φ(z) is the cdf of a standard normal random variable.\\nSo, according to the central limit theorem, irrespective of\\nthe distribution of the individual X’s the distribution of the\\nsum S  – X + …X is approximately normal for large n. This\\nis the very important result as whenever we need to deal with a\\nrandom variable which is the sum of a large number of random\\nvariables than using central limit theorem we can assume that\\nthis sum is normally distributed.\\n5.9 SAMPLING DISTRIBUTIONS\\nAs we discussed earlier in this chapter, an important\\napplication of statistics in machine learning is how to draw a\\nconclusion about a set or population based on the probability\\nmodel of random samples of the set. For example, based on\\nthe malignancy sample test results of some random tumour\\ncases we want to estimate the proportion of all tumours which\\nare malignant and thus advise the doctors on the requirement\\nor non-requirement of biopsy on each tumour case. As we can\\nn\\ni\\nn 1 n'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 259, 'page_label': '260'}, page_content='understand, different random samples may give different\\nestimates, if we can get some knowledge about the variability\\nof all possible estimates derived from the random samples,\\nthen we should be able to arrive at reasonable conclusions.\\nSome of the terminologies used in this section are defined\\nbelow:\\nPopulation is a finite set of objects being investigated.\\nRandom sample refers to a sample of objects drawn from a\\npopulation in a way that every member of the population has\\nthe same chance of being chosen.\\nSampling distribution refers to the probability distribution\\nof a random variable defined in a space of random samples.\\n5.9.1 Sampling with replacement\\nWhile choosing the samples from the population if each object\\nchosen is returned to the population before the next object is\\nchosen, then it is called the sampling with replacement. In this\\ncase, repetitions are allowed. That means, if the sample size n\\nis chosen from the population size of N, then the number of\\nsuch samples is\\n \\nN × N × cc. × N = N, because each object can be repeated.\\nAlso, the probability of each sample being chosen is the\\nsame and is \\n .\\nn'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 260, 'page_label': '261'}, page_content='For example, let’s choose a random sample of 2 patients\\nfrom a population of 3 patients {A, B, C} and replacement is\\nallowed. There can be 9 such ordered pairs, like:\\n \\n(A, A), (A, B), (A, C), (B, A), (B, B), (B, C), (C, A), (C, B), (C, C)\\n \\nThat means the number of random samples of 2 from the\\npopulation of 3 is\\n \\nN = 3= 9\\nand each of the random sample has probability \\n  of\\nbeing chosen.\\n5.9.2 Sampling without replacement\\nIn case, we don’t return the object being chosen to the\\npopulation before choosing the next object, then the random\\nsample of size n is defined as the unordered subset of n objects\\nfrom the population and called sampling without replacement.\\nThe number of such samples that can be drawn from the\\npopulation size of N is\\nn 2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 261, 'page_label': '262'}, page_content='In our previous example, the unordered sample of 2 that can\\nbe created from the population of 3 patients when replacement\\nis not allowed is\\n(A, B), (A, C), (B, C)\\nAlso, each of these 3 samples of size 2 has the probability of\\ngetting chosen as \\n5.9.3 Mean and variance of sample\\nLet us consider X as a random variable with mean μ and\\nstandard deviation σ from a population N. A random sample of\\nsize n, drawn without replacement will generate n values x ,\\nx …..,x  for X. When samples are drawn with replacement,\\nthese values are independent of each other and can be\\nconsidered as values of n independent random variables X,\\nX,…., X, each having mean µ and variance σ . The sample\\nmean is a random variable Ẋ  as\\nAs X̅  is a random variable, it also has a mean µ  and\\nvariance σ  and it is related to population parameters as:\\n1\\n2 n\\n1\\n2 n\\nẋ\\nẋ\\n2\\n2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 262, 'page_label': '263'}, page_content='for a large number of n, if X is approximately normally\\ndistributed, then X̅  will also be normally distributed.\\nWhen the samples are drawn without replacement, then the\\nsample values x , x …..,x  for random variable X are not\\nindependent. The sample mean X̅  has the mean µ  and\\nvariance σ  given by:\\nwhere, N is the size of the population and n < N. Also if X\\nis normally distributed then X̅  also has a normal distribution.\\nNow, based on the central limit theorem, if the sample size\\nof a random variable based on a finite population is large then\\nthe sample mean is approximately normally distributed\\nirrespective of the distribution of the population. For most of\\nthe practical applications of sampling, when the sample size is\\nlarge enough (as a rule of thumb ≥ 30) the sample mean is\\napproximately normally distributed. Also, when a random\\nsample is drawn from a large population, it can be assumed\\nthat the values x , x …..,x  are independent. This assumption\\nof independence is one of the key to application of probability\\ntheory in statistical inference. We use the terms like\\n‘population is much larger than the sample size’ or ‘population\\nis large compared to its sample size’, etc. to denote that the\\npopulation is large enough to make the samples independent of\\n1 2 n\\nẊ\\nẊ\\n1 2 n\\n2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 263, 'page_label': '264'}, page_content='each other. In practice, if \\n , then independence may\\nbe assumed.\\n5.10 HYPOTHESIS TESTING\\nWhile dealing with random variables a common situation is\\nwhen we have to make certain decisions or choices based on\\nthe observations or data which are random in nature. The\\nsolutions for dealing with these situations is called decision\\ntheory or hypothesis testing and it is a widely used process in\\nreal life situations. As we discussed earlier, the key component\\nof machine learning is to use a sample- based training data\\nwhich can be used to represent the larger set of actual data and\\nit is important to estimate how confidently an outcome can be\\nrelated to the behaviour of the training data so that the\\ndecisions on the actual data can be made. So, hypothesis\\ntesting is an integral part of machine learning.\\nIn terms of statistics, a hypothesis is an assumption about\\nthe probability law of the random variables. Take, e.g. a\\nrandom sample (X,….. X) of a random variable whose pdf\\non parameter κ is given by f(x, κ) = f(x , x …., x ; κ). We\\nwant to test the assumption κ = κ  against the assumption κ =\\nκ . In this case, the assumption κ = κ  is called null hypothesis\\nand is denoted by H. Assumption κ = κ  is called alternate\\nhypothesis and is denoted by H.\\n \\nH:κ = κ\\nH:κ = κ\\n \\n1 n\\n1 2 n\\n0\\n1 0\\n0 1\\n1\\n0 . . 1\\n1 . . 1'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 264, 'page_label': '265'}, page_content='A simple hypothesis is the one where all the parameters are\\nspecified with an exact value, like H or H in this case. But if\\nthe parameters don’t have an exact value, like H:κ ≠ κ  then\\nH is composite.\\nConcept of hypothesis testing is the decision process used\\nfor validating a hypothesis. We can interpret a decision process\\nby dividing an observation space, say R into two regions – R\\nand R. If x = (x ,…..x ) are the set of observation, then if x ∈\\nR the decision is in favor of Hand if x ∈  R then the\\ndecision is in favor of H. The region Ris called acceptance\\nregion as the null hypothesis is accepted and R is the rejection\\nregion. There are 4 possible decisions based on the two\\nregions in observation space:\\n1. H is true; accept H➔  this is a correct decision\\n2. H is true; reject H (which means accept H) ➔  this is an incorrect\\ndecision\\n3. H is true; accept H➔  this is a correct decision\\n4. H is true; reject H (which means accept H) ➔  this is an incorrect\\ndecision\\nSo, we can see there is the possibility of 2 correct and 2\\nincorrect decisions and the corresponding actions. The\\nerroneous decisions can be termed as:\\nType I error: reject H (or accept H) when H is true. The\\nexample of this situation is in a malignancy test of a tumour, a\\nbenign tumour is accepted as malignant tumour and\\ncorresponding treatment is started. This is also called Alpha\\nerror where good is interpreted as bad.\\nType II error: reject H (or accept H) when H is true. The\\nexample of this situation is in a malignancy test of a tumour, a\\nmalignant tumour is accepted as a benign tumour and no\\n0 1\\n1 1\\n1\\n0\\n1 1 n\\n0 0 1\\n1 0\\n1\\n0 0\\n0 0 1\\n1 1\\n1 1 0\\n0 1 0\\n1 0 1'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 265, 'page_label': '266'}, page_content='treatment for malignancy is started. This is also called Beta\\nerror where bad is interpreted as good and can have a more\\ndevastating impact.\\nThe probabilities of Type I and Type II errors are\\n \\nP = P (D| H ) = P(x ∈  R ; H )\\nP = P(D| H ) = P(x ∈  R ; H )\\n \\nwhere, D (I = 0, 1) denotes the event that the decision is for\\naccepting H. P is also denoted by α and known as level of\\nsignificance whereas P  is denoted by β and is known as the\\npower of the test. Here, α and β are not independent of each\\nother as they represent the probabilities of the event from same\\ndecision problem. So, normally a decrease in one type of error\\nleads to an increase in another type of when the sample size is\\nfixed. Though it is desirable to reduce both types of errors it is\\nonly possible by increasing the sample size. In all practical\\napplications of hypothesis testing, each of the four possible\\noutcomes and courses of actions are associated with relative\\nimportance or certain cost and thus the final goal can be to\\nreduce the overall cost.\\nSo, the probabilities of correct decisions are\\n \\nP(D| H ) = P(x ∈  R ; H )\\nP(D| H ) = P(x ∈  R ; H )\\n5.11 MONTE CARLO APPROXIMATION\\nI 1 0 1 0 \\nII 0 1 0 1 \\ni\\ni I\\nII\\n0 0 0 0 \\n1 1 1 1'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 266, 'page_label': '267'}, page_content='Though we discussed the distribution functions of the random\\nvariables, in practical situations it is difficult to compute them\\nusing the change of variables formula. Monte Carlo\\napproximation provides a simple but powerful alternative to\\nthis. Let’s first generate S samples from the distribution, as x1,\\n…, x . For these samples, we can approximate the distribution\\nof f(X) by using the empirical distribution of {f(x)} .\\nIn principle, Monte Carlo methods can be used to solve any\\nproblem which has a probabilistic interpretation. We know that\\nby the law of large numbers, integrals described by the\\nexpected value of some random variable can be approximated\\nby taking the empirical mean (or the sample mean) of\\nindependent samples of the random variable. A widely used\\nsampler is Markov chain Monte Carlo (MCMC) sampler for\\nparametrizing the probability distribution of a random\\nvariable. The main idea is to design a judicious Markov chain\\nmodel with a prescribed stationary probability distribution.\\nMonte Carlo techniques are now widely used in statistics and\\nmachine learning as well. Using the Monte Carlo technique,\\nwe can approximate the expected value of any function of a\\nrandom variable by simply drawing samples from the\\npopulation of the random variable, and then computing the\\narithmetic mean of the function applied to the samples, as\\nfollows:\\nThis is also called Monte Carlo integration, and is easier to\\nevaluate than the numerical integration which attempts to\\nevaluate the function at a fixed grid of points. Monte Carlo\\nS\\ns s = 1S'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 267, 'page_label': '268'}, page_content='evaluates the function only in places where there is a non-\\nnegligible probability.\\nFor different functions f(), the approximation of some\\nimportant quantities can be obtained:\\nmedian{ x , …, x } → median(X)\\n5.12 SUMMARY\\n1. Frequentist interpretation of probability represents the long run\\nfrequencies of events whereas the Bayesian interpretation of probability\\ntries to quantify the uncertainty of some event and thus focuses on\\ninformation rather than repeated trials.\\n2. According to the Bayes rule, p (A|B) = p (B|A) p(A)/ p (B).\\n3. A discrete random variable is expressed with the probability mass\\nfunction (pmf) p(x) = p(X = x).\\n4. A continuous random variable is expressed with the probability density\\nfunction (pdf) p(x) = p(X = x).\\n5. The cumulative distribution function (cdf) of random variable X can be\\nobtained by \\n6. The equation to find out the mean for random variable X is:\\n when X is discrete\\n when X is continuous.\\n1 s'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 268, 'page_label': '269'}, page_content='7. The equation to find out the variance for random variable X is \\n8. When we have a situation where the outcome of a trial is withered\\n‘success’ or ‘failure’, then the behaviour of the random variable X can\\nbe represented by Bernoulli distribution.\\n9. If n independent Bernoulli trials are performed and X represents the\\nnumber of success in those n trials, then X is called a binomial random\\nvariable.\\n10. To model the outcomes of tossing a K-sided die, or for experiments\\nwhere the outcome can be multiple, we can use the multinomial\\ndistribution.\\n11. The Poisson random variable has a wide range of application as it may\\nbe used as an approximation for binomial with parameter (n, p) when n\\nis large and p is small and thus np is of moderate size.\\n12. Uniform distribution, Gaussian (normal) distribution, Laplace\\ndistribution are examples for few important continuous random\\ndistributions.\\n13. If there are two random variables X and Y in the sample space of S of a\\nrandom experiment, then the pair (X, Y) is called as bivariate random\\nvariable.\\n14. The covariance between two random variables X and Y measures the\\ndegree to which X and Y are (linearly) related.\\n15. According to the central limit theorem, irrespective of the distribution\\nof the individual X’s the distribution of the sum S  – X  + … X  is\\napproximately normal for large n.\\n16. In hypothesis testing:\\n \\nType I error: reject H (or accept H) when H is true.\\nType II error: reject H (or accept H) when H is true.\\n \\n17. Using the Monte Carlo technique, we can approximate the expected\\nvalue of any function of a random variable by simply drawing samples\\nfrom the population of the random variable, and then computing the\\narithmetic mean of the function applied to the samples.\\nSAMPLE QUESTIONS\\ni n 1 n\\n0 1 0\\n1 0 1'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 269, 'page_label': '270'}, page_content='MULTIPLE - CHOICE QUESTIONS (1 MARK EACH)\\n1. The probabilistic approach used in machine learning is closely related\\nto:\\n1. Statistics\\n2. Physics\\n3. Mathematics\\n4. Psychology\\n2. This type of interpretation of probability tries to quantify the uncertainty\\nof some event and thus focuses on information rather than repeated\\ntrials.\\n1. Frequency interpretation of probability\\n2. Gaussian interpretation of probability\\n3. Machine learning interpretation of probability\\n4. Bayesian interpretation of probability\\n3. The reason the Bayesian interpretation can be used to model the\\nuncertainty of events is that it does not expect the long run frequencies\\nof the events to happen.\\n1. True\\n2. False\\n4. p(A,B) = p(A ∩ B) = p(A|B) p(B) is referred as:\\n1. Conditional probability\\n2. Unconditional probability\\n3. Bayes rule\\n4. Product rule\\n5. Based on this joint distribution on two events p(A,B), we can define\\nthe this distribution as follows:\\n6. p(A) = p(A,B) = p(A|B = b) p(B = b)\\n1. Conditional distribution\\n2. Marginal distribution\\n3. Bayes distribution\\n4. Normal distribution\\n7. We can define this probability as p(A|B) = p(A,B)/p(B) if p(B) > 0\\n1. Conditional probability\\n2. Marginal probability\\n3. Bayes probability\\n4. Normal probability\\n8. In statistical terms, this represents the weighted average score.\\n1. Variance\\n2. Mean\\n3. Median\\n4. More\\n9. The covariance between two random variables X and Y measures the\\ndegree to which X and Y are (linearly) related, which means how X\\nvaries with Y and vice versa. What is the formula for Cov (X,Y)?'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 270, 'page_label': '271'}, page_content='1. Cov(X,Y) = E(XY)−E(X)E(Y)\\n2. Cov(X,Y) = E(XY)+ E(X)E(Y)\\n3. Cov(X,Y) = E(XY)/E(X)E(Y)\\n4. Cov(X,Y) = E(X)E(Y)/ E(XY)\\n10. The binomial distribution can be used to model the outcomes of coin\\ntosses.\\n1. True\\n2. False\\n11. Two events A and B are called mutually exclusive if they can happen\\ntogether.\\n1. True\\n2. False\\nSHORT-ANSWER TYPE QUESTIONS (5 MARKS EACH)\\n1. Define the Bayesian interpretation of probability.\\n2. Define probability of a union of two events with equation.\\n3. What is joint probability? What is its formula?\\n4. What is chain rule of probability?\\n5. What is conditional probability means? What is the formula of it?\\n6. What are continuous random variables?\\n7. What are Bernoulli distributions? What is the formula of it?\\n8. What is binomial distribution? What is the formula?\\n9. What is Poisson distribution? What is the formula?\\n10. Define covariance.\\n11. Define correlation\\n12. Define sampling with replacement. Give example.\\n13. What is sampling without replacement? Give example.\\n14. What is hypothesis? Give example.\\nLONG-ANSWER TYPE QUESTIONS (10 MARKS QUESTIONS)\\n1. Let X be a discrete random variable with the following PMF \\n1. Find the range R  of the random variable X.X'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 271, 'page_label': '272'}, page_content='2. Find P(X ≤ 0.5)\\n3. Find P(0.25<X<0.75)\\n4. Find P(X = 0.2|X<0.6)\\n2. Two equal and fair dice are rolled and we observed two numbers X and\\nY.\\n1. Find R , R  and the PMFs of X and Y.\\n2. Find P(X = 2,Y = 6).\\n3. Find P(X>3|Y = 2).\\n4. If Z = X + Y. Find the range and PMF of Z.\\n5. Find P(X = 4|Z = 8).\\n3. In an exam, there were 20 multiple-choice questions. Each question had\\n44 possible options. A student knew the answer to 10 questions, but the\\nother 10 questions were unknown to him and he chose answers\\nrandomly. If the score of the student X is equal to the total number of\\ncorrect answers, then find out the PMF of X. What is P(X>15)?\\n4. The number of students arriving at a college between a time interval is a\\nPoisson random variable. On an average 10 students arrive per hour. Let\\nX be the number of students arriving from 10 am to 11:30 am. What is\\nP(10<X≤15)?\\n5. If we have two independent random variables X and Y such that\\nX~Poisson(α) and Y~Poisson(β). Define a new random variable as Z =\\nX + Y. Find out the PMF of Z.\\n6. There is a discrete random variable X with the pmf. \\nIf we define a new random variable Y = (X + 1) then\\n1. Find the range of Y.\\n2. Find the pmf of Y.\\n7. If X is a continuous random variable with PDF\\n1. \\n2. Find EX and Var(X).\\nX Y\\n2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 272, 'page_label': '273'}, page_content='3. Find P(X ≥ \\n ).\\n8. If X is a continuous random variable with pdf \\n9. If X~Uniform\\n  and Y = sin(X), then find f (y).\\n10. If X is a random variable with CDF \\n1. What kind of random variable is X: discrete, continuous, or\\nmixed?\\n2. Find the PDF of X, fX(x).\\n3. Find E(e).\\n4. Find P(X = 0|X≤0.5).\\n11. There are two random variables X and Y with joint PMF given in Table\\nbelow\\n1. Find P(X≤2, Y≤4).\\n2. Find the marginal PMFs of X and Y.\\n3. Find P(Y = 2|X = 1).\\n4. Are X and Y independent? \\n12. There is a box containing 40 white shirts and 60 black shirts. If we\\nchoose 10 shirts (without replacement) at random, then find the joint\\nPMF of X and Y where X is the number of white shirts and Y is the\\nnumber of black shirts.\\nY\\nX'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 273, 'page_label': '274'}, page_content='13. If X and Y are two jointly continuous random variables with joint PDF \\n1. Find f(x) and f(y).\\n2. Are X and Y independent to each other?\\n3. Find the conditional PDF of X given Y = y, f (x|y).\\n4. Find E[X|Y = y], for 0 ≤ y ≤ 1.\\n5. Find Var(X|Y = y), for 0 ≤ y ≤ 1.\\n14. There are 100 men on a ship. If X is the weight of the ith man on the\\nship and Xi’s are independent and identically distributed and also EX =\\nμ = 170 and σ = σ = 30. Find the probability that the total weight of\\nthe men on the ship exceeds 18,000.\\n15. Let X , X , ……, X  are the independent and identically distributed.\\nAnd have the following PMF \\nIf Y = X  + X  + … + X , estimate P(4 ≤ Y ≤ 6) using central limit\\ntheorem.\\nX Y\\nX|Y\\ni\\ni\\nXi\\n1 2 25\\n1 2 n'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 274, 'page_label': '275'}, page_content='Chapter 6\\nBayesian Concept Learning\\nOBJECTIVE OF THE CHAPTER:\\nPrinciples of probability for classification are an important\\narea of machine learning algorithms. In our practical life,\\nour decisions are affected by our prior knowledge or belief\\nabout an event. Thus, an event that is otherwise very\\nunlikely to occur may be considered by us seriously to\\noccur in certain situations if we know that in the past, the\\nevent had certainly occurred when other events were\\nobserved. The same concept is applied in machine learning\\nusing Bayes’ theorem and the related algorithms discussed\\nin this chapter. The concepts of probabilities discussed in\\nthe previous chapters are used extensively in this chapter.\\n6.1 INTRODUCTION\\nIn the last chapter, we discussed the rules of probability and\\npossible uses of probability, distribution functions, and\\nhypothesis testing principles in the machine learning domain.\\nIn this chapter, we will discuss the details of the Bayesian\\ntheorem and how it provides the basis for machine learning'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 275, 'page_label': '276'}, page_content='concepts. The technique was derived from the work of the\\n18th century mathematician Thomas Bayes. He developed the\\nfoundational mathematical principles, known as Bayesian\\nmethods, which describe the probability of events, and more\\nimportantly, how probabilities should be revised when there is\\nadditional information available.\\n6.2 WHY BAYESIAN METHODS ARE IMPORTANT?\\nBayesian learning algorithms, like the naive Bayes classifier,\\nare highly practical approaches to certain types of learning\\nproblems as they can calculate explicit probabilities for\\nhypotheses. In many cases, they are equally competitive or\\neven outperform the other learning algorithms, including\\ndecision tree and neural network algorithms.\\nBayesian classifiers use a simple idea that the training data\\nare utilized to calculate an observed probability of each class\\nbased on feature values. When the same classifier is used later\\nfor unclassified data, it uses the observed probabilities to\\npredict the most likely class for the new features. The\\napplication of the observations from the training data can also\\nbe thought of as applying our prior knowledge or prior belief\\nto the probability of an outcome, so that it has higher\\nprobability of meeting the actual or real-life outcome. This\\nsimple concept is used in Bayes’ rule and applied for training a\\nmachine in machine learning terms. Some of the real-life uses\\nof Bayesian classifiers are as follows:\\nText-based classification such as spam or junk mail filtering, author\\nidentification, or topic categorization\\nMedical diagnosis such as given the presence of a set of observed symptoms\\nduring a disease, identifying the probability of new patients having the\\ndisease\\nNetwork security such as detecting illegal intrusion or anomaly in computer\\nnetworks'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 276, 'page_label': '277'}, page_content='One of the strengths of Bayesian classifiers is that they\\nutilize all available parameters to subtly change the\\npredictions, while many other algorithms tend to ignore the\\nfeatures that have weak effects. Bayesian classifiers assume\\nthat even if few individual parameters have small effect on the\\noutcome, the collective effect of those parameters could be\\nquite large. For such learning tasks, the naive Bayes classifier\\nis most effective.\\nSome of the features of Bayesian learning methods that\\nhave made them popular are as follows:\\nPrior knowledge of the candidate hypothesis is combined with the observed\\ndata for arriving at the final probability of a hypothesis. So, two important\\ncomponents are the prior probability of each candidate hypothesis and the\\nprobability distribution over the observed data set for each possible\\nhypothesis.\\nThe Bayesian approach to learning is more flexible than the other approaches\\nbecause each observed training pattern can influence the outcome of the\\nhypothesis by increasing or decreasing the estimated probability about the\\nhypothesis, whereas most of the other algorithms tend to eliminate a\\nhypothesis if that is inconsistent with the single training pattern.\\nBayesian methods can perform better than the other methods while\\nvalidating the hypotheses that make probabilistic predictions. For example,\\nwhen starting a new software project, on the basis of the demographics of the\\nproject, we can predict the probability of encountering challenges during\\nexecution of the project.\\nThrough the easy approach of Bayesian methods, it is possible to classify\\nnew instances by combining the predictions of multiple hypotheses,\\nweighted by their respective probabilities.\\nIn some cases, when Bayesian methods cannot compute the outcome\\ndeterministically, they can be used to create a standard for the optimal\\ndecision against which the performance of other methods can be measured.\\nAs we discussed above, the success of the Bayesian method\\nlargely depends on the availability of initial knowledge about\\nthe probabilities of the hypothesis set. So, if these probabilities\\nare not known to us in advance, we have to use some\\nbackground knowledge, previous data or assumptions about\\nthe data set, and the related probability distribution functions'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 277, 'page_label': '278'}, page_content='to apply this method. Moreover, it normally involves high\\ncomputational cost to arrive at the optimal Bayes hypothesis.\\n6.3 BAYES’ THEOREM\\nBefore we discuss Bayes’ theorem and its application in\\nconcept learning, we should be clear about what is concept\\nlearning. Let us take an example of how a child starts to learn\\nmeaning of new words, e.g. ‘ball’. The child is provided with\\npositive examples of ‘objects’ which are ‘ball’. At first, the\\nchild may be confused with many different colours, shapes and\\nsizes of the balls and may also get confused with some objects\\nwhich look similar to ball, like a balloon or a globe. The\\nchild’s parent continuously feeds her positive examples like\\n‘that is a ball’, ‘this is a green ball’, ‘bring me that small ball’,\\netc. Seldom there are negative examples used for such concept\\nteaching, like ‘this is a non-ball’, but the parent may clear the\\nconfusion of the child when it points to a balloon and says it is\\na ball by saying ‘that is not a ball’. But it is observed that the\\nlearning is most influenced through positive examples rather\\nthan through negative examples, and the expectation is that the\\nchild will be able to identify the object ‘ball’ from a wide\\nvariety of objects and different types of balls kept together\\nonce the concept of a ball is clear to her. We can extend\\nthis example to explain how we can expect machines to learn\\nthrough the feeding of positive examples, which forms the\\nbasis for concept learning.\\nTo relate the above-mentioned learning concept with the\\nmathematical model of Bayes, we can correlate the learning\\nprocess of ‘meaning of a word’ as equivalent to learning, a\\nconcept using binary classification. Let us define a concept set\\nC and a corresponding function f(k). We also define f(k) = 1,\\nwhen k is within the set C and f(k) = 0 otherwise. Our aim is to\\nlearn the indicator function f that defines which elements are'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 278, 'page_label': '279'}, page_content='within the set C. So, by using the function f, we will be able to\\nclassify the element either inside or outside our concept set. In\\nBayes’ theorem, we will learn how to use standard probability\\ncalculus to determine the uncertainty about the function f, and\\nwe can validate the classification by feeding positive\\nexamples.\\nThere are few notations that will be introduced before going\\ninto the details of Bayes’ theorem. In Chapter 5, we already\\ndiscussed Bayes’ probability rule as given below:\\nwhere A and B are conditionally related events and p(A|B)\\ndenotes the probability of event A occurring when event B has\\nalready occurred.\\nLet us assume that we have a training data set D where we\\nhave noted some observed data. Our task is to determine the\\nbest hypothesis in space H by using the knowledge of D.\\n6.3.1 Prior\\nThe prior knowledge or belief about the probabilities of\\nvarious hypotheses in H is called Prior in context of Bayes’\\ntheorem. For example, if we have to determine whether a\\nparticular type of tumour is malignant for a patient, the prior\\nknowledge of such tumours becoming malignant can be used\\nto validate our current hypothesis and is a prior probability or\\nsimply called Prior.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 279, 'page_label': '280'}, page_content='Let us introduce few notations to explain the concepts. We\\nwill assume that P(h) is the initial probability of a hypothesis\\n‘h’ that the patient has a malignant tumour based only on the\\nmalignancy test, without considering the prior knowledge of\\nthe correctness of the test process or the so-called training\\ndata. Similarly, P(T) is the prior probability that the training\\ndata will be observed or, in this case, the probability of\\npositive malignancy test results. We will denote P(T|h) as the\\nprobability of observing data T in a space where ‘h’ holds true,\\nwhich means the probability of the test results showing a\\npositive value when the tumour is actually malignant.\\n6.3.2 Posterior\\nThe probability that a particular hypothesis holds for a data set\\nbased on the Prior is called the posterior probability or simply\\nPosterior. In the above example, the probability of the\\nhypothesis that the patient has a malignant tumour considering\\nthe Prior of correctness of the malignancy test is a posterior\\nprobability. In our notation, we will say that we are interested\\nin finding out P(h|T), which means whether the hypothesis\\nholds true given the observed training data T. This is called the\\nposterior probability or simply Posterior in machine learning\\nlanguage. So, the prior probability P(h), which represents the\\nprobability of the hypothesis independent of the training data\\n(Prior), now gets refined with the introduction of influence of\\nthe training data as P(h|T).\\nAccording to Bayes’ theorem'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 280, 'page_label': '281'}, page_content='combines the prior and posterior probabilities together.\\nFrom the above equation, we can deduce that P(h|T)\\nincreases as P(h) and P(T|h) increases and also as P(T)\\ndecreases. The simple explanation is that when there is more\\nprobability that T can occur independently of h then it is less\\nprobable that h can get support from T in its occurrence.\\nIt is a common question in machine learning problems to\\nfind out the maximum probable hypothesis h from a set of\\nhypotheses H (h∈H) given the observed training data T. This\\nmaximally probable hypothesis is called the maximum a\\nposteriori (MAP) hypothesis. By using Bayes’ theorem, we\\ncan identify the MAP hypothesis from the posterior\\nprobability of each candidate hypothesis:\\nand as P(T) is a constant independent of h, in this case, we\\ncan write\\n6.3.3 Likelihood\\nIn certain machine learning problems, we can further simplify\\nequation 6.1 if every hypothesis in H has equal probable priori\\nas P(h) = P(h), and then, we can determine P(h|T) from the\\nprobability P(T|h) only. Thus, P(T|h) is called the likelihood of\\ndata T given h, and any hypothesis that maximizes P(T|h) is\\ni j'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 281, 'page_label': '282'}, page_content='called the maximum likelihood (ML) hypothesis, h . See\\nfigure 6.1 and 6.2 for the conceptual and mathematical\\nrepresentation of Bayes theorem and the relationship of Prior,\\nPosterior and Likelihood.\\nFIG. 6.1  \\nBayes’ theorem\\nFIG. 6.2  \\nConcept of prior, posterior, and likelihood\\nPoints to Ponder:\\nArriving at the refined probability of an event in the light\\nof probability of a related event is a powerful concept and\\nrelates very closely with our day-to-day handling of events\\nand using our knowledge to influence the decisions.\\nML'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 282, 'page_label': '283'}, page_content='Example. Let us take the example of malignancy\\nidentification in a particular patient’s tumour as an application\\nfor Bayes rule. We will calculate how the prior knowledge of\\nthe percentage of cancer cases in a sample population and\\nprobability of the test result being correct influence the\\nprobability outcome of the correct diagnosis. We have two\\nalternative hypotheses: (1) a particular tumour is of malignant\\ntype and (2) a particular tumour is non-malignant type. The\\npriori available are—1. only 0.5% of the population has this\\nkind of tumour which is malignant, 2. the laboratory report has\\nsome amount of incorrectness as it could detect the\\nmalignancy was present only with 98% accuracy whereas\\ncould show the malignancy was not present correctly only in\\n97% of cases. This means the test predicted malignancy was\\npresent which actually was a false alarm in 2% of the cases,\\nand also missed detecting the real malignant tumour in 3% of\\nthe cases.\\nSolution: Let us denote Malignant Tumour = MT, Positive\\nLab Test = PT, Negative Lab Test = NT\\nh  = the particular tumour is of malignant type = MT in our\\nexample\\nh  = the particular tumour is not malignant type = !MT in\\nour example\\n \\nP(MT) = 0.005    P(!MT) = 0.995\\nP(PT|MT) = 0.98   P(PT|!MT) = 0.02\\nP(NT|!MT) = 0.97   P(NT|MT) = 0.03\\n \\n1\\n2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 283, 'page_label': '284'}, page_content='So, for the new patient, if the laboratory test report shows\\npositive result, let us see if we should declare this as the\\nmalignancy case or not:\\nAs P(h |PT) is higher than P(h |PT), it is clear that the\\nhypothesis h  has more probability of being true. So, hMAP =\\nh2 = !MT.\\nThis indicates that even if the posterior probability of\\nmalignancy is significantly higher than that of non-\\nmalignancy, the probability of this patient not having\\nmalignancy is still higher on the basis of the prior knowledge.\\nAlso, it should be noted that through Bayes’ theorem, we\\nidentified the probability of one hypothesis being higher than\\nthe other hypothesis, and we did not completely accept or\\nreject the hypothesis by this theorem. Furthermore, there is\\nvery high dependency on the availability of the prior data for\\nsuccessful application of Bayes’ theorem.\\n6.4 BAYES’ THEOREM AND CONCEPT LEARNING\\n2 1\\n2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 284, 'page_label': '285'}, page_content='One simplistic view of concept learning can be that if we feed\\nthe machine with the training data, then it can calculate the\\nposterior probability of the hypotheses and outputs the most\\nprobable hypothesis. This is also called brute-force Bayesian\\nlearning algorithm, and it is also observed that consistency in\\nproviding the right probable hypothesis by this algorithm is\\nvery comparable to the other algorithms.\\n6.4.1 Brute-force Bayesian algorithm\\nWe will now discuss how to use the MAP hypothesis output to\\ndesign a simple learning algorithm called brute-force map\\nlearning algorithm. Let us assume that the learner considers a\\nfinite hypothesis space H in which the learner will try to learn\\nsome target concept c:X → {0,1} where X is the instance space\\ncorresponding to H. The sequence of training examples is {(x ,\\nt ), (x , t ),…, (x , t )}, where x is the instance of X and t is\\nthe target concept of x defined as t = c(x). Without impacting\\nthe efficiency of the algorithm, we can assume that the\\nsequence of instances of x {x ,…, x } is held fixed, and then,\\nthe sequence of target values becomes T = {t ,…, t }.\\nFor calculating the highest posterior probability, we can use\\nBayes’ theorem as discussed earlier in this chapter:\\nCalculate the posterior probability of each hypothesis h in\\nH:\\nIdentify the h  with the highest posterior probability\\n1\\n1 2 2 m m i i\\ni i i\\n1 m\\n1 m\\nMAP'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 285, 'page_label': '286'}, page_content='h = argmax P(h|T)\\n \\nPlease note that calculating the posterior probability for\\neach hypothesis requires a very high volume of computation,\\nand for a large volume of hypothesis space, this may be\\ndifficult to achieve.\\nLet us try to connect the concept learning problem with the\\nproblem of identifying the h . On the basis of the\\nprobability distribution of P(h) and P(T|h), we can derive the\\nprior knowledge of the learning task. There are few important\\nassumptions to be made as follows:\\n1. The training data or target sequence T is noise free, which means that it\\nis a direct function of X only (i.e. t = c(x))\\n2. The concept c lies within the hypothesis space H\\n3. Each hypothesis is equally probable and independent of each other\\nOn the basis of assumption 3, we can say that each\\nhypothesis h within the space H has equal prior probability,\\nand also because of assumption 2, we can say that these prior\\nprobabilities sum up to 1. So, we can write\\nP(T|h) is the probability of observing the target values t in\\nthe fixed set of instances {x,…, x ) in the space where h holds\\ntrue and describes the concept c correctly. Using assumption 1\\nmentioned above, we can say that if T is consistent with h,\\nMAP h∈H\\nMAP\\ni i\\ni\\ni m'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 286, 'page_label': '287'}, page_content='then the probability of data T given the hypothesis h is 1 and is\\n0 otherwise:\\nUsing Bayes’ theorem to identify the posterior probability\\nFor the cases when h is inconsistent with the training data T,\\nusing 6.5 we get\\n, when h is inconsistent with T,\\nand when h is consistent with T\\nNow, if we define a subset of the hypothesis H which is\\nconsistent with T as H , then by using the total probability\\nequation, we get\\nD'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 287, 'page_label': '288'}, page_content='This makes 6.5 as\\nSo, with our set of assumptions about P(h) and P(T|h), we\\nget the posterior probability P(h|T) as\\nwhere H  is the number of hypotheses from the space H\\nwhich are consistent with target data set T. The interpretation\\nof this evaluation is that initially, each hypothesis has equal\\nprobability and, as we introduce the training data, the posterior\\nprobability of inconsistent hypotheses becomes zero and the\\ntotal probability that sums up to 1 is distributed equally among\\nthe consistent hypotheses in the set. So, under this condition,\\nD'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 288, 'page_label': '289'}, page_content='each consistent hypothesis is a MAP hypothesis with posterior\\nprobability \\n .\\n6.4.2 Concept of consistent learners\\nFrom the above discussion, we understand the behaviour of\\nthe general class of learner whom we call as consistent\\nlearners. So, the group of learners who commit zero error over\\nthe training data and output the hypothesis are called\\nconsistent learners. If the training data is noise free and\\ndeterministic (i.e. P(D|h) = 1 if D and h are consistent and 0\\notherwise) and if there is uniform prior probability distribution\\nover H (so, P(h ) = P(h ) for all m, n), then every consistent\\nlearner outputs the MAP hypothesis. An important application\\nof this conclusion is that Bayes’ theorem can characterize the\\nbehaviour of learning algorithms even when the algorithm\\ndoes not explicitly manipulate the probability. As it can help to\\nidentify the optimal distributions of P(h) and P(T|h) under\\nwhich the algorithm outputs the MAP hypothesis, the\\nknowledge can be used to characterize the assumptions under\\nwhich the algorithms behave optimally.\\nThough we discussed in this section a special case of\\nBayesian output which corresponds to the noise-free training\\ndata and deterministic predictions of hypotheses where P(T|h)\\ntakes on value of either 1 or 0, the theorem can be used with\\nthe same effectiveness for noisy training data and additional\\nassumptions about the probability distribution governing the\\nnoise.\\n6.4.3 Bayes optimal classifier\\nm n'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 289, 'page_label': '290'}, page_content='In this section, we will discuss the use of the MAP hypothesis\\nto answer the question what is the most probable classification\\nof the new instance given the training data. To illustrate the\\nconcept, let us assume three hypotheses h , h , and h  in the\\nhypothesis space H. Let the posterior probability of these\\nhypotheses be 0.4, 0.3, and 0.3, respectively. There is a new\\ninstance x, which is classified as true by h , but false by h  and\\nh .\\nThen the most probable classification of the new instance\\n(x) can be obtained by combining the predictions of all\\nhypotheses weighed by their corresponding posterior\\nprobabilities. By denoting the possible classification of the\\nnew instance as c from the set C, the probability P(c|T) that\\nthe correct classification for the new instance is c is\\nThe optimal classification is for which P(c|T) is maximum\\nis\\nPoints to Ponder:\\nThe approach in the Bayes optimal classifier is to calculate\\nthe most probable classification of each new instance on\\n1 2 3\\n1 2\\n3\\ni i\\ni\\ni'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 290, 'page_label': '291'}, page_content='the basis of the combined predictions of all alternative\\nhypotheses, weighted by their posterior probabilities.\\nSo, extending the above example,\\nThe set of possible outcomes for the new instance x is\\nwithin the set C = {True, False} and\\n \\nP(h  | T) = 0.4, P(False | h ) = 0, P(True | h ) = 1\\nP(h  | T) = 0.3, P(False | h ) = 1, P(True | h ) = 0\\nP(h  | T) = 0.3, P(False | h ) = 1, P(True | h ) = 0\\nThen,\\nand\\nThis method maximizes the probability that the new\\ninstance is classified correctly when the available training\\ndata, hypothesis space and the prior probabilities of the\\nhypotheses are known. This is thus also called Bayes optimal\\nclassifier.\\n6.4.4 Naïve Bayes classifier\\n1 1 1\\n2 2 2\\n3 3 3'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 291, 'page_label': '292'}, page_content='Naïve Bayes is a simple technique for building classifiers:\\nmodels that assign class labels to problem instances. The basic\\nidea of Bayes rule is that the outcome of a hypothesis can be\\npredicted on the basis of some evidence (E) that can be\\nobserved.\\nFrom Bayes rule, we observed that\\n1. A prior probability of hypothesis h or P(h): This is the probability of an\\nevent or hypothesis before the evidence is observed.\\n2. A posterior probability of h or P(h|D): This is the probability of an\\nevent after the evidence is observed within the population D.\\nPosterior Probability is of the format ‘What is the\\nprobability that a particular object belongs to class i given\\nits observed feature values?’\\nFor example, a person has height and weight of 182 cm and\\n68 kg, respectively. What is the probability that this person\\nbelongs to the class ‘basketball player’? This can be predicted\\nusing the Naïve Bayes classifier. This is known as\\nprobabilistic classifications.\\nIn machine learning, a probabilistic classifier is a classifier\\nthat can be foreseen, given a perception or information (input),\\na likelihood calculation over a set of classes, instead of just\\nyielding (outputting) the most likely class that the perception\\n(observation) should belong to. Parameter estimation for Naïve\\nBayes models uses the method of ML.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 292, 'page_label': '293'}, page_content='Bayes’ theorem is used when new information can be used\\nto revise previously determined probabilities. Depending on\\nthe particular nature of the probability model, Naïve Bayes\\nclassifiers can be trained very professionally in a supervised\\nlearning setting.\\nLet us see the basis of deriving the principles of Naïve\\nBayes classifiers. We take a learning task where each instance\\nx has some attributes and the target function (f(x)) can take any\\nvalue from the finite set of classification values C. We also\\nhave a set of training examples for target function, and the set\\nof attributes {a , a ,…, a } for the new instance are known to\\nus. Our task is to predict the classification of the new instance.\\nAccording to the approach in Bayes’ theorem, the\\nclassification of the new instance is performed by assigning\\nthe most probable target classification C  on the basis of the\\nattribute values of the new instance {a , a ,…, a }. So,\\nwhich can be rewritten using Bayes’ theorem as\\nAs combined probability of the attributes defining the new\\ninstance fully is always 1\\n1 2 n\\nMAP\\n1 2 n'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 293, 'page_label': '294'}, page_content='So, to get the most probable classifier, we have to evaluate\\nthe two terms P(a , a , c, a |c) and P(ci). In a practical\\nscenario, it is possible to calculate P(ci) by calculating the\\nfrequency of each target value c in the training data set. But\\nthe P(a , a , c, a |c) cannot be estimated easily and needs a\\nvery high effort of calculation. The reason is that the number\\nof these terms is equal to the product of number of possible\\ninstances and the number of possible target values, and thus,\\neach instance in the instance space needs to be visited many\\ntimes to arrive at the estimate of the occurrence. Thus, the\\nNaïve Bayes classifier makes a simple assumption that the\\nattribute values are conditionally independent of each other for\\nthe target value. So, applying this simplification, we can now\\nsay that for a target value of an instance, the probability of\\nobserving the combination a ,a ,…, a  is the product of\\nprobabilities of individual attributes P(a|c).\\nThen, from equation 6.7, we get the approach for the Naïve\\nBayes classifier as\\nHere, we will be able to compute P(a|c) as we have to\\ncalculate this only for the number of distinct attributes values\\n(a) times the number of distinct target values (c), which is\\nmuch smaller set than the product of both the sets. The most\\n1 2 n i\\ni\\n1 2 n i\\n1 2 n\\ni j\\ni j\\ni j'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 294, 'page_label': '295'}, page_content='important reason for the popularity of the Naïve Bayes\\nclassifier approach is that it is not required to search the whole\\nhypothesis space for this algorithm, but rather we can arrive at\\nthe target classifier by simply counting the frequencies of\\nvarious data combinations within the training example.\\nTo summarize, a Naïve Bayes classifier is a primary\\nprobabilistic classifier based on a view of applying Bayes’\\ntheorem (from Bayesian inference with strong naive)\\nindependence assumptions. The prior probabilities in Bayes’\\ntheorem that are changed with the help of newly available\\ninformation are classified as posterior probabilities.\\nA key benefit of the naive Bayes classifier is that it requires\\nonly a little bit of training information (data) to gauge the\\nparameters (mean and differences of the variables) essential\\nfor the classification (arrangement). In the Naïve Bayes\\nclassifier, independent variables are always assumed, and only\\nthe changes (variances) of the factors/variables for each class\\nshould be determined and not the whole covariance matrix.\\nBecause of the rather naïve assumption that all features of the\\ndataset are equally important and independent, this is called\\nNaïve Bayes classifier.\\nNaïve Bayes classifiers are direct linear classifiers that are\\nknown for being the straightforward, yet extremely proficient\\nresult. The modified version of Naïve Bayes classifier\\noriginates from the assumption that information collection\\n(data set) is commonly autonomous (mutually independent). In\\nmost of the practical scenarios, the ‘independence’ assumption\\nis regularly violated. However, Naïve Bayes classifiers still\\ntend to perform exceptionally well.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 295, 'page_label': '296'}, page_content='Some of the key strengths and weaknesses of Naïve Bayes\\nclassifiers are described in Table 6.1.\\n \\nTable 6.1 Strengths and Weaknesses of Bayes Classifiers\\nExample. Let us assume that we want to predict the outcome of a football\\nworld cup match on the basis of the past performance data of the playing\\nteams. We have training data available (refer Fig. 6.3) for actual match\\noutcome, while four parameters are considered – Weather Condition (Rainy,\\nOvercast, or Sunny), how many matches won were by this team out of the last\\nthree matches (one match, two matches, or three matches), Humidity\\nCondition (High or Normal), and whether they won the toss (True or False).\\nUsing Naïve Bayesian, you need to classify the conditions when this team\\nwins and then predict the probability of this team winning a particular match\\nwhen Weather Conditions = Rainy, they won two of the last three matches,\\nHumidity = Normal and they won the toss in the particular match.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 296, 'page_label': '297'}, page_content='FIG. 6.3 Training data for the Naïve Bayesian method\\n6.4.4.1 Naïve Bayes classifier steps\\nStep 1: First construct a frequency table. A frequency table is drawn for each\\nattribute against the target outcome. For example, in Figure 6.3, the various\\nattributes are (1) Weather Condition, (2) How many matches won by this team\\nin last three matches, (3) Humidity Condition, and (4) whether they won the\\ntoss and the target outcome is will they win the match or not?\\nStep 2: Identify the cumulative probability for ‘Won match = Yes’ and the\\nprobability for ‘Won match = No’ on the basis of all the attributes. Otherwise,\\nsimply multiply probabilities of all favourable conditions to derive ‘YES’\\ncondition. Multiply probabilities of all non-favourable conditions to derive\\n‘No’ condition.\\nStep 3: Calculate probability through normalization by applying the below\\nformula'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 297, 'page_label': '298'}, page_content='P(Yes) will give the overall probability of favourable\\ncondition in the given scenario.\\nP(No) will give the overall probability of non-favourable\\ncondition in the given scenario.\\nSolving the above problem with Naive Bayes\\nStep 1: Construct a frequency table. The posterior probability can be easily\\nderived by constructing a frequency table for each attribute against the target.\\nFor example, frequency of Weather Condition variable with values ‘Sunny’\\nwhen the target value Won match is ‘Yes’, is, 3/(3+4+2) = 3/9.\\nFigure 6.4 shows the frequency table thus constructed.\\nStep 2:\\nTo predict whether the team will win for given weather conditions (a ) =\\nRainy, Wins in last three matches (a ) = 2 wins, Humidity (a ) = Normal and\\nWin toss (a ) = True, we need to choose ‘Yes’ from the above table for the\\ngiven conditions.\\nFrom Bayes’ theorem, we get\\nThis equation becomes much easier to resolve if we recall that Naïve Bayes\\nclassifier assumes independence among events. This is specifically true for\\nclass-conditional independence, which means that the events are independent\\nso long as they are conditioned on the same class value. Also, we know that if\\n1\\n2 3\\n4'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 298, 'page_label': '299'}, page_content='the events are independent, then the probability rule says, P(A ∩ B) = P(A)\\nP(B), which helps in simplifying the above equation significantly as\\nP(Win match|a ∩a ∩a ∩a )\\nThis should be compared with\\nP(!Win match|a ∩a ∩a ∩a )\\nFIG. 6.4 Construct frequency table\\n1 2 3 4\\n1 2 3 4'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 299, 'page_label': '300'}, page_content='Step 3: by normalizing the above two probabilities, we can ensure that the sum\\nof these two probabilities is 1.\\nConclusion: This shows that there is 58% probability that the team will win if\\nthe above conditions become true for that particular day. Thus, Naïve Bayes\\nclassifier provides a simple yet powerful way to consider the influence of\\nmultiple attributes on the target outcome and refine the uncertainty of the\\nevent on the basis of the prior knowledge because it is able to simplify the\\ncalculation through independence assumption.\\n6.4.5 Applications of Naïve Bayes classifier\\nText classification: Naïve Bayes classifier is among the most\\nsuccessful known algorithms for learning to classify text\\ndocuments. It classifies the document where the probability of\\nclassifying the text is more. It uses the above algorithm to\\ncheck the permutation and combination of the probability of\\nclassifying a document under a particular ‘Title’. It has various\\napplications in document categorization, language detection,\\nand sentiment detection, which are very useful for traditional\\nretailers, e-retailors, and other businesses on judging the\\nsentiments of their clients on the basis of keywords in\\nfeedback forms, social media comments, etc.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 300, 'page_label': '301'}, page_content='Spam filtering: Spam filtering is the best known use of Naïve\\nBayesian text classification. Presently, almost all the email\\nproviders have this as a built-in functionality, which makes use\\nof a Naïve Bayes classifier to identify spam email on the basis\\nof certain conditions and also the probability of classifying an\\nemail as ‘Spam’. Naïve Bayesian spam sifting has turned into\\na mainstream mechanism to recognize illegitimate a spam\\nemail from an honest-to-goodness email (sometimes called\\n‘ham’). Users can also install separate email filtering\\nprogrammes. Server-side email filters such as DSPAM, Spam\\nAssassin, Spam Bayes, and ASSP make use of Bayesian spam\\nfiltering techniques, and the functionality is sometimes\\nembedded within the mail server software itself.\\nHybrid Recommender System: It uses Naïve Bayes classifier\\nand collaborative filtering. Recommender systems (used by e-\\nretailors like eBay, Alibaba, Target, Flipkart, etc.) apply\\nmachine learning and data mining techniques for filtering\\nunseen information and can predict whether a user would like\\na given resource. For example, when we log in to these retailer\\nwebsites, on the basis of the usage of texts used by the login\\nand the historical data of purchase, it automatically\\nrecommends the product for the particular login persona. One\\nof the algorithms is combining a Naïve Bayes classification\\napproach with collaborative filtering, and experimental results\\nshow that this algorithm provides better performance\\nregarding accuracy and coverage than other algorithms.\\nOnline Sentiment Analysis: The online applications use\\nsupervised machine learning (Naïve Bayes) and useful\\ncomputing. In the case of sentiment analysis, let us assume\\nthere are three sentiments such as nice, nasty, or neutral, and\\nNaïve Bayes classifier is used to distinguish between them.\\nSimple emotion modelling combines a statistically based'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 301, 'page_label': '302'}, page_content='classifier with a dynamical model. The Naïve Bayes classifier\\nemploys ‘single words’ and ‘word pairs’ like features and\\ndetermines the sentiments of the users. It allocates user\\nutterances into nice, nasty, and neutral classes, labelled as +1,\\n−1, and 0, respectively. This binary output drives a simple\\nfirst-order dynamical system, whose emotional state represents\\nthe simulated emotional state of the experiment’s\\npersonification.\\n6.4.6 Handling Continuous Numeric Features in Naïve\\nBayes Classifier\\nIn the above example, we saw that the Naïve Bayes classifier\\nmodel uses a frequency table of the training data for its\\ncalculation. Thus, each attribute data should be categorical in\\nnature so that the combination of class and feature values can\\nbe created. But this is not possible in the case of continuous\\nnumeric data as it does not have the categories of data.\\nThe workaround that is applied in these cases is discretizing\\nthe continuous data on the basis of some data range. This is\\nalso called binning as the individual categories are termed as\\nbins. For example, let us assume we want to market a certain\\ncredit card to all the customers who are visiting a particular\\nbank. We have to classify the persons who are visiting a bank\\nas either interested candidate for taking a new card or non-\\ninterested candidate for a new card, and on the basis of this\\nclassification, the representative will approach the customer\\nfor sale. In this case, the customers visit the bank continuously\\nduring banking hours and have different values for the\\nattributes we want to evaluate before classifying them into the\\ninterested/non-interested categories.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 302, 'page_label': '303'}, page_content='If we plot the number of customers visiting the bank during\\nthe 8 hours of banking time, the distribution graph will be a\\ncontinuous graph. But if we introduce a logic to categorize the\\ncustomers according to their time of entering the bank, then\\nwe will be able to put the customers in ‘bins’ or buckets for\\nour analysis. We can then try to assess what time range is best\\nsuited for targeting the customers who will have interest in the\\nnew credit card. The bins created by categorizing the\\ncustomers by their time of entry looks like Figure 6.5.\\nThis creates eight natural bins for us (or we may change the\\nnumber of bins by changing our categorizing criteria), which\\ncan now be used for Bayes analysis.\\nFIG. 6.5 The distribution of bins based on the time of entry of customers in the\\nbank\\n6.5 BAYESIAN BELIEF NETWORK'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 303, 'page_label': '304'}, page_content='We must have noted that a significant assumption in the Naïve\\nBayes classifier was that the attribute values a , a ,…, a  are\\nconditionally independent for a target value. The Naïve Bayes\\nclassifier generates optimal output when this condition is met.\\nThough this assumption significantly reduces the complexity\\nof computation, in many practical scenarios, this requirement\\nof conditional independence becomes a difficult constraint for\\nthe application of this algorithm. So, in this section, we will\\ndiscuss the approach of Bayesian Belief network, which\\nassumes that within the set of attributes, the probability\\ndistribution can have conditional probability relationship as\\nwell as conditional independence assumptions. This is\\ndifferent from the Naïve Bayes assumption of conditional\\nindependence of all the attributes as the belief network\\nprovides the flexibility of declaring a subset of the attributes as\\nconditionally dependent while leaving rest of the attributes to\\nhold the assumptions of conditional independence. The prior\\nknowledge or belief about the influence of one attribute over\\nthe other is handled through joint probabilities as discussed\\nlater in this section.\\nLet us refresh our mind on the concept of conditional\\nprobability. If an uncertain event A is conditional on a\\nknowledge or belief K, then the degree of belief in A with the\\nassumption that K is known is expressed as P(A|K).\\nTraditionally, conditional probability is expressed by joint\\nprobability as follows:\\nRearranging (6.9), we get the product rule\\n1 2 n'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 304, 'page_label': '305'}, page_content='P(A, K) = P(A|K)P(K)\\nThis can be extended for three variables or attributes as\\nP(A, K, C) = P(A|K, C)P(K, C) = P(A|K, C)P(K|C)P(C)\\nFor a set of n attributes, the generalized form of the product\\nrule becomes\\nP(A , A , …, A ) = P(A |A , …, A )P(A |A , …, A )P(A -\\n1|A )P(A ) (6.10)\\nThis generalized version of the product rule is called the\\nChain Rule.\\nFIG. 6.6 Chain rule\\nLet us understand the chain rule by using the diagram in\\nFigure 6.6. From the joint probability formula 6.10, we can\\nwrite\\n \\nP(A, B, C, D, E) = P(A|B, C, D, E)P(B|C, D, E)P(C|D, E)P(D|E)P(E)\\n \\n1 2 n 1 2 n 2 3 n n \\nn n'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 305, 'page_label': '306'}, page_content='But from Figure 6.6, it is evident that E is not related to C\\nand D, which means that the probabilities of variables C and D\\nare not influenced by E and vice versa. Similarly, A is directly\\ninfluenced only by B. By applying this knowledge of\\nindependence, we can simplify the above equation as\\n \\nP(A, B, C, D, E) = P(A|B)P(B|C, D)P(C|D)P(D)P(E)\\n \\nLet us discuss this concept of independence and conditional\\nindependence in detail in the next section.\\n6.5.1 Independence and conditional independence\\nWe represent the conditional probability of A with knowledge\\nof K as P(A|K). The variables A and K are said to be\\nindependent if P(A|K) = P(A), which means that there is no\\ninfluence of K on the uncertainty of A. Similarly, the joint\\nprobability can be written as P(A,K) = P(A)P(K).\\nExtending this concept, the variables A and K are said to be\\nconditionally independent given C if P(A|C) = P(A|K, C).\\nThis concept of conditional independence can also be\\nextended to a set of attributes. We can say that the set of\\nvariables A , A ,…, A  is conditionally independent of the set\\nof variables B , B ,…, B  given the set of variables C, C,…,\\nC if\\n \\nP(A , A ,…, A |B , B ,…, Bm, C, C,…, C) = P(A , A ,…, A |C, C,…, C)\\n1 2 n\\n1 2 m 1 2\\nl\\n1 2 n 1 2 1 2 1 1 2 n 1 2 l'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 306, 'page_label': '307'}, page_content='If we compare this definition with our assumption in the\\nNaïve Bayes classifier, we see that the Naïve Bayes classifier\\nassumes that the instance attribute A  is conditionally\\nindependent of the instance attribute A , given the target value\\nV, which can be written using the general product rule and\\napplication of conditional independence formula as\\n \\nP(A , A |V) = P(A |A , V)P(A , V) = P(A , V)P(A , V)\\n \\nA Bayesian Belief network describes the joint probability\\ndistribution of a set of attributes in their joint space. In Figure\\n6.7, a Bayesian Belief network is presented. The diagram\\nconsists of nodes and arcs. The nodes represent the discrete or\\ncontinuous variables for which we are interested to calculate\\nthe conditional probabilities. The arc represents the causal\\nrelationship of the variables.\\nThe two important information points we get from this\\nnetwork graph are used for the determining the joint\\nprobability of the variables. First, the arcs assert that the node\\nvariables are conditionally independent of its non-descendants\\nin the network given its immediate predecessors in the\\nnetwork. If two variables A and B are connected through a\\ndirected path, then B is called the descendent of A. Second, the\\nconditional probability table for each variable provides the\\nprobability distribution of that variable given the values of its\\nimmediate predecessors. We can use Bayesian probability to\\ncalculate different behaviours of the variables in Figure 6.7.\\n1\\n2\\n1 2 1 2 2 1 2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 307, 'page_label': '308'}, page_content='FIG. 6.7 Bayesian belief network\\n1. The unconditional probability that Tim is late to class –\\nP(Tim is late to class)\\n= P(Tim late|Rain Today)P(Rain Today) + P(Tim late|No Rain\\nToday)*P(No Rain Today)\\n= (0.8 × 0.1) × ( 0.1 × 0.9)\\n= 0.17\\nFrom this unconditional probability, the most important use of the\\nBayesian Belief network is to find out the revised probability on the\\nbasis of the prior knowledge. If we assume that there was rain today,\\nthen the probability table can quickly provide us the information about\\nthe probability of Paul being late to class or the probability of Tim being\\nlate to class from the probability distribution table itself. But if we do\\nnot know whether there was rain today or not, but we only know that\\nTim is late to class today, then we can arrive at the following\\nprobabilities –'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 308, 'page_label': '309'}, page_content='2. The revised probability that there was rain today – \\n3. The revised probability that Paul will be late to class today –\\nP(Paul late to class today)\\n= P(Paul late|Rain today)P(Rain today) + P(Paul late|No rain\\ntoday)P(No rain today)\\n= (0.6 × 0.47) + (0.5 × (1-0.47))\\n= 0.55\\nHere, we used the concept of hard evidence and soft\\nevidence. Hard evidence (instantiation) of a node is evidence\\nthat the state of the variable is definitely as a particular value.\\nIn our above example, we had hard evidence that ‘Tim is late\\nto class’. If a particular node is instantiated, then it will block\\npropagation of evidence further down to its child nodes. Soft\\nevidence for a node is the evidence that provides the prior\\nprobability values for the node. The node ‘Paul is late to class’\\nis soft evidenced with the prior knowledge that ‘Tim is late to\\nclass’.\\nNote\\nThere can be two main scenarios faced in the Bayesian\\nBelief network learning problem. First, the network\\nstructure might be available in advance or can be inferred\\nfrom the training data. Second, all the network variables\\neither are directly observable in each training example or'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 309, 'page_label': '310'}, page_content='some of the variables may be unobservable. Learning the\\nconditional probability tables is a straightforward problem\\nwhen the network structure is given in advance and the\\nvariables are fully observable in the training examples. But\\nin the case the network structure is available and only\\nsome of the variable values are observable in the training\\ndata, then the learning problem is more difficult. This is\\ntopic of much research, and some of the advanced topics\\nfor identifying the node values include algorithms such as\\nGradient Ascent Training and the EM algorithm.\\nThe Bayesian Belief network can represent much more\\ncomplex scenarios with dependence and independence\\nconcepts. There are three types of connections possible in a\\nBayesian Belief network.\\n \\nDiverging Connection: In this type of connection, the evidence can be\\ntransmitted between two child nodes of the same parent provided that the\\nparent is not instantiated. In Figure 6.7, we already saw the behaviour of\\ndiverging connection.\\nSerial Connection: In this type of connection, any evidence entered at the\\nbeginning of the connection can be transmitted through the directed path\\nprovided that no intermediate node on the path is instantiated (see Fig. 6.8 for\\nillustration).'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 310, 'page_label': '311'}, page_content='FIG. 6.8 Serial connection\\nConverging Connection: In this type of connection, the evidence can only be\\ntransmitted between two parents when the child (converging) node has\\nreceived some evidence and that evidence can be soft or hard (see Fig. 6.9 for\\nillustration).\\nFIG. 6.9 Convergent connection\\nAs discussed above, by using the Bayesian network, we\\nwould like to infer the value of a target variable on the basis of\\nthe observed values of some other variables. Please note that it\\nwill not be possible to infer a single value in the case of\\nrandom variables we are dealing with, but our intention is to'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 311, 'page_label': '312'}, page_content='infer the probability distribution of the target variable given\\nthe observed values of other variables. In general, the\\nBayesian network can be used to compute the probability\\ndistribution of any subset of node variables given the values or\\ndistribution of the remaining variables.\\n6.5.2 Use of the Bayesian Belief network in machine\\nlearning\\nWe have seen that the Bayesian network creates a complete\\nmodel for the variables and their relationships and thus can be\\nused to answer probabilistic queries about them. A common\\nuse of the network is to find out the updated knowledge about\\nthe state of a subset of variables, while the state of the other\\nsubset (known as the evidence variables) is observed. This\\nconcept, often known as probabilistic inference process of\\ncomputing the posterior distribution of variables, given some\\nevidences, provides a universal sufficient statistic for\\napplications related to detections. Thus if one wants to choose\\nthe values for a subset of variables in order to minimize some\\nexpected loss functions or decision errors, then this method is\\nquiet effective. In other words, the Bayesian network is a\\nmechanism for automatically applying Bayes’ theorem to\\ncomplex problems. Bayesian networks are used for modelling\\nbeliefs in domains like computational biology and\\nbioinformatics such as protein structure and gene regulatory\\nnetworks, medicines, forensics, document classification,\\ninformation retrieval, image processing, decision support\\nsystems, sports betting and gaming, property market analysis\\nand various other fields.\\n6.6 SUMMARY\\nBayesian methods introduced a basis for probabilistic learning methods that\\nconsider the knowledge about the prior probabilities of alternative'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 312, 'page_label': '313'}, page_content='hypotheses and the probability of likelihood or observing various training\\ndata given the hypothesis. It then assigns a posterior probability to each\\ncandidate hypothesis on the basis of the assumed priors and the observed\\ndata.\\nThe MAP hypothesis is the most probable hypothesis given the data. As no\\nother hypothesis is more likely, this is also the optimal hypothesis.\\nThe Bayes optimal classifier calculates the most probable classification of\\neach new instance by combining the predictions of all alternative hypotheses,\\nweighted by their posterior probabilities.\\nNaïve Bayes classifier makes the naïve assumption that the attribute values\\nare conditionally independent given the classification of the instance. This\\nsimplifying assumption considerably reduces the calculation overhead\\nwithout losing the effectiveness of the outcome. With this assumption in\\neffect, the Naïve Bayes classifier outputs the MAP classification.\\nThe Naïve Bayes classifier has been found to be useful in many practical\\napplications and is considered as one of the powerful learning methods. Even\\nwhen the assumption of conditional independence is not met, the Naïve\\nBayes classifier is quite effective in providing a standard for the other\\nlearning methods.\\nBayesian Belief network provides the mechanism to handle more practical\\nscenario of considering the conditional independence of a subset of variables\\nwhile considering the joint probability distribution of the remaining variables\\ngiven the observation.\\nSAMPLE QUESTIONS\\nMULTIPLE CHOICE QUESTIONS (1 MARK EACH)\\n1. Three companies X, Y, and Z supply 40%, 45%, and 15% of the\\nuniforms to a school. Past experience shows that 2%, 3%, and 4% of the\\nuniforms supplied by these companies are defective. If a uniform was\\nfound to be defective, what is the probability that the uniform was\\nsupplied by Company X?\\n1. \\n2. \\n3. 16/55\\n4. \\n2. A box of apples contains 10 apples, of which 6 are defective. If 3 of the\\napples are removed from the box in succession without replacement,'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 313, 'page_label': '314'}, page_content='what is the probability that all the 3 apples are defective?\\n1. (6*5*4)/ (10*10*10)\\n2. (6*5*4)/ (10*9*8)\\n3. (3*3*3)/ (10*10*10)\\n4. (3*2*1)/ (10*9*8)\\n3. Two boxes containing chocolates are placed on a table. The boxes are\\nlabelled B and B. Box B contains 6 dark chocolates and 5 white\\nchocolates. Box B contains 3 dark chocolates and 8 orange chocolates.\\nThe boxes are arranged so that the probability of selecting box B is /\\nand the probability of selecting box B is / . Sneha is blindfolded and\\nasked to select a chocolate. She will win Rs. 10,000 if she selects a dark\\nchocolate. What is the probability that Sneha will win Rs. 10,000 (that\\nis, she will select a dark chocolate)?\\n1. \\n2. \\n3. \\n4. \\n4. Two boxes containing chocolates are placed on a table. The boxes are\\nlabelled B and B. Box B contains 6 Cadbury chocolates and 5 Amul\\nchocolates. Box B contains 3 Cadbury chocolates and 8 Nestle\\nchocolates. The boxes are arranged so that the probability of selecting\\nbox B is /  and the probability of selecting box B is / . Sneha is\\nblindfolded and asked to select a chocolate. She will win Rs. 10,000 if\\nshe selects a Cadbury chocolate. If she win Rs 10,000, what is the\\nprobability that she selected a Cadbury chocolate from the first box?\\n1. \\n2. \\n3. \\n4. \\n5. In a certain basketball club, there are 4% of male players who are over 6\\nfeet tall and 1% of female players who are over 6 feet tall. The ratio of\\nmale to female players in the total player population is male:female =\\n1 2 1\\n2\\n1 3\\n2 3\\n1 2 1\\n2\\n1 3 2 3\\n1\\n2\\n1 2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 314, 'page_label': '315'}, page_content='2:3. A player is selected at random from among all those who are over 6\\nfeet tall. What is the probability that the player is a female?\\n1. 3/11\\n2. 2/5\\n3. 2/11\\n4. 1/11\\n6. The probability that a particular hypothesis holds for a data set based on\\nthe Prior is called\\n1. Independent probabilities\\n2. Posterior probabilities\\n3. Interior probabilities\\n4. Dependent probabilities\\n7. One main disadvantage of Bayesian classifiers is that they utilize all\\navailable parameters to subtly change the predictions.\\n1. True\\n2. False\\n8. In a bolt factory, machines A1, A2, and A3 manufacture respectively\\n25%, 35%, and 40% of the total output. Of these 5%, 4%, and 2% are\\ndefective bolts. A bolt is drawn at random from the product and is found\\nto be defective. What is the probability that it was manufactured by\\nmachine A2?\\n1. 0.0952\\n2. 0.452\\n3. 0.952\\n4. 0.125\\n9. Bayesian methods can perform better than the other methods while\\nvalidating the hypotheses that make probabilistic predictions.\\n1. True\\n2. False\\n10. Naïve Bayes classifier makes the naïve assumption that the attribute\\nvalues are conditionally dependent given the classification of the\\ninstance.\\n1. True\\n2. False\\nSHORT ANSWER-TYPE QUESTIONS (5 MARKS EACH)\\n1. What is prior probability? Give an example.\\n2. What is posterior probability? Give an example.\\n3. What is likelihood probability? Give an example.\\n4. What is Naïve Bayes classifier? Why is it named so?\\n5. What is optimal Bayes classifier?\\n6. Write any two features of Bayesian learning methods.\\n7. Define the concept of consistent learners.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 315, 'page_label': '316'}, page_content='8. Write any two strengths of Bayes classifier.\\n9. Write any two weaknesses of Bayes classifier.\\n10. Explain how Naïve Bayes classifier is used for\\n1. Text classification\\n2. Spam filtering\\n3. Market sentiment analysis\\nLONG ANSWER-TYPE QUESTIONS (10 MARKS EACH)\\n1. Explain the concept of Prior, Posterior, and Likelihood with an example.\\n2. How Bayes’ theorem supports the concept learning principle?\\n3. Explain Naïve Bayes classifier with an example of its use in practical\\nlife.\\n4. Is it possible to use Naïve Bayes classifier for continuous numeric data?\\nIf so, how?\\n5. What are Bayesian Belief networks? Where are they used? Can they\\nsolve all types of problems?\\n6. In an airport security checking system, the passengers are checked to\\nfind out any intruder. Let I with i∈  {0, 1} be the random variable which\\nindicates whether somebody is an intruder (i = 1) or not (i = 0) and A\\nwith a ∈  {0, 1} be the variable indicating alarm. An alarm will be\\nraised if an intruder is identified with probability P(A = 1|I = 1) = 0.98\\nand a non-intruder with probability P(A = 1|I = 0) = 0.001, which\\nimplies the error factor. In the population of passengers, the probability\\nof someone is intruder is P(I = 1) = 0.00001. What is the probability\\nthat an alarm is raised when a person actually is an intruder?\\n7. An antibiotic resistance test (random variable T) has 1% false positives\\n(i.e. 1% of those not resistance to an antibiotic show positive result in\\nthe test) and 5% false negatives (i.e. 5% of those actually resistant to an\\nantibiotic test negative). Let us assume that 2% of those tested are\\nresistant to antibiotics. Determine the probability that somebody who\\ntests positive is actually resistant (random variable D).\\n8. For preparation of the exam, a student knows that one question is to be\\nsolved in the exam which is either of types A, B, or C. The probabilities\\nof A, B, or C appearing in the exam are 30%, 20%, and 50%\\nrespectively. During the preparation, the student solved 9 of 10\\nproblems of type A, 2 of 10 problems of type B, and 6 of 10 problems\\nof type C.\\n1. What is the probability that the student will solve the problem of\\nthe exam?\\n2. Given that the student solved the problem, what is the probability\\nthat it was of type A?\\n9. A CCTV is installed in a bank to monitor the incoming customers and\\ntake a photograph. Though there are continuous flows of customers, we'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 316, 'page_label': '317'}, page_content='create bins of timeframe of 5 min each. In each time frame of 5 min,\\nthere may be a customer moving into the bank with 5% probability or\\nthere is no customer (again, for simplicity, we assume that either there is\\n1 customer or none, not the case of multiple customers). If there is a\\ncustomer, it will be detected by the CCTV with a probability of 99%. If\\nthere is no customer, the camera will take a false photograph by\\ndetecting other thing’s movement with a probability of 10%.\\n1. How many customers enter the bank on average per day (10\\nhours)?\\n2. How many false photographs (there is a photograph taken even\\nthough there is no customer) and how many missed photographs\\n(there is no photograph even though there is a customer) are there\\non average per day?\\n3. If there is a photograph, what is the probability that there is\\nindeed a customer?\\n10. Draw the Bayesian Belief network to represent the conditional\\nindependence assumptions of the Naïve Bayes classifier for the match\\nwinning prediction problem of Section 6.4.4. Construct the conditional\\nprobability table associated with the node Won Toss.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 317, 'page_label': '318'}, page_content='Chapter 7\\nSupervised Learning: Classification\\n7.1 INTRODUCTION\\nOBJECTIVE OF THE CHAPTER :\\nIn the last chapter on Bayesian Concept Learning, you\\nwere introduced to an important supervised learning\\nalgorithm – the Naïve Bayes algorithm. As we have seen,\\nit is a very simple but powerful classifier based on Bayes’\\ntheorem of conditional probability. However, other than\\nthe Naïve Bayes classifier, there are more algorithms for\\nclassification. This chapter will focus on other\\nclassification algorithms.\\nThe first algorithm we will delve into in this chapter is\\nk-Nearest Neighbour (kNN), which tries to classify\\nunlabelled data instances based on the similarity with the\\nlabelled instances in the training data.\\nThen, another critical classifier, named as decision tree,\\nwill be explained in detail. Decision tree, as the name\\nsuggests, is a classifier based on a series of logical\\ndecisions, which resembles a tree with branches.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 318, 'page_label': '319'}, page_content='Next, we will explore the random forest classifier,\\nwhich, in very simplistic terms, can be thought as a\\ncollection of many decision trees.\\nFinally, a very powerful and popular classifier named\\nSupport Vector Machine (SVM) will be explored.\\nSo, by the end of this chapter, you will gain enough\\nknowledge to start solving a classification problem by\\nusing some standard classifiers.\\n7.2 EXAMPLE OF SUPERVISED LEARNING\\nIn supervised learning, the labelled training data provide the\\nbasis for learning. According to the definition of machine\\nlearning, this labelled training data is the experience or prior\\nknowledge or belief. It is called supervised learning because\\nthe process of learning from the training data by a machine can\\nbe related to a teacher supervising the learning process of a\\nstudent who is new to the subject. Here, the teacher is the\\ntraining data.\\nTraining data is the past information with known value of\\nclass field or ‘label’. Hence, we say that the ‘training data is\\nlabelled’ in the case of supervised learning (refer Fig. 7.1).\\nContrary to this, there is no labelled training data for\\nunsupervised learning. Semi-supervised learning, as depicted\\nin Figure 7.1, uses a small amount of labelled data along with\\nunlabelled data for training.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 319, 'page_label': '320'}, page_content='FIG. 7.1 Supervised learning vs. unsupervised learning\\nIn a hospital, many patients are treated in the general wards.\\nIn comparison, the number of beds in the Intensive Care Unit\\n(ICU) is much less. So, it is always a cause of worry for the\\nhospital management that if the health condition of a number\\nof patients in the general ward suddenly aggravates and they\\nwould have to be moved to ICU. Without previous planning\\nand preparations, such a spike in demand becomes difficult for\\nthe hospital to manage. This problem can be addressed in a\\nmuch better way if it is possible to predict which of the\\npatients in the normal wards have a possibility of their health\\ncondition deteriorating and thus need to be moved to ICU.\\nThis kind of prediction problem comes under the purview of\\nsupervised learning or, more specifically, under classification.\\nThe hospital already has all past patient records. The records\\nof the patients whose health condition aggravated in the past\\nand had to be moved to ICU can form the training data for this\\nprediction problem. Test results of newly admitted patients are\\nused to classify them as high-risk or low-risk patients.\\nSome more examples of supervised learning are as follows:\\nPrediction of results of a game based on the past analysis of results'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 320, 'page_label': '321'}, page_content='Predicting whether a tumour is malignant or benign on the basis of the\\nanalysis of data\\nPrice prediction in domains such as real estate, stocks, etc.\\nDid you know?\\n‘IBM Watson Health’ developed by IBM software\\nprovides evidence-backed cancer care to each patient by\\nunderstanding millions of data points. ‘Watson for\\nOncology’ helps physicians quickly identify vital\\ninformation in a patient’s medical record, surface relevant\\nevidence, and explore various treatment options for\\npatients\\n(Source: https://www.ibm.com/watson/health/oncology-\\nand-genomics/oncology).\\n7.3 CLASSIFICATION MODEL\\nLet us consider two examples, say ‘predicting whether a\\ntumour is malignant or benign’ and ‘price prediction in the\\ndomain of real estate’. Are these two problems same in nature?\\nThe answer is ‘no’. It is true that both of them are problems\\nrelated to prediction. However, for tumour prediction, we are\\ntrying to predict which category or class, i.e. ‘malignant’ or\\n‘benign’, an unknown input data related to tumour belongs to.\\nIn the other case, that is, for price prediction, we are trying to\\npredict an absolute value and not a class.\\nWhen we are trying to predict a categorical or nominal\\nvariable, the problem is known as a classification problem. A\\nclassification problem is one where the output variable is a'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 321, 'page_label': '322'}, page_content='category such as ‘red’ or ‘blue’ or ‘malignant tumour’ or\\n‘benign tumour’, etc.\\nWhereas when we are trying to predict a numerical variable\\nsuch as ‘price’, ‘weight’, etc. the problem falls under the\\ncategory of regression.\\nNote that:\\nSupervised machine learning is as good as the data used to\\ntrain it. If the training data is poor in quality, the prediction\\nwill also be far from being precise.\\nWe can observe that in classification, the whole problem\\ncentres around assigning a label or category or class to a test\\ndata on the basis of the label or category or class information\\nthat is imparted by the training data. Because the target\\nobjective is to assign a class label, we call this type of problem\\nas a classification problem. Figure 7.2 depicts the typical\\nprocess of classification where a classification model is\\nobtained from the labelled training data by a classifier\\nalgorithm. On the basis of the model, a class label (e.g. ‘Intel’\\nas in the case of the test data referred in Fig. 7.2) is assigned to\\nthe test data.\\nA critical classification problem in the context of the\\nbanking domain is identifying potentially fraudulent\\ntransactions. Because there are millions of transactions which\\nhave to be scrutinized to identify whether a particular\\ntransaction might be a fraud transaction, it is not possible for\\nany human being to carry out this task. Machine learning is'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 322, 'page_label': '323'}, page_content='leveraged efficiently to do this task, and this is a classic case\\nof classification. On the basis of the past transaction data,\\nespecially the ones labelled as fraudulent, all new incoming\\ntransactions are marked or labelled as usual or suspicious. The\\nsuspicious transactions are subsequently segregated for a\\ncloser review.\\nFIG. 7.2 Classification model\\nIn summary, classification is a type of supervised learning\\nwhere a target feature, which is of categorical type, is\\npredicted for test data on the basis of the information imparted\\nby the training data. The target categorical feature is known as\\nclass .\\nSome typical classification problems include the following:\\nImage classification\\nDisease prediction'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 323, 'page_label': '324'}, page_content='Win–loss prediction of games\\nPrediction of natural calamity such as earthquake, flood, etc.\\nHandwriting recognition :\\nDid you know?\\nMachine learning saves lives – it can spot 52% of breast cancer cells at\\nleast a year before patients are diagnosed\\nUS Postal Service uses machine learning for handwriting recognition\\nFacebook’s news feed uses machine learning to personalize each\\nmember’s feed\\n7.4 CLASSIFICATION LEARNING STEPS\\nFirst, there is a problem which is to be solved, and then, the\\nrequired data (related to the problem, which is already stored\\nin the system) is evaluated and pre-processed based on the\\nalgorithm. Algorithm selection is a critical point in supervised\\nlearning. The result after iterative training rounds is a classifier\\nfor the problem in hand (refer Fig. 7.3).'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 324, 'page_label': '325'}, page_content='FIG. 7.3 Classification model steps\\nProblem Identification: Identifying the problem is the first\\nstep in the supervised learning model. The problem needs to\\nbe a well-formed problem,i.e. a problem with well-defined\\ngoals and benefit, which has a long-term impact.\\nIdentification of Required Data: On the basis of the problem\\nidentified above, the required data set that precisely represents\\nthe identified problem needs to be identified/evaluated. For\\nexample: If the problem is to predict whether a tumour is\\nmalignant or benign, then the corresponding patient data sets\\nrelated to malignant tumour and benign tumours are to be\\nidentified.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 325, 'page_label': '326'}, page_content='Data Pre-processing: This is related to the\\ncleaning/transforming the data set. This step ensures that all\\nthe unnecessary/irrelevant data elements are removed. Data\\npre-processing refers to the transformations applied to the\\nidentified data before feeding the same into the algorithm.\\nBecause the data is gathered from different sources, it is\\nusually collected in a raw format and is not ready for\\nimmediate analysis. This step ensures that the data is ready to\\nbe fed into the machine learning algorithm.\\nDefinition of Training Data Set: Before starting the analysis,\\nthe user should decide what kind of data set is to be used as a\\ntraining set. In the case of signature analysis, for example, the\\ntraining data set might be a single handwritten alphabet, an\\nentire handwritten word (i.e. a group of the alphabets) or an\\nentire line of handwriting (i.e. sentences or a group of words).\\nThus, a set of ‘input meta-objects’ and corresponding ‘output\\nmeta-objects’ are also gathered. The training set needs to be\\nactively representative of the real-world use of the given\\nscenario. Thus, a set of data input (X) and corresponding\\noutputs (Y) is gathered either from human experts or\\nexperiments.\\nAlgorithm Selection: This involves determining the structure\\nof the learning function and the corresponding learning\\nalgorithm. This is the most critical step of supervised learning\\nmodel. On the basis of various parameters, the best algorithm\\nfor a given problem is chosen.\\nTraining: The learning algorithm identified in the previous\\nstep is run on the gathered training set for further fine tuning.\\nSome supervised learning algorithms require the user to\\ndetermine specific control parameters (which are given as\\ninputs to the algorithm). These parameters (inputs given to'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 326, 'page_label': '327'}, page_content='algorithm) may also be adjusted by optimizing performance on\\na subset (called as validation set) of the training set.\\nEvaluation with the Test Data Set: Training data is run on\\nthe algorithm, and its performance is measured here. If a\\nsuitable result is not obtained, further training of parameters\\nmay be required.\\n7.5 COMMON CLASSIFICATION ALGORITHMS\\nLet us now delve into some common classification algorithms.\\nFollowing are the most common classification algorithms, out\\nof which we have already learnt about the Naïve Bayes\\nclassifier in Chapter 6. We will cover details of the other\\nalgorithms in this chapter.\\n1. k-Nearest Neighbour (kNN)\\n2. Decision tree\\n3. Random forest\\n4. Support Vector Machine (SVM)\\n5. Naïve Bayes classifier\\n7.5.1 k -Nearest Neighbour (kNN)\\nThe kNN algorithm is a simple but extremely powerful\\nclassification algorithm. The name of the algorithm originates\\nfrom the underlying philosophy of kNN – i.e. people having\\nsimilar background or mindset tend to stay close to each other.\\nIn other words, neighbours in a locality have a similar\\nbackground. In the same way, as a part of the kNN algorithm,\\nthe unknown and unlabelled data which comes for a prediction\\nproblem is judged on the basis of the training data set elements\\nwhich are similar to the unknown element. So, the class label\\nof the unknown element is assigned on the basis of the class\\nlabels of the similar training data set elements (metaphorically'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 327, 'page_label': '328'}, page_content='can be considered as neighbours of the unknown element). Let\\nus try to understand the algorithm with a simple data set.\\n7.5.1.1 How kNN works\\nLet us consider a very simple Student data set as depicted in\\nFigure 7.4. It consists of 15 students studying in a class. Each\\nof the students has been assigned a score on a scale of 10 on\\ntwo performance parameters – ‘Aptitude’ and\\n‘Communication’. Also, a class value is assigned to each\\nstudent based on the following criteria:\\n1. Students having good communication skills as well as a good level of\\naptitude have been classified as ‘Leader’\\n2. Students having good communication skills but not so good level of\\naptitude have been classified as ‘Speaker’\\n3. Students having not so good communication skill but a good level of\\naptitude have been classified as ‘Intel’\\nFIG. 7.4 Student data set'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 328, 'page_label': '329'}, page_content='As we have already seen in Chapter 3, while building a\\nclassification model, a part of the labelled input data is\\nretained as test data. The remaining portion of the input data is\\nused to train the model – hence known as training data. The\\nmotivation to retain a part of the data as test data is to evaluate\\nthe performance of the model. As we have seen, the\\nperformance of the classification model is measured by the\\nnumber of correct classifications made by the model when\\napplied to an unknown data set. However, it is not possible\\nduring model testing to know the actual label value of an\\nunknown data. Therefore, the test data, which is a part of the\\nlabelled input data, is used for this purpose. If the class value\\npredicted for most of the test data elements matches with the\\nactual class value that they have, then we say that the\\nclassification model possesses a good accuracy. In context of\\nthe Student data set, to keep the things simple, we assume one\\ndata element of the input data set as the test data. As depicted\\nin Figure 7.5, the record of the student named Josh is assumed\\nto be the test data. Now that we have the training data and test\\ndata identified, we can start with the modelling.\\nAs we have already discussed, in the kNN algorithm, the\\nclass label of the test data elements is decided by the class\\nlabel of the training data elements which are neighbouring, i.e.\\nsimilar in nature. But there are two challenges:\\n1. What is the basis of this similarity or when can we say that two data\\nelements are similar?\\n2. How many similar elements should be considered for deciding the class\\nlabel of each test data element?\\nTo answer the first question, though there are many\\nmeasures of similarity, the most common approach adopted by\\nkNN to measure similarity between two data elements is\\nEuclidean distance. Considering a very simple data set having'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 329, 'page_label': '330'}, page_content='two features (say f  and f ), Euclidean distance between two\\ndata elements d  and d  can be measured by\\nwhere f  = value of feature f  for data element d\\nf  = value of feature f  for data element d\\nf  = value of feature f  for data element d\\nf  = value of feature f  for data element d\\nFIG. 7.5 Segregated student data set\\nSo, as depicted in Figure 7.6, the training data points of the\\nStudent data set considering only the features ‘Aptitude’ and\\n‘Communication’ can be represented as dots in a two-\\n1 2\\n1 2\\n11 1 1\\n12 1 2\\n21 2 1\\n22 2 2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 330, 'page_label': '331'}, page_content='dimensional feature space. As shown in the figure, the training\\ndata points having the same class value are coming close to\\neach other. The reason for considering two-dimensional data\\nspace is that we are considering just the two features of the\\nStudent data set, i.e. ‘Aptitude’ and ‘Communication’, for\\ndoing the classification. The feature ‘Name’ is ignored\\nbecause, as we can understand, it has no role to play in\\ndeciding the class value. The test data point for student Josh is\\nrepresented as an asterisk in the same space. To find out the\\nclosest or nearest neighbours of the test data point, Euclidean\\ndistance of the different dots need to be calculated from the\\nasterisk. Then, the class value of the closest neighbours helps\\nin assigning the class value of the test data element.\\nFIG. 7.6 2-D representation of the student data set\\nNow, let us try to find the answer to the second question, i.e.\\nhow many similar elements should be considered. The answer\\nlies in the value of ‘k’ which is a user-defined parameter given'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 331, 'page_label': '332'}, page_content='as an input to the algorithm. In the kNN algorithm, the value\\nof ‘k’ indicates the number of neighbours that need to be\\nconsidered. For example, if the value of k is 3, only three\\nnearest neighbours or three training data elements closest to\\nthe test data element are considered. Out of the three data\\nelements, the class which is predominant is considered as the\\nclass label to be assigned to the test data. In case the value of k\\nis 1, only the closest training data element is considered. The\\nclass label of that data element is directly assigned to the test\\ndata element. This is depicted in Figure 7.7.\\nFIG. 7.7 Distance calculation between test and training points\\nLet us now try to find out the outcome of the algorithm for\\nthe Student data set we have. In other words, we want to see\\nwhat class value kNN will assign for the test data for student\\nJosh. Again, let us refer back to Figure 7.7. As is evident,\\nwhen the value of k is taken as 1, only one training data point\\nneeds to be considered. The training record for student Gouri'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 332, 'page_label': '333'}, page_content='comes as the closest one to test record of Josh, with a distance\\nvalue of 1.118. Gouri has class value ‘Intel’. So, the test data\\npoint is also assigned a class label value ‘Intel’. When the\\nvalue of k is assumed as 3, the closest neighbours of Josh in\\nthe training data set are Gouri, Susant, and Bobby with\\ndistances being 1.118, 1.414, and 1.5, respectively. Gouri and\\nBobby have class value ‘Intel’, while Susant has class value\\n‘Leader’. In this case, the class value of Josh is decided by\\nmajority voting. Because the class value of ‘Intel’ is formed by\\nthe majority of the neighbours, the class value of Josh is\\nassigned as ‘Intel’. This same process can be extended for any\\nvalue of k.\\nBut it is often a tricky decision to decide the value of k. The\\nreasons are as follows:\\nIf the value of k is very large (in the extreme case equal to the total number\\nof records in the training data), the class label of the majority class of the\\ntraining data set will be assigned to the test data regardless of the class labels\\nof the neighbours nearest to the test data.\\nIf the value of k is very small (in the extreme case equal to 1), the class value\\nof a noisy data or outlier in the training data set which is the nearest\\nneighbour to the test data will be assigned to the test data.\\nThe best k value is somewhere between these two extremes.\\nFew strategies, highlighted below, are adopted by machine\\nlearning practitioners to arrive at a value for k.\\nOne common practice is to set k equal to the square root of the number of\\ntraining records.\\nAn alternative approach is to test several k values on a variety of test data\\nsets and choose the one that delivers the best performance.\\nAnother interesting approach is to choose a larger value of k, but apply a\\nweighted voting process in which the vote of close neighbours is considered\\nmore influential than the vote of distant neighbours.\\n7.5.1.2 kNN algorithm'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 333, 'page_label': '334'}, page_content='Input: Training data set, test data set (or data points), value of\\n‘k’ (i.e. number of nearest neighbours to be considered)\\nSteps:\\nDo for all test data points\\nCalculate the distance (usually Euclidean distance) of the test data point from\\nthe different training data points.\\nFind the closest ‘k’ training data points, i.e. training data points whose\\ndistances are least from the test data point.\\nIf k = 1\\nThen assign class label of the training data point to the test data point\\nElse\\nWhichever class label is predominantly present in the training data points,\\nassign that class label to the test data point\\nEnd do\\n7.5.1.3 Why the kNN algorithm is called a lazy learner?\\nWe have already discussed in Chapter 3 that eager learners\\nfollow the general steps of machine learning, i.e. perform an\\nabstraction of the information obtained from the input data and\\nthen follow it through by a generalization step. However, as\\nwe have seen in the case of the kNN algorithm, these steps are\\ncompletely skipped. It stores the training data and directly\\napplies the philosophy of nearest neighbourhood finding to\\narrive at the classification. So, for kNN, there is no learning\\nhappening in the real sense. Therefore, kNN falls under the\\ncategory of lazy learner.\\n7.5.1.4 Strengths of the kNN algorithm\\nExtremely simple algorithm – easy to understand\\nVery effective in certain situations, e.g. for recommender system design\\nVery fast or almost no time required for the training phase'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 334, 'page_label': '335'}, page_content='7.5.1.5 Weaknesses of the kNN algorithm\\nDoes not learn anything in the real sense. Classification is done completely\\non the basis of the training data. So, it has a heavy reliance on the training\\ndata. If the training data does not represent the problem domain\\ncomprehensively, the algorithm fails to make an effective classification.\\nBecause there is no model trained in real sense and the classification is done\\ncompletely on the basis of the training data, the classification process is\\nvery slow.\\nAlso, a large amount of computational space is required to load the training\\ndata for classification.\\n7.5.1.6 Application of the kNN algorithm\\nOne of the most popular areas in machine learning where the\\nkNN algorithm is widely adopted is recommender systems. As\\nwe know, recommender systems recommend users different\\nitems which are similar to a particular item that the user seems\\nto like. The liking pattern may be revealed from past purchases\\nor browsing history and the similar items are identified using\\nthe kNN algorithm.\\nAnother area where there is widespread adoption of kNN is\\nsearching documents/ contents similar to a given\\ndocument/content. This is a core area under information\\nretrieval and is known as concept search.\\n7.5.2 Decision tree\\nDecision tree learning is one of the most widely adopted\\nalgorithms for classification. As the name indicates, it builds a\\nmodel in the form of a tree structure. Its grouping exactness is\\nfocused with different strategies, and it is exceptionally\\nproductive.\\nA decision tree is used for multi-dimensional analysis with\\nmultiple classes. It is characterized by fast execution time and'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 335, 'page_label': '336'}, page_content='ease in the interpretation of the rules. The goal of decision tree\\nlearning is to create a model (based on the past data called past\\nvector) that predicts the value of the output variable based on\\nthe input variables in the feature vector.\\nEach node (or decision node) of a decision tree corresponds\\nto one of the feature vector. From every node, there are edges\\nto children, wherein there is an edge for each of the possible\\nvalues (or range of values) of the feature associated with the\\nnode. The tree terminates at different leaf nodes (or terminal\\nnodes) where each leaf node represents a possible value for the\\noutput variable. The output variable is determined by\\nfollowing a path that starts at the root and is guided by the\\nvalues of the input variables.\\nA decision tree is usually represented in the format depicted\\nin Figure 7.8.\\nFIG. 7.8 Decision tree structure\\nEach internal node (represented by boxes) tests an attribute\\n(represented as ‘A’/‘B’ within the boxes). Each branch\\ncorresponds to an attribute value (T/F) in the above case. Each\\nleaf node assigns a classification. The first node is called as\\n‘Root’ Node. Branches from the root node are called as ‘Leaf’'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 336, 'page_label': '337'}, page_content='Nodes where ‘A’ is the Root Node (first node). ‘B’ is the\\nBranch Node. ‘T’ & ‘F’ are Leaf Nodes.\\nThus, a decision tree consists of three types of nodes:\\nRoot Node\\nBranch Node\\nLeaf Node\\nFigure 7.9 shows an example decision tree for a car driving\\n– the decision to be taken is whether to ‘Keep Going’ or to\\n‘Stop’, which depends on various situations as depicted in the\\nfigure. If the signal is RED in colour, then the car should be\\nstopped. If there is not enough gas (petrol) in the car, the car\\nshould be stopped at the next available gas station.\\nFIG. 7.9 Decision tree example\\n7.5.2.1 Building a decision tree\\nDecision trees are built corresponding to the training data\\nfollowing an approach called recursive partitioning. The'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 337, 'page_label': '338'}, page_content='approach splits the data into multiple subsets on the basis of\\nthe feature values. It starts from the root node, which is\\nnothing but the entire data set. It first selects the feature which\\npredicts the target class in the strongest way. The decision tree\\nsplits the data set into multiple partitions, with data in each\\npartition having a distinct value for the feature based on which\\nthe partitioning has happened. This is the first set of branches.\\nLikewise, the algorithm continues splitting the nodes on the\\nbasis of the feature which helps in the best partition. This\\ncontinues till a stopping criterion is reached. The usual\\nstopping criteria are –\\n1. All or most of the examples at a particular node have the same class\\n2. All features have been used up in the partitioning\\n3. The tree has grown to a pre-defined threshold limit\\nLet us try to understand this in the context of an example.\\nGlobal Technology Solutions (GTS), a leading provider of IT\\nsolutions, is coming to College of Engineering and\\nManagement (CEM) for hiring B.Tech. students. Last year\\nduring campus recruitment, they had shortlisted 18 students\\nfor the final interview. Being a company of international\\nrepute, they follow a stringent interview process to select only\\nthe best of the students. The information related to the\\ninterview evaluation results of shortlisted students (hiding the\\nnames) on the basis of different evaluation parameters is\\navailable for reference in Figure 7.10. Chandra, a student of\\nCEM, wants to find out if he may be offered a job in GTS. His\\nCGPA is quite high. His self-evaluation on the other\\nparameters is as follows:\\nCommunication – Bad; Aptitude – High; Programming skills – Bad'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 338, 'page_label': '339'}, page_content='FIG. 7.10 Training data for GTS recruitment\\nLet us try to solve this problem, i.e. predicting whether\\nChandra will get a job offer, by using the decision tree model.\\nFirst, we need to draw the decision tree corresponding to the\\ntraining data given in Figure 7.10. According to the table, job\\noffer condition (i.e. the outcome) is FALSE for all the cases\\nwhere Aptitude = Low, irrespective of other conditions. So,\\nthe feature Aptitude can be taken up as the first node of the\\ndecision tree.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 339, 'page_label': '340'}, page_content='For Aptitude = High, job offer condition is TRUE for all\\nthe cases where Communication = Good. For cases where\\nCommunication = Bad, job offer condition is TRUE for all\\nthe cases where CGPA = High.\\nFigure 7.11 depicts the complete decision tree diagram for\\nthe table given in Figure 7.10.\\nFIG. 7.11 Decision tree based on the training data\\n7.5.2.2 Searching a decision tree\\nBy using the above decision tree depicted in Figure 7.11, we\\nneed to predict whether Chandra might get a job offer for the\\ngiven parameter values: CGPA = High, Communication =\\nBad, Aptitude = High, Programming skills = Bad. There are\\nmultiple ways to search through the trained decision tree for a\\nsolution to the given prediction problem.\\nExhaustive search\\n1. Place the item in the first group (class). Recursively examine solutions\\nwith the item in the first group (class).'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 340, 'page_label': '341'}, page_content='2. Place the item in the second group (class). Recursively examine\\nsolutions with the item in the second group (class).\\n3. Repeat the above steps until the solution is reached.\\nExhaustive search travels through the decision tree\\nexhaustively, but it will take much time when the decision tree\\nis big with multiple leaves and multiple attribute values.\\nBranch and bound search\\nBranch and bound uses an existing best solution to sidestep\\nsearching of the entire decision tree in full. When the\\nalgorithm starts, the best solution is well defined to have the\\nworst possible value; thus, any solution it finds out is an\\nimprovement. This makes the algorithm initially run down to\\nthe left-most branch of the tree, even though that is unlikely to\\nproduce a realistic result. In the partitioning problem, that\\nsolution corresponds to putting every item in one group, and it\\nis an unacceptable solution. A programme can speed up the\\nprocess by using a fast heuristic to find an initial solution. This\\ncan be used as an input for branch and bound. If the heuristic\\nis right, the savings can be substantial.\\nFIG. 7.12 Decision tree based on the training data (depicting a sample path)'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 341, 'page_label': '342'}, page_content='Figure 7.12 depicts a sample path (thick line) for the\\nconditions CGPA = High, Communication = Bad, Aptitude =\\nHigh and Programming skills = Bad. According to the above\\ndecision tree, the prediction can be made as Chandra will get\\nthe job offer.\\nThere are many implementations of decision tree, the most\\nprominent ones being C5.0, CART (Classification and\\nRegression Tree), CHAID (Chi-square Automatic Interaction\\nDetector) and ID3 (Iterative Dichotomiser 3) algorithms. The\\nbiggest challenge of a decision tree algorithm is to find out\\nwhich feature to split upon. The main driver for identifying the\\nfeature is that the data should be split in such a way that the\\npartitions created by the split should contain examples\\nbelonging to a single class. If that happens, the partitions are\\nconsidered to be pure. Entropy is a measure of impurity of an\\nattribute or feature adopted by many algorithms such as ID3\\nand C5.0. The information gain is calculated on the basis of\\nthe decrease in entropy (S) after a data set is split according to\\na particular attribute (A). Constructing a decision tree is all\\nabout finding an attribute that returns the highest information\\ngain (i.e. the most homogeneous branches).\\nNote:\\nLike information gain, there are other measures like Gini\\nindex or chi-square for individual nodes to decide the\\nfeature on the basis of which the split has to be applied.\\nThe CART algorithm uses Gini index, while the CHAID\\nalgorithm uses chi-square for deciding the feature for\\napplying split.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 342, 'page_label': '343'}, page_content='7.5.2.3 Entropy of a decision tree\\nLet us say S is the sample set of training examples. Then,\\nEntropy (S) measuring the impurity of S is defined as\\nwhere c is the number of different class labels and p refers\\nto the proportion of values falling into the i-th class label.\\nFor example, with respect to the training data in Figure\\n7.10, we have two values for the target class ‘Job Offered?’ –\\nYes and No. The value of p for class value ‘Yes’ is 0.44 (i.e.\\n8/18) and that for class value ‘No’ is 0.56 (i.e. 10/18). So, we\\ncan calculate the entropy as\\n \\nEntropy(S) = -0.44 log (0.44) - 0.56 log (0.56) = 0.99.\\n7.5.2.4 Information gain of a decision tree\\nThe information gain is created on the basis of the decrease in\\nentropy (S) after a data set is split according to a particular\\nattribute (A). Constructing a decision tree is all about finding\\nan attribute that returns the highest information gain (i.e. the\\nmost homogeneous branches). If the information gain is 0, it\\nmeans that there is no reduction in entropy due to split of the\\ndata set according to that particular feature. On the other hand,\\nthe maximum amount of information gain which may happen\\nis the entropy of the data set before the split.\\ni\\n2 2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 343, 'page_label': '344'}, page_content='Information gain for a particular feature A is calculated by\\nthe difference in entropy before a split (or S ) with the\\nentropy after the split (S ).\\n \\nInformation Gain (S, A) = Entropy (S ) − Entropy (S )\\nFor calculating the entropy after split, entropy for all\\npartitions needs to be considered. Then, the weighted\\nsummation of the entropy for each partition can be taken as the\\ntotal entropy after split. For performing weighted summation,\\nthe proportion of examples falling into each partition is used\\nas weight.\\nLet us examine the value of information gain for the training\\ndata set shown in Figure 7.10. We will find the value of\\nentropy at the beginning before any split happens and then\\nagain after the split happens. We will compare the values for\\nall the cases –\\n1. when the feature ‘CGPA’ is used for the split\\n2. when the feature ‘Communication’ is used for the split\\n3. when the feature ‘Aptitude’ is used for the split\\n4. when the feature ‘Programming Skills’ is used for the split\\nFigure 7.13a gives the entropy values for the first level split\\nfor each of the cases mentioned above.\\nAs calculated, entropy of the data set before split (i.e.\\nEntropy (S )) = 0.99, and entropy of the data set after split\\n(i.e. Entropy (S )) is\\nbs\\nas\\nbs as\\nbs\\nas'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 344, 'page_label': '345'}, page_content='0.69 when the feature ‘CGPA’ is used for split\\n0.63 when the feature ‘Communication’ is used for split\\n0.52 when the feature ‘Aptitude’ is used for split\\n0.95 when the feature ‘Programming skill’ is used for split'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 345, 'page_label': '346'}, page_content='FIG. 7.13A Entropy and information gain calculation (Level 1)\\nTherefore, the information gain from the feature ‘CGPA’ =\\n0.99 − 0.69 = 0.3, whereas the information gain from the\\nfeature ‘Communication’ = 0.99 − 0.63 = 0.36. Likewise, the\\ninformation gain for ‘Aptitude’ and ‘Programming skills’ is\\n0.47 and 0.04, respectively.\\nHence, it is quite evident that among all the features,\\n‘Aptitude’ results in the best information gain when adopted\\nfor the split. So, at the first level, a split will be applied\\naccording to the value of ‘Aptitude’ or in other words,\\n‘Aptitude’ will be the first node of the decision tree formed.\\nOne important point to be noted here is that for Aptitude =\\nLow, entropy is 0, which indicates that always the result will\\nbe the same irrespective of the values of the other features.\\nHence, the branch towards Aptitude = Low will not continue\\nany further.\\nAs a part of level 2, we will thus have only one branch to\\nnavigate in this case – the one for Aptitude = High. Figure\\n7.13b presents calculations for level 2. As can be seen from\\nthe figure, the entropy value is as follows:\\n0.85 before the split\\n0.33 when the feature ‘CGPA’ is used for split\\n0.30 when the feature ‘Communication’ is used for split\\n0.80 when the feature ‘Programming skill’ is used for split'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 346, 'page_label': '347'}, page_content='Hence, the information gain after split with the features\\nCGPA, Communication and Programming Skill is 0.52, 0.55\\nand 0.05, respectively. Hence, the feature Communication\\nshould be used for this split as it results in the highest\\ninformation gain. So, at the second level, a split will be\\napplied on the basis of the value of ‘Communication’. Again,\\nthe point to be noted here is that for Communication = Good,\\nentropy is 0, which indicates that always the result will be the\\nsame irrespective of the values of the other features. Hence,\\nthe branch towards Communication = Good will not continue\\nany further.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 347, 'page_label': '348'}, page_content='FIG. 7.13B Entropy and information gain calculation (Level 2)\\nAs a part of level 3, we will thus have only one branch to\\nnavigate in this case – the one for Communication = Bad.\\nFigure 7.13c presents calculations for level 3. As can be seen\\nfrom the figure, the entropy value is as follows:\\n0.81 before the split\\n0 when the feature ‘CGPA’ is used for split'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 348, 'page_label': '349'}, page_content='0.50 when the feature ‘Programming Skill’ is used for split\\nFIG. 7.13C Entropy and information gain calculation (Level 3)'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 349, 'page_label': '350'}, page_content='Hence, the information gain after split with the feature\\nCGPA is 0.81, which is the maximum possible information\\ngain (as the entropy before split was 0.81). Hence, as obvious,\\na split will be applied on the basis of the value of ‘CGPA’.\\nBecause the maximum information gain is already achieved,\\nthe tree will not continue any further.\\n7.5.2.5 Algorithm for decision tree\\nInput: Training data set, test data set (or data points)\\nSteps:\\nDo for all attributes\\nCalculate the entropy E of the attribute F\\nif E < E  \\n   then E  = E and F  = F  \\nend if\\nEnd do\\nSplit the data set into subsets using the attribute F\\nDraw a decision tree node containing the attribute F  and split the data set\\ninto subsets\\nRepeat the above steps until the full tree is drawn covering all the attributes of\\nthe original table.\\n7.5.2.6 Avoiding overfitting in decision tree – pruning\\nThe decision tree algorithm, unless a stopping criterion is\\napplied, may keep growing indefinitely – splitting for every\\nfeature and dividing into smaller partitions till the point that\\nthe data is perfectly classified. This, as is quite evident, results\\nin overfitting problem. To prevent a decision tree getting\\noverfitted to the training data, pruning of the decision tree is\\nessential. Pruning a decision tree reduces the size of the tree\\ni i\\ni min \\nmin i min i \\nmin\\nmin'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 350, 'page_label': '351'}, page_content='such that the model is more generalized and can classify\\nunknown and unlabelled data in a better way.\\nThere are two approaches of pruning:\\nPre-pruning: Stop growing the tree before it reaches perfection.\\nPost-pruning: Allow the tree to grow entirely and then post-prune some of\\nthe branches from it.\\nIn the case of pre-pruning, the tree is stopped from further\\ngrowing once it reaches a certain number of decision nodes or\\ndecisions. Hence, in this strategy, the algorithm avoids\\noverfitting as well as optimizes computational cost. However,\\nit also stands a chance to ignore important information\\ncontributed by a feature which was skipped, thereby resulting\\nin miss out of certain patterns in the data.\\nOn the other hand, in the case of post-pruning, the tree is\\nallowed to grow to the full extent. Then, by using certain\\npruning criterion, e.g. error rates at the nodes, the size of the\\ntree is reduced. This is a more effective approach in terms of\\nclassification accuracy as it considers all minute information\\navailable from the training data. However, the computational\\ncost is obviously more than that of pre-pruning.\\n7.5.2.7 Strengths of decision tree\\nIt produces very simple understandable rules. For smaller trees, not much\\nmathematical and computational knowledge is required to understand this\\nmodel.\\nWorks well for most of the problems.\\nIt can handle both numerical and categorical variables.\\nCan work well both with small and large training data sets.\\nDecision trees provide a definite clue of which features are more useful for\\nclassification.\\n7.5.2.8 Weaknesses of decision tree'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 351, 'page_label': '352'}, page_content='Decision tree models are often biased towards features having more number\\nof possible values, i.e. levels.\\nThis model gets overfitted or underfitted quite easily.\\nDecision trees are prone to errors in classification problems with many\\nclasses and relatively small number of training examples.\\nA decision tree can be computationally expensive to train.\\nLarge trees are complex to understand.\\n7.5.2.9 Application of decision tree\\nDecision tree can be applied in a data set in which there is a\\nfinite list of attributes and each data instance stores a value for\\nthat attribute (e.g. ‘High’ for the attribute CGPA). When each\\nattribute has a small number of distinct values (e.g. ‘High’,\\n‘Medium’, ‘Low’), it is easier/quicker for the decision tree to\\nsuggest (or choose) an effective solution. This algorithm can\\nbe extended to handle real-value attributes (e.g. a floating\\npoint temperature).\\nThe most straightforward case exists when there are only\\ntwo possible values for an attribute (Boolean classification).\\nExample: Communication has only two values as ‘Good’ or\\n‘Bad’. It is also easy to extend the decision tree to create a\\ntarget function with more than two possible output values.\\nExample: CGPA can take one of the values from ‘High’,\\n‘Medium’, and ‘Low’. Irrespective of whether it is a binary\\nvalue/ multiple values, it is discrete in nature. For example,\\nAptitude can take the value of either ‘High’ or ‘Low’. It is not\\npossible to assign the value of both ‘High’ and ‘Low’ to the\\nattribute Aptitude to draw a decision tree.\\nThere should be no infinite loops on taking a decision. As\\nwe move from the root node to the next level node, it should\\nmove step-by-step towards the decision node. Otherwise, the\\nalgorithm may not give the final result for a given data. If a set'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 352, 'page_label': '353'}, page_content='of code goes in a loop, it would repeat itself forever, unless the\\nsystem crashes.\\nA decision tree can be used even for some instances with\\nmissing attributes and instances with errors in the\\nclassification of examples or in the attribute values describing\\nthose examples; such instances are handled well by decision\\ntrees, thereby making them a robust learning method.\\nPoints to Ponder\\nBalancing overfitting and underfitting in decision tree is a\\nvery tricky topic, involving more of an art than science.\\nThe way to master this art is experience in working with\\nmore number of data sets with a lot of diversity.\\n7.5.3 Random forest model\\nRandom forest is an ensemble classifier, i.e. a combining\\nclassifier that uses and combines many decision tree\\nclassifiers. Ensembling is usually done using the concept of\\nbagging with different feature sets. The reason for using large\\nnumber of trees in random forest is to train the trees enough\\nsuch that contribution from each feature comes in a number of\\nmodels. After the random forest is generated by combining the\\ntrees, majority vote is applied to combine the output of the\\ndifferent trees. A simplified random forest model is depicted in\\nFigure 7.14. The result from the ensemble model is usually\\nbetter than that from the individual decision tree models.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 353, 'page_label': '354'}, page_content='FIG. 7.14 Random forest model\\n7.5.3.1 How does random forest work?\\nThe random forest algorithm works as follows:\\n1. If there are N variables or features in the input data set, select a subset\\nof ‘m’ (m < N) features at random out of the N features. Also, the\\nobservations or data instances should be picked randomly.\\n2. Use the best split principle on these ‘m’ features to calculate the number\\nof nodes ‘d’.\\n3. Keep splitting the nodes to child nodes till the tree is grown to the\\nmaximum possible extent.\\n4. Select a different subset of the training data ‘with replacement’ to train\\nanother decision tree following steps (1) to (3). Repeat this to build and\\ntrain ‘n’ decision trees.\\n5. Final class assignment is done on the basis of the majority votes from\\nthe ‘n’ trees.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 354, 'page_label': '355'}, page_content='Points to Ponder\\nIn the random forest classifier, if the number of trees is\\nassumed to be excessively large, the model may get\\noverfitted. In an extreme case of overfitting, the\\nmodel may mimic the training data, and training error\\nmight be almost 0. However, when the model is run on an\\nunseen sample, it may result in a very high validation\\nerror.\\n7.5.3.2 Out-of-bag (OOB) error in random forest\\nIn random forests, we have seen, that each tree is constructed\\nusing a different bootstrap sample from the original data. The\\nsamples left out of the bootstrap and not used in the\\nconstruction of the i-th tree can be used to measure the\\nperformance of the model. At the end of the run, predictions\\nfor each such sample evaluated each time are tallied, and the\\nfinal prediction for that sample is obtained by taking a vote.\\nThe total error rate of predictions for such samples is termed\\nas out-of-bag (OOB) error rate.\\nThe error rate shown in the confusion matrix reflects the\\nOOB error rate. Because of this reason, the error rate displayed\\nis often surprisingly high.\\n7.5.3.3 Strengths of random forest\\nIt runs efficiently on large and expansive data sets.\\nIt has a robust method for estimating missing data and maintains precision\\nwhen a large proportion of the data is absent.\\nIt has powerful techniques for balancing errors in a class population of\\nunbalanced data sets.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 355, 'page_label': '356'}, page_content='It gives estimates (or assessments) about which features are the most\\nimportant ones in the overall classification.\\nIt generates an internal unbiased estimate (gauge) of the generalization error\\nas the forest generation progresses.\\nGenerated forests can be saved for future use on other data.\\nLastly, the random forest algorithm can be used to solve both classification\\nand regression problems.\\n7.5.3.4 Weaknesses of random forest\\nThis model, because it combines a number of decision tree models, is not as\\neasy to understand as a decision tree model.\\nIt is computationally much more expensive than a simple model like decision\\ntree.\\n7.5.3.5 Application of random forest\\nRandom forest is a very powerful classifier which combines\\nthe versatility of many decision tree models into a single\\nmodel. Because of the superior results, this ensemble model is\\ngaining wide adoption and popularity amongst the machine\\nlearning practitioners to solve a wide range of classification\\nproblems.\\n7.5.4 Support vector machines\\nSVM is a model, which can do linear classification as well as\\nregression. SVM is based on the concept of a surface, called a\\nhyperplane, which draws a boundary between data instances\\nplotted in the multi-dimensional feature space. The output\\nprediction of an SVM is one of two conceivable classes which\\nare already defined in the training data. In summary, the SVM\\nalgorithm builds an N-dimensional hyperplane model that\\nassigns future instances into one of the two possible output\\nclasses.\\n7.5.4.1 Classification using hyperplanes'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 356, 'page_label': '357'}, page_content='In SVM, a model is built to discriminate the data instances\\nbelonging to different classes. Let us assume for the sake of\\nsimplicity that the data instances are linearly separable. In this\\ncase, when mapped in a two-dimensional space, the data\\ninstances belonging to different classes fall in different sides of\\na straight line drawn in the two-dimensional space as depicted\\nin Figure 7.15a. If the same concept is extended to a multi-\\ndimensional feature space, the straight line dividing data\\ninstances belonging to different classes transforms to a\\nhyperplane as depicted in Figure 7.15b.\\nThus, an SVM model is a representation of the input\\ninstances as points in the feature space, which are mapped so\\nthat an apparent gap between them divides the instances of the\\nseparate classes. In other words, the goal of the SVM analysis\\nis to find a plane, or rather a hyperplane, which separates the\\ninstances on the basis of their classes. New examples (i.e. new\\ninstances) are then mapped into that same space and predicted\\nto belong to a class on the basis of which side of the gap the\\nnew instance will fall on. In summary, in the overall training\\nprocess, the SVM algorithm analyses input data and identifies\\na surface in the multi-dimensional feature space called the\\nhyperplane. There may be many possible hyperplanes, and one\\nof the challenges with the SVM model is to find the optimal\\nhyperplane.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 357, 'page_label': '358'}, page_content='FIG. 7.15 Linearly separable data instances\\nTraining data sets which have a substantial grouping\\nperiphery will function well with SVM. Generalization error in\\nterms of SVM is the measure of how accurately and precisely\\nthis SVM model can predict values for previously unseen data\\n(new data). A hard margin in terms of SVM means that an\\nSVM model is inflexible in classification and tries to work\\nexceptionally fit in the training set, thereby causing overfitting.\\nSupport Vectors: Support vectors are the data points\\n(representing classes), the critical component in a data set,\\nwhich are near the identified set of lines (hyperplane). If\\nsupport vectors are removed, they will alter the position of the\\ndividing hyperplane.\\nHyperplane and Margin: For an N-dimensional feature\\nspace, hyperplane is a flat subspace of dimension (N−1) that\\nseparates and classifies a set of data. For example, if we\\nconsider a two-dimensional feature space (which is nothing\\nbut a data set having two features and a class variable), a\\nhyperplane will be a one-dimensional subspace or a straight'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 358, 'page_label': '359'}, page_content='line. In the same way, for a three-dimensional feature space\\n(data set having three features and a class variable),\\nhyperplane is a two-dimensional subspace or a simple plane.\\nHowever, quite understandably, it is difficult to visualize a\\nfeature space greater than three dimensions, much like for a\\nsubspace or hyperplane having more than three dimensions.\\nMathematically, in a two-dimensional space, hyperplane can\\nbe defined by the equation:\\nc  + c X  + c X  = 0, which is nothing but an equation of a\\nstraight line.\\nExtending this concept to an N-dimensional space,\\nhyperplane can be defined by the equation:\\nc  + c X  + c X  + … + c X  = 0 which, in short, can be\\nrepresented as follows:\\nSpontaneously, the further (or more distance) from the\\nhyperplane the data points lie, the more confident we can be\\nabout correct categorization. So, when a new testing data\\npoint/data set is added, the side of the hyperplane it lands on\\nwill decide the class that we assign to it. The distance between\\nhyperplane and data points is known as margin.\\n7.5.4.2 Identifying the correct hyperplane in SVM\\nAs we have already discussed, there may be multiple options\\nfor hyperplanes dividing the data instances belonging to the\\ndifferent classes. We need to identify which one will result in\\nthe best classification. Let us examine a few scenarios before\\n0 1 1 2 2\\n0 1 1 2 2 N N'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 359, 'page_label': '360'}, page_content='arriving to that conclusion. For the sake of simplicity of\\nvisualization, the hyperplanes have been shown as straight\\nlines in most of the diagrams.\\nScenario 1\\nAs depicted in Figure 7.16, in this scenario, we have three\\nhyperplanes: A, B, and C. Now, we need to identify the correct\\nhyperplane which better segregates the two classes represented\\nby the triangles and circles. As we can see, hyperplane ‘A’ has\\nperformed this task quite well.\\nFIG. 7.16 Support vector machine: Scenario 1\\nScenario 2\\nAs depicted in Figure 7.17, we have three hyperplanes: A, B,\\nand C. We have to identify the correct hyperplane which\\nclassifies the triangles and circles in the best possible way.\\nHere, maximizing the distances between the nearest data\\npoints of both the classes and hyperplane will help us decide\\nthe correct hyperplane. This distance is called as margin.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 360, 'page_label': '361'}, page_content='In Figure 7.17b, you can see that the margin for hyperplane\\nA is high as compared to those for both B and C. Hence,\\nhyperplane A is the correct hyperplane. Another quick reason\\nfor selecting the hyperplane with higher margin (distance) is\\nrobustness. If we select a hyperplane having a lower margin\\n(distance), then there is a high probability of misclassification.\\nFIG. 7.17 Support vector machine: Scenario 2\\nScenario 3\\nUse the rules as discussed in the previous section to identify\\nthe correct hyperplane in the scenario shown in Figure 7.18.\\nSome of you might have selected hyperplane B as it has a\\nhigher margin (distance from the class) than A. But, here is the\\ncatch; SVM selects the hyperplane which classifies the classes\\naccurately before maximizing the margin. Here, hyperplane B\\nhas a classification error, and A has classified all data\\ninstances correctly. Therefore, A is the correct hyperplane.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 361, 'page_label': '362'}, page_content='FIG. 7.18 Support vector machine: Scenario 3\\nScenario 4\\nIn this scenario, as shown in Figure 7.19a, it is not possible to\\ndistinctly segregate the two classes by using a straight line, as\\none data instance belonging to one of the classes (triangle) lies\\nin the territory of the other class (circle) as an outlier.\\nOne triangle at the other end is like an outlier for the\\ntriangle class. SVM has a feature to ignore outliers and find\\nthe hyperplane that has the maximum margin (hyperplane A,\\nas shown in Fig. 7.19b). Hence, we can say that SVM is robust\\nto outliers.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 362, 'page_label': '363'}, page_content='FIG. 7.19 Support vector machine: Scenario 4\\nSo, by summarizing the observations from the different\\nscenarios, we can say that\\n1. The hyperplane should segregate the data instances belonging to the two\\nclasses in the best possible way.\\n2. It should maximize the distances between the nearest data points of both\\nthe classes, i.e. maximize the margin.\\n3. If there is a need to prioritize between higher margin and lesser\\nmisclassification, the hyperplane should try to reduce misclassifications.\\nOur next focus is to find out a way to identify a hyperplane\\nwhich maximizes the margin.\\n7.5.4.3 Maximum margin hyperplane\\nFinding the Maximum Margin Hyperplane (MMH) is nothing\\nbut identifying the hyperplane which has the largest separation\\nwith the data instances of the two classes. Though any set of\\nthree hyperplanes can do the correct classification, why do we\\nneed to search for the set of hyperplanes causing the largest\\nseparation? The answer is that doing so helps us in achieving\\nmore generalization and hence less number of issues in the\\nclassification of unknown data.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 363, 'page_label': '364'}, page_content='FIG. 7.20 Support vectors\\nSupport vectors, as can be observed in Figure 7.20, are\\ndata instances from the two classes which are closest to the\\nMMH. Quite understandably, there should be at least one\\nsupport vector from each class. The identification of support\\nvectors requires intense mathematical formulation, which is\\nout of scope of this book. However, it is fairly intuitive to\\nunderstand that modelling a problem using SVM is nothing\\nbut identifying the support vectors and MMH corresponding to\\nthe problem space.\\nIdentifying the MMH for linearly separable data\\nFinding out the MMH is relatively straightforward for the data\\nthat is linearly separable. In this case, an outer boundary needs\\nto be drawn for the data instances belonging to the different\\nclasses. These outer boundaries are known as convex hull, as\\ndepicted in Figure 7.21. The MMH can be drawn as the\\nperpendicular bisector of the shortest line (i.e. the connecting\\nline having the shortest length) between the convex hulls.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 364, 'page_label': '365'}, page_content='FIG. 7.21 Drawing the MMH for linearly separable data\\nWe have already seen earlier that a hyperplane in the N-\\ndimensional feature space can be represented by the equation: \\nUsing this equation, the objective is to find a set of values\\nfor the vector \\n  such that two hyperplanes, represented by the\\nequations below, can be specified.\\nThis is to ensure that all the data instances that belong to\\none class falls above one hyperplane and all the data instances\\nbelonging to the other class falls below another hyperplane.\\nAccording to vector geometry, the distance of these planes'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 365, 'page_label': '366'}, page_content='should be \\n . It is quite obvious that in order to maximize the\\ndistance between hyperplanes, the value of \\n  should be\\nminimized. So, in summary, the task of SVM is to solve the\\noptimization problem:\\nIdentifying the MMH for non-linearly separable data\\nNow that we have a clear understanding of how to identify the\\nMMH for a linearly separable data set, let us do some study\\nabout how non-linearly separable data needs to be handled by\\nSVM. For this, we have to use a slack variable ξ, which\\nprovides some soft margin for data instances in one class that\\nfall on the wrong side of the hyperplane. As depicted in Figure\\n7.22, a data instance belonging to the circle class falls on the\\nside of the hyperplane designated for the data instances\\nbelonging to the triangle class. The same issue also happens\\nfor a data instance belonging to the triangle class.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 366, 'page_label': '367'}, page_content='FIG. 7.22 Drawing the MMH for non-linearly separable data\\nA cost value ‘C’ is imposed on all such data instances that\\nfall on the wrong side of the hyperplane. The task of SVM is\\nnow to minimize the total cost due to such data instances in\\norder to solve the revised optimization problem:\\n7.5.4.4 Kernel trick\\nAs we have seen in the last section, one way to deal with non-\\nlinearly separable data is by using a slack variable and an\\noptimization function to minimize the cost value. However,\\nthis is not the only way to use SVM to solve machine learning\\nproblems involving non-linearly separable data sets. SVM has\\na technique called the kernel trick to deal with non-linearly\\nseparable data. As shown in Figure 7.23, these are functions\\nwhich can transform lower dimensional input space to a higher\\ndimensional space. In the process, it converts linearly non-'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 367, 'page_label': '368'}, page_content='separable data to a linearly separable data. These functions are\\ncalled kernels.\\nFIG. 7.23 Kernel trick in SVM\\nSome of the common kernel functions for transforming\\nfrom a lower dimension ‘i’ to a higher dimension ‘j’ used by\\ndifferent SVM implementations are as follows:\\nLinear kernel: It is in the form \\nPolynomial kernel: It is in the form \\nSigmoid kernel: It is in the form \\nGaussian RBF kernel: It is in the form \\nWhen data instances of the classes are closer to each other,\\nthis method can be used. The effectiveness of SVM depends\\nboth on the'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 368, 'page_label': '369'}, page_content='selection of the kernel function\\nadoption of values for the kernel parameters\\n7.5.4.5 Strengths of SVM\\nSVM can be used for both classification and regression.\\nIt is robust, i.e. not much impacted by data with noise or outliers.\\nThe prediction results using this model are very promising.\\n7.5.4.6 Weaknesses of SVM\\nSVM is applicable only for binary classification, i.e. when there are only two\\nclasses in the problem domain.\\nThe SVM model is very complex – almost like a black box when it deals\\nwith a high-dimensional data set. Hence, it is very difficult and close to\\nimpossible to understand the model in such cases.\\nIt is slow for a large dataset, i.e. a data set with either a large number of\\nfeatures or a large number of instances.\\nIt is quite memory-intensive.\\n7.5.4.7 Application of SVM\\nSVM is most effective when it is used for binary classification,\\ni.e. for solving a machine learning problem with two classes.\\nOne common problem on which SVM can be applied is in the\\nfield of bioinformatics – more specifically, in detecting cancer\\nand other genetic disorders. It can also be used in detecting the\\nimage of a face by binary classification of images into face\\nand non-face components. More such applications can be\\ndescribed.\\n7.6 SUMMARY\\n1. The objective of classification is to predict the class of unknown objects\\non the basis of prior class-related information of similar objects. This\\nlearning could be used when how to classify a given data is known or,\\nin other words, class values of data instances are available.\\n2. A critical classification problem in the context of the banking domain is\\nidentifying potentially fraudulent transactions. Because there are\\nmillions of transactions which have to be scrutinized to identify whether'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 369, 'page_label': '370'}, page_content='a particular transaction might be a fraud transaction, it is not possible\\nfor any human being to carry out this task.\\n3. Common classification algorithms are kNN, decision tree, random\\nforest, SVM, and Naïve Bayes.\\n4. The kNN algorithm is among the best and the simplest of machine\\nlearning algorithms. A new instance is classified by a majority vote of\\nits neighbours, with the instance being allocated the class that is\\npredominant among its kNN.\\n5. Decision tree learning is the most broadly utilized classifier. It is\\ncharacterized by fast execution time and ease in the interpretation of the\\nrules.\\n6. The decision tree algorithm needs to find out the attribute splitting by\\nwhich results in highest information gain. For a sample of training\\nexamples S, entropy measures the impurity of S. \\nwhere c is the number of different class labels and p refers to the\\nproportion of values falling into the i-th class label.\\n7. Information gain is created on the basis of the decrease in entropy (S)\\nafter a dataset is split based on a particular attribute (A). Information\\ngain for a particular feature A is calculated by the difference in entropy\\nbefore a split (S ) with the entropy after the split (S ).\\nInformation Gain (S, A) = Entropy (S ) - Entropy (S )\\n8. Random forest is an ensemble classifier (combining classifier) which\\nuses and combines many decision tree models. The result from an\\nensemble model is usually better than that from one of the individual\\nmodels.\\n9. An SVM is a binary linear classifier. The output prediction of an SVM\\nis one of the two conceivable classes which are already defined in the\\ntraining data. SVM is also an example of a linear classifier and a\\nmaximum margin classifier.\\n10. SVM has a new technique called the kernel trick. These are functions,\\nwhich take a lower dimensional input space and transform it to a higher\\ndimensional space and in the process converts a non-linearly separable\\nproblem to a linearly separable problem. These functions are called\\nkernels. When all the classes are closer to each other, this method can\\nbe used.\\nMULTIPLE CHOICE QUESTIONS\\n1. Predicting whether a tumour is malignant or benign is an example of?\\n1. Unsupervised Learning\\nbs as\\nbs as'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 370, 'page_label': '371'}, page_content='2. Supervised Regression Problem\\n3. Supervised Classification Problem\\n4. Categorical Attribute\\n2. Price prediction in the domain of real estate is an example of?\\n1. Unsupervised Learning\\n2. Supervised Regression Problem\\n3. Supervised Classification Problem\\n4. Categorical Attribute\\n3. Let us consider two examples, say ‘predicting whether a tumour is\\nmalignant or benign’ and ‘price prediction in the domain of real estate’.\\nThese two problems are same in nature.\\n1. TRUE\\n2. FALSE\\n4. Supervised machine learning is as good as the data used to train it.\\n1. TRUE\\n2. FALSE\\n5. Which is a type of machine learning where a target feature, which is of\\ncategorical type, is predicted for the test data on the basis of the\\ninformation imparted by the training data?\\n1. Unsupervised Learning\\n2. Supervised Regression\\n3. Supervised Classification\\n4. Categorical Attribute\\n6. Classification is a type of supervised learning where a target feature,\\nwhich is of categorical type, is predicted for the test data on the basis of\\nthe information imparted by the training data. The target categorical\\nfeature is known as?\\n1. Object\\n2. Variable\\n3. Method\\n4. Class\\n7. This is the first step in the supervised learning model.\\n1. Problem Identification\\n2. Identification of Required Data\\n3. Data Pre-processing\\n4. Definition of Training Data Set\\n8. This is the cleaning/transforming the data set in the supervised learning\\nmodel.\\n1. Problem Identification\\n2. Identification of Required Data\\n3. Data Pre-processing\\n4. Definition of Training Data Set\\n9. This refers to the transformations applied to the identified data before\\nfeeding the same into the algorithm.\\n1. Problem Identification'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 371, 'page_label': '372'}, page_content='2. Identification of Required Data\\n3. Data Pre-processing\\n4. Definition of Training Data Set\\n10. This step of supervised learning determines ‘the type of training data\\nset’.\\n1. Problem Identification\\n2. Identification of Required Data\\n3. Data Pre-processing\\n4. Definition of Training Data Set\\n11. Entire design of the programme is done over here in supervised\\nlearning.\\n1. Problem Identification\\n2. Training\\n3. Data Pre-processing\\n4. Definition of Training Data Set\\n12. Training data run on the algorithm is called as?\\n1. Program\\n2. Training\\n3. Training Information\\n4. Learned Function\\n13. SVM is an example of?\\n1. Linear Classifier and Maximum Margin Classifier\\n2. Non-linear Classifier and Maximum Margin Classifier\\n3. Linear Classifier and Minimum Margin Classifier\\n4. Non-linear Classifier and Minimum Margin Classifier\\n14. ---------- in terms of SVM means that an SVM is inflexible in\\nclassification\\n1. Hard Margin\\n2. Soft Margin\\n3. Linear Margin\\n4. Non-linear Classifier\\n15. ---------- are the data points (representing classes), the important\\ncomponent in a data set, which are near the identified set of lines\\n(hyperplane).\\n1. Hard Margin\\n2. Soft Margin\\n3. Linear Margin\\n4. Support Vectors\\n16. ---------- is a line that linearly separates and classifies a set of data.\\n1. Hyperplane\\n2. Soft Margin\\n3. Linear Margin\\n4. Support Vectors\\n17. The distance between hyperplane and data points is called as:\\n1. Hyper Plan'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 372, 'page_label': '373'}, page_content='2. Margins\\n3. Error\\n4. Support Vectors\\n18. Which of the following is true about SVM?\\n1. It is useful only in high-dimensional spaces\\n2. It always gives an approximate value\\n3. It is accurate\\n4. Understanding SVM is difficult\\n19. Which of the following is true about SVM?\\n1. It is useful only in high-dimensional spaces\\n2. It requires less memory\\n3. SVM does not perform well when we have a large data set\\n4. SVM performs well when we have a large data set\\n20. What is the meaning of hard margin in SVM?\\n1. SVM allows very low error in classification\\n2. SVM allows high amount of error in classification\\n3. Underfitting\\n4. SVM is highly flexible\\n21. What sizes of training data sets are not best suited for SVM?\\n1. Large data sets\\n2. Very small training data sets\\n3. Medium size training data sets\\n4. Training data set size does not matter\\n22. Support Vectors are near the hyperplane.\\n1. True\\n2. False\\n23. In SVM, these functions take a lower dimensional input space and\\ntransform it to a higher dimensional space.\\n1. Kernels\\n2. Vector\\n3. Support Vector\\n4. Hyperplane\\n24. Which of the following options is true about the kNN algorithm?\\n1. It can be used only for classification\\n2. It can be used only for regression\\n3. It can be used for both classification and regression\\n4. It is not possible to use for both classification and regression\\n25. Which of the following will be Euclidean distance between the two data\\npoints A(4,3) and B(2,3)?\\n1. 1\\n2. 2\\n3. 4\\n4. 8\\n26. Which of the following will be Manhattan distance between the two\\ndata points A(8,3) and B(4,3)?'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 373, 'page_label': '374'}, page_content='1. 1\\n2. 2\\n3. 4\\n4. 8\\n27. When you find many noises in data, which of the following options\\nwould you consider in kNN?\\n1. Increase the value of k\\n2. Decrease the value of k\\n3. Noise does not depend on k\\n4. K = 0\\n28. What would be the relationship between the training time taken by 1-\\nNN, 2-NN, and 3-NN?\\n1. 1-NN > 2-NN > 3-NN\\n2. 1-NN < 2-NN < 3-NN\\n3. 1-NN ~ 2-NN ~ 3-NN\\n4. None of these\\n29. Which of the following algorithms is an example of the ensemble\\nlearning algorithm?\\n1. Random Forest\\n2. Decision Tree\\n3. kNN\\n4. SVM\\n30. Which of the following is not an inductive bias in a decision tree?\\n1. It prefers longer tree over shorter tree\\n2. Trees that place nodes near the root with high information gain\\nare preferred\\n3. Overfitting is a natural phenomenon in a decision tree\\n4. Prefer the shortest hypothesis that fits the data\\nSHORT ANSWER-TYPE QUESTIONS (5 MARKS EACH)\\n1. What is supervised learning? Why it is called so?\\n2. Give an example of supervised learning in a hospital industry.\\n3. Give any three examples of supervised learning.\\n4. What is classification and regression in a supervised learning?\\n5. Give some examples of common classification algorithms.\\n6. Explain, in brief, the SVM model.\\n7. What is cost of misclassification in SVM?\\n8. Define Support Vectors in the SVM model.\\n9. Define kernel in the SVM model.\\n10. What are the factors determining the effectiveness of SVM?\\n11. What are the advantages of the SVM model?\\n12. What are the disadvantages of the SVM model?\\n13. Write notes on'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 374, 'page_label': '375'}, page_content='1. validation error in the kNN algorithm\\n2. choosing k value in the kNN algorithm\\n3. inductive bias in a decision tree\\n14. What are the advantages of the kNN algorithm?\\n15. What are the disadvantages of the kNN algorithm?\\n16. Explain, in brief, the decision tree algorithm.\\n17. What is node and leaf in decision tree?\\n18. What is entropy of a decision tree?\\n19. Define information gain in a decision tree.\\n20. Write any three strengths of the decision tree method.\\n21. Write any three weaknesses of the decision tree method.\\n22. Explain, in brief, the random forest model?\\nLONG ANSWER-TYPE QUESTIONS (10 MARKS EACH)\\n1. Distinguish between supervised learning, semi-supervised learning, and\\nunsupervised learning.\\n2. Explain any five examples of classification problems in detail.\\n3. Explain classification steps in detail.\\n4. Discuss the SVM model in detail with different scenarios.\\n5. What are the advantages and disadvantages associated with SVM?\\n6. Discuss the kNN model in detail.\\n7. Discuss the error rate and validation error in the kNN algorithm.\\n8. Discuss how to calculate the distance between the test data and the\\ntraining data for kNN.\\n9. Write the algorithm for kNN.\\n10. What is decision tree? What are the different types of nodes? Explain in\\ndetail\\n11. Explain various options of searching a decision tree.\\n12. Discuss the decision tree algorithm in detail.\\n13. What is inductive bias in a decision tree? How to avoid overfitting?\\n14. What are the strengths and weaknesses of the decision tree method?\\n15. Discuss appropriate problems for decision tree learning in detail.\\n16. Discuss the random forest model in detail. What are the features of\\nrandom forest?\\n17. Discuss OOB error and variable importance in random forest.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 375, 'page_label': '376'}, page_content='Chapter 8\\nSupervised Learning: Regression\\n8.1 INTRODUCTION\\nOBJECTIVE OF THE CHAPTER :\\nIn the last two chapters, you have got quite a good\\nconceptual overview of supervised learning algorithm for\\ncategorical data prediction. You got a detailed\\nunderstanding of all the popular models of classification\\nthat are used by machine learning practitioners to solve a\\nwide array of prediction problems where the target variable\\nis a categorical variable.\\nIn this chapter, we will build concepts on prediction of\\nnumerical variables – which is another key area of\\nsupervised learning. This area, known as regression,\\nfocuses on solving problems such as predicting value of\\nreal estate, demand forecast in retail, weather forecast, etc.\\nFirst, you will be introduced to the most popular and\\nsimplest algorithm, namely simple linear regression. This\\nmodel roots from the statistical concept of fitting a straight\\nline and the least squares method. We will explore this'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 376, 'page_label': '377'}, page_content='algorithm in detail. In this same context, we will also\\nexplore the concept of multiple linear regression.\\nWe will then briefly touch upon the other important\\nalgorithms in regression, namely multivariate adaptive\\nregression splines, logistic regression, and maximum\\nlikelihood estimation.\\nBy the end of this chapter, you will gain sufficient\\nknowledge in all the aspects of supervised learning and\\nbecome ready to start solving problems on your own.\\n8.2 EXAMPLE OF REGRESSION\\nWe have mentioned many times that real estate price\\nprediction is a problem that can be solved by supervised\\nlearning or, more specifically, by regression. So, what this\\nproblem really is? Let us delve a little deeper into the problem.\\nNew City is the primary hub of the commercial activities in\\nthe country. In the last couple of decades, with increasing\\nglobalization, commercial activities have intensified in New\\nCity. Together with that, a large number of people have come\\nand settled in the city with a dream to achieve professional\\ngrowth in their lives. As an obvious fall-out, a large number of\\nhousing projects have started in every nook and corner of the\\ncity. But the demand for apartments has still outgrown the\\nsupply. To get benefit from this boom in real estate business,\\nKaren has started a digital market agency for buying and\\nselling real estates (including apartments, independent houses,\\ntown houses, etc.). Initially, when the business was small, she\\nused to interact with buyers and sellers personally and help\\nthem arrive at a price quote – either for selling a property (for'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 377, 'page_label': '378'}, page_content='a seller) or for buying a property (for a buyer). Her long\\nexperience in real estate business helped her develop an\\nintuition on what the correct price quote of a property could be\\n– given the value of certain standard parameters such as area\\n(sq. m.) of the property, location, floor, number of years since\\npurchase, amenities available, etc. However, with the huge\\nsurge in the business, she is facing a big challenge. She is not\\nable to manage personal interactions as well as setting the\\ncorrect price quote for the properties all alone. She hired an\\nassistant for managing customer interactions. But the assistant,\\nbeing new in the real estate business, is struggling with price\\nquotations. How can Karen solve this problem?\\nFortunately, Karen has a friend, Frank, who is a data\\nscientist with in-depth knowledge in machine learning models.\\nFrank comes up with a solution to Karen’s problem. He builds\\na model which can predict the correct value of a real estate if it\\nhas certain standard inputs such as area (sq. m.) of the\\nproperty, location, floor, number of years since purchase,\\namenities available, etc. Wow, that sounds to be like Karen\\nherself doing the job! Curious to know what model Frank has\\nused? Yes, you guessed it right. He used a regression model to\\nsolve Karen’s real estate price prediction problem.\\nSo, we just discussed about one problem which can be\\nsolved using regression. In the same way, a bunch of other\\nproblems related to prediction of numerical value can be\\nsolved using the regression model. In the context of\\nregression, dependent variable (Y) is the one whose value is to\\nbe predicted, e.g. the price quote of the real estate in the\\ncontext of Karen’s problem. This variable is presumed to be\\nfunctionally related to one (say, X) or more independent\\nvariables called predictors. In the context of Karen’s problem,\\nFrank used area of the property, location, floor, etc. as'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 378, 'page_label': '379'}, page_content='predictors of the model that he built. In other words, the\\ndependent variable depends on independent variable(s) or\\npredictor(s). Regression is essentially finding a relationship\\n(or) association between the dependent variable (Y) and the\\nindependent variable(s) (X), i.e. to find the function ‘f ’ for the\\nassociation Y = f (X).\\n8.3 COMMON REGRESSION ALGORITHMS\\nThe most common regression algorithms are\\nSimple linear regression\\nMultiple linear regression\\nPolynomial regression\\nMultivariate adaptive regression splines\\nLogistic regression\\nMaximum likelihood estimation (least squares)\\n8.3.1 Simple Linear Regression\\nAs the name indicates, simple linear regression is the simplest\\nregression model which involves only one predictor. This\\nmodel assumes a linear relationship between the dependent\\nvariable and the predictor variable as shown in Figure 8.1.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 379, 'page_label': '380'}, page_content='FIG. 8.1 Simple linear regression\\nIn the context of Karen’s problem, if we take Price of a\\nProperty as the dependent variable and the Area of the\\nProperty (in sq. m.) as the predictor variable, we can build a\\nmodel using simple linear regression.\\n \\nPrice  = f(Area )\\n \\nAssuming a linear association, we can reformulate the\\nmodel as\\n \\nPrice  = a + b. Area\\nProperty Property\\nProperty Property'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 380, 'page_label': '381'}, page_content='where ‘a’ and ‘b’ are intercept and slope of the straight line,\\nrespectively.\\nJust to recall, straight lines can be defined in a slope–\\nintercept form Y = (a + bX), where a = intercept and b = slope\\nof the straight line. The value of intercept indicates the value\\nof Y when X = 0. It is known as ‘the intercept or Y intercept’\\nbecause it specifies where the straight line crosses the vertical\\nor Y-axis (refer to Fig. 8.1).\\n8.3.1.1 Slope of the simple linear regression model\\nSlope of a straight line represents how much the line in a graph\\nchanges in the vertical direction (Y-axis) over a change in the\\nhorizontal direction (X-axis) as shown in Figure 8.2.\\n \\nSlope = Change in Y/Change in X\\n \\nRise is the change in Y-axis (Y − Y ) and Run is the change\\nin X-axis (X  − X ). So, slope is represented as given below:\\n2 1\\n2 1'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 381, 'page_label': '382'}, page_content='FIG. 8.2 Rise and run representation\\nExample of slope\\nLet us find the slope of the graph where the lower point on the\\nline is represented as (−3, −2) and the higher point on the line\\nis represented as (2, 2).\\n(X , Y ) = (−3, −2) and (X , Y ) = (2, 2)\\nRise = (Y  − Y ) = (2 − (−2)) = 2 + 2 = 4\\nRun = (X  − X ) = (2 − (−3)) = 2 + 3 = 5\\nSlope = Rise/Run = 4/5 = 0.8\\n \\nThere can be two types of slopes in a linear regression\\nmodel: positive slope and negative slope. Different types of\\nregression lines based on the type of slope include\\n1 1 2 2\\n2 1\\n2 1'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 382, 'page_label': '383'}, page_content='Linear positive slope\\nCurve linear positive slope\\nLinear negative slope\\nCurve linear negative slope\\nLinear positive slope\\nA positive slope always moves upward on a graph from left to\\nright (refer to Fig. 8.3).\\nFIG. 8.3 Linear positive slope\\nSlope = Rise/Run = (Y  − Y ) / (X  − X ) = Delta (Y) / Delta(X)\\nScenario 1 for positive slope: Delta (Y) is positive and Delta (X) is positive\\nScenario 2 for positive slope: Delta (Y) is negative and Delta (X) is negative\\nCurve linear positive slope\\nFIG. 8.4 Curve linear positive slope\\n2 1 2 1'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 383, 'page_label': '384'}, page_content='Curves in these graphs (refer to Fig. 8.4) slope upward from\\nleft to right.\\n \\nSlope = (Y  − Y ) / (X  − X ) = Delta (Y) / Delta(X)\\n \\nSlope for a variable (X) may vary between two graphs, but it\\nwill always be positive; hence, the above graphs are called as\\ngraphs with curve linear positive slope.\\nLinear negative slope\\nA negative slope always moves downward on a graph from\\nleft to right. As X value (on X-axis) increases, Y value\\ndecreases (refer to Fig. 8.5).\\nFIG. 8.5 Linear negative slope\\nSlope = Rise/Run = (Y  − Y ) / (X  − X ) = Delta (Y) / Delta(X)\\nScenario 1 for negative slope: Delta (Y) is positive and Delta (X) is negative\\nScenario 2 for negative slope: Delta (Y) is negative and Delta (X) is positive\\n2 1 2 1\\n2 1 2 1'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 384, 'page_label': '385'}, page_content='Curve linear negative slope\\nFIG. 8.6 Curve linear negative slope\\nCurves in these graphs (refer to Fig. 8.6) slope downward\\nfrom left to right.\\n \\nSlope = (Y  − Y ) / (X  − X ) = Delta (Y) / Delta(X)\\n \\nSlope for a variable (X) may vary between two graphs, but it\\nwill always be negative; hence, the above graphs are called as\\ngraphs with curve linear negative slope.\\n8.3.1.2 No relationship graph\\nScatter graph shown in Figure 8.7 indicates ‘no relationship’\\ncurve as it is very difficult to conclude whether the\\nrelationship between X and Y is positive or negative.\\n2 1 2 1'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 385, 'page_label': '386'}, page_content='FIG. 8.7 No relationship graph\\n8.3.1.3 Error in simple regression\\nThe regression equation model in machine learning uses the\\nabove slope–intercept format in algorithms. X and Y values\\nare provided to the machine, and it identifies the values of a\\n(intercept) and b (slope) by relating the values of X and Y.\\nHowever, identifying the exact match of values for a and b is\\nnot always possible. There will be some error value ( ɛ )\\nassociated with it. This error is called marginal or residual\\nerror.\\n \\nY = (a + bX) + ε\\n \\nNow that we have some context of the simple regression\\nmodel, let us try to explore an example to understand clearly\\nhow to decide the parameters of the model (i.e. values of a and\\nb) for a given problem.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 386, 'page_label': '387'}, page_content='8.3.1.4 Example of simple regression\\nA college professor believes that if the grade for internal\\nexamination is high in a class, the grade for external\\nexamination will also be high. A random sample of 15\\nstudents in that class was selected, and the data is given below:\\nA scatter plot was drawn to explore the relationship between\\nthe independent variable (internal marks) mapped to X-axis\\nand dependent variable (external marks) mapped to Y-axis as\\ndepicted in Figure 8.8.\\nFIG. 8.8 Scatter plot and regression line\\nAs you can observe from the above graph, the line (i.e. the\\nregression line) does not predict the data exactly (refer to Fig.\\n8.8). Instead, it just cuts through the data. Some predictions'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 387, 'page_label': '388'}, page_content='are lower than expected, while some others are higher than\\nexpected.\\nResidual is the distance between the predicted point (on the\\nregression line) and the actual point as depicted in Figure 8.9.\\nFIG. 8.9 Residual error\\nAs we know, in simple linear regression, the line is drawn\\nusing the regression formula.\\n \\nY = (a + bX) + ε\\n \\nIf we know the values of ‘a’ and ‘b’, then it is easy to\\npredict the value of Y for any given X by using the above\\nformula. But the question is how to calculate the values of ‘a’\\nand ‘b’ for a given set of X and Y values?'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 388, 'page_label': '389'}, page_content='A straight line is drawn as close as possible over the points\\non the scatter plot. Ordinary Least Squares (OLS) is the\\ntechnique used to estimate a line that will minimize the error\\n(ε), which is the difference between the predicted and the\\nactual values of Y. This means summing the errors of each\\nprediction or, more appropriately, the Sum of the Squares of\\nthe Errors (SSE) \\nIt is observed that the SSE is least when b takes the value\\nThe corresponding value of ‘a’ calculated using the above\\nvalue of ‘b’ is\\nSo, let us calculate the value of a and b for the given\\nexample. For detailed calculation, refer to Figure 8.10.\\nCalculation summary\\nSum of X = 299\\nSum of Y = 852\\nMean X, M  = 19.93\\nMean Y, M = 56.8\\nX\\nY'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 389, 'page_label': '390'}, page_content='Sum of squares (SS ) = 226.9333\\nSum of products (SP) = 429.8\\nRegression equation = ŷ = bX + a\\na = M − bM  = 56.8 − (1.89 × 19.93) = 19.0473\\nŷ = 1.89395X + 19.0473\\nHence, for the above example, the estimated regression\\nequation is constructed on the basis of the estimated values of\\na and b:\\n \\nŷ = 1.89395X + 19.0473\\n \\nSo, in the context of the given problem, we can say\\n \\nMarks in external exam = 19.04 + 1.89 × (Marks in internal exam)\\n \\nor, M  = 19.04 + 1.89 × M\\nX\\nY X\\nExt Int'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 390, 'page_label': '391'}, page_content='FIG. 8.10 Detailed calculation of regression parameters\\nThe model built above can be represented graphically as\\nan extended version (refer to Fig. 8.11)\\na zoom-in version (refer to Fig. 8.12)\\nInterpretation of the intercept\\nAs we have already seen, the simple linear regression model\\nbuilt on the data in the example is'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 391, 'page_label': '392'}, page_content='M = 19.04 + 1.89 × M\\n \\nThe value of the intercept from the above equation is 19.05.\\nHowever, none of the internal mark is 0. So, intercept = 19.05\\nindicates that 19.05 is the portion of the external examination\\nmarks not explained by the internal examination marks.\\nExt Int'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 392, 'page_label': '393'}, page_content='FIG. 8.11 Extended version of the regression graph\\nFIG. 8.12 Zoom-in regression line\\nSlope measures the estimated change in the average value of\\nY as a result of a one-unit change in X. Here, slope = 1.89 tells\\nus that the average value of the external examination marks'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 393, 'page_label': '394'}, page_content='increases by 1.89 for each additional 1 mark in the internal\\nexamination.\\nNow that we have a complete understanding of how to build\\na simple linear regression model for a given problem, it is time\\nto summarize the algorithm.\\n8.3.1.5 OLS algorithm\\nStep 1: Calculate the mean of X and Y\\nStep 2: Calculate the errors of X and Y\\nStep 3: Get the product\\nStep 4: Get the summation of the products\\nStep 5: Square the difference of X\\nStep 6: Get the sum of the squared difference\\nStep 7: Divide output of step 4 by output of step 6 to calculate ‘b’\\nStep 8: Calculate ‘a’ using the value of ‘b’\\n8.3.1.6 Maximum and minimum point of curves\\nMaximum (shown in Fig. 8.13) and minimum points (shown\\nin Fig. 8.14) on a graph are found at points where the slope of\\nthe curve is zero. It becomes zero either from positive or\\nnegative value. The maximum point is the point on the curve\\nof the graph with the highest y-coordinate and a slope of zero.\\nThe minimum point is the point on the curve of the graph with\\nthe lowest y-coordinate and a slope of zero.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 394, 'page_label': '395'}, page_content='FIG. 8.13 Maximum point of curve\\nPoint 63 is at the maximum point for this curve (refer to Fig.\\n8.13). Point 63 is at the highest point on this curve. It has a\\ngreater y-coordinate value than any other point on the curve\\nand has a slope of zero.\\nPoint 40 (marked with an arrow in Fig. 8.14) is the\\nminimum point for this curve. Point 40 is at the lowest point\\non this curve. It has a lesser y-coordinate value than any other\\npoint on the curve and has a slope of zero.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 395, 'page_label': '396'}, page_content='FIG. 8.14 Minimum point of curve\\n8.3.2 Multiple Linear Regression\\nIn a multiple regression model, two or more independent\\nvariables, i.e. predictors are involved in the model. If we think\\nin the context of Karen’s problem, in the last section, we came\\nup with a simple linear regression by considering Price of a\\nProperty as the dependent variable and the Area of the\\nProperty (in sq. m.) as the predictor variable. However,\\nlocation, floor, number of years since purchase, amenities\\navailable, etc. are also important predictors which should not\\nbe ignored. Thus, if we consider Price of a Property (in $) as\\nthe dependent variable and Area of the Property (in sq. m.),\\nlocation, floor, number of years since purchase and amenities'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 396, 'page_label': '397'}, page_content='available as the independent variables, we can form a multiple\\nregression equation as shown below:\\n \\nPrice  = f (Area  location, floor, Ageing, Amenities)\\n \\nThe simple linear regression model and the multiple\\nregression model assume that the dependent variable is\\ncontinuous.\\nThe following expression describes the equation involving\\nthe relationship with two predictor variables, namely X  and\\nX .\\n \\nŶ = a + b X + b X\\n \\nThe model describes a plane in the three-dimensional space\\nof Ŷ, X , and X . Parameter ‘a’ is the intercept of this plane.\\nParameters ‘b ’ and ‘b ’ are referred to as partial regression\\ncoefficients. Parameter b  represents the change in the mean\\nresponse corresponding to a unit change in X  when X  is held\\nconstant. Parameter b  represents the change in the mean\\nresponse corresponding to a unit change in X  when X  is held\\nconstant.\\nConsider the following example of a multiple linear\\nregression model with two predictor variables, namely X  and\\nX  (refer to Fig. 8.15).\\nProperty Property,\\n1\\n2\\n1 1 2 2\\n1 2\\n1 2\\n1\\n1 2\\n2\\n2 1\\n1\\n2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 397, 'page_label': '398'}, page_content='Ŷ = 22 + 0.3X  + 1.2X\\n \\nFIG. 8.15 Multiple regression plane\\nMultiple regression for estimating equation when there are\\n‘n’ predictor variables is as follows:\\n \\nŶ = a + b X  + b X  + b X  + … + b X\\n \\nWhile finding the best fit line, we can fit either a polynomial\\nor curvilinear regression. These are known as polynomial or\\ncurvilinear regression, respectively.\\n1 2\\n1 1 2 2 3 3 n n'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 398, 'page_label': '399'}, page_content='8.3.3 Assumptions in Regression Analysis\\n1. The dependent variable (Y) can be calculated / predicated as a linear\\nfunction of a specific set of independent variables (X’s) plus an error\\nterm (ε).\\n2. The number of observations (n) is greater than the number of\\nparameters (k) to be estimated, i.e. n > k.\\n3. Relationships determined by regression are only relationships of\\nassociation based on the data set and not necessarily of cause and effect\\nof the defined class.\\n4. Regression line can be valid only over a limited range of data. If the line\\nis extended (outside the range of extrapolation), it may only lead to\\nwrong predictions.\\n5. If the business conditions change and the business assumptions\\nunderlying the regression model are no longer valid, then the past data\\nset will no longer be able to predict future trends.\\n6. Variance is the same for all values of X (homoskedasticity).\\n7. The error term (ε) is normally distributed. This also means that the\\nmean of the error (ε) has an expected value of 0.\\n8. The values of the error (ε) are independent and are not related to any\\nvalues of X. This means that there are no relationships between a\\nparticular X, Y that are related to another specific value of X, Y.\\nGiven the above assumptions, the OLS estimator is the Best\\nLinear Unbiased Estimator (BLUE), and this is called as\\nGauss-Markov Theorem.\\n8.3.4 Main Problems in Regression Analysis\\nIn multiple regressions, there are two primary problems:\\nmulticollinearity and heteroskedasticity.\\n8.3.4.1 Multicollinearity\\nTwo variables are perfectly collinear if there is an exact linear\\nrelationship between them. Multicollinearity is the situation in\\nwhich the degree of correlation is not only between the\\ndependent variable and the independent variable, but there is\\nalso a strong correlation within (among) the independent'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 399, 'page_label': '400'}, page_content='variables themselves. A multiple regression equation can make\\ngood predictions when there is multicollinearity, but it is\\ndifficult for us to determine how the dependent variable will\\nchange if each independent variable is changed one at a time.\\nWhen multicollinearity is present, it increases the standard\\nerrors of the coefficients. By overinflating the standard errors,\\nmulticollinearity tries to make some variables statistically\\ninsignificant when they actually should be significant (with\\nlower standard errors). One way to gauge multicollinearity is\\nto calculate the Variance Inflation Factor (VIF), which\\nassesses how much the variance of an estimated regression\\ncoefficient increases if the predictors are correlated. If no\\nfactors are correlated, the VIFs will be equal to 1.\\nThe assumption of no perfect collinearity states that there is\\nno exact linear relationship among the independent variables.\\nThis assumption implies two aspects of the data on the\\nindependent variables. First, none of the independent\\nvariables, other than the variable associated with the intercept\\nterm, can be a constant. Second, variation in the X’s is\\nnecessary. In general, the more variation in the independent\\nvariables, the better will be the OLS estimates in terms of\\nidentifying the impacts of the different independent variables\\non the dependent variable.\\n8.3.4.2 Heteroskedasticity\\nHeteroskedasticity refers to the changing variance of the error\\nterm. If the variance of the error term is not constant across\\ndata sets, there will be erroneous predictions. In general, for a\\nregression equation to make accurate predictions, the error\\nterm should be independent, identically (normally) distributed\\n(iid).'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 400, 'page_label': '401'}, page_content='Mathematically, this assumption is written as\\nwhere ‘var’ represents the variance, ‘cov’ represents the\\ncovariance, ‘u’ represents the error terms, and ‘X’ represents\\nthe independent variables.\\nThis assumption is more commonly written as\\n8.3.5 Improving Accuracy of the Linear Regression\\nModel\\nLet us understand bias and variance in the regression model\\nbefore exploring how to improve the same. The concept of\\nbias and variance is similar to accuracy and prediction.\\nAccuracy refers to how close the estimation is near the actual\\nvalue, whereas prediction refers to continuous estimation of\\nthe value.\\n \\nHigh bias = low accuracy (not close to real value)\\nHigh variance = low prediction (values are scattered)\\nLow bias = high accuracy (close to real value)\\nLow variance = high prediction (values are close to each other)'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 401, 'page_label': '402'}, page_content='Let us say we have a regression model which is highly\\naccurate and highly predictive; therefore, the overall error of\\nour model will be low, implying a low bias (high accuracy)\\nand low variance (high prediction). This is highly preferable.\\nSimilarly, we can say that if the variance increases (low\\nprediction), the spread of our data points increases, which\\nresults in less accurate prediction. As the bias increases (low\\naccuracy), the error between our predicted value and the\\nobserved values increases. Therefore, balancing out bias and\\naccuracy is essential in a regression model.\\nIn the linear regression model, it is assumed that the number\\nof observations (n) is greater than the number of parameters\\n(k) to be estimated, i.e. n > k, and in that case, the least squares\\nestimates tend to have low variance and hence will perform\\nwell on test observations.\\nHowever, if observations (n) is not much larger than\\nparameters (k), then there can be high variability in the least\\nsquares fit, resulting in overfitting and leading to poor\\npredictions.\\nIf k > n, then linear regression is not usable. This also\\nindicates infinite variance, and so, the method cannot be used\\nat all.\\nAccuracy of linear regression can be improved using the\\nfollowing three methods:\\n1. Shrinkage Approach\\n2. Subset Selection\\n3. Dimensionality (Variable) Reduction\\n8.3.5.1 Shrinkage (Regularization) approach'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 402, 'page_label': '403'}, page_content='By limiting (shrinking) the estimated coefficients, we can try\\nto reduce the variance at the cost of a negligible increase in\\nbias. This can in turn lead to substantial improvements in the\\naccuracy of the model.\\n \\nFew variables used in the multiple regression model are in\\nfact not associated with the overall response and are called as\\nirrelevant variables; this may lead to unnecessary complexity\\nin the regression model.\\nThis approach involves fitting a model involving all\\npredictors. However, the estimated coefficients are shrunken\\ntowards zero relative to the least squares estimates. This\\nshrinkage (also known as regularization) has the effect of\\nreducing the overall variance. Some of the coefficients may\\nalso be estimated to be exactly zero, thereby indirectly\\nperforming variable selection. The two best-known techniques\\nfor shrinking the regression coefficients towards zero are\\n1. ridge regression\\n2. lasso (Least Absolute Shrinkage Selector Operator)\\nRidge regression performs L2 regularization, i.e. it adds\\npenalty equivalent to square of the magnitude of coefficients\\nMinimization objective of ridge = LS Obj + α × (sum of\\nsquare of coefficients)\\nRidge regression (include all k predictors in the final model)\\nis very similar to least squares, except that the coefficients are\\nestimated by minimizing a slightly different quantity. If k > n,\\nthen the least squares estimates do not even have a unique'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 403, 'page_label': '404'}, page_content='solution, whereas ridge regression can still perform well by\\ntrading off a small increase in bias for a large decrease in\\nvariance. Thus, ridge regression works best in situations where\\nthe least squares estimates have high variance. One\\ndisadvantage with ridge regression is that it will include all k\\npredictors in the final model. This may not be a problem for\\nprediction accuracy, but it can create a challenge in model\\ninterpretation in settings in which the number of variables k is\\nquite large. Ridge regression will perform better when the\\nresponse is a function of many predictors, all with coefficients\\nof roughly equal size.\\nLasso regression performs L1 regularization, i.e. it adds\\npenalty equivalent to the absolute value of the magnitude of\\ncoefficients.\\nMinimization objective of ridge = LS Obj + α × (absolute\\nvalue of the magnitude of coefficients)\\nThe lasso overcomes this disadvantage by forcing some of\\nthe coefficients to zero value. We can say that the lasso yields\\nsparse models (involving only subset) that are simpler as well\\nas more interpretable. The lasso can be expected to perform\\nbetter in a setting where a relatively small number of\\npredictors have substantial coefficients, and the remaining\\npredictors have coefficients that are very small or equal to\\nzero.\\n8.3.5.2 Subset selection\\nIdentify a subset of the predictors that is assumed to be related\\nto the response and then fit a model using OLS on the selected\\nreduced subset of variables. There are two methods in which\\nsubset of the regression can be selected:\\nk'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 404, 'page_label': '405'}, page_content='1. Best subset selection (considers all the possible (2))\\n2. Stepwise subset selection\\n1. Forward stepwise selection (0 to k)\\n2. Backward stepwise selection (k to 0)\\nIn best subset selection, we fit a separate least squares\\nregression for each possible subset of the k predictors. For\\ncomputational reasons, best subset selection cannot be applied\\nwith very large value of predictors (k). The best subset\\nselection procedure considers all the possible (2) models\\ncontaining subsets of the p predictors.\\nThe stepwise subset selection method can be applied to\\nchoose the best subset. There are two stepwise subset\\nselection:\\n1. Forward stepwise selection (0 to k)\\n2. Backward stepwise selection (k to 0)\\nForward stepwise selection is a computationally efficient\\nalternative to best subset selection. Forward stepwise\\nconsiders a much smaller set of models, that too step by step,\\ncompared to best set selection. Forward stepwise selection\\nbegins with a model containing no predictors, and then,\\npredictors are added one by one to the model, until all the k\\npredictors are included in the model. In particular, at each step,\\nthe variable (X) that gives the highest additional improvement\\nto the fit is added.\\nBackward stepwise selection begins with the least squares\\nmodel which contains all k predictors and then iteratively\\nremoves the least useful predictor one by one.\\n8.3.5.3 Dimensionality reduction (Variable reduction)\\nk\\nk'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 405, 'page_label': '406'}, page_content='The earlier methods, namely subset selection and shrinkage,\\ncontrol variance either by using a subset of the original\\nvariables or by shrinking their coefficients towards zero. In\\ndimensionality reduction, predictors (X) are transformed, and\\nthe model is set up using the transformed variables after\\ndimensionality reduction. The number of variables is reduced\\nusing the dimensionality reduction method. Principal\\ncomponent analysis is one of the most important\\ndimensionality (variable) reduction techniques.\\n8.3.6 Polynomial Regression Model\\nPolynomial regression model is the extension of the simple\\nlinear model by adding extra predictors obtained by raising\\n(squaring) each of the original predictors to a power. For\\nexample, if there are three variables, X, X , and X  are used as\\npredictors. This approach provides a simple way to yield a\\nnon-linear fit to data.\\n \\nf(x) = c + c .X + c .X + c .X\\n \\nIn the above equation, c , c , c , and c  are the coefficients.\\nExample: Let us use the below data set of (X, Y) for degree\\n3 polynomial.\\n0 1 2 3\\n0 1 2 3\\n2 3\\n1 2 3'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 406, 'page_label': '407'}, page_content='As you can observe, the regression line (refer to Fig. 8.16)\\nis slightly curved for polynomial degree 3 with the above 15\\ndata points. The regression line will curve further if we\\nincrease the polynomial degree (refer to Fig. 8.17). At the\\nextreme value as shown below, the regression line will be\\noverfitting into all the original values of X.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 407, 'page_label': '408'}, page_content='FIG. 8.16 Polynomial regression degree 3\\nFIG. 8.17 Polynomial regression degree 14'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 408, 'page_label': '409'}, page_content='8.3.7 Logistic Regression\\nLogistic regression is both classification and regression\\ntechnique depending on the scenario used. Logistic regression\\n(logit regression) is a type of regression analysis used for\\npredicting the outcome of a categorical dependent variable\\nsimilar to OLS regression. In logistic regression, dependent\\nvariable (Y) is binary (0,1) and independent variables (X) are\\ncontinuous in nature. The probabilities describing the possible\\noutcomes (probability that Y = 1) of a single trial are modelled\\nas a logistic function of the predictor variables. In the logistic\\nregression model, there is no R  to gauge the fit of the overall\\nmodel; however, a chi-square test is used to gauge how well\\nthe logistic regression model fits the data. The goal of logistic\\nregression is to predict the likelihood that Y is equal to 1\\n(probability that Y = 1 rather than 0) given certain values of X.\\nThat is, if X and Y have a strong positive linear relationship,\\nthe probability that a person will have a score of Y = 1 will\\nincrease as values of X increase. So, we are predicting\\nprobabilities rather than the scores of the dependent variable.\\nFor example, we might try to predict whether or not a small\\nproject will succeed or fail on the basis of the number of years\\nof experience of the project manager handling the project. We\\npresume that those project managers who have been managing\\nprojects for many years will be more likely to succeed. This\\nmeans that as X (the number of years of experience of project\\nmanager) increases, the probability that Y will be equal to 1\\n(success of the new project) will tend to increase. If we take a\\nhypothetical example in which 60 already executed projects\\nwere studied and the years of experience of project managers\\nranges from 0 to 20 years, we could represent this tendency to\\nincrease the probability that Y = 1 with a graph.\\n2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 409, 'page_label': '410'}, page_content='To illustrate this, it is convenient to segregate years of\\nexperience into categories (i.e. 0–8, 9–16, 17–24, 25–32, 33–\\n40). If we compute the mean score on Y (averaging the 0s and\\n1s) for each category of years of experience, we will get\\nsomething like\\nWhen the graph is drawn for the above values of X and Y, it\\nappears like the graph in Figure 8.18. As X increases, the\\nprobability that Y = 1 increases. In other words, when the\\nproject manager has more years of experience, a larger\\npercentage of projects succeed. A perfect relationship\\nrepresents a perfectly curved S rather than a straight line, as\\nwas the case in OLS regression. So, to model this relationship,\\nwe need some fancy algebra / mathematics that accounts for\\nthe bends in the curve.\\nAn explanation of logistic regression begins with an\\nexplanation of the logistic function, which always takes values\\nbetween zero and one. The logistic formulae are stated in\\nterms of the probability that Y = 1, which is referred to as P.\\nThe probability that Y is 0 is 1 − P.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 410, 'page_label': '411'}, page_content='FIG. 8.18 Logistic regression\\nThe ‘ln’ symbol refers to a natural logarithm and a + bX is\\nthe regression line equation. Probability (P) can also be\\ncomputed from the regression equation. So, if we know the\\nregression equation, we could, theoretically, calculate the\\nexpected probability that Y = 1 for a given value of X.\\n‘exp’ is the exponent function, which is sometimes also\\nwritten as e.\\nLet us say we have a model that can predict whether a\\nperson is male or female on the basis of their height. Given a'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 411, 'page_label': '412'}, page_content='height of 150 cm, we need to predict whether the person is\\nmale or female.\\nWe know that the coefficients of a = −100 and b = 0.6.\\nUsing the above equation, we can calculate the probability of\\nmale given a height of 150 cm or more formally P(male|height\\n= 150).\\n \\ny = e^(a + b × X)/(1 + e^(a + b × X))\\ny = exp ( −100 + 0.6 × 150)/(1 + EXP( −100 + 0.6 × X)\\ny = 0.000046\\n \\nor a probability of near zero that the person is a male.\\nAssumptions in logistic regression\\nThe following assumptions must hold when building a logistic\\nregression model:\\nThere exists a linear relationship between logit function and independent\\nvariables\\nThe dependent variable Y must be categorical (1/0) and take binary value,\\ne.g. if pass then Y = 1; else Y = 0\\nThe data meets the ‘iid’ criterion, i.e. the error terms, ε, are independent from\\none another and identically distributed\\nThe error term follows a binomial distribution [n, p]\\nn = # of records in the data\\np = probability of success (pass, responder)\\n8.3.8 Maximum Likelihood Estimation\\nThe coefficients in a logistic regression are estimated using a\\nprocess called Maximum Likelihood Estimation (MLE). First,\\nlet us understand what is likelihood function before moving to'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 412, 'page_label': '413'}, page_content='MLE. A fair coin outcome flips equally heads and tails of the\\nsame number of times. If we toss the coin 10 times, it is\\nexpected that we get five times Head and five times Tail.\\nLet us now discuss about the probability of getting only\\nHead as an outcome; it is 5/10 = 0.5 in the above case.\\nWhenever this number (P) is greater than 0.5, it is said to be in\\nfavour of Head. Whenever P is lesser than 0.5, it is said to be\\nagainst the outcome of getting Head.\\nLet us represent ‘n’ flips of coin as X , X , X ,…, X . Now\\nX can take the value of 1 or 0.\\n   X = 1 if Head is the outcome\\n   X = 0 if Tail is the outcome\\nWhen we use the Bernoulli distribution represents each flip\\nof the coin:\\nEach observation X is independent and also identically\\ndistributed (iid), and the joint distribution simplifies to a\\nproduct of distributions.\\nwhere #H is the number of flips that resulted in the expected\\noutcome (heads in this case).\\nThe likelihood equation is\\n1 2 3 n\\ni\\ni\\ni\\ni'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 413, 'page_label': '414'}, page_content='But the likelihood function is not a probability. The\\nlikelihood for some coins may be 0.25 or 0 or 1.\\nMLE is about predicting the value for the parameters that\\nmaximizes the likelihood function.\\n8.4 SUMMARY\\nIn supervised learning, when we are trying to predict a real-value variable\\nsuch as ‘Price’, ‘Weight’, etc., the problem falls under the category of\\nregression. A regression problem tries to forecast results as a continuous\\noutput.\\nDependent Variable (Y) is the value to be predicted. This variable is\\npresumed to be functionally related to the independent variable (X). In other\\nwords, dependent variable(s) depends on independent variable(s).\\nIndependent Variable (X) is called as predictor. The independent variable (X)\\nis used in a regression model to estimate the value of the dependent variable\\n(Y).\\nRegression is essentially finding a relationship (or) association between the\\ndependent variable (Y) and the independent variables (X).\\nIf the regression involves only one independent variable, it is called simple\\nregression. Thus, if we take ‘Price of a used car’ as the dependent variable\\nand the ‘Year of manufacturing of the car’ as the independent variable, we\\ncan build a simple regression.\\nSlope represents how much the line in a graph changes in the vertical\\ndirection (Y-axis) over a change in the horizontal direction (X-axis). Slope is\\nalso referred as the rate of change in a graph.\\nMaximum and minimum points on a graph are found at points where the\\nslope of the curve is zero. It becomes zero either from positive or from\\nnegative value.\\nIf two or more independent variables are involved, it is called multiple\\nregression. Thus, if we take ‘Price of a used car’ as the dependent variable\\nand year of manufacturing (Year), brand of the car (Brand), and mileage run'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 414, 'page_label': '415'}, page_content='(Miles run) as the independent variables, we can form a multiple regression\\nequation as given below:\\n \\nPrice of a used car ($) = function (Year, Brand, Miles run)\\n \\nMulticollinearity is the situation in which the degree of correlation is not\\nonly between the dependent variable and the independent variable, but there\\nalso exists a strong correlation within (among) the independent variables\\nitself.\\nHeteroskedasticity refers to the changing variance of the error term. If the\\nvariance of the error term is not constant across data sets, there will be\\nerroneous predictions. In general, for a regression equation to make accurate\\npredictions, the error term should be independent, normally (identically)\\ndistributed (iid). The error terms should not be related to each other.\\nAccuracy of linear regression can be improved using the following three\\nmethods:\\n1. Shrinkage Approach\\n2. Subset Selection\\n3. Dimensionality Reduction\\nPolynomial regression model is the extension of the simple linear model by\\nadding extra predictors, obtained by raising (squaring) each of the original\\npredictors to a power. For example, if there are three variables, X, X , and X\\nare used as predictors.\\nIn logistic regression, the dependent variable (Y) is binary (0,1) and\\nindependent variables (X) are continuous in nature. The probabilities\\ndescribing the possible outcomes (probability that Y = 1) of a single trial are\\nmodelled as a function of the explanatory (predictor) variables by using a\\nlogistic function.\\nSAMPLE QUESTIONS\\nMULTIPLE-CHOICE QUESTIONS\\n1. When we are trying to predict a real-value variable such as ‘$’,\\n‘Weight’, the problem falls under the category of\\n1. Unsupervised learning\\n2. Supervised regression problem\\n3. Supervised classification problem\\n4. Categorical attribute\\n2. Price prediction of crude oil is an example of\\n1. Unsupervised learning\\n2 3'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 415, 'page_label': '416'}, page_content='2. Supervised regression problem\\n3. Supervised classification problem\\n4. Categorical attribute\\n3. Value to be predicted in machine learning is called as\\n1. Slope\\n2. Regression\\n3. Independent variable\\n4. Dependent variable\\n4. This is called as predictor.\\n1. Slope\\n2. Regression\\n3. Independent variable\\n4. Dependent variable\\n5. This is essentially finding a relationship (or) association between the\\ndependent variable (Y) and the independent variables (X).\\n1. Slope\\n2. Regression\\n3. Classification\\n4. Categorization\\n6. If the regression involves only one independent variable, it is called as\\n1. Multiple regression\\n2. One regression\\n3. Simple regression\\n4. Independent regression\\n7. Which equation represents simple imperfect relationship?\\n1. Y = (a + bx) + ε\\n2. Y = (a + bx)\\n3. DY = Change in Y / Change in X\\n4. Y = a + b1X1 + b2X2\\n8. Which equation represents simple perfect relationship?\\n1. Y = (a + bx) + ε\\n2. Y = (a + bx)\\n3. DY = Change in Y / Change in X\\n4. Y = a + b1X1 + b2X2\\n9. What is the formula for slope in a simple linear equation?\\n1. Y = (a + bx) + ε\\n2. Y = (a + bx)\\n3. DY = Change in Y / Change in X\\n4. Y = a + b1X1 + b2X2\\n10. This slope always moves upward on a graph from left to right.\\n1. Multilinear slope\\n2. No relationship slope\\n3. Negative slope\\n4. Positive slope\\n11. This slope always moves downwards on a graph from left to right.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 416, 'page_label': '417'}, page_content='1. Multilinear slope\\n2. No relationship slope\\n3. Negative slope\\n4. Positive slope\\n12. Maximum and minimum points on a graph are found at points where\\nthe slope of the curve is\\n1. Zero\\n2. One\\n3. 0.5\\n4. Random number\\n13. In the OLS algorithm, the first step is\\n1. Calculate the mean of Y and X\\n2. Calculate the errors of X and Y\\n3. Get the product (multiply)\\n4. Sum the products\\n14. In the OLS algorithm, the last step is\\n1. Calculate ‘a’ using the value of ‘b’\\n2. Calculate ‘b’ using the value of ‘a’\\n3. Get the product (multiply)\\n4. Sum the products\\n15. Which equation below is called as Unexplained Variation?\\n1. SSR (Sum of Squares due to Regression)\\n2. SSE (Sum of Squares due to Error)\\n3. SST (Sum of Squares Total):\\n4. R-square (R2)\\n16. Which equation below is called as Explained Variation?\\n1. SSR (Sum of Squares due to Regression)\\n2. SSE (Sum of Squares due to Error)\\n3. SST (Sum of Squares Total):\\n4. R-square (R2)\\n17. When new predictors (X) are added to the multiple linear regression\\nmodel, how does R  behave?\\n1. Decreasing\\n2. Increasing or decreasing\\n3. Increasing and decreasing\\n4. Increasing or remains constant\\n18. Predicting stochastic events precisely is not possible.\\n1. True\\n2. False\\nSHORT ANSWER-TYPE QUESTIONS (5 MARKS EACH)\\n1. What is a dependent variable and an independent variable in a linear\\nequation?\\n2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 417, 'page_label': '418'}, page_content='2. What is simple linear regression? Give one example.\\n3. Define slope in a linear regression.\\n4. Find the slope of the graph where the lower point on the line is\\nrepresented as (−3, −2) and the higher point on the line is represented as\\n(2, 2).\\n5. What are the conditions of a positive slope in linear regression?\\n6. What are the conditions of a negative slope in linear regression?\\n7. What is multiple linear regression?\\n8. Define sum of squares due to error in multiple linear regression.\\n9. Define sum of squares due to regression in multiple linear regression.\\n10. What is multicollinearity in regression equation?\\n11. What is heteroskedasticity?\\n12. Explain ridge regression.\\n13. Explain lasso regression.\\n14. What is polynomial regression?\\n15. Explain basis function.\\n16. Explain logistic regression.\\nLONG ANSWER-TYPE QUESTIONS (10 MARKS EACH)\\n1. Define simple linear regression using a graph explaining slope and\\nintercept.\\n2. Explain rise, run, and slope in a graph.\\n3. Explain slope, linear positive slope, and linear negative slope in a graph\\nalong with various conditions leading to the slope.\\n4. Explain curve linear negative slope and curve linear positive slope in a\\ngraph.\\n5. Explain maximum and minimum point of curves through a graph.\\n6. Explain ordinary least square with formula for a and b.\\n7. Explain the OLS algorithm with steps.\\n8. What is standard error of the regression? Draw a graph to represent the\\nsame.\\n9. Explain multiple linear regression with an example.\\n10. Explain the assumptions in regression analysis and BLUE concept.\\n11. Explain two main problems in regression analysis.\\n12. How to improve accuracy of the linear regression model?\\n13. Explain polynomial regression model in detail with an example.\\n14. Explain logistic regression in detail.\\n15. What are the assumptions in logistic regression?\\n16. Discuss maximum likelihood estimation in detail.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 418, 'page_label': '419'}, page_content='Chapter 9\\nUnsupervised Learning\\nOBJECTIVE OF THE CHAPTER :\\nWe have discussed how to train our machines with past\\ndata on the basis of which they can learn, gain intelligence,\\nand apply that intelligence on a new set of data. There are,\\nhowever, situations when we do not have any prior\\nknowledge of the data set we are working with, but we still\\nwant to discover interesting relationships among the\\nattributes of the data or group the data in logical segments\\nfor easy analysis. The task of the machine is then to\\nidentify this knowledge without any prior training and that\\nis the space of unsupervised learning. In this chapter, we\\nwill discuss how Clustering algorithms help in grouping\\ndata sets into logical segments and the association analysis\\nwhich enables to identify a pattern or relationship of\\nattributes within the data set. An interesting application of\\nthe association analysis is the Market Basket Analysis,\\nwhich is used widely by retailers and advertisers across the\\nglobe.\\n9.1 INTRODUCTION'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 419, 'page_label': '420'}, page_content='Unsupervised learning is a machine learning concept where\\nthe unlabelled and unclassified information is analysed to\\ndiscover hidden knowledge. The algorithms work on the data\\nwithout any prior training, but they are constructed in such a\\nway that they can identify patterns, groupings, sorting order,\\nand numerous other interesting knowledge from the set of\\ndata.\\n9.2 UNSUPERVISED VS SUPERVISED LEARNING\\nTill now, we have discussed about supervised learning where\\nthe aim was to predict the outcome variable Y on the basis of\\nthe feature set X :X :… X , and we discussed methods such as\\nregression and classification for the same. We will now\\nintroduce the concept of unsupervised learning where the\\nobjective is to observe only the features X :X :… X ; we are\\nnot going to predict any outcome variable, but rather our\\nintention is to find out the association between the features or\\ntheir grouping to understand the nature of the data. This\\nanalysis may reveal an interesting correlation between the\\nfeatures or a common behaviour within the subgroup of the\\ndata, which provides better understanding of the data.\\nIn terms of statistics, a supervised learning algorithm will\\ntry to learn the probability of outcome Y for a particular input\\nX, which is called the posterior probability. Unsupervised\\nlearning is closely related to density estimation in statistics.\\nHere, every input and the corresponding targets are\\nconcatenated to create a new set of input such as {(X , Y ),\\n(X , Y ),…, (X , Y )}, which leads to a better understanding of\\nthe correlation of X and Y; this probability notation is called\\nthe joint probability.\\nLet us take an example of how unsupervised learning helps\\nin pushing movie promotions to the correct group of people. In\\n1 2 n\\n1 2 n\\n1 1\\n2 2 n n'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 420, 'page_label': '421'}, page_content='earlier days, movie promotions were blind push of the same\\ndata to all demography, such that everyone used to watch the\\nsame posters or trailers irrespective of their choice or\\npreference. So, in most of the cases, the person watching the\\npromotion or trailer would end up ignoring it, which leads to\\nwaste of effort and money on the promotion. But with the\\nadvent of smart devices and apps, there is now a huge database\\navailable to understand what type of movie is liked by what\\nsegment of the demography. Machine learning helps to find\\nout the pattern or the repeated behaviour of the smaller\\ngroups/clusters within this database to provide the intelligence\\nabout liking or disliking of certain types of movies by different\\ngroups within the demography. So, by using this intelligence,\\nthe smart apps can push only the relevant movie promotions or\\ntrailers to the selected groups, which will significantly increase\\nthe chance of targeting the right interested person for the\\nmovie.\\nWe will discuss two methods in this chapter for explaining\\nthe principle underlying unsupervised learning – Clustering\\nand Association Analysis. Clustering is a broad class of\\nmethods used for discovering unknown subgroups in data,\\nwhich is the most important concept in unsupervised learning.\\nAnother technique is Association Analysis which identifies a\\nlow-dimensional representation of the observations that can\\nexplain the variance and identify the association rule for the\\nexplanation.\\n9.3 APPLICATION OF UNSUPERVISED LEARNING\\nBecause of its flexibility that it can work on uncategorized and\\nunlabelled data, there are many domains where unsupervised\\nlearning finds its application. Few examples of such\\napplications are as follows:'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 421, 'page_label': '422'}, page_content='Segmentation of target consumer populations by an advertisement consulting\\nagency on the basis of few dimensions such as demography, financial data,\\npurchasing habits, etc. so that the advertisers can reach their target\\nconsumers efficiently\\nAnomaly or fraud detection in the banking sector by identifying the pattern\\nof loan defaulters\\nImage processing and image segmentation such as face recognition,\\nexpression identification, etc.\\nGrouping of important characteristics in genes to identify important\\ninfluencers in new areas of genetics\\nUtilization by data scientists to reduce the dimensionalities in sample data to\\nsimplify modelling\\nDocument clustering and identifying potential labelling options\\nToday, unsupervised learning is used in many areas\\ninvolving Artificial Intelligence (AI) and Machine Learning\\n(ML). Chat bots, self-driven cars, and many more recent\\ninnovations are results of the combination of unsupervised and\\nsupervised learning.\\nSo, in this chapter, we will cover two major aspects of\\nunsupervised learning, namely Clustering which helps in\\nsegmentation of the set of objects into groups of similar\\nobjects and Association Analysis which is related to the\\nidentification of relationships among objects in a data set.\\n9.4 CLUSTERING\\nClustering refers to a broad set of techniques for finding\\nsubgroups, or clusters, in a data set on the basis of the\\ncharacteristics of the objects within that data set in such a\\nmanner that the objects within the group are similar (or related\\nto each other) but are different from (or unrelated to) the\\nobjects from the other groups. The effectiveness of clustering\\ndepends on how similar or related the objects within a group\\nare or how different or unrelated the objects in different groups\\nare from each other. It is often domain specific to define what'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 422, 'page_label': '423'}, page_content='is meant by two objects to be similar or dissimilar and thus is\\nan important aspect of the unsupervised machine learning task.\\nAs an example, suppose we want to run some\\nadvertisements of a new movie for a countrywide promotional\\nactivity. We have data for the age, location, financial\\ncondition, and political stability of the people in different parts\\nof the country. We may want to run a different type of\\ncampaign for the different parts grouped according to the data\\nwe have. Any logical grouping obtained by analysing the\\ncharacteristics of the people will help us in driving the\\ncampaigns in a more targeted way. Clustering analysis can\\nhelp in this activity by analysing different ways to group the\\nset of people and arriving at different types of clusters.\\nThere are many different fields where cluster analysis is\\nused effectively, such as\\nText data mining: this includes tasks such as text categorization, text\\nclustering, document summarization, concept extraction, sentiment analysis,\\nand entity relation modelling\\nCustomer segmentation: creating clusters of customers on the basis of\\nparameters such as demographics, financial conditions, buying habits, etc.,\\nwhich can be used by retailers and advertisers to promote their products in\\nthe correct segment\\nAnomaly checking: checking of anomalous behaviours such as fraudulent\\nbank transaction, unauthorized computer intrusion, suspicious movements on\\na radar scanner, etc.\\nData mining: simplify the data mining task by grouping a large number of\\nfeatures from an extremely large data set to make the analysis manageable\\nIn this section, we will discuss the methods related to the\\nmachine learning task of clustering, which involves finding\\nnatural groupings of data. The focus will be on\\nhow clustering tasks differ from classification tasks and how clustering\\ndefines groups'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 423, 'page_label': '424'}, page_content='a classic and easy-to-understand clustering algorithm, namely k-means,\\nwhich is used for clustering along with the k-medoids algorithm\\napplication of clustering in real-life scenarios\\n9.4.1 Clustering as a machine learning task\\nThe primary driver of clustering knowledge is discovery rather\\nthan prediction, because we may not even know what we are\\nlooking for before starting the clustering analysis. So,\\nclustering is defined as an unsupervised machine learning task\\nthat automatically divides the data into clusters or groups of\\nsimilar items. The analysis achieves this without prior\\nknowledge of the types of groups required and thus can\\nprovide an insight into the natural groupings within the data\\nset. The primary guideline of clustering task is that the data\\ninside a cluster should be very similar to each other but very\\ndifferent from those outside the cluster. We can assume that\\nthe definition of similarity might vary across applications, but\\nthe basic idea is always the same, that is, to create the group\\nsuch that related elements are placed together. Using this\\nprinciple, whenever a large set of diverse and varied data is\\npresented for analysis, clustering enables to represent the data\\nin a smaller number of groups. It helps to reduce the\\ncomplexity and provides insight into patterns of relationships\\nto generate meaningful and actionable structures within the\\ndata. The effectiveness of clustering is measured by the\\nhomogeneity within a group as well as the difference between\\ndistinct groups. See Figure 9.1 for reference.\\nFrom the above discussion, it may seem that through\\nclustering, we are trying to label the objects with class labels.\\nBut clustering is somewhat different from the classification\\nand numeric prediction discussed in supervised learning\\nchapters. In each of these cases, the goal was to create a model\\nthat relates features to an outcome or to other features and the'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 424, 'page_label': '425'}, page_content='model identifies patterns within the data. In contrast, clustering\\ncreates new data. Unlabelled objects are given a cluster label\\nwhich is inferred entirely from the relationship of attributes\\nwithin the data.\\nLet us take an example. You were invited to take a session\\non Machine Learning in a reputed university for induction of\\ntheir professors on the subject. Before you create the material\\nfor the session, you want to know the level of acquaintance of\\nthe professors on the subject so that the session is successful.\\nBut you do not want to ask the inviting university, but rather\\ndo some analysis on your own on the basis of the data\\navailable freely. As Machine Learning is the intersection of\\nStatistics and Computer Science, you focused on identifying\\nthe professors from these two areas also. So, you searched the\\nlist of research publications of these professors from the\\ninternet, and by using the machine learning algorithm, you\\nnow want to group the papers and thus infer the expertise of\\nthe professors into three buckets – Statistics, Computer\\nScience, and Machine Learning.\\nAfter plotting the number of publications of these professors\\nin the two core areas, namely Statistics and Computer Science,\\nyou obtain a scatter plot as shown in Figure 9.2.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 425, 'page_label': '426'}, page_content='FIG. 9.1 Unsupervised learning – clustering\\nFIG. 9.2 Data set for the conference attendees'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 426, 'page_label': '427'}, page_content='Some inferences that can be derived from the pattern\\nanalysis of the data is that there seems to be three groups or\\nclusters emerging from the data. The pure statisticians have\\nvery less Computer Science-related papers, whereas the pure\\nComputer Science professors have less number of statistics-\\nrelated papers than Computer Science-related papers. There is\\na third cluster of professors who have published papers on\\nboth these areas and thus can be assumed to be the persons\\nknowledgeable in machine learning concepts, as shown in\\nFigure 9.3.\\nThus, in the above problem, we used visual indication of\\nlogical grouping of data to identify a pattern or cluster and\\nlabelled the data in three different clusters. The main driver for\\nour clustering was the closeness of the points to each other to\\nform a group. The clustering algorithm uses a very similar\\napproach to measure how closely the data points are related\\nand decides whether they can be labelled as a homogeneous\\ngroup. In the next section, we will discuss few important\\nalgorithms for clustering.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 427, 'page_label': '428'}, page_content='FIG. 9.3 Clusters for the conference attendees\\n9.4.2 Different types of clustering techniques\\nThe major clustering techniques are\\nPartitioning methods,\\nHierarchical methods, and\\nDensity-based methods.\\nTheir approach towards creating the clusters, way to\\nmeasure the quality of the clusters, and applicability are\\ndifferent. Table 9.1 summarizes the main characteristics of\\neach method for each reference.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 428, 'page_label': '429'}, page_content='Table 9.1 Different Clustering Methods\\nWe will discuss each of these methods and their related\\ntechniques in details in the following sections.\\n9.4.3 Partitioning methods\\nTwo of the most important algorithms for partitioning-based\\nclustering are k-means and k-medoid. In the k-means\\nalgorithm, the centroid of the prototype is identified for\\nclustering, which is normally the mean of a group of points.\\nSimilarly, the k-medoid algorithm identifies the medoid which\\nis the most representative point for a group of points. We can\\nalso infer that in most cases, the centroid does not correspond\\nto an actual data point, whereas medoid is always an actual\\ndata point. Let us discuss both these algorithms in detail.\\n9.4.3.1 K-means - A centroid-based technique\\nThis is one of the oldest and most popularly used algorithm for\\nclustering. The basic principles used by this algorithm also'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 429, 'page_label': '430'}, page_content='serves as the basis for other more sophisticated and complex\\nalgorithms. Table 9.2 provides the strengths and weaknesses\\nof this algorithm.\\nThe principle of the k-means algorithm is to assign each of\\nthe ‘n’ data points to one of the K clusters where ‘K’ is a user-\\ndefined parameter as the number of clusters desired. The\\nobjective is to maximize the homogeneity within the clusters\\nand also to maximize the differences between the clusters. The\\nhomogeneity and differences are measured in terms of the\\ndistance between the objects or points in the data set.\\nAlgorithm 9.1 shows the simple algorithm of K-means\\nStep 1: Select K points in the data space and mark them as initial centroids\\nloop\\nStep 2: Assign each point in the data space to the nearest centroid to form K\\nclusters\\nStep 3: Measure the distance of each point in the cluster from the centroid\\nStep 4: Calculate the Sum of Squared Error (SSE) to measure the quality of\\nthe clusters (described later in this chapter)\\nStep 5: Identify the new centroid of each cluster on the basis of distance\\nbetween points\\nStep 6: Repeat Steps 2 to 5 to refine until centroids do not change\\nend loop'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 430, 'page_label': '431'}, page_content='Table 9.2 Strengths and Weaknesses of K-means\\nLet us understand this algorithm with an example. In Figure\\n9.4, we have certain set of data points, and we will apply the k-\\nmeans algorithm to find out the clusters generated from this\\ndata set. Let us fix K = 4, implying that we want to create four\\nclusters out of this data set. As the first step, we assign four\\nrandom points from the data set as the centroids, as\\nrepresented by the * signs, and we assign the data points to the\\nnearest centroid to create four clusters. In the second step, on\\nthe basis of the distance of the points from the corresponding\\ncentroids, the centroids are updated and points are reassigned\\nto the updated centroids. After three iterations, we found that\\nthe centroids are not moving as there is no scope for\\nrefinement, and thus, the k-means algorithm will terminate.\\nThis provides us the most logical four groupings or cluster of\\nthe data sets where the homogeneity within the groups is\\nhighest and difference between the groups is maximum.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 431, 'page_label': '432'}, page_content='FIG. 9.4 Clustering concept – before and after clustering\\nChoosing appropriate number of clusters\\nOne of the most important success factors in arriving at correct\\nclustering is to start with the correct number of cluster\\nassumptions. Different numbers of starting cluster lead to\\ncompletely different types of data split. It will always help if\\nwe have some prior knowledge about the number of clusters\\nand we start our k-means algorithm with that prior knowledge.\\nFor example, if we are clustering the data of the students of a\\nuniversity, it is always better to start with the number of\\ndepartments in that university. Sometimes, the business needs\\nor resource limitations drive the number of required clusters.\\nFor example, if a movie maker wants to cluster the movies on\\nthe basis of combination of two parameters – budget of the\\nmovie: high or low, and casting of the movie: star or non-star,\\nthen there are 4 possible combinations, and thus, there can be\\nfour clusters to split the data.\\nFor a small data set, sometimes a rule of thumb that is\\nfollowed is'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 432, 'page_label': '433'}, page_content='which means that K is set as the square root of n/2 for a data\\nset of n examples. But unfortunately, this thumb rule does not\\nwork well for large data sets. There are several statistical\\nmethods to arrive at the suitable number of clusters.\\nElbow method\\nThis method tries to measure the homogeneity or\\nheterogeneity within the cluster and for various values of ‘K’\\nand helps in arriving at the optimal ‘K’. From Figure 9.5, we\\ncan see the homogeneity will increase or heterogeneity will\\ndecrease with increasing ‘K’ as the number of data points\\ninside each cluster reduces with this increase. But these\\niterations take significant computation effort, and after a\\ncertain point, the increase in homogeneity benefit is no longer\\nin accordance with the investment required to achieve it, as is\\nevident from the figure. This point is known as the elbow\\npoint, and the ‘K’ value at this point produces the optimal\\nclustering performance. There are a large number of\\nalgorithms to calculate the homogeneity and heterogeneity of\\nthe clusters, which are not discussed in this book.\\nChoosing the initial centroids\\nAnother key step for the k-means algorithm is to choose the\\ninitial centroids properly. One common practice is to choose\\nrandom points in the data space on the basis of the number of\\ncluster requirement and refine the points as we move into the\\niterations. But this often leads to higher squared error in the\\nfinal clustering, thus resulting in sub-optimal clustering\\nsolution. The assumption for selecting random centroids is that'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 433, 'page_label': '434'}, page_content='multiple subsequent runs will minimize the SSE and identify\\nthe optimal clusters. But this is often not true on the basis of\\nthe spread of the data set and the number of clusters sought.\\nSo, one effective approach is to employ the hierarchical\\nclustering technique on sample points from the data set and\\nthen arrive at sample K clusters. The centroids of these initial\\nK clusters are used as the initial centroids. This approach is\\npractical when the data set has small number of points and K is\\nrelatively small compared to the data points. There are\\nprocedures such as bisecting k-means and use of post-\\nprocessing to fix initial clustering issues; these procedures can\\nproduce better quality initial centroids and thus better SSE for\\nthe final clusters.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 434, 'page_label': '435'}, page_content='FIG. 9.5 Elbow point to determine the appropriate number of clusters\\nRecomputing cluster centroids\\nWe discussed in the k-means algorithm that the iterative step is\\nto recalculate the centroids of the data set after each iteration.\\nThe proximities of the data points from each other within a\\ncluster is measured to minimize the distances. The distance of\\nthe data point from its nearest centroid can also be calculated\\nto minimize the distances to arrive at the refined centroid. The\\nEuclidean distance between two data points is measured as\\nfollows:'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 435, 'page_label': '436'}, page_content='Using this function, the distance between the example data\\nand its nearest centroid and the objective is calculated to\\nminimize this distance. The measure of quality of clustering\\nuses the SSE technique. The formula used is as follows:\\nwhere dist() calculates the Euclidean distance between the\\ncentroid c of the cluster C and the data points x in the cluster.\\nThe summation of such distances over all the ‘K’ clusters\\ngives the total sum of squared error. As you can understand,\\nthe lower the SSE for a clustering solution, the better is the\\nrepresentative position of the centroid. Thus, in our clustering\\nalgorithm in Algorithm 9.1, the recomputation of the\\ncentroid involves calculating the SSE of each new centroid\\nand arriving at the optimal centroid identification. After the\\ncentroids are repositioned, the data points nearest to the\\ncentroids are assigned to form the refined clusters. It is\\nobserved that the centroid that minimizes the SSE of the\\ncluster is its mean. One limitation of the squared error method\\nis that in the case of presence of outliers in the data set, the\\nsquared error can distort the mean value of the clusters.\\nLet us use this understanding to identify the cluster step for\\nthe data set in Figure 9.6. Assume that the number of cluster\\nrequirement, K = 4. We will randomly select four cluster\\ncentroids as indicated by four different colours in Figure 9.7.\\ni i'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 436, 'page_label': '437'}, page_content='Now, on the basis of the proximity of the data points in this\\ndata set to the centroids, we partition the data set into four\\nsegments as represented by dashed lines in Figure 9.8. This\\ndiagram is called Voronoi diagram which creates the\\nboundaries of the clusters. We got the initial four clusters,\\nnamely C 1 , C 2 , C 3 , and C 4 , created by the dashed lines\\nfrom the vertex of the clusters, which is the point with the\\nmaximal distance from the centre of the clusters. It is now easy\\nto understand the areas covered by each cluster and the data\\npoints within each cluster through this representation.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 437, 'page_label': '438'}, page_content='FIG. 9.6 Clustering of data set'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 438, 'page_label': '439'}, page_content='FIG. 9.7 Clustering with initial centroids\\nFIG. 9.8 Iteration 1: Four clusters and distance of points from the centroids\\nThe next step is to calculate the SSE of this clustering and\\nupdate the position of the centroids. We can also proceed by\\nour understanding that the new centroid should be the mean of\\nthe data points in the respective clusters. The distances of the\\ndata points currently marked as Cluster C from the centroid of\\ncluster C are marked as a  , a  ,…, a  in the figure and those\\ndetermine the homogeneity within cluster C. On the other\\nhand, the distances of data points of cluster C from the\\ncentroid of cluster C determine the heterogeneity among these\\ntwo different clusters. Our aim is to minimize the homogeneity\\nwithin the clusters and maximize the heterogeneity among the\\ndifferent clusters. So, the revised centroids are as shown in\\nFigure 9.9.\\n3\\n3 i1 i2 in\\n3\\n4\\n3'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 439, 'page_label': '440'}, page_content='We can also find out that the cluster boundaries are refined\\non the basis of the new centroids and the identification of the\\nnearest centroids for the data points and reassigning them to\\nthe new centroids. The new points reclaimed by each cluster\\nare shown in the diagram.\\nThe k-means algorithm continues with the update of the\\ncentroid according to the new cluster and reassignment of the\\npoints, until no more data points are changed due to the\\ncentroid shift. At this point, the algorithm stops. Figure 9.10\\nshows the final clustering of the data set we used. The\\ncomplexity of the k-means algorithm is O ( nKt ), where ‘ n ’\\nis the total number of data points or objects in the data set, K is\\nthe number of clusters, and ‘ t ’ is the number of iterations.\\nNormally, ‘ K ’ and ‘ t ’ are kept much smaller than ‘ n ’, and\\nthus, the k-means method is relatively scalable and efficient in\\nprocessing large data sets.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 440, 'page_label': '441'}, page_content='FIG. 9.9 Iteration 2: Centroids recomputed and points redistributed among the\\nclusters according to the nearest centroid'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 441, 'page_label': '442'}, page_content='FIG. 9.10 Iteration 3: Final cluster arrangement: Centroids recomputed and points\\nredistributed among the clusters according to the nearest centroid\\nPoints to Ponder:\\nBecause of the distance-based approach from the centroid\\nto all points in the data set, the k-means method may not\\nalways converge to the global optimum and often\\nterminates at a local optimum. The result of the clustering\\nlargely depends on the initial random selection of cluster\\ncentres.\\nk-means often produce local optimum and not global optimum.\\nAlso, the result depends on the initial selection of random\\ncluster centroids. It is a common practice to run the k-means\\nalgorithm multiple times with different cluster centres to\\nidentify the optimal clusters. The necessity to set the initial ‘K’\\nvalues is also perceived as a disadvantage of the k-means\\nalgorithm. There are methods to overcome this problem, such\\nas defining a range for ‘K’ and then comparing the results of\\nclustering with those different ‘K’ values to arrive at the best\\npossible cluster. The ways to improve cluster performance is\\nan area of further study, and many different techniques are\\nemployed to achieve that. This is out of scope of this book but\\ncan be pursued for advanced machine learning studies.\\nNote:'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 442, 'page_label': '443'}, page_content='Clustering is often used as the first step of identifying the\\nsubgroups within a unlabeled set of data which then is\\nused for classifying the new observed data. At the\\nbeginning we are not clear about the pattern or classes that\\nexist within the unlabeled data set. By using the clustering\\nalgorithm at that stage we find out the groups of similar\\nobjects within the data set and form sub-groups and\\nclasses. Later when a new object is observed, then using\\nthe classification algorithms we try to place that into one\\nof the sub-groups identified in the earlier stage. Let’s take\\nan example. We are running a software testing activity and\\nwe identified a set of defects in the software. For easy\\nallocation of these defects to different developer groups,\\nthe team is trying to identify similar groups of defects.\\nOften text analytics is used as the guiding principle for\\nidentifying this similarity. Suppose there are 4 sub-groups\\nof defects identified, namely, GUI related defects, Business\\nlogic related defects, Missing requirement defects and\\nDatabase related defects. Based on this grouping, the team\\nidentified the developers to whom the defects should be\\nsent for fixing. As the testing continues, there are new\\ndefects getting created. We have the categories of defects\\nidentified now and thus the team can use classification\\nalgorithms to assign the new defect to one of the 4\\nidentified groups or classes which will make it easy to\\nidentify the developer who should be fixing it.\\n9.4.4 K-Medoids: a representative object-based\\ntechnique\\nAs discussed earlier, the k-means algorithm is sensitive to\\noutliers in the data set and inadvertently produces skewed\\nclusters when the means of the data points are used as'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 443, 'page_label': '444'}, page_content='centroids. Let us take an example of eight data points, and for\\nsimplicity, we can consider them to be 1-D data with values 1,\\n2, 3, 5, 9, 10, 11, and 25. Point 25 is the outlier, and it affects\\nthe cluster formation negatively when the mean of the points is\\nconsidered as centroids.\\nWith K = 2, the initial clusters we arrived at are {1, 2, 3, 6}\\nand {9, 10, 11, 25}.\\nThe mean of the cluster \\nand the mean of the cluster \\nSo, the SSE within the clusters is\\nIf we compare this with the cluster {1, 2, 3, 6, 9} and {10,\\n11, 25},\\nthe mean of the cluster \\nand the mean of the cluster'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 444, 'page_label': '445'}, page_content='So, the SSE within the clusters is\\nBecause the SSE of the second clustering is lower, k-means\\ntend to put point 9 in the same cluster with 1, 2, 3, and 6\\nthough the point is logically nearer to points 10 and 11. This\\nskewedness is introduced due to the outlier point 25, which\\nshifts the mean away from the centre of the cluster.\\nk-medoids provides a solution to this problem. Instead of\\nconsidering the mean of the data points in the cluster, k-\\nmedoids considers k representative data points from the\\nexisting points in the data set as the centre of the clusters. It\\nthen assigns the data points according to their distance from\\nthese centres to form k clusters. Note that the medoids in this\\ncase are actual data points or objects from the data set and not\\nan imaginary point as in the case when the mean of the data\\nsets within cluster is used as the centroid in the k-means\\ntechnique. The SSE is calculated as\\nwhere o is the representative point or object of cluster C.\\nThus, the k-medoids method groups n objects in k clusters\\nby minimizing the SSE. Because of the use of medoids from\\nthe actual representative data points, k-medoids is less\\ninfluenced by the outliers in the data. One of the practical\\nimplementation of the k-medoids principle is the Partitioning\\ni i'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 445, 'page_label': '446'}, page_content='Around Medoids (PAM) algorithm. Refer to Algorithm 2\\ntable:\\nAlgorithm 2: PAM\\nStep 1: Randomly choose k points in the data set as the initial representative\\npoints\\nloop\\nStep 2: Assign each of the remaining points to the cluster which has the\\nnearest representative point\\nStep 3: Randomly select a non-representative point o in each cluster\\nStep 4: Swap the representative point o with o and compute the new SSE\\nafter swapping\\nStep 5: If SSEnew < SSEold, then swap o with o to form the new set of k\\nrepresentative objects;\\nStep 6: Refine the k clusters on the basis of the nearest representative point.\\nLogic continues until there is no change\\nend loop\\nIn this algorithm, we replaced the current representative\\nobject with a non-representative object and checked if it\\nimproves the quality of clustering. In the iterative process, all\\npossible replacements are attempted until the quality of\\nclusters no longer improves.\\nIf o ,…, o are the current set of representative objects or\\nmedoids and there is a non-representative object o, then to\\ndetermine whether o is a good replacement of o (1 ≤ j ≤ k),\\nthe distance of each object x is calculated from its nearest\\nmedoid from the set {o , o ,…, o , o, o ,…, o} and the\\nSSE is calculated. If the SSE after replacing o with o\\ndecreases, it means that o represents the cluster better than o,\\nand the data points in the set are reassigned according to the\\nnearest medoids now.\\nr\\nj r\\nj r\\n1 k\\nr\\nr j\\n1 2 j−1 r j+1 k\\nj r\\nr j'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 446, 'page_label': '447'}, page_content='FIG. 9.11 PAM algorithm: Reassignment of points to different clusters\\nAs shown in Figure 9.11, point p was belonging to the\\ncluster with medoid o  in the first iteration, but after o was\\nreplaced by o, it was found that p is nearest to the new\\nrandom medoid and thus gets assigned to it. In this way, the\\nclusters get refined after each medoid is replaced with a new\\nnon-representative medoid. Each time a reassignment is done,\\nthe SSE based on the new medoid is calculated. The difference\\nbetween the SSE before and after the swap indicates whether\\nor not the replacement is improving the quality of the\\nclustering by bringing the most similar points together.\\nPoints to Ponder:\\nk-medoids methods like PAM works well for small set of\\ndata, but they are not scalable for large set of data because\\nof computational overhead. A sample-based technique is\\nused in the case of large data set where the sample should\\nbe a good representative of the whole data set.\\ni\\nj+1 j\\nr i'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 447, 'page_label': '448'}, page_content='Though the k-medoids algorithm provides an effective way\\nto eliminate the noise or outliers in the data set, which was the\\nproblem in the k-means algorithm, it is expensive in terms of\\ncalculations. The complexity of each iteration in the k-medoids\\nalgorithm is O(k(n - k) ). For large value of ‘n’ and ‘k’, this\\ncalculation becomes much costlier than that of the k-means\\nalgorithm.\\n9.4.5 Hierarchical clustering\\nTill now, we have discussed the various methods for\\npartitioning the data into different clusters. But there are\\nsituations when the data needs to be partitioned into groups at\\ndifferent levels such as in a hierarchy. The hierarchical\\nclustering methods are used to group the data into hierarchy or\\ntree-like structure. For example, in a machine learning\\nproblem of organizing employees of a university in different\\ndepartments, first the employees are grouped under the\\ndifferent departments in the university, and then within each\\ndepartment, the employees can be grouped according to their\\nroles such as professors, assistant professors, supervisors, lab\\nassistants, etc. This creates a hierarchical structure of the\\nemployee data and eases visualization and analysis. Similarly,\\nthere may be a data set which has an underlying hierarchy\\nstructure that we want to discover and we can use the\\nhierarchical clustering methods to achieve that.\\nThere are two main hierarchical clustering methods:\\nagglomerative clustering and divisive clustering.\\nAgglomerative clustering is a bottom-up technique which\\nstarts with individual objects as clusters and then iteratively\\nmerges them to form larger clusters. On the other hand, the\\ndivisive method starts with one cluster with all given objects\\n2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 448, 'page_label': '449'}, page_content='and then splits it iteratively to form smaller clusters. See\\nFigure 9.12.\\nThe agglomerative hierarchical clustering method uses the\\nbottom-up strategy. It starts with each object forming its own\\ncluster and then iteratively merges the clusters according to\\ntheir similarity to form larger clusters. It terminates either\\nwhen a certain clustering condition imposed by the user is\\nachieved or all the clusters merge into a single cluster.\\nThe divisive hierarchical clustering method uses a top-down\\nstrategy. The starting point is the largest cluster with all the\\nobjects in it, and then, it is split recursively to form smaller\\nand smaller clusters, thus forming the hierarchy. The end of\\niterations is achieved when the objects in the final clusters are\\nsufficiently homogeneous to each other or the final clusters\\ncontain only one object or the user-defined clustering\\ncondition is achieved.\\nIn both these cases, it is important to select the split and\\nmerger points carefully, because the subsequent splits or\\nmergers will use the result of the previous ones and there is no\\noption to perform any object swapping between the clusters or\\nrectify the decisions made in previous steps, which may result\\nin poor clustering quality at the end.\\nA dendrogram is a commonly used tree structure\\nrepresentation of step-by-step creation of hierarchical\\nclustering. It shows how the clusters are merged iteratively (in\\nthe case of agglomerative clustering) or split iteratively (in the\\ncase of divisive clustering) to arrive at the optimal clustering\\nsolution. Figure 9.13 shows a dendro-gram with four levels\\nand how the objects are merged or split at each level to arrive\\nat the hierarchical clustering.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 449, 'page_label': '450'}, page_content='FIG. 9.12 Agglomerative and divisive hierarchical clustering\\nOne of the core measures of proximities between clusters is\\nthe distance between them. There are four standard methods to\\nmeasure the distance between clusters:\\nLet C and C be the two clusters with n and n respectively.\\np and p represents the points in clusters C and C\\nrespectively. We will denote the mean of cluster C as m\\ni j i j\\ni j i j\\ni i.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 450, 'page_label': '451'}, page_content='Refer to Figure 9.14 for understanding the concept of these\\ndistances.\\nFIG. 9.13 Dendrogram representation of hierarchical clustering\\nFIG. 9.14 Distance measure in algorithmic methods\\nOften the distance measure is used to decide when to\\nterminate the clustering algorithm. For example, in an\\nagglomerative clustering, the merging iterations may be'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 451, 'page_label': '452'}, page_content='stopped once the MIN distance between two neighbouring\\nclusters becomes less than the user-defined threshold. So,\\nwhen an algorithm uses the minimum distance D  to\\nmeasure the distance between the clusters, then it is referred to\\nas nearest neighbour clustering algorithm, and if the decision\\nto stop the algorithm is based on a user-defined limit on D ,\\nthen it is called single linkage algorithm.\\nOn the other hand, when an algorithm uses the maximum\\ndistance D  to measure the distance between the clusters,\\nthen it is referred to as furthest neighbour clustering algorithm,\\nand if the decision to stop the algorithm is based on a user-\\ndefined limit on D  then it is called complete linkage\\nalgorithm.\\nAs minimum and maximum measures provide two extreme\\noptions to measure distance between the clusters, they are\\nprone to the outliers and noisy data. Instead, the use of mean\\nand average distance helps in avoiding such problem and\\nprovides more consistent results.\\n9.4.6 Density-based methods - DBSCAN\\nYou might have noticed that when we used the partitioning\\nand hierarchical clustering methods, the resulting clusters are\\nspherical or nearly spherical in nature. In the case of the other\\nshaped clusters such as S-shaped or uneven shaped clusters,\\nthe above two types of method do not provide accurate results.\\nThe density-based clustering approach provides a solution to\\nidentify clusters of arbitrary shapes. The principle is based on\\nidentifying the dense area and sparse area within the data set\\nand then run the clustering algorithm. DBSCAN is one of the\\npopular density-based algorithm which creates clusters by\\nusing connected regions with high density.\\nmin\\nmin\\nmax\\nmax'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 452, 'page_label': '453'}, page_content='9.5 FINDING PATTERN USING ASSOCIATION RULE\\nAssociation rule presents a methodology that is useful for\\nidentifying interesting relationships hidden in large data sets. It\\nis also known as association analysis, and the discovered\\nrelationships can be represented in the form of association\\nrules comprising a set of frequent items. A common\\napplication of this analysis is the Market Basket Analysis\\nthat retailers use for cross-selling of their products. For\\nexample, every large grocery store accumulates a large volume\\nof data about the buying pattern of the customers. On the basis\\nof the items purchased together, the retailers can push some\\ncross-selling either by placing the items bought together in\\nadjacent areas or creating some combo offer with those\\ndifferent product types. The below association rule signifies\\nthat people who have bought bread and milk have often bought\\negg also; so, for the retailer, it makes sense that these items are\\nplaced together for new opportunities for cross-selling.\\n \\n{Bread, Milk} → {Egg}\\n \\nThe application of association analysis is also widespread in\\nother domains such as bioinformatics, medical diagnosis,\\nscientific data analysis, and web data mining. For example, by\\ndiscovering the interesting relationship between food habit and\\npatients developing breast cancer, a new cancer prevention\\nmechanism can be found which will benefit thousands of\\npeople in the world. In this book, we will mainly illustrate the\\nanalysis techniques by using the market basket example, but it\\ncan be used more widely across domains to identify\\nassociation among items in transactional data. The huge pool'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 453, 'page_label': '454'}, page_content='of data generated everyday through tracked transactions such\\nas barcode scanner, online purchase, and inventory tracking\\nsystems has enabled for machine learning systems to learn\\nfrom this wealth of data. We will discuss the methods for\\nfinding useful associations in large databases by using simple\\nstatistical performance measures while managing the\\npeculiarities of working with such transactional data. One\\nsignificant challenge in working with the large volume of data\\nis that it may be computationally very expensive to discover\\npatterns from such data. Moreover, there may be cases when\\nsome of the associations occurred by chance, which can lead\\nto potentially false knowledge. While discussing the\\nassociation analysis, we will discuss both these points.\\nWe will use the transaction data in the table below for our\\nexamples of association analysis. This simplified version of\\nthe market basket data will show how the association rules can\\nbe effectively used for the market basket analysis.\\n9.5.1 Definition of common terms\\nLet us understand few common terminologies used in\\nassociation analysis.\\n9.5.1.1 Itemset\\nOne or more items are grouped together and are surrounded by\\nbrackets to indicate that they form a set, or more specifically,\\nan itemset that appears in the data with some regularity. For\\nexample, in Table 9.3, {Bread, Milk, Egg} can be grouped\\ntogether to form an itemset as those are frequently bought\\ntogether. To generalize this concept, if I = {i , i ,…, i } are the\\nitems in a market basket data and T = {t , t ,…, t } are the set\\nof all the transactions, then each transaction t contains a\\nsubset of items from I. A collection of zero or more items is\\n1 2 n\\n1 2 n\\ni'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 454, 'page_label': '455'}, page_content='called an itemset. A null itemset is the one which does not\\ncontain any item. In the association analysis, an itemset is\\ncalled k-itemset if it contains k number of items. Thus, the\\nitemset {Bread, Milk, Egg} is a three-itemset.\\n \\nTable 9.3 Market Basket Transaction Data\\n9.5.1.2 Support count\\nSupport count denotes the number of transactions in which a\\nparticular itemset is present. This is a very important property\\nof an itemset as it denotes the frequency of occurrence for the\\nitemset. This is expressed as\\nwhere |{}| denotes the number of elements in a set\\nIn Table 9.3, the itemset {Bread, Milk, Egg} occurs\\ntogether three times and thus have a support count of 3.\\n9.5.2 Association rule'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 455, 'page_label': '456'}, page_content='The result of the market basket analysis is expressed as a set of\\nassociation rules that specify patterns of relationships among\\nitems. A typical rule might be expressed as{Bread, Milk}\\n→{Egg}, which denotes that if Bread and Milk are purchased,\\nthen Egg is also likely to be purchased. Thus, association rules\\nare learned from subsets of itemsets. For example, the\\npreceding rule was identified from the set of {Bread, Milk,\\nEgg}.\\nIt should be noted that an association rule is an expression\\nof X → Y where X and Y are disjoint itemsets, i.e. X ∩ Y = 0.\\nSupport and confidence are the two concepts that are used\\nfor measuring the strength of an association rule. Support\\ndenotes how often a rule is applicable to a given data set.\\nConfidence indicates how often the items in Y appear in\\ntransactions that contain X in a total transaction of N.\\nConfidence denotes the predictive power or accuracy of the\\nrule. So, the mathematical expressions are\\nIn our data set 9.3, if we consider the association rule\\n{Bread, Milk} → {Egg}, then from the above formula'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 456, 'page_label': '457'}, page_content='It is important to understand the role of support and\\nconfidence in the association analysis. A low support may\\nindicate that the rule has occurred by chance. Also, from its\\napplication perspective, this rule may not be a very attractive\\nbusiness investment as the items are seldom bought together\\nby the customers. Thus, support can provide the intelligence of\\nidentifying the most interesting rules for analysis.\\nSimilarly, confidence provides the measurement for\\nreliability of the inference of a rule. Higher confidence of a\\nrule X → Y denotes more likelihood of to be present in\\ntransactions that contain X as it is the estimate of the\\nconditional probability of Y given X.\\nAlso, understand that the confidence of X leading to Y is not\\nthe same as the confidence of Y leading to X. In our example,\\nconfidence of {Bread, Milk} → {Egg} = 0.75 but confidence\\nof \\n  Here, the rule {Bread,\\nMilk} → {Egg} is the strong rule.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 457, 'page_label': '458'}, page_content='Association rules were developed in the context of Big Data\\nand data science and are not used for prediction. They are used\\nfor unsupervised knowledge discovery in large databases,\\nunlike the classification and numeric prediction algorithms.\\nStill we will find that association rule learners are closely\\nrelated to and share many features of the classification rule\\nlearners. As association rule learners are unsupervised, there is\\nno need for the algorithm to be trained; this means that no\\nprior labelling of the data is required. The programme is\\nsimply run on a data set in the hope that interesting\\nassociations are found.\\nObviously, the downside is that there is not an easy way to\\nobjectively measure the performance of a rule learner, aside\\nfrom evaluating them for qualitative usefulness. Also, note\\nthat the association rule analysis is used to search for\\ninteresting connections among a very large number of\\nvariables. Though human beings are capable of such insight\\nquite intuitively, sometimes it requires expert-level knowledge\\nor a great deal of experience to achieve the performance of a\\nrule-learning algorithm. Additionally, some data may be too\\nlarge or complex for humans to decipher and analyse so easily.\\n9.5.3 The apriori algorithm for association rule learning\\nAs discussed earlier, the main challenge of discovering an\\nassociation rule and learning from it is the large volume of\\ntransactional data and the related complexity. Because of the\\nvariation of features in transactional data, the number of\\nfeature sets within a data set usually becomes very large. This\\nleads to the problem of handling a very large number of\\nitemsets, which grows exponentially with the number of\\nfeatures. If there are k items which may or may not be part of\\nan itemset, then there is 2 ways of creating itemsets withk'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 458, 'page_label': '459'}, page_content='those items. For example, if a seller is dealing with 100\\ndifferent items, then the learner need to evaluate 2  = 1 × e\\nitemsets for arriving at the rule, which is computationally\\nimpossible. So, it is important to filter out the most important\\n(and thus manageable in size) itemsets and use the resources\\non those to arrive at the reasonably efficient association rules.\\nThe first step for us is to decide the minimum support and\\nminimum confidence of the association rules. From a set of\\ntransaction T, let us assume that we will find out all the rules\\nthat have support ≥ minS and confidence ≥ minC, where minS\\nand minC are the support and confidence thresholds,\\nrespectively, for the rules to be considered acceptable. Now,\\neven if we put the minS = 20% and minC = 50%, it is seen that\\nmore than 80% of the rules are discarded; this means that a\\nlarge portion of the computational efforts could have been\\navoided if the itemsets for consideration were first pruned and\\nthe itemsets which cannot generate association rules with\\nreasonable support and confidence were removed. The\\napproach to achieve this goal is discussed below.\\nStep 1: decouple the support and confidence requirements.\\nAccording to formula 9.8, the support of the rule X → Y is\\ndependent only on the support of its corresponding itemsets.\\nFor example, all the below rules have the same support as their\\nitemsets are the same {Bread, Milk, Egg}:\\n{Bread, Milk} → {Egg}\\n{Bread, Egg} → {Milk}\\n{Egg, Milk} → {Bread}\\n{Bread} → {Egg, Milk}\\n{Milk} → {Bread, Egg}\\n{Egg} → {Bread, Milk}\\nSo, the same treatment can be applied to this association\\nrule on the basis of the frequency of the itemset. In this case, if\\n100 30'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 459, 'page_label': '460'}, page_content='the itemset {Bread, Milk, Egg} is rare in the basket\\ntransactions, then all these six rules can be discarded without\\ncomputing their individual support and confidence values.\\nThis identifies some important strategies for arriving at the\\nassociation rules:\\n1. Generate Frequent Itemset: Once the minS is set for a particular\\nassignment, identify all the itemsets that satisfy minS. These itemsets\\nare called frequent itemsets.\\n2. Generate Rules: From the frequent itemsets found in the previous step,\\ndiscover all the high confidence rules. These are called strong rules.\\nPlease note that the computation requirement for identifying\\nfrequent itemsets is more intense than the rule generation. So,\\ndifferent techniques have been evolved to optimize the\\nperformance for frequent itemset generation as well as rule\\ndiscovery as discussed in the next section.\\n9.5.4 Build the apriori principle rules\\nOne of the most widely used algorithm to reduce the number\\nof itemsets to search for the association rule is known as\\nApriori. It has proven to be successful in simplifying the\\nassociation rule learning to a great extent. The principle got its\\nname from the fact that the algorithm utilizes a simple prior\\nbelief (i.e. a priori) about the properties of frequent itemsets:\\nIf an itemset is frequent, then all of its subsets must also be\\nfrequent.\\nThis principle significantly restricts the number of itemsets\\nto be searched for rule generation. For example, if in a market\\nbasket analysis, it is found that an item like ‘Salt’ is not so\\nfrequently bought along with the breakfast items, then it is fine\\nto remove all the itemsets containing salt for rule generation as'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 460, 'page_label': '461'}, page_content='their contribution to the support and confidence of the rule will\\nbe insignificant.\\nThe converse also holds true:\\nIf an itemset is frequent, then all the supersets must be\\nfrequent too.\\nThese are very powerful principles which help in pruning\\nthe exponential search space based on the support measure and\\nis known as support-based pruning. The key property of the\\nsupport measure used here is that the support for an itemset\\nnever exceeds the support for its subsets. This is also known as\\nthe anti-monotone property of the support measure.\\nLet us use the transaction data in Table 9.3 to illustrate the\\nApriori principle and its use. From the full itemset of six items\\n{Bread, Milk, Egg, Butter, Salt, Apple}, there are 2  ways to\\ncreate baskets or itemsets (including the null itemset) as shown\\nin Figure 9.15:\\nFIG. 9.15 Sixty-four ways to create itemsets from 6 items\\n6'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 461, 'page_label': '462'}, page_content='Without applying any filtering logic, the brute-force\\napproach would involve calculating the support count for each\\nitemset in Figure 9.16. Thus, by comparing each item in the\\ngenerated itemset with the actual transactions mentioned in\\nTable 9.3, we can determine the support count of the itemset.\\nFor example, if {Bread, Milk} is present in any transactions in\\nTable 9.3, then its support count will be incremented by 1. As\\nwe can understand, this is a very computation heavy activity,\\nand as discussed earlier, many of the computations may get\\nwasted at a later point of time because some itemsets will be\\nfound to be infrequent in the transactions. To get an idea of the\\ntotal computations to be done, the number of comparisons to\\nbe done is T × N × L, where T is the number of transactions (6\\nin our case), N is the number of candidate itemsets (64 in our\\ncase), and L is the maximum transaction width (6 in our case).\\nFIG. 9.16 Discarding the itemsets consisting of Salt\\nLet us apply the Apriori principle on this data set to reduce\\nthe number of candidate itemsets (N). We could identify from\\nthe transaction Table 9.3 that Salt is an infrequent item. So, by'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 462, 'page_label': '463'}, page_content='applying the Apriori principle, we can say that all the itemsets\\nwhich are superset of Salt will be infrequent and thus can be\\ndiscarded from comparison to discover the association rule as\\nshown in Figure 9.16.\\nThis approach reduces the computation effort for a good\\nnumber of itemsets and will make our search process more\\nefficient. Thus, in each such iteration, we can determine the\\nsupport count of each itemset, and on the basis of the min\\nsupport value fixed for our analysis, any itemset in the\\nhierarchy that does not meet the min support criteria can be\\ndiscarded to make the rule generation faster and easier.\\nTo generalize the example in order to build a set of rules\\nwith the Apriori principle, we will use the Apriori principle\\nthat states that all subsets of a frequent itemset must also be\\nfrequent. In other words, if {X, Y} is frequent, then both {X}\\nand {Y} must be frequent. Also by definition, the support\\nmetric indicates how frequently an itemset appears in the data.\\nThus, if we know that {X} does not meet a desired support\\nthreshold, there is no reason to consider {X, Y} or any itemset\\ncontaining {X}; it cannot possibly be frequent.\\nThis logic of the Apriori algorithm excludes potential\\nassociation rules prior to actually evaluating them. The actual\\nprocess of creating rules involves two phases:\\nIdentifying all itemsets that meet a minimum support threshold set for the\\nanalysis\\nCreating rules from these itemsets that meet a minimum confidence\\nthreshold which identifies the strong rules\\nThe first phase involves multiple iterations where each\\nsuccessive iteration requires evaluating the support of storing a\\nset of increasingly large itemsets. For instance, iteration 1'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 463, 'page_label': '464'}, page_content='evaluates the set of one-item itemsets (one-itemsets), iteration\\n2 involves evaluating the two-itemsets, etc.. The result of each\\niteration N is a set of all N-itemsets that meet the minimum\\nsupport threshold. Normally, all the itemsets from iteration N\\nare combined in order to generate candidate itemsets for\\nevaluation in iteration N + 1, but by applying the Apriori\\nprinciple, we can eliminate some of them even before the next\\niteration starts. If {X}, {Y}, and {Z} are frequent in iteration 1\\nwhile {W} is not frequent, then iteration 2 will consider only\\n{X, Y}, {X, Z}, and {Y, Z}. We can see that the algorithm\\nneeds to evaluate only three itemsets rather than the six that\\nwould have been evaluated if sets containing W had not been\\neliminated by apriori.\\nBy continuing with the iterations, let us assume that during\\niteration 2, it is discovered that {X, Y} and {Y, Z} are frequent,\\nbut {X, Z} is not. Although iteration 3 would normally begin\\nby evaluating the support for {X, Y, Z}, this step need not\\noccur at all. The Apriori principle states that {X, Y, Z} cannot\\nbe frequent, because the subset {X, Z} is not. Therefore, in\\niteration 3, the algorithm may stop as no new itemset can be\\ngenerated.\\nOnce we identify the qualifying itemsets for analysis, the\\nsecond phase of the Apriori algorithm begins. For the given set\\nof frequent itemsets, association rules are generated from all\\npossible subsets. For example, {X, Y} would result in\\ncandidate rules for {X} → {Y} and {Y} → {X}. These rules\\nare evaluated against a minimum confidence threshold, and\\nany rule that does not meet the desired confidence level is\\neliminated, thus finally yielding the set of strong rules.\\nThough the Apriori principle is widely used in the market\\nbasket analysis and other applications of association rule help'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 464, 'page_label': '465'}, page_content='in the discovery of new relationship among objects, there are\\ncertain strengths and weaknesses we need to keep in mind\\nbefore employing it over the target data set:\\n9.6 SUMMARY\\nUnsupervised learning is a machine learning concept where the unlabelled\\nand unclassified information is analysed to discover hidden knowledge. The\\nalgorithm works on the data without any prior training, but they are\\nconstructed in such a way that they can identify patterns, groupings, sorting\\norder, and numerous other interesting knowledge from the set of data.\\nClustering refers to a broad set of techniques for finding subgroups, or\\nclusters, in a data set based on the characteristics of the objects within the\\ndata set itself in such a manner that the objects within the group are similar\\n(or related to each other) but are different from (or unrelated to) the objects\\nfrom the other groups.\\nThe major clustering techniques are classified in three board categories\\nPartitioning methods,\\nHierarchical methods, and\\nDensity-based methods.\\nk-means and k-medoids are the most popular partitioning techniques.\\nThe principle of k-means algorithm is to assign each of the n data points to\\none of the k clusters, where k is a user-defined parameter as the number of\\nclusters desired. The objective is to maximize the homogeneity within the\\nclusters and to maximize the differences between the clusters. The\\nhomogeneity and differences are measured in terms of the distance between\\nthe points.\\nk-medoids considers representative data points from the existing points in the\\ndata set as the centre of the clusters. It then assigns the data points according\\nto their distance from these centres to form the clusters.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 465, 'page_label': '466'}, page_content='The hierarchical clustering methods are used to group the data into hierarchy\\nor tree-like structure.\\nThere are two main hierarchical clustering methods: agglomerative\\nclustering and divisive clustering. Agglomerative clustering is a bottom-up\\ntechnique which starts with individual objects as clusters and then iteratively\\nmerges them to form larger clusters. On the other hand, the divisive method\\nstarts with one cluster with all given objects and then splits it iteratively to\\nform smaller clusters.\\nDBSCAN is one of the density-based clustering approaches that provide a\\nsolution to identify clusters of arbitrary shapes. The principle is based on\\nidentifying the dense area and sparse area within the data set and then\\nrunning the clustering algorithm.\\nAssociation rule presents a methodology that is useful for identifying\\ninteresting relationships hidden in large data sets. It is also known as\\nassociation analysis, and the discovered relationships can be represented in\\nthe form of association rules comprising a set of frequent items.\\nA common application of this analysis is the Market Basket Analysis that\\nretailers use for cross-selling of their products.\\nSAMPLE QUESTIONS\\nMULTIPLE CHOICE QUESTIONS (1 MARK EACH)\\n1. k-means clustering algorithm is an example of which type of clustering\\nmethod?\\n1. Hierarchical\\n2. Partitioning\\n3. Density based\\n4. Random\\n2. Which of the below statement describes the difference between\\nagglomerative and divisive clustering techniques correctly?\\n1. Agglomerative is a bottom-up technique, but divisive is a top-\\ndown technique\\n2. Agglomerative is a top-down technique, but divisive is a bottom-\\nup technique\\n3. Agglomerative technique can start with a single cluster\\n4. Divisive technique can end with a single cluster\\n3. Which of the below is an advantage of k-medoids algorithm over k-\\nmeans algorithm?\\n1. both are equally error prone\\n2. k-medoids can handle larger data set than k-means\\n3. k-medoids helps in reducing the effect of the outliers in the\\nobjects\\n4. k-medoids needs less computation to arrive at the final clustering'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 466, 'page_label': '467'}, page_content='4. The principle underlying the Market Basket Analysis is known as\\n1. Association rule\\n2. Bisecting rule\\n3. k-means\\n4. Bayes’ theorem\\n5. A Voronoi diagram is used in which type of clustering?\\n1. Hierarchical\\n2. Partitioning\\n3. Density based\\n4. Intuition based\\n6. SSE of a clustering measures:\\n1. Initial number of set clusters\\n2. Number of clusters generated\\n3. Cost of clustering\\n4. Quality of clustering\\n7. One of the disadvantages of k-means algorithm is that the outliers may\\nreduce the quality of the final clustering.\\n1. True\\n2. False\\n8. Which of the following can be possible termination conditions in K-\\nMeans?\\n1. For a fixed number of iterations.\\n2. Assignment of observations to clusters does not change between\\niterations. Except for cases with a bad local minimum.\\n3. Centroids do not change between successive iterations.\\n4. All of the above\\n9. Which of the following clustering algorithm is most sensitive to\\noutliers?\\n1. K-means clustering algorithm\\n2. K-medians clustering algorithm\\n3. K-medoids clustering algorithm\\n4. K-modes clustering algorithm\\n10. In which of the following situations the K-Means clustering fails to give\\ngood results?\\n1. Data points with outliers\\n2. Data points with different densities\\n3. Data points with round shapes\\n4. All of the above\\nSHORT ANSWER-TYPE QUESTIONS (5 MARKS EACH)\\n1. How unsupervised learning is different from supervised learning?\\nExplain with some examples.\\n2. Mention few application areas of unsupervised learning.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 467, 'page_label': '468'}, page_content='3. What are the broad three categories of clustering techniques? Explain\\nthe characteristics of each briefly.\\n4. Describe how the quality of clustering is measured in the k-means\\nalgorithm?\\n5. Describe the main difference in the approach of k-means and k-medoids\\nalgorithms with a neat diagram.\\n6. What is a dendrogram? Explain its use.\\n7. What is SSE? What is its use in the context of the k-means algorithm?\\n8. Explain the k-means method with a step-by-step algorithm.\\n9. Describe the concept of single link and complete link in the context of\\nhierarchical clustering.\\n10. How apriori principle helps in reducing the calculation overhead for a\\nmarket basket analysis? Provide an example to explain.\\nLONG ANSWER-TYPE QUESTIONS (10 MARKS EACH)\\n1. You are given a set of one-dimensional data points: {5, 10, 15, 20, 25,\\n30, 35}. Assume that k = 2 and first set of random centroid is selected\\nas {15, 32} and then it is refined with {12, 30}.\\n1. Create two clusters with each set of centroid mentioned above\\nfollowing the k-means approach\\n2. Calculate the SSE for each set of centroid\\n2. Explain how the Market Basket Analysis uses the concepts of\\nassociation analysis.\\n3. Explain the Apriori algorithm for association rule learning with an\\nexample.\\n4. How the distance between clusters is measured in hierarchical\\nclustering? Explain the use of this measure in making decision on when\\nto stop the iteration.\\n5. How to recompute the cluster centroids in the k-means algorithm?\\n6. Discuss one technique to choose the appropriate number of clusters at\\nthe beginning of clustering exercise.\\n7. Discuss the strengths and weaknesses of the k-means algorithm.\\n8. Explain the concept of clustering with a neat diagram.\\n9. During a research work, you found 7 observations as described with the\\ndata points below. You want to create 3 clusters from these observations\\nusing K-means algorithm. After first iteration, the clusters C1, C2, C3\\nhas following observations:\\nC1: {(2,2), (4,4), (6,6)}\\nC2: {(0,4), (4,0)}\\nC3: {(5,5), (9,9)}\\nIf you want to run a second iteration then what will be the cluster\\ncentroids? What will be the SSE of this clustering?'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 468, 'page_label': '469'}, page_content='10. In a software project, the team is trying to identify the similarity of\\nsoftware defects identified during testing. They wanted to create 5\\nclusters of similar defects based on the text analytics of the defect\\ndescriptions. Once the 5 clusters of defects are identified, any new\\ndefect created is to be classified as one of the types identified through\\nclustering. Explain this approach through a neat diagram. Assume 20\\nDefect data points which are clustered among 5 clusters and k-means\\nalgorithm was used.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 469, 'page_label': '470'}, page_content='Chapter 10\\nBasics of Neural Network\\nOBJECTIVE OF THE CHAPTER :\\nIn the last 9 chapters, you have been introduced to the\\nconcepts of machine learning in great details. You started\\nwith how to decide whether a problem can be solved with\\nmachine learning, and once you have made that decision,\\nyou learnt how to start with the modelling of the problem\\nin the machine learning paradigm. In that context, you\\nwere introduced to three types of machine learning –\\nsupervised, unsupervised, and reinforcement. Then, you\\nexplored all different popular algorithms of supervised and\\nunsupervised learning. Now, you have gained quite some\\nbackground on machine learning basics, and it is time for\\nyou to get introduced to the concept of neural network.\\nWell, you have already seen right at the beginning of the\\nbook how the machine learning process maps with the\\nhuman learning process. Now, it is time to see how the\\nhuman nervous system has been mimicked in the computer\\nworld in the form of an artificial neural network or simply\\na neural network. This chapter gives a brief view of neural\\nnetworks and how it helps in different forms of learning.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 470, 'page_label': '471'}, page_content='10.1 INTRODUCTION\\nIn the previous chapters, we were slowly unveiling the\\nmystery of machine learning, i.e. how a machine learns to\\nperform tasks and improves with time from experience, by\\nexpert guidance or by itself. Machine learning, as we have\\nseen, mimics the human form of learning. On the other hand,\\nhuman learning, or for that matter every action of a human\\nbeing, is controlled by the nervous system. In any human\\nbeing, the nervous system coordinates the different actions by\\ntransmitting signals to and from different parts of the body.\\nThe nervous system is constituted of a special type of cell,\\ncalled neuron or nerve cell, which has special structures\\nallowing it to receive or send signals to other neurons.\\nNeurons connect with each other to transmit signals to or\\nreceive signals from other neurons. This structure essentially\\nforms a network of neurons or a neural network.\\nBy virtue of billions of networked neurons that it possesses,\\nthe biological neural network is a massively large and complex\\nparallel computing network. It is because of this massive\\nparallel computing network that the nervous system helps\\nhuman beings to perform actions or take decisions at a speed\\nand with such ease that the fastest supercomputer of the world\\nwill also be envy of. For example, let us think of the superb\\nflying catches taken by the fielders in the cricket world cup. It\\nis a combination of superior calculation based on past\\ncricketing experience, understanding of local on-ground\\nconditions, and anticipation of how hard the ball has been hit\\nthat the fielder takes the decision about when to jump, where\\nto jump, and how much to jump. This is a highly complex\\ntask, and you may think that not every human being is skilled\\nenough for such a magical action. In that case, let us think of'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 471, 'page_label': '472'}, page_content='something much simpler. Let us consider about a very simple,\\ndaily activity, namely swimming in the pool. Apparently,\\nswimming may look very trivial, but think about the parallel\\nactions and decisions that need to be taken to swim. It needs\\nthe right combination of body position, limb movement,\\nbreathe in/out, etc. to swim. To add to the challenge is the fact\\nthat water is few hundred times denser than air. So, to orient\\nthe body movement in that scale difference is a difficult\\nproposition in itself. Coordinating the actions and taking the\\ndecisions for such a complex task, which may appear to be\\ntrivial, are possible because of the massive parallel complex\\nnetwork, i.e. the neural network.\\nThe fascinating capability of the biological neural network\\nhas inspired the inception of artificial neural network (ANN).\\nAn ANN is made up of artificial neurons. In its most generic\\nform, an ANN is a machine designed to model the functioning\\nof the nervous system or, more specifically, the neurons. The\\nonly difference is that the biological form of neuron is\\nreplicated in the electronic or digital form of neuron. Digital\\nneurons or artificial neurons form the smallest processing units\\nof the ANNs. As we move on, let us first do a deep dive into\\nthe structure of the biological neuron and then try to see how\\nthat has been modelled in the artificial neuron.\\n10.2 UNDERSTANDING THE BIOLOGICAL NEURON\\nThe human nervous system has two main parts –\\nthe central nervous system (CNS) consisting of the brain and spinal cord\\nthe peripheral nervous system consisting of nerves and ganglia outside the\\nbrain and spinal cord.\\nThe CNS integrates all information, in the form of signals,\\nfrom the different parts of the body. The peripheral nervous\\nsystem, on the other hand, connects the CNS with the limbs'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 472, 'page_label': '473'}, page_content='and organs. Neurons are basic structural units of the CNS. A\\nneuron is able to receive, process, and transmit information in\\nthe form of chemical and electrical signals. Figure 10.1\\npresents the structure of a neuron. It has three main parts to\\ncarry out its primary functionality of receiving and\\ntransmitting information:\\n1. Dendrites – to receive signals from neighbouring neurons.\\n2. Soma – main body of the neuron which accumulates the signals coming\\nfrom the different dendrites. It ‘fires’ when a sufficient amount of signal\\nis accumulated.\\n3. Axon – last part of the neuron which receives signal from soma, once\\nthe neuron ‘fires’, and passes it on to the neighbouring neurons through\\nthe axon terminals (to the adjacent dendrite of the neighbouring\\nneurons).\\nThere is a very small gap between the axon terminal of one\\nneuron and the adjacent dendrite of the neighbouring neuron.\\nThis small gap is known as synapse. The signals transmitted\\nthrough synapse may be excitatory or inhibitory.\\nFIG. 10.1 Structure of biological neuron\\nPoints to Ponder:'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 473, 'page_label': '474'}, page_content='The adult human brain, which forms the main part of the\\ncentral nervous system, is approximately 1.3 kg in weight\\nand 1200 cm in volume. It is estimated to contain about\\n100 billion (i.e. 10 ) neurons and 10 times more glial or\\nglue cells. Glial cells act as support cells for the neurons. It\\nis believed that neurons represent about 10% of all cells in\\nthe brain. On an average, each neuron is connected to 10\\nof other neurons, which means that altogether there are\\n10  connections.\\nThe axon, a human neuron, is 10–12 μm in diameter.\\nEach synapse spans a gap of about a millionth of an inch\\nwide.\\n10.3 EXPLORING THE ARTIFICIAL NEURON\\nThe biological neural network has been modelled in the form\\nof ANN with artificial neurons simulating the function of\\nbiological neurons. As depicted in Figure 10.2, input signal x\\n(x , x , …, x ) comes to an artificial neuron. Each neuron has\\nthree major components:\\n1. A set of ‘i’ synapses having weight w. A signal x forms the input to the i-th\\nsynapse having weight w. The value of weight w may be positive or\\nnegative. A positive weight has an excitatory effect, while a negative weight\\nhas an inhibitory effect on the output of the summation junction, y .\\n2. A summation junction for the input signals is weighted by the respective\\nsynaptic weight. Because it is a linear combiner or adder of the weighted\\ninput signals, the output of the summation junction, y , can be expressed as\\nfollows:\\ni\\n1 2 n\\ni i\\ni i\\nsum\\nsum\\n3\\n11\\n5\\n16'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 474, 'page_label': '475'}, page_content='[Note: Typically, a neural network also includes a bias which adjusts the\\ninput of the activation function. However, for the sake of simplicity, we are\\nignoring bias for the time being. In the case of a bias ‘b’, the value of y\\nwould have been as follows:\\n3. A threshold activation function (or simply activation function, also called\\nsquashing function) results in an output signal only when an input signal\\nexceeding a specific threshold value comes as an input. It is similar in\\nbehaviour to the biological neuron which transmits the signal only when the\\ntotal input signal meets the firing threshold.\\nFIG. 10.2 Structure of an artificial neuron\\nOutput of the activation function, y , can be expressed as\\nfollows:\\n10.4 TYPES OF ACTIVATION FUNCTIONS\\nThere are different types of activation functions. The most\\ncommonly used activation functions are highlighted below.\\nsum\\nout'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 475, 'page_label': '476'}, page_content='10.4.1 Identity function\\nIdentity function is used as an activation function for the input\\nlayer. It is a linear function having the form\\nAs obvious, the output remains the same as the input.\\n10.4.2 Threshold/step function\\nStep/threshold function is a commonly used activation\\nfunction. As depicted in Figure 10.3a, step function gives 1 as\\noutput if the input is either 0 or positive. If the input is\\nnegative, the step function gives 0 as output. Expressing\\nmathematically,\\nFIG. 10.3 Step and threshold functions'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 476, 'page_label': '477'}, page_content='The threshold function (depicted in Fig. 10.3b) is almost\\nlike the step function, with the only difference being the fact\\nthat θ is used as a threshold value instead of 0. Expressing\\nmathematically,\\n10.4.3 ReLU (Rectified Linear Unit) function\\nReLU is the most popularly used activation function in the\\nareas of convolutional neural networks and deep learning. It is\\nof the form\\nThis means that f(x) is zero when x is less than zero and f(x)\\nis equal to x when x is above or equal to zero. Figure 10.4\\ndepicts the curve for a ReLU activation function.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 477, 'page_label': '478'}, page_content='FIG. 10.4 ReLU function\\nThis function is differentiable, except at a single point x = 0.\\nIn that sense, the derivative of a ReLU is actually a sub-\\nderivative.\\n10.4.4 Sigmoid function\\nSigmoid function, depicted in Figure 10.5, is by far the most\\ncommonly used activation function in neural networks. The\\nneed for sigmoid function stems from the fact that many\\nlearning algorithms require the activation function to be\\ndifferentiable and hence continuous. Step function is not\\nsuitable in those situations as it is not continuous. There are\\ntwo types of sigmoid function:\\n1. Binary sigmoid function\\n2. Bipolar sigmoid function\\n10.4.4.1 Binary sigmoid function\\nA binary sigmoid function, depicted in Figure 10.5a, is of the\\nform'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 478, 'page_label': '479'}, page_content='where k = steepness or slope parameter of the sigmoid\\nfunction. By varying the value of k, sigmoid functions with\\ndifferent slopes can be obtained. It has range of (0, 1).\\nFIG. 10.5 Sigmoid function\\nThe slope at origin is k/4. As the value of k becomes very\\nlarge, the sigmoid function becomes a threshold function.\\n10.4.4.2 Bipolar sigmoid function\\nA bipolar sigmoid function, depicted in Figure 10.5b, is of the\\nform'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 479, 'page_label': '480'}, page_content='The range of values of sigmoid functions can be varied\\ndepending on the application. However, the range of (−1, +1)\\nis most commonly adopted.\\n10.4.5 Hyperbolic tangent function\\nHyperbolic tangent function is another continuous activation\\nfunction, which is bipolar in nature. It is a widely adopted\\nactivation function for a special type of neural network known\\nas backpropagation network (discussed elaborately in Section\\n10.8). The hyperbolic tangent function is of the form\\nThis function is similar to the bipolar sigmoid function.\\nNote that all the activation functions defined in Sections\\n10.4.2 and 10.4.4 have values ranging between 0 and 1.\\nHowever, in some cases, it is desirable to have values ranging\\nfrom −1 to +1. In that case, there will be a need to reframe the\\nactivation function. For example, in the case of step function,\\nthe revised definition would be as follows:\\nDid you know?'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 480, 'page_label': '481'}, page_content='Works related to neural network dates long back to the\\n1940s. Warren McCulloch and Walter Pitts in 1943 created\\na computational network inspired by the biological\\nprocesses in the brain. This model paved the way for\\nneural networks.\\nThe next notable work in the area of neural network was\\nin the late 1940s by the Canadian psychologist Donald\\nOlding Hebb. He proposed a learning hypothesis based on\\nthe neurons and synaptic connection between neurons.\\nThis became popular as Hebbian learning.\\nIn 1958, the American psychologist Frank Rosenblatt\\nrefined the Hebbian concept and evolved the concept of\\nperceptron. It was almost at the same time, in 1960, that\\nProfessor Bernard Widrow of Stanford University came up\\nwith an early single-layer ANN named ADALINE\\n(Adaptive Linear Neuron or later Adaptive Linear\\nElement). Marvin Lee Minsky and Seymour Aubrey\\nPapert stated two key issues of perceptron in their research\\npaper in 1969. The first issue stated was the\\ninability of perceptron of processing the XOR (exclusive-\\nOR) circuit. The other issue was lack of processing power\\nof computers to effectively handle the large neural\\nnetworks.\\nFrom here till the middle of the 1970s, there was a\\nslowdown in the research work related to neural networks.\\nThere was a renewed interest generated in neural networks\\nand learning with Werbos’ backpropagation algorithm in\\n1975. Slowly, as the computing ability of the computers\\nincreased drastically, a lot of research on neural network\\nhas been conducted in the later decades. Neural network'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 481, 'page_label': '482'}, page_content='evolved further as deep neural networks and started to be\\nimplemented in solving large-scale learning problems such\\nas image recognition, thereby getting a more popular name\\nas deep learning.\\n10.5 EARLY IMPLEMENTATIONS OF ANN\\n10.5.1 McCulloch–Pitts model of neuron\\nThe McCulloch–Pitts neural model (depicted in Fig. 10.6),\\nwhich was the earliest ANN model, has only two types of\\ninputs – excitatory and inhibitory. The excitatory inputs have\\nweights of positive magnitude and the inhibitory weights have\\nweights of negative magnitude. The inputs of the McCulloch–\\nPitts neuron could be either 0 or 1. It has a threshold function\\nas activation function. So, the output signal y  is 1 if the\\ninput y  is greater than or equal to a given threshold value,\\nelse 0.\\nSimple McCulloch–Pitts neurons can be used to design\\nlogical operations. For that purpose, the connection weights\\nneed to be correctly decided along with the threshold function\\n(rather the threshold value of the activation function). Let us\\ntake a small example.\\nout\\nsum'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 482, 'page_label': '483'}, page_content='FIG. 10.6 McCulloch–Pitts neuron\\nJohn carries an umbrella if it is sunny or if it is raining.\\nThere are four given situations. We need to decide when John\\nwill carry the umbrella. The situations are as follows:\\nSituation 1 – It is not raining nor is it sunny.\\nSituation 2 – It is not raining, but it is sunny.\\nSituation 3 – It is raining, and it is not sunny.\\nSituation 4 – Wow, it is so strange! It is raining as well as it is sunny.\\nTo analyse the situations using the McCulloch–Pitts neural\\nmodel, we can consider the input signals as follows:\\nx  → Is it raining?\\nx  → Is it sunny?\\nSo, the value of both x  and x  can be either 0 or 1. We can\\nuse the value of both weights x  and x  as 1 and a threshold\\nvalue of the activation function as 1. So, the neural model will\\nlook as Figure 10.7a.\\n1\\n2\\n1 2\\n1 2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 483, 'page_label': '484'}, page_content='FIG. 10.7 McCulloch–Pitts neural model (illustration)\\nFormally, we can say,\\nThe truth table built with respect to the problem is depicted\\nin Figure 10.7b. From the truth table, we can conclude that in\\nthe situations where the value of y  is 1, John needs to carry\\nan umbrella. Hence, he will need to carry an umbrella in\\nsituations 2, 3, and 4. Surprised, as it looks like a typical logic\\nproblem related to ‘OR’ function? Do not worry, it is really an\\nimplementation of logical OR using the McCulloch–Pitts\\nneural model.\\n10.5.2 Rosenblatt’s perceptron\\nRosenblatt’s perceptron is built around the McCulloch–Pitts\\nneural model. The perceptron, as depicted in Figure 10.7,\\nreceives a set of input x , x ,…, x . The linear combiner or the\\nadder node computes the linear combination of the inputs\\napplied to the synapses with synaptic weights being w, w,\\n…, w. Then, the hard limiter checks whether the resulting\\nout\\n1 2 n\\n1 2\\nn'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 484, 'page_label': '485'}, page_content='sum is positive or negative. If the input of the hard limiter\\nnode is positive, the output is +1, and if the input is negative,\\nthe output is −1. Mathematically, the hard limiter input is\\nHowever, perceptron includes an adjustable value or bias as\\nan additional weight w. This additional weight w is attached\\nto a dummy input x , which is always assigned a value of 1.\\nThis consideration modifies the above equation to\\nThe output is decided by the expression\\nThe objective of perceptron is to classify a set of inputs into\\ntwo classes, c  and c . This can be done using a very simple\\ndecision rule – assign the inputs x , x , x , …, x  to c  if the\\noutput of the perceptron, i.e. y , is +1 and c  if y  is −1. So,\\nfor an n-dimensional signal space, i.e. a space for ‘n’ input\\nsignals x , x , x , …, x , the simplest form of perceptron will\\nhave two decision regions, resembling two classes, separated\\nby a hyperplane defined by\\n0 0\\n0\\n1 2\\n0 1 2 n 1\\nout 2 out\\n0 1 2 n'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 485, 'page_label': '486'}, page_content='FIG. 10.8 Rosenblatt’s perceptron\\nTherefore, for two input signals denoted by variables x  and\\nx , the decision boundary is a straight line of the form\\nSo, for a perceptron having the values of synaptic weights\\nw, w, and w as −2, ½, and ¼, respectively, the linear\\ndecision boundary will be of the form\\nSo, any point (x , x ) which lies above the decision\\nboundary, as depicted by Figure 10.9, will be assigned to class\\nc  and the points which lie below the boundary are assigned to\\nclass c .\\n1\\n2\\n0 1 2\\n1 2\\n1\\n2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 486, 'page_label': '487'}, page_content='FIG. 10.9 Perceptron decision boundary\\nLet us examine if this perceptron is able to classify a set of\\npoints given below:\\np  = (5, 2) and p  = (−1, 12) belonging to c\\np  = (3, −5) and p  = (−2, −1) belonging to c\\nAs depicted in Figure 10.10, we can see that on the basis of\\nactivation function output, only points p  and p  generate an\\noutput of 1. Hence, they are assigned to class c  as expected.\\nOn the other hand, p  and p  points having activation function\\noutput as negative generate an output of 0. Hence, they are\\nassigned to class c , again as expected.\\nFIG. 10.10 Class assignment through perceptron\\n1 2 1\\n3 4 2\\n1 2\\n1\\n3 4\\n2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 487, 'page_label': '488'}, page_content='The same classification is obtained by mapping the points in\\nthe input space, as shown in Figure 10.11.\\nFIG. 10.11 Classification by decision boundary\\nThus, we can see that for a data set with linearly separable\\nclasses, perceptrons can always be employed to solve\\nclassification problems using decision lines (for two-\\ndimensional space), decision planes (for three-dimensional\\nspace), or decision hyperplanes (for n-dimensional space).\\nAppropriate values of the synaptic weights w, w, w, …,\\nw can be obtained by training a perceptron. However, one\\nassumption for perceptron to work properly is that the two\\nclasses should be linearly separable (as depicted in Figure\\n10.12a), i.e. the classes should be sufficiently separated from\\neach other. Otherwise, if the classes are non-linearly separable\\n(as depicted in Figure 10.12b), then the classification problem\\ncannot be solved by perceptron.\\n0 1 2\\nn'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 488, 'page_label': '489'}, page_content='FIG. 10.12 Class separability\\n10.5.2.1 Multi-layer perceptron\\nA basic perceptron works very successfully for data sets which\\npossess linearly separable patterns. However, in practical\\nsituation, that is an ideal situation to have. This was exactly\\nthe point driven by Minsky and Papert in their work (1969).\\nThey showed that a basic perceptron is not able to learn to\\ncompute even a simple 2-bit XOR. Why is that so? Let us try\\nto understand.\\nFigure 10.13 is the truth table highlighting output of a 2-bit\\nXOR function.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 489, 'page_label': '490'}, page_content='FIG. 10.13 Class separability of XOR function output\\nAs we can see in Figure 10.13, the data is not linearly\\nseparable. Only a curved decision boundary can separate the\\nclasses properly.\\nTo address this issue, the other option is to use two decision\\nlines in place of one. Figure 10.14 shows how a linear decision\\nboundary with two decision lines can clearly partition the data.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 490, 'page_label': '491'}, page_content='FIG. 10.14 Classification with two decision lines in XOR function output\\nThis is the philosophy used to design the multi-layer\\nperceptron model. The major highlights of this model are as\\nfollows:\\nThe neural network contains one or more intermediate layers between the\\ninput and the output nodes, which are hidden from both input and output\\nnodes.\\nEach neuron in the network includes a non-linear activation function that is\\ndifferentiable.\\nThe neurons in each layer are connected with some or all the neurons in the\\nprevious layer.\\nThe diagram in Figure 10.15 resembles a fully connected\\nmulti-layer perceptron with multiple hidden layers between the\\ninput and output layers. It is called fully connected because\\nany neuron in any layer of the perceptron is connected with all\\nneurons (or input nodes in the case of the first hidden layer) in\\nthe previous layer. The signals flow from one layer to another\\nlayer from left to right.\\n10.5.3 ADALINE network model'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 491, 'page_label': '492'}, page_content='Adaptive Linear Neural Element (ADALINE) is an early\\nsingle-layer ANN developed by Professor Bernard Widrow of\\nStanford University. As depicted in Figure 10.16, it has only\\noutput neuron. The output value can be +1 or −1. A bias input\\nx  (where x  = 1) having a weight w is added. The activation\\nfunction is such that if the weighted sum is positive or 0, then\\nthe output is 1, else it is −1. Formally, we can say,\\nFIG. 10.15 Multi-layer perceptron\\n0 0 0'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 492, 'page_label': '493'}, page_content='FIG. 10.16 ADALINE network\\nThe supervised learning algorithm adopted by the\\nADALINE network is known as Least Mean Square (LMS) or\\nDelta rule.\\nA network combining a number of ADALINEs is termed as\\nMADALINE (many ADALINE). MADALINE networks can\\nbe used to solve problems related to nonlinear separability.\\nNote:\\nBoth perceptron and ADALINE are neural network\\nmodels. Both of them are classifiers for binary\\nclassification. They have linear decision boundary and use\\na threshold activation function.\\n10.6 ARCHITECTURES OF NEURAL NETWORK\\nAs we have seen till now, ANN is a computational system\\nconsisting of a large number of interconnected units called'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 493, 'page_label': '494'}, page_content='artificial neurons. The connection between artificial neurons\\ncan transmit signal from one neuron to another. There are\\nmultiple possibilities for connecting the neurons based on\\nwhich architecture we are going to adopt for a specific\\nsolution. Some of the choices are listed below:\\nThere may be just two layers of neuron in the network – the input and output\\nlayer.\\nOther than the input and output layers, there may be one or more\\nintermediate ‘hidden’ layers of neuron.\\nThe neurons may be connected with one or more of the neurons in the next\\nlayer.\\nThe neurons may be connected with all neurons in the next layer.\\nThere may be single or multiple output signals. If there are multiple output\\nsignals, they might be connected with each other.\\nThe output from one layer may become input to neurons in the same or\\npreceding layer.\\n10.6.1 Single-layer feed forward network\\nSingle-layer feed forward is the simplest and most basic\\narchitecture of ANNs. It consists of only two layers as\\ndepicted in Figure 10.17 – the input layer and the output layer.\\nThe input layer consists of a set of ‘m’ input neurons X , X ,\\n…, X  connected to each of the ‘n’ output neurons Y , Y , …,\\nY . The connections carry weights w , w , …, w . The input\\nlayer of neurons does not conduct any processing – they pass\\nthe input signals to the output neurons. The computations are\\nperformed only by the neurons in the output layer. So, though\\nit has two layers of neurons, only one layer is performing the\\ncomputation. This is the reason why the network is known as\\nsingle layer in spite of having two layers of neurons. Also, the\\nsignals always flow from the input layer to the output layer.\\nHence, this network is known as feed forward.\\nThe net signal input to the output neurons is given by\\n1 2\\nm 1 2\\nn 11 12 mn'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 494, 'page_label': '495'}, page_content='for the k-th output neuron. The signal output from each\\noutput neuron will depend on the activation function used.\\nFIG. 10.17 Single-layer feed forward\\n10.6.2 Multi-layer feed forward ANNs\\nThe multi-layer feed forward network is quite similar to the\\nsingle-layer feed forward network, except for the fact that\\nthere are one or more intermediate layers of neurons between\\nthe input and the output layers. Hence, the network is termed\\nas multi-layer. The structure of this network is depicted in\\nFigure 10.18.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 495, 'page_label': '496'}, page_content='FIG. 10.18 Multi-layer feed forward\\nEach of the layers may have varying number of neurons.\\nFor example, the one shown in Figure 10.18 has ‘m’ neurons\\nin the input layer and ‘r’ neurons in the output layer, and there\\nis only one hidden layer with ‘n’ neurons.\\nThe net signal input to the neuron in the hidden layer is\\ngiven by\\nfor the k-th hidden layer neuron. The net signal input to the\\nneuron in the output layer is given by\\nfor the k-th output layer neuron.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 496, 'page_label': '497'}, page_content='10.6.3 Competitive network\\nThe competitive network is almost the same in structure as the\\nsingle-layer feed forward network. The only difference is that\\nthe output neurons are connected with each other (either\\npartially or fully). Figure 10.19 depicts a fully connected\\ncompetitive network.\\nFIG. 10.19 Competitive network\\nIn competitive networks, for a given input, the output\\nneurons compete amongst themselves to represent the input. It\\nrepresents a form of unsupervised learning algorithm in ANN\\nthat is suitable to find clusters in a data set.\\n10.6.4 Recurrent network\\nWe have seen that in feed forward networks, signals always\\nflow from the input layer towards the output layer (through the\\nhidden layers in the case of multi-layer feed forward\\nnetworks), i.e. in one direction. In the case of recurrent neural\\nnetworks, there is a small deviation. There is a feedback loop,'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 497, 'page_label': '498'}, page_content='as depicted in Figure 10.20, from the neurons in the output\\nlayer to the input layer neurons. There may also be self-loops.\\nFIG. 10.20 Recurrent neural network\\n10.7 LEARNING PROCESS IN ANN\\nNow that we have a clear idea about neurons, how they form\\nnetworks using different architectures, what is activation\\nfunction in a neuron, and what are the different choices of\\nactivation functions, it is time to relate all these to our main\\nfocus, i.e. learning. First, we need to understand what is\\nlearning in the context of ANNs? There are four major aspects\\nwhich need to be decided:\\n1. The number of layers in the network\\n2. The direction of signal flow\\n3. The number of nodes in each layer\\n4. The value of weights attached with each interconnection between neurons'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 498, 'page_label': '499'}, page_content='10.7.1 Number of layers\\nAs we have seen earlier, a neural network may have a single\\nlayer or multi-layer. In the case of a single layer, a set of\\nneurons in the input layer receives signal, i.e. a single feature\\nper neuron, from the data set. The value of the feature is\\ntransformed by the activation function of the input neuron. The\\nsignals processed by the neurons in the input layer are then\\nforwarded to the neurons in the output layer. The neurons in\\nthe output layer use their own activation function to generate\\nthe final prediction.\\nMore complex networks may be designed with multiple\\nhidden layers between the input layer and the output layer.\\nMost of the multi-layer networks are fully connected.\\n10.7.2 Direction of signal flow\\nIn certain networks, termed as feed forward networks, signal is\\nalways fed in one direction, i.e. from the input layer towards\\nthe output layer through the hidden layers, if there is any.\\nHowever, certain networks, such as the recurrent network, also\\nallow signals to travel from the output layer to the input layer.\\nThis is also an important consideration for choosing the\\ncorrect learning model.\\n10.7.3 Number of nodes in layers\\nIn the case of a multi-layer network, the number of nodes in\\neach layer can be varied. However, the number of nodes or\\nneurons in the input layer is equal to the number of features of\\nthe input data set. Similarly, the number of output nodes will\\ndepend on possible outcomes, e.g. number of classes in the\\ncase of supervised learning. So, the number of nodes in each'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 499, 'page_label': '500'}, page_content='of the hidden layers is to be chosen by the user. A larger\\nnumber of nodes in the hidden layer help in improving the\\nperformance. However, too many nodes may result in\\noverfitting as well as an increased computational expense.\\n10.7.4 Weight of interconnection between neurons\\nDeciding the value of weights attached with each\\ninterconnection between neurons so that a specific learning\\nproblem can be solved correctly is quite a difficult problem by\\nitself. Let us try to understand it in the context of a problem.\\nLet us take a step back and look at the problem in Section\\n10.6.2.\\nWe have a set of points with known labels as given below.\\nWe have to train an ANN model using this data, so that it can\\nclassify a new test data, say p  (3, −2).\\np  = (5, 2) and p  = (−1, 12) belonging to c\\np  = (3, −5) and p  = (−2, −1) belonging to c\\nWhen we were discussing the problem in Section 10.6.2, we\\nassumed the values of the synaptic weights w, w, and w as\\n−2, ½, and ¼, respectively. But where on earth did we get\\nthose values from? Will we get these weight values for every\\nlearning problem that we will attempt to solve using ANN?\\nThe answer is a big NO.\\nFor solving a learning problem using ANN, we can start\\nwith a set of values for the synaptic weights and keep doing\\nchanges to those values in multiple iterations. In the case of\\nsupervised learning, the objective to be pursued is to reduce\\nthe number of misclassifications. Ideally, the iterations for\\nmaking changes in weight values should be continued till there\\nis no misclassification. However, in practice, such a stopping\\n5\\n1 2 1\\n3 4 2\\n0 1 2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 500, 'page_label': '501'}, page_content='criterion may not be possible to achieve. Practical stopping\\ncriteria may be the rate of misclassification less than a specific\\nthreshold value, say 1%, or the maximum number of iterations\\nreaches a threshold, say 25, etc. There may be other practical\\nchallenges to deal with, such as the rate of misclassification is\\nnot reducing progressively. This may become a bigger\\nproblem when the number of interconnections and hence the\\nnumber of weights keeps increasing. There are ways to deal\\nwith those challenges, which we will see in more details in the\\nnext section.\\nSo, to summarize, learning process using ANN is a\\ncombination of multiple aspects – which include deciding the\\nnumber of hidden layers, number of nodes in each of the\\nhidden layers, direction of signal flow, and last but not the\\nleast, deciding the connection weights.\\nMulti-layer feed forward network is a commonly adopted\\narchitecture. It has been observed that a neural network with\\neven one hidden layer can be used to reasonably approximate\\nany continuous function. The learning method adopted to train\\na multi-layer feed forward network is termed as\\nbackpropagation, which we will study in the next section.\\n10.8 BACK PROPAGATION\\nWe have already seen that one of the most critical activities of\\ntraining an ANN is to assign the inter-neuron connection\\nweights. It can be a very intense work, more so for the neural\\nnetworks having a high number of hidden layers or a high\\nnumber of nodes in a layer. In 1986, an efficient method of\\ntraining an ANN was discovered. In this method, errors, i.e.\\ndifference in output values of the output layer and the expected\\nvalues, are propagated back from the output layer to the\\npreceding layers. Hence, the algorithm implementing this'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 501, 'page_label': '502'}, page_content='method is known as backpropagation, i.e. propagating the\\nerrors backward to the preceding layers.\\nThe backpropagation algorithm is applicable for multi-layer\\nfeed forward networks. It is a supervised learning algorithm\\nwhich continues adjusting the weights of the connected\\nneurons with an objective to reduce the deviation of the output\\nsignal from the target output. This algorithm consists of\\nmultiple iterations, also known as epochs. Each epoch consists\\nof two phases –\\nA forward phase in which the signals flow from the neurons in the input\\nlayer to the neurons in the output layer through the hidden layers. The\\nweights of the interconnections and activation functions are used during the\\nflow. In the output layer, the output signals are generated.\\nA backward phase in which the output signal is compared with the expected\\nvalue. The computed errors are propagated backwards from the output to the\\npreceding layers. The errors propagated back are used to adjust the\\ninterconnection weights between the layers.\\nThe iterations continue till a stopping criterion is reached.\\nFigure 10.21 depicts a reasonably simplified version of the\\nbackpropagation algorithm.\\nOne main part of the algorithm is adjusting the\\ninterconnection weights. This is done using a technique termed\\nas gradient descent. In simple terms, the algorithm calculates\\nthe partial derivative of the activation function by each\\ninterconnection weight to identify the ‘gradient’ or extent of\\nchange of the weight required to minimize the cost function.\\nQuite understandably, therefore, the activation function needs\\nto be differentiable. Let us try to understand this in a bit more\\ndetails.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 502, 'page_label': '503'}, page_content='FIG. 10.21 Backpropagation algorithm\\nPoints to Ponder:\\nA real-world simile for the gradient descent algorithm is a\\nblind person trying to come down from a hill top without\\nanyone to assist. The person is not able to see. So, for the\\nperson who is not able to see the path of descent, the only'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 503, 'page_label': '504'}, page_content='option is to check in which direction the land slope feels to\\nbe downward. One challenge of this approach arises when\\nthe person reaches a point which feels to be the lowest, as\\nall the points surrounding it is higher in slope, but in\\nreality it is not so. Such a point, which is local minima and\\nnot global minima, may be deceiving and stalls the\\nalgorithm before reaching the real global minima.\\nWe have already seen that multi-layer neural networks have\\nmultiple hidden layers. During the learning phase, the\\ninterconnection weights are adjusted on the basis of the errors\\ngenerated by the network, i.e. difference in the output signal of\\nthe network vis-à-vis the expected value. These errors\\ngenerated at the output layer are propagated back to the\\npreceding layers. Because of the backward propagation of\\nerrors which happens during the learning phase, these\\nnetworks are also called back-propagation networks or simply\\nbackpropagation nets. One such backpropagation net with one\\nhidden layer is depicted in Figure 10.22. In this network, X  is\\nthe bias input to the hidden layer and Y  is the bias input to the\\noutput layer.\\nThe net signal input to the hidden layer neurons is given by\\nfor the k-th neuron in the hidden layer. If f  is the activation\\nfunction of the hidden layer, then\\n0\\n0\\ny'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 504, 'page_label': '505'}, page_content='FIG. 10.22 Backpropagation net\\nThe net signal input to the output layer neurons is given by\\nfor the k-th neuron in the output layer. Note that the input\\nsignals to X  and Y  are assumed as 1. If f  is the activation\\nfunction of the hidden layer, then\\nIf t  is the target output of the k-th output neuron, then the\\ncost function defined as the squared error of the output layer is\\ngiven by\\n0 0 z\\nk'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 505, 'page_label': '506'}, page_content='Note:\\nThere are two types of gradient descent algorithms. When\\na full data set is used in one shot to compute the gradient,\\nit is known as full batch gradient descent. In the case of\\nstochastic gradient descent, which is also known as\\nincremental gradient descent, smaller samples of the data\\nare taken iteratively and used in the gradient computation.\\nThus, stochastic gradient descent tries to identify the\\nglobal minima.\\nSo, as a part of the gradient descent algorithm, partial\\nderivative of the cost function E has to be done with respect to\\neach of the interconnection weights w′ w′ w′\\nMathematically, it can be represented as follows:\\nfor the interconnection weight between the j-th neuron in\\nthe hidden layer and the k-th neuron in the output layer. This\\nexpression can be deduced to\\n01, 02, … , nr.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 506, 'page_label': '507'}, page_content='If we assume \\n as a component of\\nthe weight adjustment needed for weight w  corresponding to\\nthe k-th output neuron, then\\nOn the basis of this, the weights and bias need to be updated\\nas follows:\\nNote that ‘α’ is the learning rate of the neural network.\\nIn the same way, we can perform the calculations for the\\ninterconnection weights between the input and hidden layers.\\nThe weights and bias for the interconnection between the input\\nand hidden layers need to be updated as follows:\\njk'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 507, 'page_label': '508'}, page_content='Note:\\nLearning rate is a user parameter which increases or\\ndecreases the speed with which the interconnection\\nweights of a neural network is to be adjusted. If the\\nlearning rate is too high, the adjustment done as a part of\\nthe gradient descent process may miss the minima. Then,\\nthe training may not converge, and it may even diverge.\\nOn the other hand, if the learning rate is too low, the\\noptimization may consume more time because of the small\\nsteps towards the minima.\\nA balanced approach is to start the training with a\\nrelatively large learning rate (say 0.1), because in the\\nbeginning, random weights are far from optimal. Then, the\\nlearning rate should be decreased during training (say\\nexponentially lower values, e.g. 0.01, 0.001, etc.) to allow\\nmore fine-grained weight updates.\\n10.9 DEEP LEARNING\\nNeural networks, as we have already seen in this chapter, are a\\nclass of machine learning algorithms. As we have also seen,\\nthere are multiple choices of architectures for neural networks,'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 508, 'page_label': '509'}, page_content='multi-layer neural network being one of the most adopted\\nones. However, in a multi-layer neural network, as we keep\\nincreasing the number of hidden layers, the computation\\nbecomes very expensive. Going beyond two to  three layers\\nbecomes quite difficult computationally. The only way to\\nhandle such intense computation is by using graphics\\nprocessing unit (GPU) computing.\\nWhen we have less number of hidden layers – at the\\nmaximum two to three layers, it is a normal neural network,\\nwhich is sometimes given the fancy name ‘shallow neural\\nnetwork’. However, when the number of layers increases, it is\\ntermed as deep neural network. One of the earliest deep neural\\nnetworks had three hidden layers. Deep learning is a more\\ncontemporary branding of deep neural networks, i.e.\\nmultilayer neural networks having more than three layers.\\nMore detailed understanding of deep learning is beyond the\\nscope of this book.\\n10.10 SUMMARY\\nArtificial neural network (ANN) is inspired by the biological neural network.\\nIn an ANN, the biological form of neuron is replicated in the electronic or\\ndigital form of neuron.\\nEach neuron has three major components:\\nA set of synapses having weight\\nA summation junction for the input signals weighted by the respective\\nsynaptic weight\\nA threshold activation function results in an output signal only when\\nan input signal exceeding a specific threshold value comes as an input\\nDifferent types of activation functions include:\\nThreshold/Step function\\nIdentity function\\nReLU function\\nSigmoid function\\nHyperbolic tangent function\\nSome of the earlier implementations of ANN include\\nMcCulloch–Pitts Model of Neuron\\nRosenblatt’s Perceptron'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 509, 'page_label': '510'}, page_content='ADALINE Network Model\\nSome of the architectures of neural network include\\nSingle-layer feed forward ANNs: simplest and most basic architecture\\nconsisting on an input and output layer.\\nMulti-layer feed forward ANNs – there are one or more hidden\\nintermediate layers of neurons between the input and the output layers\\nCompetitive network – similar to single-layer feed forward network.\\nThe only difference is that the output neurons are connected with each\\nother (either partially or fully).\\nRecurrent networks – single or multi-layer, with a feedback loop from\\nthe neurons in the output layer to the input layer neurons. There may\\nalso be self-loops.\\nThere are four major aspects which need to be decided while learning using\\nANN\\nThe number of layers in the network\\nThe direction of signal flow\\nThe number of nodes in each layer\\nThe value of weights attached with each interconnection between\\nneurons\\nIn the backpropagation algorithm, errors are propagated back from the output\\nlayer to the preceding layers. The backpropagation algorithm is applicable\\nfor multi-layer feed forward network.\\nA technique termed as gradient descent is used to adjust the interconnection\\nweights between neurons of different layers.\\nWhen the number of hidden layers in multi-layer neural networks is higher\\nthan three, it is termed as deep neural network. Deep learning is a more\\ncontemporary branding of deep neural networks.\\nSAMPLE QUESTIONS\\nMULTIPLE CHOICE QUESTIONS\\n1. ANN is made up of ___ neurons.\\n1. biological\\n2. digital\\n3. logical\\n4. none of the above\\n2. Biological neuron does not consist:\\n1. dendrite\\n2. synapse\\n3. soma\\n4. axon\\n3. A neuron is able to ___ information in the form of chemical and\\nelectrical signals.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 510, 'page_label': '511'}, page_content='1. receive\\n2. process\\n3. transmit\\n4. all of the above\\n4. A summation junction is a ____ for the input signals.\\n1. tokenizer\\n2. multiplier\\n3. linear combiner\\n4. None of the above\\n5. Step function gives __ as output if the input is either 0 or positive.\\n1. 0\\n2. −1\\n3. 1\\n4. None of the above\\n6. A binary sigmoid function has range of ___.\\n1. (−1, +1)\\n2. (0, −1)\\n3. (0, 1)\\n4. (1, 0)\\n7. A bipolar sigmoid function has range of ___.\\n1. (−1, +1)\\n2. (0, −1)\\n3. (0, 1)\\n4. (1, 0)\\n8. The inputs of the McCulloch–Pitts neuron could be ___.\\n1. either −1 or 1\\n2. either 0 or 1\\n3. either 0 or −1\\n4. none of the above\\n9. Single-layer perceptron is able to deal with\\n1. linearly separable data\\n2. non-linearly separable data\\n3. linearly inseparable data\\n4. none of the above\\n10. Single-layer feed forward network consists of ___ layers.\\n1. two\\n2. one\\n3. three\\n4. many\\n11. Multi-layer feed forward network consists of ___ layers.\\n1. two\\n2. one\\n3. three\\n4. many\\n12. In competitive networks, output neurons are connected with __.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 511, 'page_label': '512'}, page_content='1. each other\\n2. input neurons\\n3. synapse\\n4. none of the above\\n13. Recurrent networks\\n1. are similar to multi-layer feed forward networks\\n2. may have self-loops\\n3. have feedback loops\\n4. all of the above\\n14. Which of the following are critical aspects of learning in ANN?\\n1. The number of layers in the network\\n2. The number of nodes in each layer\\n3. The interconnection weights\\n4. all of the above\\n15. In the backpropagation algorithm, multiple iterations are known as\\n1. degree\\n2. epoch\\n3. cardinality\\n4. none of the above\\n16. Each epoch consists of phases –\\n1. forward\\n2. backward\\n3. both a and b\\n4. none of the above\\n17. Deep neural networks generally have more than ___ hidden layers.\\n1. 1\\n2. 2\\n3. 3\\n4. none of the above\\n18. To handle intense computation of deep learning, ___ is needed.\\n1. parallel computing\\n2. CPU-based traditional computing\\n3. GPU computing\\n4. none of the above\\nSHORT ANSWER-TYPE QUESTIONS ( 5 MARKS EACH )\\n1. What is the function of a summation junction of a neuron? What is\\nthreshold activation function?\\n2. What is a step function? What is the difference of step function with\\nthreshold function?\\n3. Explain the McCulloch–Pitts model of neuron.\\n4. Explain the ADALINE network model.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 512, 'page_label': '513'}, page_content='5. What is the constraint of a simple perceptron? Why it may fail with a\\nreal-world data set?\\n6. What is linearly inseparable problem? What is the role of the hidden\\nlayer?\\n7. Explain XOR problem in case of a simple perceptron.\\n8. Design a multi-layer perceptron to implement A XOR B.\\n9. Explain the single-layer feed forward architecture of ANN.\\n10. Explain the competitive network architecture of ANN.\\n11. Consider a multi-layer feed forward neural network. Enumerate and\\nexplain steps in the backpropagation algorithm used to train the\\nnetwork.\\n12. What are the advantages and disadvantages of neural networks?\\n13. Write short notes on any two of the following:\\n1. Biological neuron\\n2. ReLU function\\n3. Single-layer feed forward ANN\\n4. Gradient descent\\n5. Recurrent networks\\nLONG ANSWER-TYPE QUESTIONS ( 10 MARKS EACH )\\n1. Describe the structure of an artificial neuron. How is it similar to a\\nbiological neuron? What are its main components?\\n2. What are the different types of activation functions popularly used?\\nExplain each of them.\\n3. 1. Explain, in details, Rosenblatt’s perceptron model. How can a set\\nof data be classified using a simple perceptron?\\n2. Use a simple perceptron with weights w, w, and w as −1, 2,\\nand 1, respectively, to classify data points (3, 4); (5, 2); (1, −3);\\n(−8, −3); (−3, 0).\\n4. Explain the basic structure of a multi-layer perceptron. Explain how it\\ncan solve the XOR problem.\\n5. What is artificial neural network (ANN)? Explain some of the salient\\nhighlights in the different architectural options for ANN.\\n6. Explain the learning process of an ANN. Explain, with example, the\\nchallenge in assigning synaptic weights for the interconnection between\\nneurons? How can this challenge be addressed?\\n7. Explain, in details, the backpropagation algorithm. What are the\\nlimitations of this algorithm?\\n8. Describe, in details, the process of adjusting the interconnection weights\\nin a multi-layer neural network.\\n9. What are the steps in the backpropagation algorithm? Why a multi-layer\\nneural network is required?\\n10. 1. Write short notes on any two of the following:\\n0 1 2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 513, 'page_label': '514'}, page_content='1. Artificial neuron\\n2. Multi-layer perceptron\\n3. Deep learning\\n4. Learning rate\\n2. Write the difference between (any two)\\n1. Activation function vs threshold function\\n2. Step function vs sigmoid function\\n3. Single layer vs multi-layer perceptron'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 514, 'page_label': '515'}, page_content='Chapter 11\\nOther Types of Learning\\nOBJECTIVE OF THE CHAPTER :\\nIn the previous chapters, you have got good understanding\\nof supervised learning algorithm and unsupervised\\nlearning algorithm. In this chapter, we will discuss learning\\ntypes which were not covered in the earlier chapters which\\nincludes representation learning, active learning, instance-\\nbased Learning, association rule, and ensemble learning.\\nBy the end of this chapter, you will gain sufficient\\nknowledge in all the aspects of learning and become ready\\nto start solving problems on your own.\\n11.1 INTRODUCTION\\nIn this chapter, we will discuss learning types which were not\\ncovered in the earlier chapters which include representation\\nlearning, active learning, instance-based Learning, association\\nrule, and ensemble learning.\\n11.2 REPRESENTATION LEARNING'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 515, 'page_label': '516'}, page_content='Another name for representation learning is feature learning.\\nWhile many enterprises nowadays retain substantial amounts\\nof business-related data, most of those are often unstructured\\nand unlabelled. Creating new labels is generally a time-\\nconsuming and expensive endeavour. As a result, machine\\nlearning algorithms that can straight away mine structures\\nfrom unlabelled data to improve the business performance are\\nreasonably valued. Representation learning is one such type\\nwhere the extraction from the overall unlabelled data happens\\nusing a ‘neural network’. The main objective of representation\\nlearning (feature learning) is to find an appropriate\\nrepresentation of data based on features to perform a machine\\nlearning task. Representation learning has become a field in\\nitself because of its popularity. Refer the chapter 4 titled\\n‘Basics of Feature Engineering’ to understand more on feature\\nand feature selection process. An example of some of the\\ncharacteristics of a triangle are its corners (or vertices), sides,\\nand degree of angle. Every triangle has three corners and three\\nangles. Sum of the three angles of a triangle is equal to 180°.\\nThe types of triangles with their characteristics are given\\nbelow.\\nConsider you are tasked to classify/identify the types of\\ntriangles (Refer Table 11.1). The way you do that is to find\\nunique characteristics of the type of triangle given as input.\\nThe system may work something like this\\nInput – Triangular image\\nRepresentation – Three angles, length of sides.\\nModel – It gets an input representation (angles, length of sides) and applies\\nrules as per Table 11.1 to detect the type of triangle.\\nGreat! Now, we have a primary representational working\\nsystem to detect the type of triangle.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 516, 'page_label': '517'}, page_content='What happens when we begin to input shapes like square,\\nrectangle, circle, or all sort of shapes instead of a triangle?\\nWhat if the image contains both a triangle and a circle?\\nDesigning to adapt to this kind of features requires deep\\ndomain expertise as we start working with real-world\\napplications. Deep Learning goes one step further and\\nlearns/tries to learn features on its own. All we would do is to\\nfeed in an image as input and let the system learn features like\\nhuman beings do.\\nRepresentation learnings are most widely used in words,\\nspeech recognition, signal processing, music, and image\\nidentification. For example, a word can have meaning based\\non the context.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 517, 'page_label': '518'}, page_content='Table 11.1 Types of Triangle\\nFIG. 11.1 Generation versus recognition\\nIn Figure 11.1, V represents the input data, and H represents\\nthe causes. When the causes (H) explains the data (V) we\\nobserve, it is called as Recognition. When unknown causes\\n(H) are combined, it can also generate the data (V), it is called\\nas generative (or) Generation.\\nRepresentation learning is inspired by the fact that machine\\nlearning tasks such as classification regularly requires\\nnumerical inputs that are mathematically and computationally\\neasy to process. However, it is difficult to define the features\\nalgorithmically for real-world objects such as images and\\nvideos. An alternative is to discover such features or'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 518, 'page_label': '519'}, page_content='representations just through examination, without trusting on\\nexplicit algorithms. So, representation learning can be either\\nsupervised or unsupervised.\\n1. Supervised neural networks and multilayer perceptron\\n2. Independent component analysis (unsupervised)\\n3. Autoencoders (unsupervised) and\\n4. Various forms of clustering.\\n11.2.1 Supervised neural networks and multilayer\\nperceptron\\nRefer the chapter 10 titled ‘Basics of Neural Network’ to\\nunderstand more on the topic of the neural network.\\n11.2.2 Independent component analysis (Unsupervised)\\nIndependent component analysis, or ICA is similar to principal\\ncomponent analysis (PCA), with the exception that the\\ncomponents are independent and uncorrelated. ICA is used\\nprimarily for separating unknown source signals from their\\nobserved linear mixtures after the linear mixture with an\\nunfamiliar matrix (A) . Nil information is known about the\\nsources or the mixing process except that there are N different\\nrecorded mixtures. The job is to recuperate a version(U) of the\\nsources (S) which are identical except for scaling and\\npermutation, by finding a square matrix (W) specifying spatial\\nfilters that linearly invert the mixing process, i.e. U = WX.\\nMaximizing the joint entropy, H(y), of the output of a neural\\nprocessor minimizes the mutual information among the output\\ncomponents (Refer Fig. 11.2).'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 519, 'page_label': '520'}, page_content='FIG. 11.2 Independent component analysis\\n11.2.3 Autoencoders\\nAutoencoders belong to the neural network family (Refer Fig.\\n11.3) and are similar to Principal Component Analysis (PCA).\\nThey are more flexible than PCA. Autoencoders represents\\nboth linear and non-linear transformation, whereas PCA\\nrepresents only linear transformation. The neural network’s\\n(autoencoder) target output is its input (x) in a different form\\n(x’). In Autoencoders, the dimensionality of the input is equal\\nto the dimensionality of the output, and essentially what we\\nwant is x’ = x. x’ = Decode (Encode(x))'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 520, 'page_label': '521'}, page_content='FIG. 11.3 Autoencoders\\nTo get the value from the hidden layer, we multiply the\\n(input to hidden) weights by the input. To get the value from\\nthe output, we multiply (hidden to output) weights by the\\nhidden layer values.\\nZ = f (Wx)\\nY = g (Vz)\\nSo Y = g(V f (Wx)) = VWx\\nSo Objective Function J = ∑ [Xn – VWx (n)]\\nHeuristics for Autoencoders\\n2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 521, 'page_label': '522'}, page_content='Autoencoder training algorithm can be summarized as\\nbelow:\\nFor each input x\\n1. Do a feed-forward pass to compute activations at all hidden layers, then\\nat the output layer to obtain an output X \\n2. Measure the deviation X  from the input X\\n3. Backpropagate the error through the net and perform weight updates.\\n11.2.4 Various forms of clustering\\nRefer chapter 9 titled ‘Unsupervised Learning’ to know more\\nabout various forms of Clustering.\\n11.3 ACTIVE LEARNING\\nActive learning is a type of semi-supervised learning in which\\na learning algorithm can interactively query (question) the user\\nto obtain the desired outputs at new data points.\\nLet P be the population set of all data under consideration.\\nFor example, all male students studying in a college are known\\nto have a particularly exciting study pattern.\\nDuring each iteration, I, P (total population) is broken up\\ninto three subsets.\\n1. P(K, i): Data points where the label is known (K).\\n2. P(U, i): Data points where the label is unknown (U)\\n3. P(C, i): A subset of P(U, i)that is chosen (C) to be labelled.\\nCurrent research in this type of active learning concentrates\\non identifying the best method P(C, i) to chose (C) the data\\npoints.\\n′\\n′'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 522, 'page_label': '523'}, page_content='11.3.1 Heuristics for active learning\\n1. Start with a pool of unlabelled data P(U, i)\\n2. Pick a few points at random and get their labels P(C, i)\\n3. Repeat\\nSome active learning algorithms are built upon aupport vector\\nmachines (SVMs) to determine the data points to label. Such\\nmethods usually calculate the margin (W) of each unlabelled\\ndatum in P(U, i) and assume W as an n-dimensional space\\ndistance commencing covering datum and the splitting\\nhyperplane.\\nMinimum marginal hyperplane (MMH) techniques consent\\nthat the data records with the minimum margin (W) are those\\nthat the SVM is most uncertain about and therefore should be\\nretained in P(C, i) to be labelled. Other comparable\\napproaches, such as Maximum Marginal Hyperplane, select\\ndata with the significant W (n-dimensional space). Other\\napproaches to select a combination of the smallest as well as\\nlargest Ws.\\n11.3.2 Active learning query strategies\\nFew logics for determining which data points should be\\nlabelled are\\n1. Uncertainty sampling\\n2. Query by committee\\n3. Expected model change\\n4. Expected error reduction\\n5. Variance reduction\\nUncertainty sampling: In this method, the active learning\\nalgorithm first tries to label the points for which the current'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 523, 'page_label': '524'}, page_content='model is least specific (as this means a lot of confusion) on the\\ncorrect output.\\nQuery by committee: A range of models are trained on the\\ncurrent labelled data, and vote on the output for unlabelled\\ndata; label those points for which the ‘committee’ disagrees\\nthe most.\\nExpected model change: In this method, the active learning\\nalgorithm first tries to label the points that would most change\\nthe existing model itself.\\nExpected error reduction: In this method, the active learning\\nalgorithm first tries to label the points that would mostly\\nreduce the model’s generalization error.\\nVariance reduction: In this method, the active learning\\nalgorithm first tries to label the points that would reduce\\noutput variance (which is also one of the components of error).\\n11.4 INSTANCE-BASED LEARNING (MEMORY-BASED LEARNING)\\nIn instance-based learning (memory-based learning), the\\nalgorithm compares new problem instances with instances\\nalready seen in training, which have been stored in memory\\nfor reference rather than performing explicit generalization. It\\nis called instance-based as it constructs hypotheses from the\\ntraining instances (memories) themselves. Computational\\ncomplexity O(n) of the hypothesis can grow when we have\\nsome data (n). It can quickly adapt its model to previously\\nunseen data. New instances are either stored or thrown away\\nbased on the previously set criteria.\\nExamples of instance-based learning include K-nearest\\nneighbour algorithm (K-NN), kernel machines (support vector'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 524, 'page_label': '525'}, page_content='machine, PCA), and radial basis function (RBF) networks.\\nAll these algorithms store already known instances and\\ncompute distances or similarities between this instance and the\\ntraining instances to make the final decision. Instance\\nreduction algorithm can be used to overcome memory\\ncomplexity problem and overfitting problems. Let us look into\\nradial basis function (RBF) in detail.\\n11.4.1 Radial basis function\\nRadial basis function (RBF) networks typically have three\\nlayers: one input layer, one hidden layer with a non-linear RBF\\nactivation function, and one linear output layer (Refer Fig.\\n11.4).\\nFIG. 11.4 Radial Basis Function\\nIn the above diagram, an input vector(x) is used as input to\\nall radial basis functions, each with different parameters. The\\ninput layer (X) is merely a fan-out layer, and no processing\\nhappens here. The second layer (hidden) performs radial basis\\nfunctions, which are the non-linear mapping from the input\\nspace (Vector X) into a higher order dimensional space. The\\noutput of the network (Y) is a linear combination of the'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 525, 'page_label': '526'}, page_content='outputs from radial basis functions. If pattern classification is\\nrequired (in Y), then a hard-limiter or sigmoid function could\\nbe placed on the output neurons to give 0/1 output values.\\nThe distinctive part of the radial basis function network\\n(RFFN) is the procedure implemented in the hidden layer. The\\nidea is that the patterns in the input space form clusters.\\nBeyond this area (clustering area, RFB function area), the\\nvalue drops dramatically. The Gaussian function is the most\\ncommonly used radial-basis function. In an RBF network, r is\\nthe distance from the cluster centre.\\nSpace (distance) computed from the cluster centre is usually\\nthe Euclidean distance. For each neuron that is part of the\\nhidden layer, the weights represent the coordinates of the\\ncentre of the cluster. Therefore, when that neuron receives an\\ninput pattern, X, the distance is found using the following\\nequation:\\nThe variable sigma, σ, denotes the width or radius of the\\nbell-shape and is to be determined by calculation. When the\\ndistance from the centre of the Gaussian reaches σ, the output\\ndrops from 1 to 0.6.\\nThe output y(t) is a weighted sum of the outputs of the\\nhidden layer, given by'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 526, 'page_label': '527'}, page_content='where\\nu(t) is the input\\nϕ(.) is an arbitrary non-linear radial basis function\\n‖ . ‖  denotes the norm that is usually assumed to be\\nEuclidean\\nc are the known centres of the radial basis functions\\nw are the weights of the function\\nIn radial functions, the response decreases (or increases)\\nmonotonically with distance from a central point and they are\\nradially symmetric. The centre, the distance scale, and the\\nexact shape of the radial function are the necessary\\nconsiderations of the defined model. The most commonly used\\nradial function is the Gaussian radial filter, which in case of a\\nscalar input is\\nIts parameters are its centre c and its radius β (width),\\nillustrates a Gaussian RBF with centre c = 0 and radius β = 1.\\nA Gaussian RBF monotonically decreases with distance from\\nthe centre.\\n11.4.2 Pros and cons of instance-based learning method\\ni\\ni'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 527, 'page_label': '528'}, page_content='Advantage: Instance-based learning (IBL) methods are particularly well\\nsuited to problems where the target function is highly complex, but can also be\\ndefined by a group of not as much of complex local approximations.\\nThe disadvantage I: The cost of classification of new instances can be high (a\\nmajority of the calculation takes place at this stage).\\nDisadvantage II: Many IBL approaches usually study all characteristics of the\\ninstances leading to dimensionality-related problems.\\n11.5 ASSOCIATION RULE LEARNING ALGORITHM\\nAssociation rule learning is a method that extracts rules that\\nbest explain observed relationships between variables in data.\\nThese rules can discover important and commercially useful\\nassociations in large multidimensional data sets that can be\\nexploited by an organization. The most popular association\\nrule learning algorithms are Apriori algorithm and Eclat\\nprocess.\\n11.5.1 Apriori algorithm\\nApriori is designed to function on a large database containing\\nvarious types of transactions (for example, collections of\\nproducts bought by customers, or details of websites visited by\\ncustomers frequently). Apriori algorithm uses a ‘bottom-up’\\nmethod, where repeated subsets are extended one item at a\\ntime (a step is known as candidate generation, and groups of\\ncandidates are tested against the data). The Apriori Algorithm\\nis a powerful algorithm for frequent mining of itemsets for\\nboolean association rules. Refer chapter 9 titled ‘unsupervised\\nlearning’ to understand more on this algorithm.\\n11.5.2 Eclat algorithm\\nECLAT stands for ‘Equivalence Class Clustering and bottom-\\nup Lattice Traversal’. Eclat algorithm is another set of\\nfrequent itemset generation similar to Apriori algorithm. Three'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 528, 'page_label': '529'}, page_content='traversal approaches such as ‘Top-down’, ‘Bottom-up’, and\\nHybrid approaches are supported. Transaction IDs (TID list)\\nare stored here. It represents the data in vertical format.\\nStep 1: Get Transaciton IDs : tidlist for each item (DB scan).\\nStep 2. Transaciton IDs (tidlist) of {a} is exactly the list of transactions\\ncovering {a}.\\nStep 3. Intersect tidlist of {a} with the tidlists of all other items, resulting in\\ntidlists of {a,b}, {a,c}, {a,d}, … = {a}–conditional database (if {a} removed).\\nStep 4. Repeat from 1 on {a}–conditional database 5. Repeat for all other\\nitems.\\n11.6 ENSEMBLE LEARNING ALGORITHM\\nEnsemble means collaboration/joint/group. Ensemble methods\\nare models that contain many weaker models that are\\nautonomously trained and whose forecasts are combined\\napproximately to create the overall prediction model. More\\nefforts are required to study the types of weak learners and\\nvarious ways to combine them.\\nThe most popular ensemble learning algorithms are\\nBootstrap Aggregation (bagging)\\nBoosting\\nAdaBoost\\nStacked Generalization (blending)\\nGradient Boosting Machines (GBM)\\nGradient Boosted Regression Trees (GBRT)\\nRandom Forest\\n11.6.1 Bootstrap aggregation (Bagging)\\nBootstrap Aggregation (bagging) works using the principle of\\nthe Bootstrap sampling method. Given a training data set D'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 529, 'page_label': '530'}, page_content='containing m examples, bootstrap drawing method draws a\\nsample of training examples D, by selecting m examples in\\nuniform random with replacement. It comprises two phases\\nnamely Training phase and Classification Phase.\\nTraining Phase:\\n1. Initialize the parameters\\n2. D = {Ф }\\n3. H = the number of classification\\n4. For k = 1 to h\\n5. Take a bootstrap sample S  from training set S\\n6. Build the classifier D using S  as a training set\\n7. D = D⋃D\\n8. Return D\\nClassification Phase:\\n1. Run D, D,………..D on the input k\\n2. The class with a maximum number of the vote is chosen as the label for\\nX.\\nIn the above example, the original sample has eight data\\npoints. All the Bootstrap samples also have eight data points.\\ni\\nk\\nk k\\ni\\n1 2 k'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 530, 'page_label': '531'}, page_content='Within the same bootstrap sample, data points are repeated due\\nto selection with the replacement.\\n11.6.2 Boosting\\nJust like bagging, boosting is another key ensemble-based\\ntechnique. Boosting is an iterative technique. It decreases the\\nbiasing error. If an observation was classified incorrectly, then\\nit boosts the weight of that observation. In this type of\\nensemble, weaker learning models are trained on resampled\\ndata, and the outcomes are combined using a weighted voting\\napproach based on the performance of different models.\\nAdaptive boosting or AdaBoost is a special variant of a\\nboosting algorithm. It is based on the idea of generating weak\\nlearners and learning slowly.\\n11.6.3 Gradient boosting machines (GBM)\\nAs we know already, boosting is the machine learning\\nalgorithm, which boosts week learner to strong learner. We\\nmay already know that a learner estimates a target function\\nfrom training data. Gradient boosting is the boosting with\\nGradient.\\nGradient: Consider L as a function of an ith variable, other\\nN − 1 variables are fixed at the current point, by calculating\\nderivative of L at that point (L is differentiable), we have – if\\nthe derivative is positive/negative – a decrease/increase value\\nof the ith variable in order to make the loss smaller (*).\\nApplying the above calculation for all i = 1… N, we will get N\\nderivative values forming a vector, so-called gradient of an N-\\nvariable function at the current point.\\n11.7 REGULARIZATION ALGORITHM'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 531, 'page_label': '532'}, page_content='This is an extension made to another method (typically\\nregression method) that penalizes models based on their\\ncomplexity, favouring simpler models that are also better at\\ngeneralizing. Regularization algorithms have been listed\\nseparately here because they are famous, influential, and\\ngenerally simple modifications made to other methods.\\nThe most popular regularization algorithms are\\nRidge Regression\\nLeast Absolute Shrinkage and Selection Operator (LASSO)\\nElastic Net\\nLeast-Angle Regression (LARS)\\nRefer chapter 8 titled “Supervised Learning: Regression”\\nfor Ridge regression and the LASSO method.\\nElastic Net\\nWhen we are working with high-dimensional data sets with\\na large number of independent variables, correlations\\n(relationships) amid the variables can be often result in\\nmulticollinearity. These correlated variables which are strictly\\nrelated can sometimes form groups or clusters called as an\\nelastic net of correlated variables. We would want to include\\nthe complete group in the model selection even if just one\\nvariable has been selected.\\n11.8 SUMMARY\\nAnother name for Representation learning is feature learning.\\nRepresentation learnings are most widely used in words, speech recognition,\\nsignal processing, music, and images identification. For example, a word can\\nhave meaning based on the context.\\nIndependent component analysis, or ICA) is similar to principal components\\nanalysis (PCA), with the exception that the components are independent and\\nuncorrelated.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 532, 'page_label': '533'}, page_content='Autoencoders belongs to neural network family and are similar to PCA\\n(Principal Component Analysis). They are more flexible than PCA.\\nActive learning is a type of semi-supervised learning in which a learning\\nalgorithm can interactively query (question) the user to obtain the desired\\noutputs at new data points.\\nUncertainty sampling: In this method, the active learning algorithm first tries\\nto label the points for which the current model is least specific (a that means\\nmuch confusion) on the correct output.\\nIn instance-based learning (memory-based learning), the algorithm compares\\nnew problem instances with instances already seen in training, which have\\nbeen stored in memory for reference rather than performing explicit\\ngeneralization.\\nExamples of instance-based learning include K-nearest neighbor algorithm\\n(K-NN), Kernel machines (Support Vector Machine, PCA) and Radial Basis\\nFunction networks (RBF networks).\\nRadial basis function (RBF) networks typically have three layers: one input\\nlayer, one hidden layer with a non-linear RBF activation function and one\\nlinear output layer.\\nAssociation rule learning is methods that extract rules that best explain\\nobserved relationships between variables in data. These rules can discover\\nimportant and commercially useful associations in large multidimensional\\ndatasets that can be exploited by an organization.\\nApriori is designed to function on a large database containing various types\\nof transactions (for example, collections of products bought by customers, or\\ndetails of websites visited by customers frequently).\\nECLAT stands for “Equivalence Class Clustering and bottom-up Lattice\\nTraversal.” Eclat algorithm is another set of frequent itemset generation\\nsimilar to Apriori algorithm. Three traversal approaches such as “Top-\\ndown,” “Bottom-up” and Hybrid approaches are supported.\\nEnsemble means collaboration/joint/Group. Ensemble methods are models\\nthat contain many weaker models that are autonomously trained and whose\\nforecasts are joined approximately to create the overall prediction model.\\nSAMPLE QUESTIONS\\nMULTIPLE-CHOICE QUESTIONS (1 MARK QUESTIONS)\\n1. Another name for Representation learning is ___\\n1. Feature learning\\n2. Active learning\\n3. Instance-based learning\\n4. Ensemble learnings\\n2. In representation learning, when unknown causes (H) are combined, it\\ncan also generate the data (V), it is called as ___'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 533, 'page_label': '534'}, page_content='1. Recognition\\n2. Generation\\n3. Representative\\n4. Combinative\\n3. When the causes (H) explains the data (V) we observe, it is called as\\n___\\n1. Recognition\\n2. Generation\\n3. Representative\\n4. Combinative\\n4. When unknown causes (H) are combined, it can also generate the data\\n(V), it is called as ___\\n1. Recognition\\n2. Generation\\n3. Representative\\n4. Combinative\\n5. Independent component analysis is similar to\\n1. Dependent analysis\\n2. Sub Component analysis\\n3. Data analysis\\n4. Principal component analysis\\n6. In this method, the active learning algorithm first tries to label the\\npoints for which the current model is least specific (as this means a lot\\nof confusion) on the correct output.\\n1. Expected model change\\n2. Query by committee\\n3. Uncertainty sampling\\n4. Expected error reduction\\n7. A range of models are trained on the current labeled data, and vote on\\nthe output for unlabeled data; label those points for which the ‘Group of\\npeople’ disagrees the most.\\n1. Expected model change\\n2. Query by committee\\n3. Uncertainty sampling\\n4. Expected error reduction\\n8. In this method, the active learning algorithm first tries to label the\\npoints that would most change the existing model itself\\n1. Expected model change\\n2. Query by committee\\n3. Uncertainty sampling\\n4. Expected error reduction\\n9. In this method, the active learning algorithm first tries to label the\\npoints that would mostly reduce the model’s generalization problem.\\n1. Expected model change\\n2. Query by committee'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 534, 'page_label': '535'}, page_content='3. Uncertainty sampling\\n4. Expected error reduction\\n10. Which of the following is not a type of Ensemble learning algorithms?\\n1. Boosting\\n2. AdaBoost\\n3. GBM\\n4. Elastic Net\\nSHORT-ANSWER TYPE QUESTIONS (5 MARKS QUESTIONS)\\n1. What is the main objective of Representation Learning?\\n2. What are the applications of Representational Learning?\\n3. Define Independent Component Analysis? What is the primary usage of\\nit?\\n4. What is Uncertainty sampling in Active learning?\\n5. What is Query by committee in Active learning?\\n6. What are the Pros and Cons of Instance-Based Learning (IBL) Method?\\n7. Write notes on Elastic Net\\n8. List down various types of regularization algorithms\\n9. What is Gradient?\\n10. What is Bootstraped Aggregation (Bagging)?\\n11. List down various types of Ensemble learning algorithms\\nLONG-ANSWER TYPE QUESTIONS (10 MARKS QUESTIONS)\\n1. Derive primary representational working system to detect the type of\\ntriangle.\\n2. Discuss Generation and Recognition in Representation Learning\\n3. Discuss Independent component analysis (unsupervised)\\n4. Discuss Autoencoders in detail with diagram\\n5. What is active learning? Explain its heuristics\\n6. Discuss various Active learning Query Strategies\\n7. Discuss Instance-based Learning (Memory-based learning)\\n8. Discuss Radial Basis Function in detail\\n9. Discuss various Association Rule Learning Algorithm in detail\\n10. What is Ensemble Learning Algorithm? Discuss various types.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 535, 'page_label': '536'}, page_content='Appendix A\\nProgramming Machine Learning in R\\nA.1 PRE-REQUISITES\\nBefore starting with machine learning programming in R, we\\nneed to fulfil certain pre-requisites. Quite understandably, the\\nfirst and foremost activity is to install R and get started with\\nthe basic programming interface of R, i.e. R console. Then, we\\nneed to become familiar with the R commands and scripting in\\nR. In this section, we will have a step-by-step guide of\\nfulfilling these pre-requisites.\\nA.1.1 Install R in Your System\\nR 3.5.0 or higher (https://cran.r-project.org/bin/windows/base/)\\nRStudio 1.1.453 or higher (Optional, only if you want to leverage the\\nadvantage of using an integrated development environment (IDE).\\nOtherwise, R console is sufficient.)\\n(https://www.rstudio.com/products/rstudio/download/)\\nA.1.1.1 Note\\nThe Comprehensive R Archive Network (or CRAN) is a worldwide network\\nof ftp and web servers that store identical and up-to-date versions of code\\nand documentation for R.\\nRStudio is a free and open-source IDE for R.\\nA.1.2 Know How to Manage R scripts'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 536, 'page_label': '537'}, page_content='Open new / pre-existing scripts in R console as shown in Figure A.1: \\nCopyright © The R Foundation\\nFIG. A.1 Opening a script in R console\\nScripts can be written / edited on the R editor window, and console\\ncommands can be directly executed on the R console window as shown in'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 537, 'page_label': '538'}, page_content='Figure A.2: \\nCopyright © The R Foundation\\nFIG. A.2 Writing code in R console\\nA.1.3 Know How to do Basic Programming Using R\\nA.1.3.1 Introduction to basic R commands\\nTry each of the following commands from the command\\nprompt in R console (or from RStudio, if you want).'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 538, 'page_label': '539'}, page_content=''),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 539, 'page_label': '540'}, page_content='Note:\\n“#” is used for inserting inline comments\\n<- and = are alternative / synonymous assignment\\noperators\\nA.1.3.2 Basic data types in R\\nVector: This data structure contains similar types of data, i.e. integer, double,\\nlogical, complex, etc. The function c () is used to create vectors.\\n> num <- c(1,3.4,-2,-10.85) #numeric vector\\n> char <- c(“a”,“hello”,“280”) #character vector\\n> bool <- c(TRUE,FALSE,TRUE,FALSE,FALSE) #logical\\nvector\\n> print(num)\\n[1] 1.0 3.4 -2.0 -10.85\\n \\nMatrix: Matrix is a 2-D data structure and can be created using the matrix ()\\nfunction. The values for rows and columns can be defined using ‘nrow’ and\\n‘ncol’ arguments. However, providing both is not required as the other\\ndimension is automatically acquired with the help of the length parameter\\n(first index : last index). \\nList: This data structure is slightly different from vectors in the sense that it\\ncontains a mixture of different data types A list is created using the list ()'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 540, 'page_label': '541'}, page_content='function. \\nFactor: The factor stores the nominal values as a vector of integers in the\\nrange [1…k] (where k is the number of unique values in the nominal\\nvariable) and an internal vector of character strings (the original values)\\nmapped to these items.\\n> data <- c(‘A’,‘B’,‘B’,‘C’,‘A’,‘B’,‘C’,‘C’,‘B’)\\n> fact <- factor(data)\\n> fact\\n[1] A B B C A B C C B\\nLevels: A B C\\n> table(fact) #arranges argument item in a tabular\\nformat fact\\nA B C #unique items mapped to frequencies\\n2 4 3\\n \\nData frame: This data structure is a special case of list where each\\ncomponent is of the same length. Data frame is created using the frame ()\\nfunction.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 541, 'page_label': '542'}, page_content='A.1.3.3 Loops\\nFor loop\\nSyntax:\\nfor (variable in sequence)\\n{\\n(loop_body)\\n}\\nExample: Printing squares of all integers from 1 to 3.\\nfor (i in c(1:3))\\n{\\nj = i*i\\nprint(j)\\n}\\n[1] 1\\n[1] 4\\n[1] 9\\nWhile loop\\nSyntax:'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 542, 'page_label': '543'}, page_content='while (condition)\\n{\\n(loop_body)\\n}\\nExample: Printing squares of all integers from 1 to 3.\\ni <- 1\\nwhile(i<=3)\\n{\\nsqr <- i*i\\nprint(sqr)\\ni <- i+1\\n}\\n[1] 1\\n[1] 4\\n[1] 9\\nIf-else statement\\nSyntax:\\nif (condition 1)'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 543, 'page_label': '544'}, page_content='{\\nStatement 1\\n}\\nelse if (condition 2)\\n{\\nStatement 2\\n}\\nelse\\n{\\nStatement 4\\n}\\nExample:\\nx = 0\\nif (x > 0)\\n{\\nprint(“positive”)\\n} else if (x == 0)'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 544, 'page_label': '545'}, page_content='{\\nprint(“zero”)\\n} else\\n{\\nprint(“negative”)\\n}\\n[1] “zero”\\nA.1.3.4 Writing functions\\nWriting a function (in a script):\\nSyntax:\\nfunction_name <- function(argument_list)\\n{\\n(function_body)\\n}\\nExample: Function to calculate factorial\\nof an input number n.\\nfactorial <- function (n)\\n{'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 545, 'page_label': '546'}, page_content='fact<-1\\nfor(i in 1:n)\\n{\\nfact <-fact*i\\n}\\nreturn(fact)\\n}\\nRunning the function (after compiling the script by using\\nsource (‘script_name’)):\\n> f <- factorial(6)\\n> print(f)\\n[1] 720\\nA.1.3.5 Mathematical operations on data types\\nVectors:\\n> n <- 10\\n> m <- 5\\n> n + m #addition\\n[1] 15\\n> n – m #subtraction\\n[1] 5\\n> n * m #multiplication\\n[1] 50\\n> n / m #division\\n[1] 2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 546, 'page_label': '547'}, page_content='Matrices: \\nA.1.3.6 Basic data handling commands'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 547, 'page_label': '548'}, page_content='> data <- read.csv(“auto-mpg.csv”) #\\nUploads data from a .csv file\\n> class(data) # To find the type of the\\ndata set object loaded\\n[1] “data.frame”\\n> dim(data) # To find the dimensions, i.e.\\nthe number of rows and columns of the data\\nset loaded\\n[1] 398 9\\n> nrow(data) # To find only the number of\\nrows\\n[1] 398\\n> ncol(data) # To find only the number of\\ncolumns\\n[1] 9\\n> names (data) #\\n[1] “mpg” “cylinders” “displacement”\\n“horsepower” “weight”\\n[6] “acceleration” “model.year” “origin”\\n“car.name”\\n> head (data, 3) # To display the top 3\\nrows'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 548, 'page_label': '549'}, page_content='> tail(data, 3) # To display the bottom 3\\nrows\\n> View(data) # To view the data frame\\ncontents in a separate UI\\n> data[1,9] # Will return cell value of\\nthe 1st row, 9th column of a data frame\\n[1] chevrolet chevelle malibu\\n> write.csv(data, “new-auto.csv”) # To\\nwrite the contents of a data frame object\\nto a .csv file\\n> rbind(data[1:15,], data[25:35,]) # Bind\\nsets of rows\\n> cbind(data[,3], data[,5]) # Bind sets of\\ncolumns, with the same number of rows\\n> data <- data[!data$model.year > 74,]\\n#Remove all rows with model year greater\\nthan 74\\nNote:\\nFor advanced data manipulation, the dplyr library of R\\n(developed by Hadley Wickham et al) can be leveraged. It\\nis the next version of the plyr package focused on working\\nwith data frames (hence the name “d”plyr).'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 549, 'page_label': '550'}, page_content='A.1.3.7 Advanced data manipulation commands\\n> library(“dplyr”)\\n# Functions to project specific columns\\n> select (data, cylinders) #Selects a\\nspecific feature\\n> select(data, -cylinders) # De-selects a\\nspecific feature\\n> select(data,2) #selects columns by\\ncolumn index\\n> select(data,2:3) #selects columns by\\ncolumn index range\\n> select(data,starts_with(“Cyl”))#Selects\\nfeatures by pattern match\\nSome additional options to project data elements on the\\nbasis of conditions are as follows:\\nends_with () = Select columns that end with a character string\\ncontains () = Select columns that contain a character string\\nmatches () = Select columns that match a regular expression\\none_of () = Select column names that are from a group of names\\n# Functions to select specific rows\\n> filter(data, cylinders == 8) #selects\\nrows based on conditions'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 550, 'page_label': '551'}, page_content='> filter(data, cylinders == 8, model.year\\n> 75)\\nIn R, the pipe (%>%) operator allows to pipe the output\\nfrom one function to the input of another function. Instead of\\nnesting functions (reading from inside to outside), the idea of\\npiping is to read the functions from left to right.\\n> data %>% select(2:7) %>%\\nfilter(cylinders == 8, model.year > 75)\\n%>% head(3)\\n> arrange(data,model.year) #Sorts\\nascending rows by a feature\\n> arrange(data,- model.year) #Sorts\\ndescending rows by a feature\\n> mutate(data,Total = mpg*2) #Adds new\\ncolumns'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 551, 'page_label': '552'}, page_content=''),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 552, 'page_label': '553'}, page_content='A.2 PREPARING TO MODEL\\nNow that we are reasonably familiar with the basic R\\ncommands, we have acquired the ability to start machine\\nlearning programming in R. But before starting the actual\\nmodelling work, we have to first understand the data using the\\nconcepts highlighted in Chapter 2. Also, there might be some\\nissues in the data, which we will reveal during data\\nexploration. We have to remediate that too.\\nSo first, let us find out how to do data exploration in R.\\nThere are two ways to explore and understand data:\\n1. By using certain statistical functions to understand the central tendency\\nand spread of the data\\n2. By visually observing the data in the form of plots or graphs\\nA.2.1 Basic Statistical Functions for Data Exploration\\nLet us start with the first approach of understanding the data\\nthrough statistical techniques. As we have seen in Chapter 2,\\nfor any data set, it is critical to understand the central tendency\\nand spread of the data. We have also seen that the standard\\nstatistical measures used are as follows:\\n1. Measures of central tendency – mean, median, mode\\n2. Measures of data spread\\n1. Dispersion of data – variance, standard deviation\\n2. Position of the different data values – quartiles, interquartile\\nrange (IQR)\\nIn R, there is a function summary, which generates the\\nsummary statistics of the attributes of a data set. It gives the\\nfirst basic understanding of the data set, which can trigger\\nthoughts about the data set and the anomalies that may be\\npresent. We will use another diagnostic function, str, which\\ncompactly provides the structure of a data frame along with'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 553, 'page_label': '554'}, page_content='the data types of the different attributes. So, let us start\\nexploring a data set Auto MPG data set from the University\\nof California, Irvine (UCI) machine learning repository. We\\nwill run the str and summary commands for the Auto MPG\\ndata set.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 554, 'page_label': '555'}, page_content='Looking closely at the output of the summary command,\\nthere are six measures listed for the attributes (well, most of\\nthem). These are\\n1. Min. – minimum value of the attribute\\n2. 1st Qu. – first quartile (for details, refer to Chapter 2)\\n3. Median\\n4. Mean\\n5. 3rd Qu. – third quartile (for details, refer to Chapter 2)\\n6. Max. – maximum value of the attribute\\nThese measures give quite good understanding of the data\\nset attributes. Now, note that the attribute car.name is not\\nshowing these values and showing something else. Why is that\\nso and what are the values that it is showing? Let us try to\\nunderstand the reason for this difference.\\nThe attribute car.name is a nominal, i.e. categorical\\nattribute. As we have already seen in Chapter 2, mathematical\\nor statistical operations are not possible for a nominal variable.\\nHence, only the unique values for the attribute along with the\\nnumber of occurrences or frequency are given. We can obtain\\nan exhaustive list of nominal attributes using the following R\\ncommand.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 555, 'page_label': '556'}, page_content='Next, let us try to explore whether any variable has any\\nissue with the data values where a cleaning may be required.\\nAs discussed in Chapter 2, there may be two primary data\\nissues: missing values and outliers.\\nLet us first try to determine whether there is any missing\\nvalue for any of the attributes. Let us use a small piece of R\\ncode to find out whether there is any missing/ unwanted value\\nfor an attribute in the data. If there is such issue, return the\\nrows in which the attribute has missing/unwanted values. By\\nchecking all the attributes, we find that the attribute\\n‘horsepower’ has missing values.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 556, 'page_label': '557'}, page_content='There are six rows in the data set, which have missing\\nvalues for the attribute ‘horsepower’. We will have to\\nremediate these rows before we proceed with the modelling\\nactivities. We will do that shortly.\\nThe easiest and most effective method to detect outliers is\\nfrom the box plot of the attributes. In the box plot, outliers are\\nvery clearly highlighted. When we explore the attributes using\\nbox plots in a short while, we will have a clear view of this\\naspect.\\nLet us quickly see the other R commands for obtaining\\nstatistical measures of the numeric attributes.\\n> range(data$mpg) #Gives minimum and\\nmaximum values\\n[1] 9.0 46.6'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 557, 'page_label': '558'}, page_content='> diff(range(data$mpg))\\n[1] 37.6\\n> quantile(data$mpg)\\n0% 25% 50% 75% 100%\\n9.0 17.5 23.0 29.0 46.6\\n> IQR(data$mpg)\\n[1] 11.5\\n> mean(data$mpg)\\n[1] 23.51457\\n> median(data$mpg)\\n[1] 23\\n> var(data$mpg)\\n[1] 61.08961\\n> sd(data$mpg)\\n[1] 7.815984\\nNote:'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 558, 'page_label': '559'}, page_content='To perform data exploration (as well as data visualization),\\nthe ggplot2 library of R can be leveraged. Created by\\nHadley Wickham, the ggplot2 library offers a\\ncomprehensive graphics module for creating elaborate and\\ncomplex plots.\\nA.2.2 Basic Plots for Data Exploration\\nTo start using the library functions of ggplot2, we need to load\\nthe library as follows:\\n> library(ggplot2)\\nLet us now understand the different graphs that are used for\\ndata exploration and how to generate them using R code.\\nA.2.2.1 Box plot\\nSyntax: boxplot (x, data, notch, varwidth, names, main)\\nUsage:\\n> boxplot(iris) # Iris is a popular data\\nset used in machine learning, which comes\\nbundled in R installation\\nA separate window opens in R console with the box plot\\ngenerated as shown in Figure A.3.\\nAs we can see, Figure A.3 shows the box plot of the entire\\niris data set, i.e. for all the features in the iris data set, there is a\\ncomponent or box plot in the overall plot. However, if we want'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 559, 'page_label': '560'}, page_content='to review individual features separately, we can do that too\\nusing the following R command.\\n> boxplot(iris$Sepal.Width,\\nmain=“Boxplot”, ylab = “Sepal.Width”)\\nThe output of the command, i.e. the box plot of an\\nindividual feature, sepal width, of the iris data set is shown in\\nFigure A.4.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 560, 'page_label': '561'}, page_content='FIG. A.3 Box plot of an entire data set\\nFIG. A.4 Box plot of a specific feature'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 561, 'page_label': '562'}, page_content='A.2.2.2 Histogram\\nSyntax: hist (v, main, xlab, xlim, ylim, breaks, col, border)\\nUsage:\\n> hist(iris$Sepal.Length, main =\\n“Histogram”, xlab = “Sepal Length”, col =\\n“blue”, border = “green”)\\nThe output of the command, i.e. the histogram of an\\nindividual feature, petal length, of the iris data set is shown in\\nFigure A.5.\\nFIG. A.5 Histogram of a specific feature'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 562, 'page_label': '563'}, page_content='A.2.2.3 Scatterplot\\nSyntax: plot (x, y, main, xlab, ylab, xlim, ylim, axes)\\nUsage:\\n>\\nplot(Sepal.Length~Petal.Length,data=iris,m\\nain=“Scatter Plot”,xlab=“Petal\\nLength”,ylab=“Sepal Length”)\\n> abline(lm(iris $Sepal.Length~\\niris$Petal.Length), col=“red”) # Fit a\\nregression line (red) to show the trend\\nThe output of the command, i.e. the scatter plot of the\\nfeature pair petal length and sepal length of the iris data set, is\\nshown in Figure A.6.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 563, 'page_label': '564'}, page_content='FIG. A.6 Scatter plot of petal length vs sepal length\\nA.2.3 Data Pre-Processing\\nThe primary data pre-processing activities are remediating\\ndata issues related to outliers and missing values. Also, feature\\nsubset selection is quite a critical area of data pre-processing.\\nLet us understand how to write programmes for achieving\\nthese purposes.\\nA.2.3.1 Handling outliers and missing values\\nAs we saw in Chapter 2, the primary measures for remediating\\noutliers and missing values are as follows:\\nRemoving specific rows containing outliers/missing values\\nImputing the value (i.e. outlier/missing value) with a standard statistical\\nmeasure, e.g. mean or median or mode for that attribute\\nEstimate the value (i.e. outlier/missing value) on the basis of value of the\\nattribute in similar records and replace with the estimated value.\\nCap the values within 1.5 times IQR limits'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 564, 'page_label': '565'}, page_content='Removing outliers/missing values\\nWe have to first identify the outliers. We have already seen in\\nboxplots that outliers are clearly visible when we draw the box\\nplot of a specific attribute. Hence, we can use the same\\nconcept as shown in the following code:\\n> outliers <-\\nboxplot.stats(data$mpg)$out\\nThen, those rows can be removed using the code:\\n> data <- data[!(data$mpg == outliers),]\\nOR,\\n> data <- data[-which(data$mpg ==\\noutliers),]\\nFor missing value identification and removal, the below\\ncode is used:\\n> data1 <- data[!\\n(is.na(data$horsepower)),]\\nImputing standard values\\nThe code for identification of outliers or missing values will\\nremain the same. For imputation, depending on which\\nstatistical function is to be used for imputation, the code will\\nbe as follows:\\n> library(dplyr)'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 565, 'page_label': '566'}, page_content='# Only the affected rows are identified\\nand the value of the attribute is\\ntransformed to the mean value of the\\nattribute\\n> imputedrows <- data[which(data$mpg ==\\noutliers),] %>% mutate(mpg =\\nmean(data$mpg))\\n# Affected rows are removed from the data\\nset\\n> outlier_removed_rows <- data[-\\nwhich(data$mpg == outliers),]\\n# Recombine the imputed row and the\\nremaining part of the data set\\n> data <- rbind(outlier_removed_rows,\\nimputedrows)\\nAlmost the same code can be used for imputing missing\\nvalues with the only difference being in the identification of\\nthe relevant rows.\\n> imputedrows <-\\ndata[(is.na(data$horsepower)),]%>% mutate\\n(horsepower = mean(data$horsepower))\\n> missval_removed_rows <- data[!\\n(is.na(data$horsepower)),]\\n> data <- rbind(outlier_removed_rows,\\nimputedrows)'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 566, 'page_label': '567'}, page_content='Capping of values\\nThe code for identification of outlier values will remain the\\nsame. For capping, generally a value of 1.5 times the IQR is\\nused for imputation, and the code will be as follows:\\n> library(dplyr)\\n> outliers <- boxplot.stats(data$mpg)$out\\n> imputedrows <- data[which(data$mpg ==\\noutliers),] %>% mutate(mpg =\\n1.5*IQR(data$mpg))\\n> outlier_removed_rows <- data[-\\nwhich(data$mpg == outliers),]\\n> data <- rbind(outlier_removed_rows,\\nimputedrows)\\nA.3 MODELLING AND EVALUATION\\nNote:\\nThe caret package (short for Classification And\\nREgression Training) contains functions to streamline the\\nmodel training process for complex regression and\\nclassification problems. The package contains tools for\\ndata splitting\\ndifferent pre-processing functionalities\\nfeature selection\\nmodel tuning using resampling\\nmodel performance evaluation'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 567, 'page_label': '568'}, page_content='as well as other functionalities.\\nA.4 MODEL TRAINING\\nTo start using the functions of the caret package, we need to\\ninclude the caret as follows:\\n> library(caret)\\nA.4.1 Holdout\\nThe first step before the start of modelling, in the case of\\nsupervised learning, is to load the input data, holdout a portion\\nof the input data as test data, and use the remaining portion as\\ntraining data for building the model. Below is the standard\\nprocedure to do it.\\n> inputdata <- read.csv(“btissue.csv”)\\n> split = 0.7 #Ratio in which the input\\ndata is to be split to training and test\\ndata. A split value = 0.7 indicates 70% of\\nthe data will be training data, i.e. 30%\\nof the input data is retained as test data\\n> set.seed(123) # This step is optional,\\nneeded for result reproducibility\\n> trainIndex <- createDataPartition (y =\\ninputdata$class, p = split, list = FALSE)\\n# Does a stratified random split\\nof data into training and test sets'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 568, 'page_label': '569'}, page_content='> train_ds <- inputdata [trainIndex,]\\n> test_ds <- inputdata [-trainIndex,]\\nA.4.2 K-Fold Cross-Validation\\nLet us do a 10-fold cross-validation. For creating the cross-\\nvalidation, functions from the caret package can be used as\\nfollows:\\nNext, we perform the data holdout.\\ntrain_ds <- inputdata [-\\nten_folds$Fold01,]\\ntest_ds <- inputdata [ten_folds$Fold01,]\\nNote:\\nWhen we perform data holdout, i.e. splitting of the input\\ndata into training and test data sets, the records selected for\\neach set are picked randomly. So, it is obvious that'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 569, 'page_label': '570'}, page_content='executing the same code or R function may result in\\ndifferent training data sets. The model trained will also be\\nsomewhat different.\\nIn R, there is a set.seed function which sets the starting\\npoint of the random number generator used internally to\\npick the records. This ensures that random numbers of a\\nspecific sequence are used every time, and hence, the same\\nrecords (i.e. records having the same sequence number) are\\npicked every time and the model is trained in the same\\nway. This is extremely critical for the reproducibility of\\nresults, i.e. every time, the same machine learning program\\ngenerates the same set of results. The code is as follows:\\n> set.seed (5)\\nA.4.3 Bootstrap Sampling\\nTo generate a bootstrap sample for any statistics, R package\\nboot can be used. A sample code is given below.\\n> install.packages (“boot”)\\n> library (boot)\\nmyfunc <- function (){\\n# body of the function …  \\nreturn (someval) \\n}\\n> bootcorr <- boot(data = mydata,\\nstatistic = myfunc, R = 500) # R is the'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 570, 'page_label': '571'}, page_content='number of bootstrap samples\\nA.4.4 Training the Model\\nOnce the model preparatory steps such as data holdout, etc. are\\ncompleted, the actual training starts using the following code\\nor similar codes.\\n> model <- train(class ~ ., method =\\n“rpart”, data = train_ds) # Code for the\\ndecision tree model\\nA.4.5 Evaluating Model Performance\\nThere are different ways to evaluate the different models in\\nsupervised and unsupervised learning. Some of them, as we\\nhave seen, have been discussed in Chapter 3. Now, it is time to\\nsee how we can implement them through R code.\\nA.4.5.1 Supervised learning - classification\\nIn supervised learning, model accuracy is the most critical\\nmeasure for evaluating a model’s performance. There are also\\nother measures such as sensitivity, specificity, precision, recall,\\netc., each of which we have studied in Chapter 3. R library\\ncaret gives a function confusionMatrix to reveal the confusion\\nmatrix of a model, and on the basis of the confusion matrix,\\nvalues of the different measures, namely accuracy, sensitivity,\\nspecificity, precision, recall, etc., are obtained. Figure A.7\\npresents a snapshot of the confusionMatrix output for a\\nspecific data set.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 571, 'page_label': '572'}, page_content='FIG. A.7 Performance evaluation of a classification model (Decision Tree)\\n> predictions <- predict(model, test_ds,\\nna.action = na.pass)\\n> cm <- confusionMatrix(predictions,\\ntest_ds$class)\\n> print(cm)\\n> print(cm$overall[‘Accuracy’])\\nA.4.5.2 Supervised learning - regression\\nThe summary function, when applied to a regression model,\\ndisplays the different performance measures such as residual'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 572, 'page_label': '573'}, page_content='standard error, multiple R-squared, etc., both for simple and\\nmultiple linear regression.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 573, 'page_label': '574'}, page_content='A.4.5.3 Unsupervised learning - clustering\\nAs we have seen in Chapter 3, there are two popular measures\\nof cluster quality: purity and silhouette width. Purity can be\\ncalculated only when class label is known for the data set\\nsubjected to clustering. On the other hand, silhouette width can\\nbe calculated for any data set.\\nPurity\\nWe will use a Lower Back Pain Symptoms data set released by\\nKaggle (https://www.kaggle.com/sammy123/lower-back-pain-\\nsymptoms-dataset). The data set spine.csv consists of 310\\nobservations and 13 attributes (12 numeric predictors, 1 binary\\nclass attribute).\\n> library(fpc)\\n> data <- read.csv(“spine.csv”) #Loading\\nthe Kaggle data set\\n> data_wo_class <- data[,-length(data)]\\n#Stripping off the class attribute from\\nthe data set before clustering\\n> class <- data[,length(data)] #Storing\\nthe class attribute in a separate variable\\nfor later use\\n> dis = dist(data_wo_class)^2'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 574, 'page_label': '575'}, page_content='> res = kmeans(data_wo_class,2) #Can use\\nother clustering algorithms too\\n#Let us define a custom function to\\ncompare cluster value with the original\\nclass value and calculate the percentage\\nmatch\\nClusterPurity <- function(clusters,\\nclasses) {\\nsum(apply(table(classes, clusters), 2,\\nmax)) / length(clusters)\\n}\\n> ClusterPurity(res$cluster, class)\\nOutput\\n[1] 0.6774194\\nSilhouette width\\nUse the R library cluster to find out/plot the silhouette width\\nof the clusters formed. The piece of code below clusters the\\nrecords in the data set spinem.csv (the same data set as\\nspine.csv with the target variable removed) using the k-means\\nalgorithm and then calculates the silhouette width of the\\nclusters formed.\\n> library (cluster)\\n> data <- read.csv(“spinem.csv”)'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 575, 'page_label': '576'}, page_content='> dis = dist(data)^2\\n> res = kmeans(data,2) #Can use other\\nclustering algorithms too\\n> sil_width <- silhouette (res$cluster,\\ndis)\\n> sil_summ <- summary(sil_width)\\n> sil_summ$clus.avg.widths # Returns\\nsilhouette width of each cluster\\n> sil_summ$avg.width # Returns silhouette\\nwidth of the overall data set\\nOutput\\nSilhouette width of each cluster:\\n1          2\\n0.7473583 0.1921082\\nSilhouette width of the overall data set:\\n[1] 0.5413785\\nA.5 FEATURE ENGINEERING\\nA.5.1 Feature Construction\\nFor performing feature construction, we can use mutate\\nfunction of the dplyr package. Following is a small code for'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 576, 'page_label': '577'}, page_content='feature construction using the iris data set.\\nA.5.1.1 Dummy coding categorical (nominal) variables\\nAs in the above case, we can use the dummy.code function of\\nthe psych package to encode categorical variables. Following\\nis a small code for the same.\\n> library(psych)\\n> age <- c(18,20,23,19,18,22)\\n> city <- c(‘City A’,‘City B’,‘City\\nA’,‘City C’,‘City B’)\\n> data <- data.frame(age, city)\\n> data'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 577, 'page_label': '578'}, page_content='A.5.1.2 Encoding categorical (ordinal) variables'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 578, 'page_label': '579'}, page_content='A.5.1.3 Transforming numeric (continuous) features to\\ncategorical features'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 579, 'page_label': '580'}, page_content='A.5.2 Feature Extraction\\nA.5.2.1 Principal Component Analysis (PCA)'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 580, 'page_label': '581'}, page_content='For performing principal component analysis (PCA), we can\\nuse the prcomp function of the stats package. Following is the\\ncode using the iris data set. PCA should be applied to the\\npredictors. The class variable can be used to visualize the\\nprincipal components.\\nThe output of the biplot function is given in Figure A.8'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 581, 'page_label': '582'}, page_content='FIG. A.8 Principal components of the iris data set\\nA.5.2.2 Singular Value Decomposition (SVD)\\nFor performing singular value decomposition, we can use the\\nsvd function of the stats package. Following is the code using\\nthe iris data set.\\n> sing_val_decomp <- svd(iris[,1:4])\\n> print(sing_val_decomp$d)\\nOutput:\\n[1] 95.959914 17.761034 3.460931 1.884826'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 582, 'page_label': '583'}, page_content='A.5.2.3 Linear Discriminant Analysis (LDA)\\nFor performing linear discriminant analysis, we can use the\\nlda function of the MASS package. Following is the code\\nusing the UCI data set btissue.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 583, 'page_label': '584'}, page_content='The output of the above function is given in Figure A.9\\nFIG. A.9 LDA of the btissue data set\\nA.5.3 Feature Subset Selection\\nFeature subset selection is a topic of intense research. There\\nare many approaches to select a subset of features which can\\nimprove model performance. It is not possible to cover all\\nsuch approaches as a part of this text. However, only for basic'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 584, 'page_label': '585'}, page_content='feature selection functionalities, the FSelector package of R\\ncan be used.\\nlibrary(FSelector)\\ndata <- iris\\nfeat_subset <- cfs(Species ~ ., data) #\\nSelects feature subset using correlation\\nand entropy measures for continuous and\\ndiscrete data\\nBelow is a programme to perform feature subset selection\\nbefore applying the same for training a model.\\nlibrary(caret)\\nlibrary(FSelector)\\ninputdata <- read.csv(“apndcts.csv”)\\nsplit = 0.7\\nset.seed(123)\\ntrainIndex <- createDataPartition (y =\\ninputdata$class, p = split, list = FALSE)\\ntrain_ds <- inputdata [trainIndex,]\\ntest_ds <- inputdata [-trainIndex,]\\nfeat_subset <- cfs(class ~ ., train_ds)\\n#Feature selection done class <-'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 585, 'page_label': '586'}, page_content='train_ds$class\\ntrain_ds <- cbind(train_ds[feat_subset],\\nclass) # Subset training data created\\nclass <- test_ds$class\\ntest_ds <- cbind(test_ds[feat_subset],\\nclass) # Subset test data created\\ntrain_ds$class <-\\nas.factor(train_ds$class)\\ntest_ds$class <- as.factor(test_ds$class)\\n# Applying Decision Tree classifier\\nhere. Any other model can also be applied\\n…\\nmodel <- train(class ~ ., method =\\n“rpart”, data = train_ds) predictions <-\\npredict(model, test_ds, na.action =\\nna.pass)\\ncm <- confusionMatrix(predictions,\\ntest_ds$class)\\ncm$overall[‘Accuracy’]\\nNote:'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 586, 'page_label': '587'}, page_content='The e1071 package is an important R package which\\ncontains many statistical functions along with some critical\\nclassification algorithms such as Naïve Bayes and support\\nvector machine (SVM). It is created by David Meyer and\\nteam and maintained by David Meyer.\\nA.6 MACHINE LEARNING MODELS\\nA.6.1 Supervised Learning – Classification\\nIn Chapters 6, 7, and 8, conceptual overview of different\\nsupervised learning algorithms has been presented. Now, you\\nwill understand how to implement them using R. For the sake\\nof simplicity, the code for implementing each of the algorithms\\nis kept as consistent as possible. Also, we have used\\nbenchmark data sets from UCI repository (these data sets will\\nalso be available online, refer to the URL\\nhttps://archive.ics.uci.edu/ml).\\nA.6.1.1 Naïve Bayes classifier\\nTo implement this classifier, the naiveBayes function of the\\ne1071 package has been used. The full code for the\\nimplementation is given below.\\n> library(caret)\\n> library (e1071)\\n> inputdata <- read.csv(“apndcts.csv”)\\n> split = 0.7'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 587, 'page_label': '588'}, page_content='> set.seed(123)\\n> trainIndex <- createDataPartition (y =\\ninputdata$class, p = split, list = FALSE)\\n> train_ds <- inputdata [trainIndex,]\\n> test_ds <- inputdata [-trainIndex,]\\n> train_ds$class <-\\nas.factor(train_ds$class) #Pre-processing\\nstep\\n> test_ds$class <-\\nas.factor(test_ds$class) #Pre-processing\\nstep\\n> model <- naiveBayes (class ~ ., data =\\ntrain_ds)\\n> predictions <- predict(model, test_ds,\\nna.action = na.pass)\\n> cm <- confusionMatrix(predictions,\\ntest_ds $class)\\n> cm$overall[‘Accuracy’]\\nOutput Accuracy:\\n0.8387097\\nA.6.1.2 kNN classifier'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 588, 'page_label': '589'}, page_content='To implement this classifier, the knn function of the class\\npackage has been used. The full code for the implementation is\\ngiven below.\\n> library(caret)\\n> library (class)\\n> inputdata <- read.csv(“apndcts.csv”)\\n> split = 0.7\\n> set.seed(123)\\n> trainIndex <- createDataPartition (y =\\ninputdata$class, p = split, list = FALSE)\\n> train_ds <- inputdata [trainIndex,]\\n> test_ds <- inputdata [-trainIndex,]\\n> train_ds$class <-\\nas.factor(train_ds$class) #Pre-processing\\nstep\\n> test_ds$class <-\\nas.factor(test_ds$class) #Pre-processing\\nstep\\n> model <- knn(train_ds, test_ds,\\ntrain_ds$class, k = 3)\\n> cm <- confusionMatrix(model,\\ntest_ds$class)'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 589, 'page_label': '590'}, page_content='> cm$overall[‘Accuracy’]\\nOutput Accuracy:\\n1\\nA.6.1.3 Decision tree classifier\\nTo implement this classifier implementation, the train function\\nof the caret package can be used with a parameter method =\\n“rpart” to indicate that the train function will use the decision\\ntree classifier. Optionally, the rpart function of the rpart\\npackage can also be used. The full code for the\\nimplementation is given below.\\n> library(caret)\\n> library(rpart) # Optional\\n> inputdata <- read.csv(“apndcts.csv”)\\n> split = 0.7\\n> set.seed(123)\\n> trainIndex <- createDataPartition (y =\\ninputdata$class, p = split, list = FALSE)\\n> train_ds <- inputdata [trainIndex,]\\n> test_ds <- inputdata [-trainIndex,]\\n> train_ds$class <-\\nas.factor(train_ds$class) #Pre-processing'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 590, 'page_label': '591'}, page_content='step\\n> test_ds$class <-\\nas.factor(test_ds$class) #Pre-processing\\nstep\\n> model <- train(class ~ ., method =\\n“rpart”, data = train_ds, prox = TRUE)\\n# May also use the rpart function as shown\\nbelow …\\n#> model <- rpart(formula = class ~ .,\\ndata = train_ds) # Optional\\n> predictions <- predict(model, test_ds,\\nna.action = na.pass)\\n> cm <- confusionMatrix(predictions,\\ntest_ds $class)\\n> cm$overall[‘Accuracy’]\\nOutput Accuracy:\\n0.8387097\\nA.6.1.4 Random forest classifier\\nTo implement this classifier, the train function of the caret\\npackage can be used with the parameter method = “rf” to\\nindicate that the train function will use the decision tree\\nclassifier. Optionally, the randomForest function of the'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 591, 'page_label': '592'}, page_content='randomForest package can also be used. The full code for the\\nimplementation is given below.\\n> library(caret)\\n> library(randomForest) # Optional\\n> inputdata <- read.csv(“apndcts.csv”)\\n> split = 0.7\\n> set.seed(123)\\n> trainIndex <- createDataPartition (y =\\ninputdata$class, p = split, list = FALSE)\\n> train_ds <- inputdata [trainIndex,]\\n> test_ds <- inputdata [-trainIndex,]\\n> train_ds$class <-\\nas.factor(train_ds$class) #Pre-processing\\nstep\\n> test_ds$class <-\\nas.factor(test_ds$class) #Pre-processing\\nstep\\n> model <- train(class ~ ., method = “rf”,\\ndata = train_ds, prox = TRUE)\\n# May also use the randomForest function\\nas shown below …'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 592, 'page_label': '593'}, page_content='#> model <- randomForest(class ~ . , data\\n= train_ds, ntree=400)\\n> predictions <- predict(model, test_ds,\\nna.action = na.pass)\\n> cm <- confusionMatrix(predictions,\\ntest_ds $class)\\n> cm$overall[‘Accuracy’]\\nOutput Accuracy:\\n0.8387097\\nA.6.1.5 SVM classifier\\nTo implement this classifier, the svm function of the e1071\\npackage has been used. The full code for the implementation is\\ngiven below.\\n> library(caret)\\n> library (e1071)\\n> inputdata <- read.csv(“apndcts.csv”)\\n> split = 0.7\\n> set.seed(123)\\n> trainIndex <- createDataPartition (y =\\ninputdata$class, p = split, list = FALSE)'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 593, 'page_label': '594'}, page_content='> train_ds <- inputdata [trainIndex,]\\n> test_ds <- inputdata [-trainIndex,]\\n> train_ds$class <-\\nas.factor(train_ds$class) #Pre-processing\\nstep\\n> test_ds$class <-\\nas.factor(test_ds$class) #Pre-processing\\nstep\\n> model <- svm(class ~ ., data = train_ds)\\n> predictions <- predict(model, test_ds,\\nna.action = na.pass)\\n> cm <- confusionMatrix(predictions,\\ntest_ds $class)\\n> cm$overall[‘Accuracy’]\\nOutput Accuracy:\\n0.8709677\\nA.6.2 Supervised Learning – Regression\\nAs discussed in Chapter 9, two main algorithms used for\\nregression are simple linear regression and multiple linear\\nregression. Implementation of both the algorithms in R code is\\nshown below.\\n> data <- read.csv(“auto-mpg.csv”)'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 594, 'page_label': '595'}, page_content='> attach(data) # Data set is attached to\\nthe R search path, which means that while\\nevaluating a variable, objects in the data\\nset can be accessed by simply giving their\\nnames. So, instead of “data$mpg”, simply\\ngiving “mpg” will suffice.\\n> data <-\\ndata[!is.na(as.numeric(as.character(horsep\\nower))),]\\n> data <- mutate(data, horsepower =\\nas.numeric(horsepower))\\n> outliers_mpg <- boxplot.stats(mpg)$out\\n> data <- data[-which(mpg ==\\noutliers_mpg),]\\n> outliers_cylinders <-\\nboxplot.stats(cylinders)$out\\n> data <- data[-which(cylinders ==\\noutliers_cylinders),]\\n> outliers_displacement <-\\nboxplot.stats(displacement)$out\\n> data <- data[-which(displacement ==\\noutliers_displacement),]\\n> outliers_weight <-\\nboxplot.stats(weight)$out'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 595, 'page_label': '596'}, page_content='> data <- data[-which(weight ==\\noutliers_weight),]\\n> outliers_acceleration <-\\nboxplot.stats(acceleration)$out\\n> data <- data[-which(acceleration ==\\noutliers_acceleration),]\\nA.6.2.1 Simple linear regression\\n> reg_pred <- lm(mpg ~ cylinders)\\n> summary(reg_pred)\\nA.6.2.2 Multiple linear regression\\n> reg_pred <- lm(mpg ~ cylinders +\\ndisplacement)\\n> reg_pred <- lm(mpg ~ cylinders + weight\\n+ acceleration)\\n> reg_pred <- lm(mpg ~ cylinders +\\ndisplacement + horsepower + weight +\\nacceleration)\\nA.6.3 Unsupervised Learning\\nTo implement the k-means algorithm, the k-means function of\\nthe cluster package has been used. Also, because silhouette\\nwidth is a more generic performance measure, it has been used\\nhere. The complete R code for the implementation is given\\nbelow.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 596, 'page_label': '597'}, page_content='> library (cluster)\\n> data <- read.csv(“spinem.csv”)\\n> dis = dist(data)^2\\n> res = kmeans(data,2)\\n> sil_width <- silhouette (res$cluster,\\ndis)\\n> sil_summ <- summary(sil_width)\\n> sil_summ$avg.width # Returns silhouette\\nwidth of the overall data set\\nOutput Accuracy:\\n[1] 0.5508955\\nA.6.4 Neural Network\\nTo implement this classifier, the neuralnet function of the\\nneuralnet package has been used. The full code for the\\nimplementation is given below.\\nA.6.4.1 Single-layer feedforward neural network\\n> library(caret)\\n> library(neuralnet)\\n> inputdata <- read.csv(“apndcts.csv”)\\n> split = 0.7'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 597, 'page_label': '598'}, page_content='> set.seed(123)\\n> trainIndex <- createDataPartition (y =\\ninputdata$class, p = split, list = FALSE)\\n> train_ds <- inputdata [trainIndex,]\\n> test_ds <- inputdata [-trainIndex,]\\n> model <- neuralnet(class ~ At1 + At2 +\\nAt3 + At4 + At5 + At6 + At7, train_ds)\\n> plot(model)\\nOutput is presented in Figure A.10.\\nA.6.4.2 Multi-layer feedforward neural network\\n> library(caret)\\n> library(neuralnet)\\n> inputdata <- read.csv(“apndcts.csv”)\\n> split = 0.7 > set.seed(123)\\n> trainIndex <- createDataPartition (y =\\ninputdata$class, p = split, list = FALSE)\\n> train_ds <- inputdata [trainIndex,]\\n> test_ds <- inputdata [-trainIndex,]'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 598, 'page_label': '599'}, page_content='> model <- neuralnet(class ~ At1 + At2 +\\nAt3 + At4 + At5 + At6 + At7, train_ds,\\nhidden = 3) # Multi-layer NN\\n> plot(model)'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 599, 'page_label': '600'}, page_content='FIG. A.10 Single-layer NN\\nOutput is presented in Figure A.11.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 600, 'page_label': '601'}, page_content='FIG. A.11 Multi-layer NN\\nA.7 MACHINE LEARNING L AB SCHEDULE'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 601, 'page_label': '602'}, page_content=''),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 602, 'page_label': '603'}, page_content=''),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 603, 'page_label': '604'}, page_content=''),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 604, 'page_label': '605'}, page_content=''),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 605, 'page_label': '606'}, page_content='Appendix B\\nProgramming Machine Learning in\\nPython\\nB.1 PRE-REQUISITES\\nB.1.1 Install anaconda in your system\\nBefore starting to do machine learning programming in\\nPython, please install Python Anaconda distribution from\\nhttps://anaconda.org/anaconda/python. To write any python\\nscript, open any text editor and write your python script and\\nsave with the extension .py. You may also use the Spyder\\n(Scientific PYthon Development EnviRonment), which is a\\npowerful and interactive development environment for the\\nPython language with advanced editing, interactive testing,\\ndebugging, and introspection features. In this chapter, all the\\nexamples have been created working in Spyder.\\nB.1.2 Know how to manage Python scripts\\nOpen new/pre-existing .py script in Spyder as shown in Figure B.1:\\nB.1.3 Know how to do basic programming using Python\\nB.1.3.1 Introduction to basic Python commands\\nFirst install a couple of basic libraries of Python namely'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 606, 'page_label': '607'}, page_content='‘os’ for using operating system dependent functions\\n‘pandas’ for extensive data manipulation functionalities\\nIn general, Annaconda Python by default contains all the\\nbasic required packages for starting programming in machine\\nlearning. But in case, any extra package is needed, ‘pip’ is the\\nrecommended installer.\\nCopyright © 2018. Python Software Foundation\\nFIG. B.1 Opening a script in Spyder'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\Data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 607, 'page_label': '608'}, page_content='Command: pip install\\nSyntax: pip install ‘SomePackage’\\nAfter installing, libraries need to be loaded by invoking the\\ncommand:\\nCommand: import <<library-name>>\\nSyntax: import os\\nTry out each of the following commands.'),\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for pdf data loading\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "pdf_loader=DirectoryLoader( #specify config of loader\n",
    "    \"../Data/pdf\", #file dir\n",
    "    glob=\"**/*.pdf\", #pattern to match\n",
    "    show_progress=True,\n",
    "    loader_cls=PyPDFLoader\n",
    ")\n",
    "\n",
    "pdfdoc=pdf_loader.load()\n",
    "pdfdoc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e16b8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDF files to process\n",
      "\n",
      "Processing: Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf\n",
      "  ✓ Loaded 392 pages\n",
      "\n",
      "Processing: Machine Learning ( etc.) (z-lib.org).pdf\n",
      "  ✓ Loaded 741 pages\n",
      "\n",
      "Total documents loaded: 1133\n"
     ]
    }
   ],
   "source": [
    "#chunking\n",
    "\n",
    "from pathlib import Path\n",
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08a7cb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1133 documents into 2094 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: Andreas C. Müller & Sarah Guido\n",
      "Introduction to \n",
      "Machine \n",
      "Learning  \n",
      "with P y t h o n   \n",
      "A GUIDE FOR DATA SCIENTISTS...\n",
      "Metadata: {'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 0, 'page_label': 'Cover', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 0, 'page_label': 'Cover', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Andreas C. Müller & Sarah Guido\\nIntroduction to \\nMachine \\nLearning  \\nwith P y t h o n   \\nA GUIDE FOR DATA SCIENTISTS'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 2, 'page_label': 'i', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Andreas C. Müller and Sarah Guido\\nIntroduction to Machine Learning\\nwith Python\\nA Guide for Data Scientists\\nBoston Farnham Sebastopol TokyoBeijing Boston Farnham Sebastopol TokyoBeijing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 3, 'page_label': 'ii', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='978-1-449-36941-5\\n[LSI]\\nIntroduction to Machine Learning with Python\\nby Andreas C. Müller and Sarah Guido\\nCopyright © 2017 Sarah Guido, Andreas Müller. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles ( http://safaribooksonline.com). For more information, contact our corporate/\\ninstitutional sales department: 800-998-9938 or corporate@oreilly.com.\\nEditor: Dawn Schanafelt\\nProduction Editor: Kristen Brown\\nCopyeditor: Rachel Head\\nProofreader: Jasmine Kwityn\\nIndexer: Judy McConville\\nInterior Designer: David Futato\\nCover Designer: Karen Montgomery\\nIllustrator: Rebecca Demarest\\nOctober 2016:  First Edition\\nRevision History for the First Edition\\n2016-09-22: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781449369415 for release details.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 3, 'page_label': 'ii', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='October 2016:  First Edition\\nRevision History for the First Edition\\n2016-09-22: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781449369415 for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Introduction to Machine Learning with\\nPython, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk. If any code samples or other technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 3, 'page_label': 'ii', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 4, 'page_label': 'iii', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Table of Contents\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  vii\\n1. Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\\nWhy Machine Learning?                                                                                                   1\\nProblems Machine Learning Can Solve                                                                      2\\nKnowing Y our Task and Knowing Y our Data                                                            4\\nWhy Python?                                                                                                                      5\\nscikit-learn                                                                                                                          5'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 4, 'page_label': 'iii', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='scikit-learn                                                                                                                          5\\nInstalling scikit-learn                                                                                                     6\\nEssential Libraries and Tools                                                                                            7\\nJupyter Notebook                                                                                                           7\\nNumPy                                                                                                                             7\\nSciPy                                                                                                                                 8\\nmatplotlib                                                                                                                        9'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 4, 'page_label': 'iii', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='matplotlib                                                                                                                        9\\npandas                                                                                                                            10\\nmglearn                                                                                                                          11\\nPython 2 Versus Python 3                                                                                               12\\nVersions Used in this Book                                                                                             12\\nA First Application: Classifying Iris Species                                                                13\\nMeet the Data                                                                                                                14\\nMeasuring Success: Training and Testing Data                                                        17'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 4, 'page_label': 'iii', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Measuring Success: Training and Testing Data                                                        17\\nFirst Things First: Look at Y our Data                                                                        19\\nBuilding Y our First Model: k-Nearest Neighbors                                                    20\\nMaking Predictions                                                                                                      22\\nEvaluating the Model                                                                                                   22\\nSummary and Outlook                                                                                                   23\\niii'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 5, 'page_label': 'iv', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='2. Supervised Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  25\\nClassification and Regression                                                                                         25\\nGeneralization, Overfitting, and Underfitting                                                             26\\nRelation of Model Complexity to Dataset Size                                                         29\\nSupervised Machine Learning Algorithms                                                                  29\\nSome Sample Datasets                                                                                                 30\\nk-Nearest Neighbors                                                                                                    35\\nLinear Models                                                                                                               45'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 5, 'page_label': 'iv', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Linear Models                                                                                                               45\\nNaive Bayes Classifiers                                                                                                 68\\nDecision Trees                                                                                                               70\\nEnsembles of Decision Trees                                                                                      83\\nKernelized Support Vector Machines                                                                        92\\nNeural Networks (Deep Learning)                                                                          104\\nUncertainty Estimates from Classifiers                                                                      119\\nThe Decision Function                                                                                              120'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 5, 'page_label': 'iv', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='The Decision Function                                                                                              120\\nPredicting Probabilities                                                                                             122\\nUncertainty in Multiclass Classification                                                                 124\\nSummary and Outlook                                                                                                 127\\n3. Unsupervised Learning and Preprocessing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  131\\nTypes of Unsupervised Learning                                                                                 131\\nChallenges in Unsupervised Learning                                                                        132\\nPreprocessing and Scaling                                                                                            132'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 5, 'page_label': 'iv', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Preprocessing and Scaling                                                                                            132\\nDifferent Kinds of Preprocessing                                                                             133\\nApplying Data Transformations                                                                               134\\nScaling Training and Test Data the Same Way                                                       136\\nThe Effect of Preprocessing on Supervised Learning                                           138\\nDimensionality Reduction, Feature Extraction, and Manifold Learning              140\\nPrincipal Component Analysis (PCA)                                                                    140\\nNon-Negative Matrix Factorization (NMF)                                                           156\\nManifold Learning with t-SNE                                                                                 163'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 5, 'page_label': 'iv', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Manifold Learning with t-SNE                                                                                 163\\nClustering                                                                                                                        168\\nk-Means Clustering                                                                                                    168\\nAgglomerative Clustering                                                                                         182\\nDBSCAN                                                                                                                     187\\nComparing and Evaluating Clustering Algorithms                                              191\\nSummary of Clustering Methods                                                                             207\\nSummary and Outlook                                                                                                 208'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 5, 'page_label': 'iv', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Summary and Outlook                                                                                                 208\\n4. Representing Data and Engineering Features. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  211\\nCategorical Variables                                                                                                     212\\nOne-Hot-Encoding (Dummy Variables)                                                                213\\niv | Table of Contents'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 6, 'page_label': 'v', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Numbers Can Encode Categoricals                                                                         218\\nBinning, Discretization, Linear Models, and Trees                                                   220\\nInteractions and Polynomials                                                                                      224\\nUnivariate Nonlinear Transformations                                                                      232\\nAutomatic Feature Selection                                                                                        236\\nUnivariate Statistics                                                                                                    236\\nModel-Based Feature Selection                                                                                238\\nIterative Feature Selection                                                                                         240'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 6, 'page_label': 'v', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Iterative Feature Selection                                                                                         240\\nUtilizing Expert Knowledge                                                                                         242\\nSummary and Outlook                                                                                                 250\\n5. Model Evaluation and Improvement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  251\\nCross-Validation                                                                                                            252\\nCross-Validation in scikit-learn                                                                               253\\nBenefits of Cross-Validation                                                                                     254\\nStratified k-Fold Cross-Validation and Other Strategies                                      254'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 6, 'page_label': 'v', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Stratified k-Fold Cross-Validation and Other Strategies                                      254\\nGrid Search                                                                                                                     260\\nSimple Grid Search                                                                                                    261\\nThe Danger of Overfitting the Parameters and the Validation Set                     261\\nGrid Search with Cross-Validation                                                                          263\\nEvaluation Metrics and Scoring                                                                                   275\\nKeep the End Goal in Mind                                                                                      275\\nMetrics for Binary Classification                                                                             276'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 6, 'page_label': 'v', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Metrics for Binary Classification                                                                             276\\nMetrics for Multiclass Classification                                                                       296\\nRegression Metrics                                                                                                     299\\nUsing Evaluation Metrics in Model Selection                                                        300\\nSummary and Outlook                                                                                                 302\\n6. Algorithm Chains and Pipelines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  305\\nParameter Selection with Preprocessing                                                                    306\\nBuilding Pipelines                                                                                                          308'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 6, 'page_label': 'v', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Building Pipelines                                                                                                          308\\nUsing Pipelines in Grid Searches                                                                                 309\\nThe General Pipeline Interface                                                                                    312\\nConvenient Pipeline Creation with make_pipeline                                              313\\nAccessing Step Attributes                                                                                          314\\nAccessing Attributes in a Grid-Searched Pipeline                                                 315\\nGrid-Searching Preprocessing Steps and Model Parameters                                  317\\nGrid-Searching Which Model To Use                                                                        319'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 6, 'page_label': 'v', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Grid-Searching Which Model To Use                                                                        319\\nSummary and Outlook                                                                                                 320\\n7. Working with Text Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  323\\nTypes of Data Represented as Strings                                                                         323\\nTable of Contents | v'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 7, 'page_label': 'vi', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Example Application: Sentiment Analysis of Movie Reviews                                 325\\nRepresenting Text Data as a Bag of Words                                                                 327\\nApplying Bag-of-Words to a Toy Dataset                                                               329\\nBag-of-Words for Movie Reviews                                                                            330\\nStopwords                                                                                                                       334\\nRescaling the Data with tf–idf                                                                                      336\\nInvestigating Model Coefficients                                                                                 338\\nBag-of-Words with More Than One Word (n-Grams)                                            339\\nAdvanced Tokenization, Stemming, and Lemmatization                                        344'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 7, 'page_label': 'vi', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Bag-of-Words with More Than One Word (n-Grams)                                            339\\nAdvanced Tokenization, Stemming, and Lemmatization                                        344\\nTopic Modeling and Document Clustering                                                               347\\nLatent Dirichlet Allocation                                                                                       348\\nSummary and Outlook                                                                                                 355\\n8. Wrapping Up. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  357\\nApproaching a Machine Learning Problem                                                               357\\nHumans in the Loop                                                                                                  358'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 7, 'page_label': 'vi', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Humans in the Loop                                                                                                  358\\nFrom Prototype to Production                                                                                    359\\nTesting Production Systems                                                                                         359\\nBuilding Y our Own Estimator                                                                                     360\\nWhere to Go from Here                                                                                                361\\nTheory                                                                                                                          361\\nOther Machine Learning Frameworks and Packages                                           362\\nRanking, Recommender Systems, and Other Kinds of Learning                       363\\nProbabilistic Modeling, Inference, and Probabilistic Programming                  363'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 7, 'page_label': 'vi', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Ranking, Recommender Systems, and Other Kinds of Learning                       363\\nProbabilistic Modeling, Inference, and Probabilistic Programming                  363\\nNeural Networks                                                                                                        364\\nScaling to Larger Datasets                                                                                         364\\nHoning Y our Skills                                                                                                     365\\nConclusion                                                                                                                      366\\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  367\\nvi | Table of Contents'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 8, 'page_label': 'vii', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Preface\\nMachine learning is an integral part of many commercial applications and research\\nprojects today, in areas ranging from medical diagnosis and treatment to finding your\\nfriends on social networks. Many people think that machine learning can only be\\napplied by large companies with extensive research teams. In this book, we want to\\nshow you how easy it can be to build machine learning solutions yourself, and how to\\nbest go about it. With the knowledge in this book, you can build your own system for\\nfinding out how people feel on Twitter, or making predictions about global warming.\\nThe applications of machine learning are endless and, with the amount of data avail‐\\nable today, mostly limited by your imagination.\\nWho Should Read This Book\\nThis book is for current and aspiring machine learning practitioners looking to\\nimplement solutions to real-world machine learning problems. This is an introduc‐\\ntory book requiring no previous knowledge of machine learning or artificial intelli‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 8, 'page_label': 'vii', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='implement solutions to real-world machine learning problems. This is an introduc‐\\ntory book requiring no previous knowledge of machine learning or artificial intelli‐\\ngence (AI). We focus on using Python and the scikit-learn library, and work\\nthrough all the steps to create a successful machine learning application. The meth‐\\nods we introduce will be helpful for scientists and researchers, as well as data scien‐\\ntists working on commercial applications. Y ou will get the most out of the book if you\\nare somewhat familiar with Python and the NumPy and matplotlib libraries.\\nWe made a conscious effort not to focus too much on the math, but rather on the\\npractical aspects of using machine learning algorithms. As mathematics (probability\\ntheory, in particular) is the foundation upon which machine learning is built, we\\nwon’t go into the analysis of the algorithms in great detail. If you are interested in the\\nmathematics of machine learning algorithms, we recommend the book The Elements'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 8, 'page_label': 'vii', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='won’t go into the analysis of the algorithms in great detail. If you are interested in the\\nmathematics of machine learning algorithms, we recommend the book The Elements\\nof Statistical Learning  (Springer) by Trevor Hastie, Robert Tibshirani, and Jerome\\nFriedman, which is available for free at the authors’ website . We will also not describe\\nhow to write machine learning algorithms from scratch, and will instead focus on\\nvii'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 9, 'page_label': 'viii', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='how to use the large array of models already implemented in scikit-learn and other\\nlibraries.\\nWhy We Wrote This Book\\nThere are many books on machine learning and AI. However, all of them are meant\\nfor graduate students or PhD students in computer science, and they’re full of\\nadvanced mathematics. This is in stark contrast with how machine learning is being\\nused, as a commodity tool in research and commercial applications. Today, applying\\nmachine learning does not require a PhD. However, there are few resources out there\\nthat fully cover all the important aspects of implementing machine learning in prac‐\\ntice, without requiring you to take advanced math courses. We hope this book will\\nhelp people who want to apply machine learning without reading up on years’ worth\\nof calculus, linear algebra, and probability theory.\\nNavigating This Book\\nThis book is organized roughly as follows:\\n• Chapter 1  introduces the fundamental concepts of machine learning and its'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 9, 'page_label': 'viii', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='of calculus, linear algebra, and probability theory.\\nNavigating This Book\\nThis book is organized roughly as follows:\\n• Chapter 1  introduces the fundamental concepts of machine learning and its\\napplications, and describes the setup we will be using throughout the book.\\n• Chapters 2 and 3 describe the actual machine learning algorithms that are most\\nwidely used in practice, and discuss their advantages and shortcomings.\\n• Chapter 4 discusses the importance of how we represent data that is processed by\\nmachine learning, and what aspects of the data to pay attention to.\\n• Chapter 5 covers advanced methods for model evaluation and parameter tuning,\\nwith a particular focus on cross-validation and grid search.\\n• Chapter 6 explains the concept of pipelines for chaining models and encapsulat‐\\ning your workflow.\\n• Chapter 7 shows how to apply the methods described in earlier chapters to text\\ndata, and introduces some text-specific processing techniques.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 9, 'page_label': 'viii', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='ing your workflow.\\n• Chapter 7 shows how to apply the methods described in earlier chapters to text\\ndata, and introduces some text-specific processing techniques.\\n• Chapter 8 offers a high-level overview, and includes references to more advanced\\ntopics.\\nWhile Chapters 2 and 3 provide the actual algorithms, understanding all of these\\nalgorithms might not be necessary for a beginner. If you need to build a machine\\nlearning system ASAP , we suggest starting with Chapter 1 and the opening sections of\\nChapter 2, which introduce all the core concepts. Y ou can then skip to “Summary and\\nOutlook” on page 127 in Chapter 2, which includes a list of all the supervised models\\nthat we cover. Choose the model that best fits your needs and flip back to read the\\nviii | Preface'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 10, 'page_label': 'ix', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='section devoted to it for details. Then you can use the techniques in Chapter 5 to eval‐\\nuate and tune your model.\\nOnline Resources\\nWhile studying this book, definitely refer to the scikit-learn website for more in-\\ndepth documentation of the classes and functions, and many examples. There is also\\na video course created by Andreas Müller, “ Advanced Machine Learning with scikit-\\nlearn, ” that supplements this book. Y ou can find it at http://bit.ly/\\nadvanced_machine_learning_scikit-learn.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program ele‐\\nments such as variable or function names, databases, data types, environment\\nvariables, statements, and keywords. Also used for commands and module and\\npackage names.\\nConstant width bold'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 10, 'page_label': 'ix', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='ments such as variable or function names, databases, data types, environment\\nvariables, statements, and keywords. Also used for commands and module and\\npackage names.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by values deter‐\\nmined by context.\\nThis element signifies a tip or suggestion.\\nThis element signifies a general note.\\nPreface | ix'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 11, 'page_label': 'x', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='This icon indicates a warning or caution.\\nUsing Code Examples\\nSupplemental material (code examples, IPython notebooks, etc.) is available for\\ndownload at https://github.com/amueller/introduction_to_ml_with_python.\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 11, 'page_label': 'x', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='cant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “An Introduction to Machine Learning\\nwith Python by Andreas C. Müller and Sarah Guido (O’Reilly). Copyright 2017 Sarah\\nGuido and Andreas Müller, 978-1-449-36941-5. ”\\nIf you feel your use of code examples falls outside fair use or the permission given\\nabove, feel free to contact us at permissions@oreilly.com.\\nSafari® Books Online\\nSafari Books Online is an on-demand digital library that deliv‐\\ners expert content in both book and video form from the\\nworld’s leading authors in technology and business.\\nTechnology professionals, software developers, web designers, and business and crea‐\\ntive professionals use Safari Books Online as their primary resource for research,\\nproblem solving, learning, and certification training.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 11, 'page_label': 'x', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='tive professionals use Safari Books Online as their primary resource for research,\\nproblem solving, learning, and certification training.\\nSafari Books Online offers a range of plans and pricing  for enterprise, government,\\neducation, and individuals.\\nMembers have access to thousands of books, training videos, and prepublication\\nmanuscripts in one fully searchable database from publishers like O’Reilly Media,\\nPrentice Hall Professional, Addison-Wesley Professional, Microsoft Press, Sams, Que,\\nx | Preface'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 12, 'page_label': 'xi', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Peachpit Press, Focal Press, Cisco Press, John Wiley & Sons, Syngress, Morgan Kauf‐\\nmann, IBM Redbooks, Packt, Adobe Press, FT Press, Apress, Manning, New Riders,\\nMcGraw-Hill, Jones & Bartlett, Course Technology, and hundreds more. For more\\ninformation about Safari Books Online, please visit us online.\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. Y ou can access this page at http://bit.ly/intro-machine-learning-python.\\nTo comment or ask technical questions about this book, send email to bookques‐\\ntions@oreilly.com.\\nFor more information about our books, courses, conferences, and news, see our web‐\\nsite at http://www.oreilly.com.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 12, 'page_label': 'xi', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='tions@oreilly.com.\\nFor more information about our books, courses, conferences, and news, see our web‐\\nsite at http://www.oreilly.com.\\nFind us on Facebook: http://facebook.com/oreilly\\nFollow us on Twitter: http://twitter.com/oreillymedia\\nWatch us on Y ouTube: http://www.youtube.com/oreillymedia\\nAcknowledgments\\nFrom Andreas\\nWithout the help and support of a large group of people, this book would never have\\nexisted.\\nI would like to thank the editors, Meghan Blanchette, Brian MacDonald, and in par‐\\nticular Dawn Schanafelt, for helping Sarah and me make this book a reality.\\nI want to thank my reviewers, Thomas Caswell, Olivier Grisel, Stefan van der Walt,\\nand John Myles White, who took the time to read the early versions of this book and\\nprovided me with invaluable feedback—in addition to being some of the corner‐\\nstones of the scientific open source ecosystem.\\nPreface | xi'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 13, 'page_label': 'xii', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='I am forever thankful for the welcoming open source scientific Python community,\\nespecially the contributors to scikit-learn. Without the support and help from this\\ncommunity, in particular from Gael Varoquaux, Alex Gramfort, and Olivier Grisel, I\\nwould never have become a core contributor to scikit-learn or learned to under‐\\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\\nutors who donate their time to improve and maintain this package.\\nI’m also thankful for the discussions with many of my colleagues and peers that hel‐\\nped me understand the challenges of machine learning and gave me ideas for struc‐\\nturing a textbook. Among the people I talk to about machine learning, I specifically\\nwant to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe,\\nHugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\\nand Dan Cervone.\\nMy thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 13, 'page_label': 'xii', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Hugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\\nand Dan Cervone.\\nMy thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader\\nof an early version of this book, and helped me shape it in many ways.\\nOn the personal side, I want to thank my parents, Harald and Margot, and my sister,\\nMiriam, for their continuing support and encouragement. I also want to thank the\\nmany people in my life whose love and friendship gave me the energy and support to\\nundertake such a challenging task.\\nFrom Sarah\\nI would like to thank Meg Blanchette, without whose help and guidance this project\\nwould not have even existed. Thanks to Celia La and Brian Carlson for reading in the\\nearly days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\\nto DTS, for your everlasting and endless support.\\nxii | Preface'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 14, 'page_label': '1', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 1\\nIntroduction\\nMachine learning is about extracting knowledge from data. It is a research field at the\\nintersection of statistics, artificial intelligence, and computer science and is also\\nknown as predictive analytics or statistical learning. The application of machine\\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\\nmatic recommendations of which movies to watch, to what food to order or which\\nproducts to buy, to personalized online radio and recognizing your friends in your\\nphotos, many modern websites and devices have machine learning algorithms at their\\ncore. When you look at a complex website like Facebook, Amazon, or Netflix, it is\\nvery likely that every part of the site contains multiple machine learning models.\\nOutside of commercial applications, machine learning has had a tremendous influ‐\\nence on the way data-driven research is done today. The tools introduced in this book'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 14, 'page_label': '1', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Outside of commercial applications, machine learning has had a tremendous influ‐\\nence on the way data-driven research is done today. The tools introduced in this book\\nhave been applied to diverse scientific problems such as understanding stars, finding\\ndistant planets, discovering new particles, analyzing DNA sequences, and providing\\npersonalized cancer treatments.\\nY our application doesn’t need to be as large-scale or world-changing as these exam‐\\nples in order to benefit from machine learning, though. In this chapter, we will\\nexplain why machine learning has become so popular and discuss what kinds of\\nproblems can be solved using machine learning. Then, we will show you how to build\\nyour first machine learning model, introducing important concepts along the way.\\nWhy Machine Learning?\\nIn the early days of “intelligent” applications, many systems used handcoded rules of\\n“if ” and “else” decisions to process data or adjust to user input. Think of a spam filter'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 14, 'page_label': '1', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Why Machine Learning?\\nIn the early days of “intelligent” applications, many systems used handcoded rules of\\n“if ” and “else” decisions to process data or adjust to user input. Think of a spam filter\\nwhose job is to move the appropriate incoming email messages to a spam folder. Y ou\\ncould make up a blacklist of words that would result in an email being marked as\\n1'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 15, 'page_label': '2', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='spam. This would be an example of using an expert-designed rule system to design an\\n“intelligent” application. Manually crafting decision rules is feasible for some applica‐\\ntions, particularly those in which humans have a good understanding of the process\\nto model. However, using handcoded rules to make decisions has two major disad‐\\nvantages:\\n• The logic required to make a decision is specific to a single domain and task.\\nChanging the task even slightly might require a rewrite of the whole system.\\n• Designing rules requires a deep understanding of how a decision should be made\\nby a human expert.\\nOne example of where this handcoded approach will fail is in detecting faces in\\nimages. Today, every smartphone can detect a face in an image. However, face detec‐\\ntion was an unsolved problem until as recently as 2001. The main problem is that the\\nway in which pixels (which make up an image in a computer) are “perceived” by the'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 15, 'page_label': '2', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='tion was an unsolved problem until as recently as 2001. The main problem is that the\\nway in which pixels (which make up an image in a computer) are “perceived” by the\\ncomputer is very different from how humans perceive a face. This difference in repre‐\\nsentation makes it basically impossible for a human to come up with a good set of\\nrules to describe what constitutes a face in a digital image.\\nUsing machine learning, however, simply presenting a program with a large collec‐\\ntion of images of faces is enough for an algorithm to determine what characteristics\\nare needed to identify a face.\\nProblems Machine Learning Can Solve\\nThe most successful kinds of machine learning algorithms are those that automate\\ndecision-making processes by generalizing from known examples. In this setting,\\nwhich is known as supervised learning, the user provides the algorithm with pairs of\\ninputs and desired outputs, and the algorithm finds a way to produce the desired out‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 15, 'page_label': '2', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='which is known as supervised learning, the user provides the algorithm with pairs of\\ninputs and desired outputs, and the algorithm finds a way to produce the desired out‐\\nput given an input. In particular, the algorithm is able to create an output for an input\\nit has never seen before without any help from a human. Going back to our example\\nof spam classification, using machine learning, the user provides the algorithm with a\\nlarge number of emails (which are the input), together with information about\\nwhether any of these emails are spam (which is the desired output). Given a new\\nemail, the algorithm will then produce a prediction as to whether the new email is\\nspam.\\nMachine learning algorithms that learn from input/output pairs are called supervised\\nlearning algorithms because a “teacher” provides supervision to the algorithms in the\\nform of the desired outputs for each example that they learn from. While creating a'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 15, 'page_label': '2', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='learning algorithms because a “teacher” provides supervision to the algorithms in the\\nform of the desired outputs for each example that they learn from. While creating a\\ndataset of inputs and outputs is often a laborious manual process, supervised learning\\nalgorithms are well understood and their performance is easy to measure. If your\\napplication can be formulated as a supervised learning problem, and you are able to\\n2 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 16, 'page_label': '3', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='create a dataset that includes the desired outcome, machine learning will likely be\\nable to solve your problem.\\nExamples of supervised machine learning tasks include:\\nIdentifying the zip code from handwritten digits on an envelope\\nHere the input is a scan of the handwriting, and the desired output is the actual\\ndigits in the zip code. To create a dataset for building a machine learning model,\\nyou need to collect many envelopes. Then you can read the zip codes yourself\\nand store the digits as your desired outcomes.\\nDetermining whether a tumor is benign based on a medical image\\nHere the input is the image, and the output is whether the tumor is benign. To\\ncreate a dataset for building a model, you need a database of medical images. Y ou\\nalso need an expert opinion, so a doctor needs to look at all of the images and\\ndecide which tumors are benign and which are not. It might even be necessary to\\ndo additional diagnosis beyond the content of the image to determine whether'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 16, 'page_label': '3', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='decide which tumors are benign and which are not. It might even be necessary to\\ndo additional diagnosis beyond the content of the image to determine whether\\nthe tumor in the image is cancerous or not.\\nDetecting fraudulent activity in credit card transactions\\nHere the input is a record of the credit card transaction, and the output is\\nwhether it is likely to be fraudulent or not. Assuming that you are the entity dis‐\\ntributing the credit cards, collecting a dataset means storing all transactions and\\nrecording if a user reports any transaction as fraudulent.\\nAn interesting thing to note about these examples is that although the inputs and out‐\\nputs look fairly straightforward, the data collection process for these three tasks is\\nvastly different. While reading envelopes is laborious, it is easy and cheap. Obtaining\\nmedical imaging and diagnoses, on the other hand, requires not only expensive\\nmachinery but also rare and expensive expert knowledge, not to mention the ethical'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 16, 'page_label': '3', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='medical imaging and diagnoses, on the other hand, requires not only expensive\\nmachinery but also rare and expensive expert knowledge, not to mention the ethical\\nconcerns and privacy issues. In the example of detecting credit card fraud, data col‐\\nlection is much simpler. Y our customers will provide you with the desired output, as\\nthey will report fraud. All you have to do to obtain the input/output pairs of fraudu‐\\nlent and nonfraudulent activity is wait.\\nUnsupervised algorithms are the other type of algorithm that we will cover in this\\nbook. In unsupervised learning, only the input data is known, and no known output\\ndata is given to the algorithm. While there are many successful applications of these\\nmethods, they are usually harder to understand and evaluate.\\nExamples of unsupervised learning include:\\nIdentifying topics in a set of blog posts\\nIf you have a large collection of text data, you might want to summarize it and'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 16, 'page_label': '3', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Examples of unsupervised learning include:\\nIdentifying topics in a set of blog posts\\nIf you have a large collection of text data, you might want to summarize it and\\nfind prevalent themes in it. Y ou might not know beforehand what these topics\\nare, or how many topics there might be. Therefore, there are no known outputs.\\nWhy Machine Learning? | 3'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 17, 'page_label': '4', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Segmenting customers into groups with similar preferences\\nGiven a set of customer records, you might want to identify which customers are\\nsimilar, and whether there are groups of customers with similar preferences. For\\na shopping site, these might be “parents, ” “bookworms, ” or “gamers. ” Because you\\ndon’t know in advance what these groups might be, or even how many there are,\\nyou have no known outputs.\\nDetecting abnormal access patterns to a website\\nTo identify abuse or bugs, it is often helpful to find access patterns that are differ‐\\nent from the norm. Each abnormal pattern might be very different, and you\\nmight not have any recorded instances of abnormal behavior. Because in this\\nexample you only observe traffic, and you don’t know what constitutes normal\\nand abnormal behavior, this is an unsupervised problem.\\nFor both supervised and unsupervised learning tasks, it is important to have a repre‐\\nsentation of your input data that a computer can understand. Often it is helpful to'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 17, 'page_label': '4', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='For both supervised and unsupervised learning tasks, it is important to have a repre‐\\nsentation of your input data that a computer can understand. Often it is helpful to\\nthink of your data as a table. Each data point that you want to reason about (each\\nemail, each customer, each transaction) is a row, and each property that describes that\\ndata point (say, the age of a customer or the amount or location of a transaction) is a\\ncolumn. Y ou might describe users by their age, their gender, when they created an\\naccount, and how often they have bought from your online shop. Y ou might describe\\nthe image of a tumor by the grayscale values of each pixel, or maybe by using the size,\\nshape, and color of the tumor.\\nEach entity or row here is known as a sample (or data point) in machine learning,\\nwhile the columns—the properties that describe these entities—are called features.\\nLater in this book we will go into more detail on the topic of building a good repre‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 17, 'page_label': '4', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='while the columns—the properties that describe these entities—are called features.\\nLater in this book we will go into more detail on the topic of building a good repre‐\\nsentation of your data, which is called feature extraction or feature engineering. Y ou\\nshould keep in mind, however, that no machine learning algorithm will be able to\\nmake a prediction on data for which it has no information. For example, if the only\\nfeature that you have for a patient is their last name, no algorithm will be able to pre‐\\ndict their gender. This information is simply not contained in your data. If you add\\nanother feature that contains the patient’s first name, you will have much better luck,\\nas it is often possible to tell the gender by a person’s first name.\\nKnowing Your Task and Knowing Your Data\\nQuite possibly the most important part in the machine learning process is under‐\\nstanding the data you are working with and how it relates to the task you want to'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 17, 'page_label': '4', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Quite possibly the most important part in the machine learning process is under‐\\nstanding the data you are working with and how it relates to the task you want to\\nsolve. It will not be effective to randomly choose an algorithm and throw your data at\\nit. It is necessary to understand what is going on in your dataset before you begin\\nbuilding a model. Each algorithm is different in terms of what kind of data and what\\nproblem setting it works best for. While you are building a machine learning solution,\\nyou should answer, or at least keep in mind, the following questions:\\n4 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 18, 'page_label': '5', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='• What question(s) am I trying to answer? Do I think the data collected can answer\\nthat question?\\n• What is the best way to phrase my question(s) as a machine learning problem?\\n• Have I collected enough data to represent the problem I want to solve?\\n• What features of the data did I extract, and will these enable the right\\npredictions?\\n• How will I measure success in my application?\\n• How will the machine learning solution interact with other parts of my research\\nor business product?\\nIn a larger context, the algorithms and methods in machine learning are only one\\npart of a greater process to solve a particular problem, and it is good to keep the big\\npicture in mind at all times. Many people spend a lot of time building complex\\nmachine learning solutions, only to find out they don’t solve the right problem.\\nWhen going deep into the technical aspects of machine learning (as we will in this\\nbook), it is easy to lose sight of the ultimate goals. While we will not discuss the ques‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 18, 'page_label': '5', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='When going deep into the technical aspects of machine learning (as we will in this\\nbook), it is easy to lose sight of the ultimate goals. While we will not discuss the ques‐\\ntions listed here in detail, we still encourage you to keep in mind all the assumptions\\nthat you might be making, explicitly or implicitly, when you start building machine\\nlearning models.\\nWhy Python?\\nPython has become the lingua franca for many data science applications. It combines\\nthe power of general-purpose programming languages with the ease of use of\\ndomain-specific scripting languages like MATLAB or R. Python has libraries for data\\nloading, visualization, statistics, natural language processing, image processing, and\\nmore. This vast toolbox provides data scientists with a large array of general- and\\nspecial-purpose functionality. One of the main advantages of using Python is the abil‐\\nity to interact directly with the code, using a terminal or other tools like the Jupyter'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 18, 'page_label': '5', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='special-purpose functionality. One of the main advantages of using Python is the abil‐\\nity to interact directly with the code, using a terminal or other tools like the Jupyter\\nNotebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\\nmentally iterative processes, in which the data drives the analysis. It is essential for\\nthese processes to have tools that allow quick iteration and easy interaction.\\nAs a general-purpose programming language, Python also allows for the creation of\\ncomplex graphical user interfaces (GUIs) and web services, and for integration into\\nexisting systems.\\nscikit-learn\\nscikit-learn is an open source project, meaning that it is free to use and distribute,\\nand anyone can easily obtain the source code to see what is going on behind the\\nWhy Python? | 5'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 19, 'page_label': '6', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='scenes. The scikit-learn project is constantly being developed and improved, and it\\nhas a very active user community. It contains a number of state-of-the-art machine\\nlearning algorithms, as well as comprehensive documentation about each algorithm.\\nscikit-learn is a very popular tool, and the most prominent Python library for\\nmachine learning. It is widely used in industry and academia, and a wealth of tutori‐\\nals and code snippets are available online. scikit-learn works well with a number of\\nother scientific Python tools, which we will discuss later in this chapter.\\nWhile reading this, we recommend that you also browse the scikit-learn user guide \\nand API documentation for additional details on and many more options for each\\nalgorithm. The online documentation is very thorough, and this book will provide\\nyou with all the prerequisites in machine learning to understand it in detail.\\nInstalling scikit-learn\\nscikit-learn depends on two other Python packages, NumPy and SciPy. For plot‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 19, 'page_label': '6', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='you with all the prerequisites in machine learning to understand it in detail.\\nInstalling scikit-learn\\nscikit-learn depends on two other Python packages, NumPy and SciPy. For plot‐\\nting and interactive development, you should also install matplotlib, IPython, and\\nthe Jupyter Notebook. We recommend using one of the following prepackaged\\nPython distributions, which will provide the necessary packages:\\nAnaconda\\nA Python distribution made for large-scale data processing, predictive analytics,\\nand scientific computing. Anaconda comes with NumPy, SciPy, matplotlib,\\npandas, IPython, Jupyter Notebook, and scikit-learn. Available on Mac OS,\\nWindows, and Linux, it is a very convenient solution and is the one we suggest\\nfor people without an existing installation of the scientific Python packages. Ana‐\\nconda now also includes the commercial Intel MKL library for free. Using MKL\\n(which is done automatically when Anaconda is installed) can give significant'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 19, 'page_label': '6', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='conda now also includes the commercial Intel MKL library for free. Using MKL\\n(which is done automatically when Anaconda is installed) can give significant\\nspeed improvements for many algorithms in scikit-learn.\\nEnthought Canopy\\nAnother Python distribution for scientific computing. This comes with NumPy,\\nSciPy, matplotlib, pandas, and IPython, but the free version does not come with\\nscikit-learn. If you are part of an academic, degree-granting institution, you\\ncan request an academic license and get free access to the paid subscription ver‐\\nsion of Enthought Canopy. Enthought Canopy is available for Python 2.7.x, and\\nworks on Mac OS, Windows, and Linux.\\nPython(x,y)\\nA free Python distribution for scientific computing, specifically for Windows.\\nPython(x,y) comes with NumPy, SciPy, matplotlib, pandas, IPython, and\\nscikit-learn.\\n6 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 20, 'page_label': '7', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='1 If you are unfamiliar with NumPy or matplotlib, we recommend reading the first chapter of the SciPy Lec‐\\nture Notes.\\nIf you already have a Python installation set up, you can use pip to install all of these\\npackages:\\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\\nEssential Libraries and Tools\\nUnderstanding what scikit-learn is and how to use it is important, but there are a\\nfew other libraries that will enhance your experience. scikit-learn is built on top of\\nthe NumPy and SciPy scientific Python libraries. In addition to NumPy and SciPy, we\\nwill be using pandas and matplotlib. We will also introduce the Jupyter Notebook,\\nwhich is a browser-based interactive programming environment. Briefly, here is what\\nyou should know about these tools in order to get the most out of scikit-learn.1\\nJupyter Notebook\\nThe Jupyter Notebook is an interactive environment for running code in the browser.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 20, 'page_label': '7', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='you should know about these tools in order to get the most out of scikit-learn.1\\nJupyter Notebook\\nThe Jupyter Notebook is an interactive environment for running code in the browser.\\nIt is a great tool for exploratory data analysis and is widely used by data scientists.\\nWhile the Jupyter Notebook supports many programming languages, we only need\\nthe Python support. The Jupyter Notebook makes it easy to incorporate code, text,\\nand images, and all of this book was in fact written as a Jupyter Notebook. All of the\\ncode examples we include can be downloaded from GitHub.\\nNumPy\\nNumPy is one of the fundamental packages for scientific computing in Python. It\\ncontains functionality for multidimensional arrays, high-level mathematical func‐\\ntions such as linear algebra operations and the Fourier transform, and pseudorandom\\nnumber generators.\\nIn scikit-learn, the NumPy array is the fundamental data structure. scikit-learn'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 20, 'page_label': '7', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='tions such as linear algebra operations and the Fourier transform, and pseudorandom\\nnumber generators.\\nIn scikit-learn, the NumPy array is the fundamental data structure. scikit-learn\\ntakes in data in the form of NumPy arrays. Any data you’re using will have to be con‐\\nverted to a NumPy array. The core functionality of NumPy is the ndarray class, a\\nmultidimensional ( n-dimensional) array. All elements of the array must be of the\\nsame type. A NumPy array looks like this:\\nIn[2]:\\nimport numpy as np\\nx = np.array([[1, 2, 3], [4, 5, 6]])\\nprint(\"x:\\\\n{}\".format(x))\\nEssential Libraries and Tools | 7'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 21, 'page_label': '8', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Out[2]:\\nx:\\n[[1 2 3]\\n [4 5 6]]\\nWe will be using NumPy a lot in this book, and we will refer to objects of the NumPy\\nndarray class as “NumPy arrays” or just “arrays. ”\\nSciPy\\nSciPy is a collection of functions for scientific computing in Python. It provides,\\namong other functionality, advanced linear algebra routines, mathematical function\\noptimization, signal processing, special mathematical functions, and statistical distri‐\\nbutions. scikit-learn draws from SciPy’s collection of functions for implementing\\nits algorithms. The most important part of SciPy for us is scipy.sparse: this provides\\nsparse matrices, which are another representation that is used for data in scikit-\\nlearn. Sparse matrices are used whenever we want to store a 2D array that contains\\nmostly zeros:\\nIn[3]:\\nfrom scipy import sparse\\n# Create a 2D NumPy array with a diagonal of ones, and zeros everywhere else\\neye = np.eye(4)\\nprint(\"NumPy array:\\\\n{}\".format(eye))\\nOut[3]:\\nNumPy array:\\n[[ 1.  0.  0.  0.]\\n [ 0.  1.  0.  0.]'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 21, 'page_label': '8', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='# Create a 2D NumPy array with a diagonal of ones, and zeros everywhere else\\neye = np.eye(4)\\nprint(\"NumPy array:\\\\n{}\".format(eye))\\nOut[3]:\\nNumPy array:\\n[[ 1.  0.  0.  0.]\\n [ 0.  1.  0.  0.]\\n [ 0.  0.  1.  0.]\\n [ 0.  0.  0.  1.]]\\nIn[4]:\\n# Convert the NumPy array to a SciPy sparse matrix in CSR format\\n# Only the nonzero entries are stored\\nsparse_matrix = sparse.csr_matrix(eye)\\nprint(\"\\\\nSciPy sparse CSR matrix:\\\\n{}\".format(sparse_matrix))\\nOut[4]:\\nSciPy sparse CSR matrix:\\n  (0, 0)    1.0\\n  (1, 1)    1.0\\n  (2, 2)    1.0\\n  (3, 3)    1.0\\n8 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 22, 'page_label': '9', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Usually it is not possible to create dense representations of sparse data (as they would\\nnot fit into memory), so we need to create sparse representations directly. Here is a\\nway to create the same sparse matrix as before, using the COO format:\\nIn[5]:\\ndata = np.ones(4)\\nrow_indices = np.arange(4)\\ncol_indices = np.arange(4)\\neye_coo = sparse.coo_matrix((data, (row_indices, col_indices)))\\nprint(\"COO representation:\\\\n{}\".format(eye_coo))\\nOut[5]:\\nCOO representation:\\n  (0, 0)    1.0\\n  (1, 1)    1.0\\n  (2, 2)    1.0\\n  (3, 3)    1.0\\nMore details on SciPy sparse matrices can be found in the SciPy Lecture Notes.\\nmatplotlib\\nmatplotlib is the primary scientific plotting library in Python. It provides functions\\nfor making publication-quality visualizations such as line charts, histograms, scatter\\nplots, and so on. Visualizing your data and different aspects of your analysis can give\\nyou important insights, and we will be using matplotlib for all our visualizations.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 22, 'page_label': '9', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='plots, and so on. Visualizing your data and different aspects of your analysis can give\\nyou important insights, and we will be using matplotlib for all our visualizations.\\nWhen working inside the Jupyter Notebook, you can show figures directly in the\\nbrowser by using the %matplotlib notebook and %matplotlib inline commands.\\nWe recommend using %matplotlib notebook , which provides an interactive envi‐\\nronment (though we are using %matplotlib inline  to produce this book). For\\nexample, this code produces the plot in Figure 1-1:\\nIn[6]:\\n%matplotlib inline\\nimport matplotlib.pyplot as plt\\n# Generate a sequence of numbers from -10 to 10 with 100 steps in between\\nx = np.linspace(-10, 10, 100)\\n# Create a second array using sine\\ny = np.sin(x)\\n# The plot function makes a line chart of one array against another\\nplt.plot(x, y, marker=\"x\")\\nEssential Libraries and Tools | 9'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 23, 'page_label': '10', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 1-1. Simple line plot of the sine function using matplotlib\\npandas\\npandas is a Python library for data wrangling and analysis. It is built around a data\\nstructure called the DataFrame that is modeled after the R DataFrame. Simply put, a\\npandas DataFrame is a table, similar to an Excel spreadsheet. pandas provides a great\\nrange of methods to modify and operate on this table; in particular, it allows SQL-like\\nqueries and joins of tables. In contrast to NumPy, which requires that all entries in an\\narray be of the same type, pandas allows each column to have a separate type (for\\nexample, integers, dates, floating-point numbers, and strings). Another valuable tool\\nprovided by pandas is its ability to ingest from a great variety of file formats and data‐\\nbases, like SQL, Excel files, and comma-separated values (CSV) files. Going into\\ndetail about the functionality of pandas is out of the scope of this book. However,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 23, 'page_label': '10', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='bases, like SQL, Excel files, and comma-separated values (CSV) files. Going into\\ndetail about the functionality of pandas is out of the scope of this book. However,\\nPython for Data Analysis  by Wes McKinney (O’Reilly, 2012) provides a great guide.\\nHere is a small example of creating a DataFrame using a dictionary:\\nIn[7]:\\nimport pandas as pd\\n# create a simple dataset of people\\ndata = {\\'Name\\': [\"John\", \"Anna\", \"Peter\", \"Linda\"],\\n        \\'Location\\' : [\"New York\", \"Paris\", \"Berlin\", \"London\"],\\n        \\'Age\\' : [24, 13, 53, 33]\\n       }\\ndata_pandas = pd.DataFrame(data)\\n# IPython.display allows \"pretty printing\" of dataframes\\n# in the Jupyter notebook\\ndisplay(data_pandas)\\n10 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 24, 'page_label': '11', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='This produces the following output:\\nAge Location Name\\n0 24 New York John\\n1 13 Paris Anna\\n2 53 Berlin Peter\\n3 33 London Linda\\nThere are several possible ways to query this table. For example:\\nIn[8]:\\n# Select all rows that have an age column greater than 30\\ndisplay(data_pandas[data_pandas.Age > 30])\\nThis produces the following result:\\nAge Location Name\\n2 53 Berlin Peter\\n3 33 London Linda\\nmglearn\\nThis book comes with accompanying code, which you can find on GitHub. The\\naccompanying code includes not only all the examples shown in this book, but also\\nthe mglearn library. This is a library of utility functions we wrote for this book, so\\nthat we don’t clutter up our code listings with details of plotting and data loading. If\\nyou’re interested, you can look up all the functions in the repository, but the details of\\nthe mglearn module are not really important to the material in this book. If you see a\\ncall to mglearn in the code, it is usually a way to make a pretty picture quickly, or to'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 24, 'page_label': '11', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the mglearn module are not really important to the material in this book. If you see a\\ncall to mglearn in the code, it is usually a way to make a pretty picture quickly, or to\\nget our hands on some interesting data.\\nThroughout the book we make ample use of NumPy, matplotlib\\nand pandas. All the code will assume the following imports:\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport mglearn\\nWe also assume that you will run the code in a Jupyter Notebook\\nwith the %matplotlib notebook  or %matplotlib inline  magic\\nenabled to show plots. If you are not using the notebook or these\\nmagic commands, you will have to call plt.show to actually show\\nany of the figures.\\nEssential Libraries and Tools | 11'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 25, 'page_label': '12', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='2 The six package can be very handy for that.\\nPython 2 Versus Python 3\\nThere are two major versions of Python that are widely used at the moment: Python 2\\n(more precisely, 2.7) and Python 3 (with the latest release being 3.5 at the time of\\nwriting). This sometimes leads to some confusion. Python 2 is no longer actively\\ndeveloped, but because Python 3 contains major changes, Python 2 code usually does\\nnot run on Python 3. If you are new to Python, or are starting a new project from\\nscratch, we highly recommend using the latest version of Python 3 without changes.\\nIf you have a large codebase that you rely on that is written for Python 2, you are\\nexcused from upgrading for now. However, you should try to migrate to Python 3 as\\nsoon as possible. When writing any new code, it is for the most part quite easy to\\nwrite code that runs under Python 2 and Python 3.2 If you don’t have to interface with\\nlegacy software, you should definitely use Python 3. All the code in this book is writ‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 25, 'page_label': '12', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='write code that runs under Python 2 and Python 3.2 If you don’t have to interface with\\nlegacy software, you should definitely use Python 3. All the code in this book is writ‐\\nten in a way that works for both versions. However, the exact output might differ\\nslightly under Python 2.\\nVersions Used in this Book\\nWe are using the following versions of the previously mentioned libraries in this\\nbook:\\nIn[9]:\\nimport sys\\nprint(\"Python version: {}\".format(sys.version))\\nimport pandas as pd\\nprint(\"pandas version: {}\".format(pd.__version__))\\nimport matplotlib\\nprint(\"matplotlib version: {}\".format(matplotlib.__version__))\\nimport numpy as np\\nprint(\"NumPy version: {}\".format(np.__version__))\\nimport scipy as sp\\nprint(\"SciPy version: {}\".format(sp.__version__))\\nimport IPython\\nprint(\"IPython version: {}\".format(IPython.__version__))\\nimport sklearn\\nprint(\"scikit-learn version: {}\".format(sklearn.__version__))\\n12 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 26, 'page_label': '13', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Out[9]:\\nPython version: 3.5.2 |Anaconda 4.1.1 (64-bit)| (default, Jul  2 2016, 17:53:06)\\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\\npandas version: 0.18.1\\nmatplotlib version: 1.5.1\\nNumPy version: 1.11.1\\nSciPy version: 0.17.1\\nIPython version: 5.1.0\\nscikit-learn version: 0.18\\nWhile it is not important to match these versions exactly, you should have a version\\nof scikit-learn that is as least as recent as the one we used.\\nNow that we have everything set up, let’s dive into our first application of machine\\nlearning.\\nThis book assumes that you have version 0.18 or later of scikit-\\nlearn. The model_selection module was added in 0.18, and if you\\nuse an earlier version of scikit-learn, you will need to adjust the\\nimports from this module.\\nA First Application: Classifying Iris Species\\nIn this section, we will go through a simple machine learning application and create\\nour first model. In the process, we will introduce some core concepts and terms.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 26, 'page_label': '13', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In this section, we will go through a simple machine learning application and create\\nour first model. In the process, we will introduce some core concepts and terms.\\nLet’s assume that a hobby botanist is interested in distinguishing the species of some\\niris flowers that she has found. She has collected some measurements associated with\\neach iris: the length and width of the petals and the length and width of the sepals, all\\nmeasured in centimeters (see Figure 1-2).\\nShe also has the measurements of some irises that have been previously identified by\\nan expert botanist as belonging to the species setosa, versicolor, or virginica. For these\\nmeasurements, she can be certain of which species each iris belongs to. Let’s assume\\nthat these are the only species our hobby botanist will encounter in the wild.\\nOur goal is to build a machine learning model that can learn from the measurements\\nof these irises whose species is known, so that we can predict the species for a new\\niris.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 26, 'page_label': '13', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Our goal is to build a machine learning model that can learn from the measurements\\nof these irises whose species is known, so that we can predict the species for a new\\niris.\\nA First Application: Classifying Iris Species | 13'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 27, 'page_label': '14', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 1-2. Parts of the iris flower\\nBecause we have measurements for which we know the correct species of iris, this is a\\nsupervised learning problem. In this problem, we want to predict one of several\\noptions (the species of iris). This is an example of a classification problem. The possi‐\\nble outputs (different species of irises) are called classes. Every iris in the dataset\\nbelongs to one of three classes, so this problem is a three-class classification problem.\\nThe desired output for a single data point (an iris) is the species of this flower. For a\\nparticular data point, the species it belongs to is called its label.\\nMeet the Data\\nThe data we will use for this example is the Iris dataset, a classical dataset in machine\\nlearning and statistics. It is included in scikit-learn in the datasets module. We\\ncan load it by calling the load_iris function:\\nIn[10]:\\nfrom sklearn.datasets import load_iris\\niris_dataset = load_iris()'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 27, 'page_label': '14', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='can load it by calling the load_iris function:\\nIn[10]:\\nfrom sklearn.datasets import load_iris\\niris_dataset = load_iris()\\nThe iris object that is returned by load_iris is a Bunch object, which is very similar\\nto a dictionary. It contains keys and values:\\n14 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 28, 'page_label': '15', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[11]:\\nprint(\"Keys of iris_dataset: \\\\n{}\".format(iris_dataset.keys()))\\nOut[11]:\\nKeys of iris_dataset:\\ndict_keys([\\'target_names\\', \\'feature_names\\', \\'DESCR\\', \\'data\\', \\'target\\'])\\nThe value of the key DESCR is a short description of the dataset. We show the begin‐\\nning of the description here (feel free to look up the rest yourself):\\nIn[12]:\\nprint(iris_dataset[\\'DESCR\\'][:193] + \"\\\\n...\")\\nOut[12]:\\nIris Plants Database\\n====================\\nNotes\\n----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive att\\n...\\n----\\nThe value of the key target_names is an array of strings, containing the species of\\nflower that we want to predict:\\nIn[13]:\\nprint(\"Target names: {}\".format(iris_dataset[\\'target_names\\']))\\nOut[13]:\\nTarget names: [\\'setosa\\' \\'versicolor\\' \\'virginica\\']\\nThe value of feature_names is a list of strings, giving the description of each feature:\\nIn[14]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 28, 'page_label': '15', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Out[13]:\\nTarget names: [\\'setosa\\' \\'versicolor\\' \\'virginica\\']\\nThe value of feature_names is a list of strings, giving the description of each feature:\\nIn[14]:\\nprint(\"Feature names: \\\\n{}\".format(iris_dataset[\\'feature_names\\']))\\nOut[14]:\\nFeature names:\\n[\\'sepal length (cm)\\', \\'sepal width (cm)\\', \\'petal length (cm)\\',\\n \\'petal width (cm)\\']\\nThe data itself is contained in the target and data fields. data contains the numeric\\nmeasurements of sepal length, sepal width, petal length, and petal width in a NumPy\\narray:\\nA First Application: Classifying Iris Species | 15'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 29, 'page_label': '16', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[15]:\\nprint(\"Type of data: {}\".format(type(iris_dataset[\\'data\\'])))\\nOut[15]:\\nType of data: <class \\'numpy.ndarray\\'>\\nThe rows in the data array correspond to flowers, while the columns represent the\\nfour measurements that were taken for each flower:\\nIn[16]:\\nprint(\"Shape of data: {}\".format(iris_dataset[\\'data\\'].shape))\\nOut[16]:\\nShape of data: (150, 4)\\nWe see that the array contains measurements for 150 different flowers. Remember\\nthat the individual items are called samples in machine learning, and their properties\\nare called features. The shape of the data array is the number of samples multiplied by\\nthe number of features. This is a convention in scikit-learn, and your data will\\nalways be assumed to be in this shape. Here are the feature values for the first five\\nsamples:\\nIn[17]:\\nprint(\"First five columns of data:\\\\n{}\".format(iris_dataset[\\'data\\'][:5]))\\nOut[17]:\\nFirst five columns of data:\\n[[ 5.1  3.5  1.4  0.2]\\n [ 4.9  3.   1.4  0.2]\\n [ 4.7  3.2  1.3  0.2]\\n [ 4.6  3.1  1.5  0.2]'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 29, 'page_label': '16', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Out[17]:\\nFirst five columns of data:\\n[[ 5.1  3.5  1.4  0.2]\\n [ 4.9  3.   1.4  0.2]\\n [ 4.7  3.2  1.3  0.2]\\n [ 4.6  3.1  1.5  0.2]\\n [ 5.   3.6  1.4  0.2]]\\nFrom this data, we can see that all of the first five flowers have a petal width of 0.2 cm\\nand that the first flower has the longest sepal, at 5.1 cm.\\nThe target array contains the species of each of the flowers that were measured, also\\nas a NumPy array:\\nIn[18]:\\nprint(\"Type of target: {}\".format(type(iris_dataset[\\'target\\'])))\\nOut[18]:\\nType of target: <class \\'numpy.ndarray\\'>\\ntarget is a one-dimensional array, with one entry per flower:\\n16 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 30, 'page_label': '17', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[19]:\\nprint(\"Shape of target: {}\".format(iris_dataset[\\'target\\'].shape))\\nOut[19]:\\nShape of target: (150,)\\nThe species are encoded as integers from 0 to 2:\\nIn[20]:\\nprint(\"Target:\\\\n{}\".format(iris_dataset[\\'target\\']))\\nOut[20]:\\nTarget:\\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\\n 2 2]\\nThe meanings of the numbers are given by the iris[\\'target_names\\'] array:\\n0 means setosa, 1 means versicolor, and 2 means virginica.\\nMeasuring Success: Training and Testing Data\\nWe want to build a machine learning model from this data that can predict the spe‐\\ncies of iris for a new set of measurements. But before we can apply our model to new\\nmeasurements, we need to know whether it actually works—that is, whether we\\nshould trust its predictions.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 30, 'page_label': '17', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='cies of iris for a new set of measurements. But before we can apply our model to new\\nmeasurements, we need to know whether it actually works—that is, whether we\\nshould trust its predictions.\\nUnfortunately, we cannot use the data we used to build the model to evaluate it. This\\nis because our model can always simply remember the whole training set, and will\\ntherefore always predict the correct label for any point in the training set. This\\n“remembering” does not indicate to us whether our model will generalize well (in\\nother words, whether it will also perform well on new data).\\nTo assess the model’s performance, we show it new data (data that it hasn’t seen\\nbefore) for which we have labels. This is usually done by splitting the labeled data we\\nhave collected (here, our 150 flower measurements) into two parts. One part of the\\ndata is used to build our machine learning model, and is called the training data or'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 30, 'page_label': '17', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='have collected (here, our 150 flower measurements) into two parts. One part of the\\ndata is used to build our machine learning model, and is called the training data or\\ntraining set. The rest of the data will be used to assess how well the model works; this\\nis called the test data, test set, or hold-out set.\\nscikit-learn contains a function that shuffles the dataset and splits it for you: the\\ntrain_test_split function. This function extracts 75% of the rows in the data as the\\ntraining set, together with the corresponding labels for this data. The remaining 25%\\nof the data, together with the remaining labels, is declared as the test set. Deciding\\nA First Application: Classifying Iris Species | 17'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 31, 'page_label': '18', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"how much data you want to put into the training and the test set respectively is some‐\\nwhat arbitrary, but using a test set containing 25% of the data is a good rule of thumb.\\nIn scikit-learn, data is usually denoted with a capital X, while labels are denoted by\\na lowercase y. This is inspired by the standard formulation f(x)=y in mathematics,\\nwhere x is the input to a function and y is the output. Following more conventions\\nfrom mathematics, we use a capital X because the data is a two-dimensional array (a\\nmatrix) and a lowercase y because the target is a one-dimensional array (a vector).\\nLet’s call train_test_split on our data and assign the outputs using this nomencla‐\\nture:\\nIn[21]:\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(\\n    iris_dataset['data'], iris_dataset['target'], random_state=0)\\nBefore making the split, the train_test_split function shuffles the dataset using a\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 31, 'page_label': '18', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"iris_dataset['data'], iris_dataset['target'], random_state=0)\\nBefore making the split, the train_test_split function shuffles the dataset using a\\npseudorandom number generator. If we just took the last 25% of the data as a test set,\\nall the data points would have the label 2, as the data points are sorted by the label\\n(see the output for iris['target'] shown earlier). Using a test set containing only\\none of the three classes would not tell us much about how well our model generalizes,\\nso we shuffle our data to make sure the test data contains data from all classes.\\nTo make sure that we will get the same output if we run the same function several\\ntimes, we provide the pseudorandom number generator with a fixed seed using the\\nrandom_state parameter. This will make the outcome deterministic, so this line will\\nalways have the same outcome. We will always fix the random_state in this way when\\nusing randomized procedures in this book.\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 31, 'page_label': '18', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='always have the same outcome. We will always fix the random_state in this way when\\nusing randomized procedures in this book.\\nThe output of the train_test_split function is X_train, X_test, y_train, and\\ny_test, which are all NumPy arrays. X_train contains 75% of the rows of the dataset,\\nand X_test contains the remaining 25%:\\nIn[22]:\\nprint(\"X_train shape: {}\".format(X_train.shape))\\nprint(\"y_train shape: {}\".format(y_train.shape))\\nOut[22]:\\nX_train shape: (112, 4)\\ny_train shape: (112,)\\n18 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 32, 'page_label': '19', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[23]:\\nprint(\"X_test shape: {}\".format(X_test.shape))\\nprint(\"y_test shape: {}\".format(y_test.shape))\\nOut[23]:\\nX_test shape: (38, 4)\\ny_test shape: (38,)\\nFirst Things First: Look at Your Data\\nBefore building a machine learning model it is often a good idea to inspect the data,\\nto see if the task is easily solvable without machine learning, or if the desired infor‐\\nmation might not be contained in the data.\\nAdditionally, inspecting your data is a good way to find abnormalities and peculiari‐\\nties. Maybe some of your irises were measured using inches and not centimeters, for\\nexample. In the real world, inconsistencies in the data and unexpected measurements\\nare very common.\\nOne of the best ways to inspect data is to visualize it. One way to do this is by using a\\nscatter plot. A scatter plot of the data puts one feature along the x-axis and another\\nalong the y-axis, and draws a dot for each data point. Unfortunately, computer'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 32, 'page_label': '19', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='scatter plot. A scatter plot of the data puts one feature along the x-axis and another\\nalong the y-axis, and draws a dot for each data point. Unfortunately, computer\\nscreens have only two dimensions, which allows us to plot only two (or maybe three)\\nfeatures at a time. It is difficult to plot datasets with more than three features this way.\\nOne way around this problem is to do a pair plot, which looks at all possible pairs of\\nfeatures. If you have a small number of features, such as the four we have here, this is\\nquite reasonable. Y ou should keep in mind, however, that a pair plot does not show\\nthe interaction of all of features at once, so some interesting aspects of the data may\\nnot be revealed when visualizing it this way.\\nFigure 1-3 is a pair plot of the features in the training set. The data points are colored\\naccording to the species the iris belongs to. To create the plot, we first convert the\\nNumPy array into a pandas DataFrame. pandas has a function to create pair plots'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 32, 'page_label': '19', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"according to the species the iris belongs to. To create the plot, we first convert the\\nNumPy array into a pandas DataFrame. pandas has a function to create pair plots\\ncalled scatter_matrix. The diagonal of this matrix is filled with histograms of each\\nfeature:\\nIn[24]:\\n# create dataframe from data in X_train\\n# label the columns using the strings in iris_dataset.feature_names\\niris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)\\n# create a scatter matrix from the dataframe, color by y_train\\ngrr = pd.scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15), marker='o',\\n                        hist_kwds={'bins': 20}, s=60, alpha=.8, cmap=mglearn.cm3)\\nA First Application: Classifying Iris Species | 19\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 33, 'page_label': '20', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 1-3. Pair plot of the Iris dataset, colored by class label\\nFrom the plots, we can see that the three classes seem to be relatively well separated\\nusing the sepal and petal measurements. This means that a machine learning model\\nwill likely be able to learn to separate them.\\nBuilding Your First Model: k-Nearest Neighbors\\nNow we can start building the actual machine learning model. There are many classi‐\\nfication algorithms in scikit-learn that we could use. Here we will use a k-nearest\\nneighbors classifier, which is easy to understand. Building this model only consists of\\nstoring the training set. To make a prediction for a new data point, the algorithm\\nfinds the point in the training set that is closest to the new point. Then it assigns the\\nlabel of this training point to the new data point.\\n20 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 34, 'page_label': '21', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='The k in k-nearest neighbors signifies that instead of using only the closest neighbor\\nto the new data point, we can consider any fixed number k of neighbors in the train‐\\ning (for example, the closest three or five neighbors). Then, we can make a prediction\\nusing the majority class among these neighbors. We will go into more detail about\\nthis in Chapter 2; for now, we’ll use only a single neighbor.\\nAll machine learning models in scikit-learn are implemented in their own classes,\\nwhich are called Estimator classes. The k-nearest neighbors classification algorithm\\nis implemented in the KNeighborsClassifier class in the neighbors module. Before\\nwe can use the model, we need to instantiate the class into an object. This is when we\\nwill set any parameters of the model. The most important parameter of KNeighbor\\nsClassifier is the number of neighbors, which we will set to 1:\\nIn[25]:\\nfrom sklearn.neighbors import KNeighborsClassifier\\nknn = KNeighborsClassifier(n_neighbors=1)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 34, 'page_label': '21', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"sClassifier is the number of neighbors, which we will set to 1:\\nIn[25]:\\nfrom sklearn.neighbors import KNeighborsClassifier\\nknn = KNeighborsClassifier(n_neighbors=1)\\nThe knn object encapsulates the algorithm that will be used to build the model from\\nthe training data, as well the algorithm to make predictions on new data points. It will\\nalso hold the information that the algorithm has extracted from the training data. In\\nthe case of KNeighborsClassifier, it will just store the training set.\\nTo build the model on the training set, we call the fit method of the knn object,\\nwhich takes as arguments the NumPy array X_train containing the training data and\\nthe NumPy array y_train of the corresponding training labels:\\nIn[26]:\\nknn.fit(X_train, y_train)\\nOut[26]:\\nKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\\n           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\\n           weights='uniform')\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 34, 'page_label': '21', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"knn.fit(X_train, y_train)\\nOut[26]:\\nKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\\n           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\\n           weights='uniform')\\nThe fit method returns the knn object itself (and modifies it in place), so we get a\\nstring representation of our classifier. The representation shows us which parameters\\nwere used in creating the model. Nearly all of them are the default values, but you can\\nalso find n_neighbors=1, which is the parameter that we passed. Most models in\\nscikit-learn have many parameters, but the majority of them are either speed opti‐\\nmizations or for very special use cases. Y ou don’t have to worry about the other\\nparameters shown in this representation. Printing a scikit-learn model can yield\\nvery long strings, but don’t be intimidated by these. We will cover all the important\\nparameters in Chapter 2. In the remainder of this book, we will not show the output\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 34, 'page_label': '21', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='very long strings, but don’t be intimidated by these. We will cover all the important\\nparameters in Chapter 2. In the remainder of this book, we will not show the output\\nof fit because it doesn’t contain any new information.\\nA First Application: Classifying Iris Species | 21'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 35, 'page_label': '22', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Making Predictions\\nWe can now make predictions using this model on new data for which we might not\\nknow the correct labels. Imagine we found an iris in the wild with a sepal length of\\n5 cm, a sepal width of 2.9 cm, a petal length of 1 cm, and a petal width of 0.2 cm.\\nWhat species of iris would this be? We can put this data into a NumPy array, again by\\ncalculating the shape—that is, the number of samples (1) multiplied by the number of\\nfeatures (4):\\nIn[27]:\\nX_new = np.array([[5, 2.9, 1, 0.2]])\\nprint(\"X_new.shape: {}\".format(X_new.shape))\\nOut[27]:\\nX_new.shape: (1, 4)\\nNote that we made the measurements of this single flower into a row in a two-\\ndimensional NumPy array, as scikit-learn always expects two-dimensional arrays\\nfor the data.\\nTo make a prediction, we call the predict method of the knn object:\\nIn[28]:\\nprediction = knn.predict(X_new)\\nprint(\"Prediction: {}\".format(prediction))\\nprint(\"Predicted target name: {}\".format(\\n       iris_dataset[\\'target_names\\'][prediction]))\\nOut[28]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 35, 'page_label': '22', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[28]:\\nprediction = knn.predict(X_new)\\nprint(\"Prediction: {}\".format(prediction))\\nprint(\"Predicted target name: {}\".format(\\n       iris_dataset[\\'target_names\\'][prediction]))\\nOut[28]:\\nPrediction: [0]\\nPredicted target name: [\\'setosa\\']\\nOur model predicts that this new iris belongs to the class 0, meaning its species is\\nsetosa. But how do we know whether we can trust our model? We don’t know the cor‐\\nrect species of this sample, which is the whole point of building the model!\\nEvaluating the Model\\nThis is where the test set that we created earlier comes in. This data was not used to\\nbuild the model, but we do know what the correct species is for each iris in the test\\nset.\\nTherefore, we can make a prediction for each iris in the test data and compare it\\nagainst its label (the known species). We can measure how well the model works by\\ncomputing the accuracy, which is the fraction of flowers for which the right species\\nwas predicted:\\n22 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 36, 'page_label': '23', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[29]:\\ny_pred = knn.predict(X_test)\\nprint(\"Test set predictions:\\\\n {}\".format(y_pred))\\nOut[29]:\\nTest set predictions:\\n [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0 2]\\nIn[30]:\\nprint(\"Test set score: {:.2f}\".format(np.mean(y_pred == y_test)))\\nOut[30]:\\nTest set score: 0.97\\nWe can also use the score method of the knn object, which will compute the test set\\naccuracy for us:\\nIn[31]:\\nprint(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))\\nOut[31]:\\nTest set score: 0.97\\nFor this model, the test set accuracy is about 0.97, which means we made the right\\nprediction for 97% of the irises in the test set. Under some mathematical assump‐\\ntions, this means that we can expect our model to be correct 97% of the time for new\\nirises. For our hobby botanist application, this high level of accuracy means that our\\nmodel may be trustworthy enough to use. In later chapters we will discuss how we\\ncan improve performance, and what caveats there are in tuning a model.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 36, 'page_label': '23', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='model may be trustworthy enough to use. In later chapters we will discuss how we\\ncan improve performance, and what caveats there are in tuning a model.\\nSummary and Outlook\\nLet’s summarize what we learned in this chapter. We started with a brief introduction\\nto machine learning and its applications, then discussed the distinction between\\nsupervised and unsupervised learning and gave an overview of the tools we’ll be\\nusing in this book. Then, we formulated the task of predicting which species of iris a\\nparticular flower belongs to by using physical measurements of the flower. We used a\\ndataset of measurements that was annotated by an expert with the correct species to\\nbuild our model, making this a supervised learning task. There were three possible\\nspecies, setosa, versicolor, or virginica, which made the task a three-class classification\\nproblem. The possible species are called classes in the classification problem, and the\\nspecies of a single iris is called its label.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 36, 'page_label': '23', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='problem. The possible species are called classes in the classification problem, and the\\nspecies of a single iris is called its label.\\nThe Iris dataset consists of two NumPy arrays: one containing the data, which is\\nreferred to as X in scikit-learn, and one containing the correct or desired outputs,\\nSummary and Outlook | 23'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 37, 'page_label': '24', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='which is called y. The array X is a two-dimensional array of features, with one row per\\ndata point and one column per feature. The array y is a one-dimensional array, which\\nhere contains one class label, an integer ranging from 0 to 2, for each of the samples.\\nWe split our dataset into a training set, to build our model, and a test set, to evaluate\\nhow well our model will generalize to new, previously unseen data.\\nWe chose the k-nearest neighbors classification algorithm, which makes predictions\\nfor a new data point by considering its closest neighbor(s) in the training set. This is\\nimplemented in the KNeighborsClassifier class, which contains the algorithm that\\nbuilds the model as well as the algorithm that makes a prediction using the model.\\nWe instantiated the class, setting parameters. Then we built the model by calling the\\nfit method, passing the training data ( X_train) and training outputs ( y_train) as'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 37, 'page_label': '24', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='We instantiated the class, setting parameters. Then we built the model by calling the\\nfit method, passing the training data ( X_train) and training outputs ( y_train) as\\nparameters. We evaluated the model using the score method, which computes the\\naccuracy of the model. We applied the score method to the test set data and the test\\nset labels and found that our model is about 97% accurate, meaning it is correct 97%\\nof the time on the test set.\\nThis gave us the confidence to apply the model to new data (in our example, new\\nflower measurements) and trust that the model will be correct about 97% of the time.\\nHere is a summary of the code needed for the whole training and evaluation\\nprocedure:\\nIn[32]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    iris_dataset[\\'data\\'], iris_dataset[\\'target\\'], random_state=0)\\nknn = KNeighborsClassifier(n_neighbors=1)\\nknn.fit(X_train, y_train)\\nprint(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))\\nOut[32]:\\nTest set score: 0.97'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 37, 'page_label': '24', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='knn = KNeighborsClassifier(n_neighbors=1)\\nknn.fit(X_train, y_train)\\nprint(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))\\nOut[32]:\\nTest set score: 0.97\\nThis snippet contains the core code for applying any machine learning algorithm\\nusing scikit-learn. The fit, predict, and score methods are the common inter‐\\nface to supervised models in scikit-learn, and with the concepts introduced in this\\nchapter, you can apply these models to many machine learning tasks. In the next\\nchapter, we will go into more depth about the different kinds of supervised models in\\nscikit-learn and how to apply them successfully.\\n24 | Chapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 38, 'page_label': '25', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 2\\nSupervised Learning\\nAs we mentioned earlier, supervised machine learning is one of the most commonly\\nused and successful types of machine learning. In this chapter, we will describe super‐\\nvised learning in more detail and explain several popular supervised learning algo‐\\nrithms. We already saw an application of supervised machine learning in Chapter 1:\\nclassifying iris flowers into several species using physical measurements of the\\nflowers.\\nRemember that supervised learning is used whenever we want to predict a certain\\noutcome from a given input, and we have examples of input/output pairs. We build a\\nmachine learning model from these input/output pairs, which comprise our training\\nset. Our goal is to make accurate predictions for new, never-before-seen data. Super‐\\nvised learning often requires human effort to build the training set, but afterward\\nautomates and often speeds up an otherwise laborious or infeasible task.\\nClassification  and Regression'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 38, 'page_label': '25', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='vised learning often requires human effort to build the training set, but afterward\\nautomates and often speeds up an otherwise laborious or infeasible task.\\nClassification  and Regression\\nThere are two major types of supervised machine learning problems, called classifica‐\\ntion and regression.\\nIn classification, the goal is to predict a class label, which is a choice from a predefined\\nlist of possibilities. In Chapter 1 we used the example of classifying irises into one of\\nthree possible species. Classification is sometimes separated into binary classification,\\nwhich is the special case of distinguishing between exactly two classes, and multiclass\\nclassification, which is classification between more than two classes. Y ou can think of\\nbinary classification as trying to answer a yes/no question. Classifying emails as\\neither spam or not spam is an example of a binary classification problem. In this\\nbinary classification task, the yes/no question being asked would be “Is this email'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 38, 'page_label': '25', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='either spam or not spam is an example of a binary classification problem. In this\\nbinary classification task, the yes/no question being asked would be “Is this email\\nspam?”\\n25'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 39, 'page_label': '26', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='1 We ask linguists to excuse the simplified presentation of languages as distinct and fixed entities.\\nIn binary classification we often speak of one class being the posi‐\\ntive class and the other class being the negative class. Here, positive\\ndoesn’t represent having benefit or value, but rather what the object\\nof the study is. So, when looking for spam, “positive” could mean\\nthe spam class. Which of the two classes is called positive is often a\\nsubjective matter, and specific to the domain.\\nThe iris example, on the other hand, is an example of a multiclass classification prob‐\\nlem. Another example is predicting what language a website is in from the text on the\\nwebsite. The classes here would be a pre-defined list of possible languages.\\nFor regression tasks, the goal is to predict a continuous number, or a floating-point\\nnumber in programming terms (or real number in mathematical terms). Predicting a\\nperson’s annual income from their education, their age, and where they live is an'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 39, 'page_label': '26', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='number in programming terms (or real number in mathematical terms). Predicting a\\nperson’s annual income from their education, their age, and where they live is an\\nexample of a regression task. When predicting income, the predicted value is an\\namount, and can be any number in a given range. Another example of a regression\\ntask is predicting the yield of a corn farm given attributes such as previous yields,\\nweather, and number of employees working on the farm. The yield again can be an\\narbitrary number.\\nAn easy way to distinguish between classification and regression tasks is to ask\\nwhether there is some kind of continuity in the output. If there is continuity between\\npossible outcomes, then the problem is a regression problem. Think about predicting\\nannual income. There is a clear continuity in the output. Whether a person makes\\n$40,000 or $40,001 a year does not make a tangible difference, even though these are'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 39, 'page_label': '26', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='annual income. There is a clear continuity in the output. Whether a person makes\\n$40,000 or $40,001 a year does not make a tangible difference, even though these are\\ndifferent amounts of money; if our algorithm predicts $39,999 or $40,001 when it\\nshould have predicted $40,000, we don’t mind that much.\\nBy contrast, for the task of recognizing the language of a website (which is a classifi‐\\ncation problem), there is no matter of degree. A website is in one language, or it is in\\nanother. There is no continuity between languages, and there is no language that is\\nbetween English and French.1\\nGeneralization, \\nOverfitting,  and Underfitting\\nIn supervised learning, we want to build a model on the training data and then be\\nable to make accurate predictions on new, unseen data that has the same characteris‐\\ntics as the training set that we used. If a model is able to make accurate predictions on\\nunseen data, we say it is able to generalize from the training set to the test set. We'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 39, 'page_label': '26', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='tics as the training set that we used. If a model is able to make accurate predictions on\\nunseen data, we say it is able to generalize from the training set to the test set. We\\nwant to build a model that is able to generalize as accurately as possible.\\n26 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 40, 'page_label': '27', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='2 In the real world, this is actually a tricky problem. While we know that the other customers haven’t bought a\\nboat from us yet, they might have bought one from someone else, or they may still be saving and plan to buy\\none in the future.\\nUsually we build a model in such a way that it can make accurate predictions on the\\ntraining set. If the training and test sets have enough in common, we expect the\\nmodel to also be accurate on the test set. However, there are some cases where this\\ncan go wrong. For example, if we allow ourselves to build very complex models, we\\ncan always be as accurate as we like on the training set.\\nLet’s take a look at a made-up example to illustrate this point. Say a novice data scien‐\\ntist wants to predict whether a customer will buy a boat, given records of previous\\nboat buyers and customers who we know are not interested in buying a boat. 2 The\\ngoal is to send out promotional emails to people who are likely to actually make a'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 40, 'page_label': '27', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='boat buyers and customers who we know are not interested in buying a boat. 2 The\\ngoal is to send out promotional emails to people who are likely to actually make a\\npurchase, but not bother those customers who won’t be interested.\\nSuppose we have the customer records shown in Table 2-1.\\nTable 2-1. Example data about customers\\nAge Number of \\ncars owned\\nOwns house Number of children Marital status Owns a dog Bought a boat\\n66 1 yes 2 widowed no yes\\n52 2 yes 3 married no yes\\n22 0 no 0 married yes no\\n25 1 no 1 single no no\\n44 0 no 2 divorced yes no\\n39 1 yes 2 married yes no\\n26 1 no 2 single no no\\n40 3 yes 1 married yes no\\n53 2 yes 2 divorced no yes\\n64 2 yes 3 divorced no no\\n58 2 yes 2 married yes yes\\n33 1 no 1 single no no\\nAfter looking at the data for a while, our novice data scientist comes up with the fol‐\\nlowing rule: “If the customer is older than 45, and has less than 3 children or is not\\ndivorced, then they want to buy a boat. ” When asked how well this rule of his does,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 40, 'page_label': '27', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='lowing rule: “If the customer is older than 45, and has less than 3 children or is not\\ndivorced, then they want to buy a boat. ” When asked how well this rule of his does,\\nour data scientist answers, “It’ s 100 percent accurate!” And indeed, on the data that is\\nin the table, the rule is perfectly accurate. There are many possible rules we could\\ncome up with that would explain perfectly if someone in this dataset wants to buy a\\nboat. No age appears twice in the data, so we could say people who are 66, 52, 53, or\\nGeneralization, Overfitting,  and Underfitting  | 27'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 41, 'page_label': '28', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='3 And also provably, with the right math.\\n58 years old want to buy a boat, while all others don’t. While we can make up many\\nrules that work well on this data, remember that we are not interested in making pre‐\\ndictions for this dataset; we already know the answers for these customers. We want\\nto know if new customers are likely to buy a boat. We therefore want to find a rule that\\nwill work well for new customers, and achieving 100 percent accuracy on the training\\nset does not help us there. We might not expect that the rule our data scientist came\\nup with will work very well on new customers. It seems too complex, and it is sup‐\\nported by very little data. For example, the “or is not divorced” part of the rule hinges\\non a single customer.\\nThe only measure of whether an algorithm will perform well on new data is the eval‐\\nuation on the test set. However, intuitively 3 we expect simple models to generalize'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 41, 'page_label': '28', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='on a single customer.\\nThe only measure of whether an algorithm will perform well on new data is the eval‐\\nuation on the test set. However, intuitively 3 we expect simple models to generalize\\nbetter to new data. If the rule was “People older than 50 want to buy a boat, ” and this\\nwould explain the behavior of all the customers, we would trust it more than the rule\\ninvolving children and marital status in addition to age. Therefore, we always want to\\nfind the simplest model. Building a model that is too complex for the amount of\\ninformation we have, as our novice data scientist did, is called overfitting. Overfitting\\noccurs when you fit a model too closely to the particularities of the training set and\\nobtain a model that works well on the training set but is not able to generalize to new\\ndata. On the other hand, if your model is too simple—say, “Everybody who owns a\\nhouse buys a boat”—then you might not be able to capture all the aspects of and vari‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 41, 'page_label': '28', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='data. On the other hand, if your model is too simple—say, “Everybody who owns a\\nhouse buys a boat”—then you might not be able to capture all the aspects of and vari‐\\nability in the data, and your model will do badly even on the training set. Choosing\\ntoo simple a model is called underfitting.\\nThe more complex we allow our model to be, the better we will be able to predict on\\nthe training data. However, if our model becomes too complex, we start focusing too\\nmuch on each individual data point in our training set, and the model will not gener‐\\nalize well to new data.\\nThere is a sweet spot in between that will yield the best generalization performance.\\nThis is the model we want to find.\\nThe trade-off between overfitting and underfitting is illustrated in Figure 2-1.\\n28 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 42, 'page_label': '29', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-1. Trade-off of model complexity against training and test accuracy\\nRelation of Model Complexity to Dataset Size\\nIt’s important to note that model complexity is intimately tied to the variation of\\ninputs contained in your training dataset: the larger variety of data points your data‐\\nset contains, the more complex a model you can use without overfitting. Usually, col‐\\nlecting more data points will yield more variety, so larger datasets allow building\\nmore complex models. However, simply duplicating the same data points or collect‐\\ning very similar data will not help.\\nGoing back to the boat selling example, if we saw 10,000 more rows of customer data,\\nand all of them complied with the rule “If the customer is older than 45, and has less\\nthan 3 children or is not divorced, then they want to buy a boat, ” we would be much\\nmore likely to believe this to be a good rule than when it was developed using only\\nthe 12 rows in Table 2-1.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 42, 'page_label': '29', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='than 3 children or is not divorced, then they want to buy a boat, ” we would be much\\nmore likely to believe this to be a good rule than when it was developed using only\\nthe 12 rows in Table 2-1.\\nHaving more data and building appropriately more complex models can often work\\nwonders for supervised learning tasks. In this book, we will focus on working with\\ndatasets of fixed sizes. In the real world, you often have the ability to decide how\\nmuch data to collect, which might be more beneficial than tweaking and tuning your\\nmodel. Never underestimate the power of more data.\\nSupervised Machine Learning Algorithms\\nWe will now review the most popular machine learning algorithms and explain how\\nthey learn from data and how they make predictions. We will also discuss how the\\nconcept of model complexity plays out for each of these models, and provide an over‐\\nSupervised Machine Learning Algorithms | 29'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 43, 'page_label': '30', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='4 Discussing all of them is beyond the scope of the book, and we refer you to the scikit-learn documentation\\nfor more details.\\nview of how each algorithm builds a model. We will examine the strengths and weak‐\\nnesses of each algorithm, and what kind of data they can best be applied to. We will\\nalso explain the meaning of the most important parameters and options. 4 Many algo‐\\nrithms have a classification and a regression variant, and we will describe both.\\nIt is not necessary to read through the descriptions of each algorithm in detail, but\\nunderstanding the models will give you a better feeling for the different ways\\nmachine learning algorithms can work. This chapter can also be used as a reference\\nguide, and you can come back to it when you are unsure about the workings of any of\\nthe algorithms.\\nSome Sample Datasets\\nWe will use several datasets to illustrate the different algorithms. Some of the datasets\\nwill be small and synthetic (meaning made-up), designed to highlight particular'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 43, 'page_label': '30', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Some Sample Datasets\\nWe will use several datasets to illustrate the different algorithms. Some of the datasets\\nwill be small and synthetic (meaning made-up), designed to highlight particular\\naspects of the algorithms. Other datasets will be large, real-world examples.\\nAn example of a synthetic two-class classification dataset is the forge dataset, which\\nhas two features. The following code creates a scatter plot ( Figure 2-2) visualizing all\\nof the data points in this dataset. The plot has the first feature on the x-axis and the\\nsecond feature on the y-axis. As is always the case in scatter plots, each data point is\\nrepresented as one dot. The color and shape of the dot indicates its class:\\nIn[2]:\\n# generate dataset\\nX, y = mglearn.datasets.make_forge()\\n# plot dataset\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\nplt.legend([\"Class 0\", \"Class 1\"], loc=4)\\nplt.xlabel(\"First feature\")\\nplt.ylabel(\"Second feature\")\\nprint(\"X.shape: {}\".format(X.shape))\\nOut[2]:\\nX.shape: (26, 2)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 43, 'page_label': '30', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='plt.legend([\"Class 0\", \"Class 1\"], loc=4)\\nplt.xlabel(\"First feature\")\\nplt.ylabel(\"Second feature\")\\nprint(\"X.shape: {}\".format(X.shape))\\nOut[2]:\\nX.shape: (26, 2)\\n30 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 44, 'page_label': '31', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-2. Scatter plot of the forge dataset\\nAs you can see from X.shape, this dataset consists of 26 data points, with 2 features.\\nTo illustrate regression algorithms, we will use the synthetic wave dataset. The wave\\ndataset has a single input feature and a continuous target variable (or response) that\\nwe want to model. The plot created here ( Figure 2-3) shows the single feature on the\\nx-axis and the regression target (the output) on the y-axis:\\nIn[3]:\\nX, y = mglearn.datasets.make_wave(n_samples=40)\\nplt.plot(X, y, \\'o\\')\\nplt.ylim(-3, 3)\\nplt.xlabel(\"Feature\")\\nplt.ylabel(\"Target\")\\nSupervised Machine Learning Algorithms | 31'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 45, 'page_label': '32', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-3. Plot of the wave dataset, with the x-axis showing the feature and the y-axis\\nshowing the regression target\\nWe are using these very simple, low-dimensional datasets because we can easily visu‐\\nalize them—a printed page has two dimensions, so data with more than two features\\nis hard to show. Any intuition derived from datasets with few features (also called\\nlow-dimensional datasets) might not hold in datasets with many features ( high-\\ndimensional datasets). As long as you keep that in mind, inspecting algorithms on\\nlow-dimensional datasets can be very instructive.\\nWe will complement these small synthetic datasets with two real-world datasets that\\nare included in scikit-learn. One is the Wisconsin Breast Cancer dataset ( cancer,\\nfor short), which records clinical measurements of breast cancer tumors. Each tumor\\nis labeled as “benign” (for harmless tumors) or “malignant” (for cancerous tumors),\\nand the task is to learn to predict whether a tumor is malignant based on the meas‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 45, 'page_label': '32', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='is labeled as “benign” (for harmless tumors) or “malignant” (for cancerous tumors),\\nand the task is to learn to predict whether a tumor is malignant based on the meas‐\\nurements of the tissue.\\nThe data can be loaded using the load_breast_cancer function from scikit-learn:\\nIn[4]:\\nfrom sklearn.datasets import load_breast_cancer\\ncancer = load_breast_cancer()\\nprint(\"cancer.keys(): \\\\n{}\".format(cancer.keys()))\\n32 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 46, 'page_label': '33', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Out[4]:\\ncancer.keys():\\ndict_keys([\\'feature_names\\', \\'data\\', \\'DESCR\\', \\'target\\', \\'target_names\\'])\\nDatasets that are included in scikit-learn are usually stored as\\nBunch objects, which contain some information about the dataset\\nas well as the actual data. All you need to know about Bunch objects\\nis that they behave like dictionaries, with the added benefit that you\\ncan access values using a dot (as in bunch.key instead of\\nbunch[\\'key\\']).\\nThe dataset consists of 569 data points, with 30 features each:\\nIn[5]:\\nprint(\"Shape of cancer data: {}\".format(cancer.data.shape))\\nOut[5]:\\nShape of cancer data: (569, 30)\\nOf these 569 data points, 212 are labeled as malignant and 357 as benign:\\nIn[6]:\\nprint(\"Sample counts per class:\\\\n{}\".format(\\n      {n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))}))\\nOut[6]:\\nSample counts per class:\\n{\\'benign\\': 357, \\'malignant\\': 212}\\nTo get a description of the semantic meaning of each feature, we can have a look at\\nthe feature_names attribute:\\nIn[7]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 46, 'page_label': '33', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Out[6]:\\nSample counts per class:\\n{\\'benign\\': 357, \\'malignant\\': 212}\\nTo get a description of the semantic meaning of each feature, we can have a look at\\nthe feature_names attribute:\\nIn[7]:\\nprint(\"Feature names:\\\\n{}\".format(cancer.feature_names))\\nOut[7]:\\nFeature names:\\n[\\'mean radius\\' \\'mean texture\\' \\'mean perimeter\\' \\'mean area\\'\\n \\'mean smoothness\\' \\'mean compactness\\' \\'mean concavity\\'\\n \\'mean concave points\\' \\'mean symmetry\\' \\'mean fractal dimension\\'\\n \\'radius error\\' \\'texture error\\' \\'perimeter error\\' \\'area error\\'\\n \\'smoothness error\\' \\'compactness error\\' \\'concavity error\\'\\n \\'concave points error\\' \\'symmetry error\\' \\'fractal dimension error\\'\\n \\'worst radius\\' \\'worst texture\\' \\'worst perimeter\\' \\'worst area\\'\\n \\'worst smoothness\\' \\'worst compactness\\' \\'worst concavity\\'\\n \\'worst concave points\\' \\'worst symmetry\\' \\'worst fractal dimension\\']\\nSupervised Machine Learning Algorithms | 33'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 47, 'page_label': '34', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='5 This is called the binomial coefficient, which is the number of combinations of k elements that can be selected\\nfrom a set of n elements. Often this is written as nk and spoken as “n choose k”—in this case, “13 choose 2. ”\\nY ou can find out more about the data by reading cancer.DESCR if you are interested.\\nWe will also be using a real-world regression dataset, the Boston Housing dataset.\\nThe task associated with this dataset is to predict the median value of homes in sev‐\\neral Boston neighborhoods in the 1970s, using information such as crime rate, prox‐\\nimity to the Charles River, highway accessibility, and so on. The dataset contains 506\\ndata points, described by 13 features:\\nIn[8]:\\nfrom sklearn.datasets import load_boston\\nboston = load_boston()\\nprint(\"Data shape: {}\".format(boston.data.shape))\\nOut[8]:\\nData shape: (506, 13)\\nAgain, you can get more information about the dataset by reading the DESCR attribute'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 47, 'page_label': '34', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='boston = load_boston()\\nprint(\"Data shape: {}\".format(boston.data.shape))\\nOut[8]:\\nData shape: (506, 13)\\nAgain, you can get more information about the dataset by reading the DESCR attribute\\nof boston. For our purposes here, we will actually expand this dataset by not only\\nconsidering these 13 measurements as input features, but also looking at all products\\n(also called interactions) between features. In other words, we will not only consider\\ncrime rate and highway accessibility as features, but also the product of crime rate\\nand highway accessibility. Including derived feature like these is called feature engi‐\\nneering, which we will discuss in more detail in Chapter 4. This derived dataset can be\\nloaded using the load_extended_boston function:\\nIn[9]:\\nX, y = mglearn.datasets.load_extended_boston()\\nprint(\"X.shape: {}\".format(X.shape))\\nOut[9]:\\nX.shape: (506, 104)\\nThe resulting 104 features are the 13 original features together with the 91 possible'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 47, 'page_label': '34', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='X, y = mglearn.datasets.load_extended_boston()\\nprint(\"X.shape: {}\".format(X.shape))\\nOut[9]:\\nX.shape: (506, 104)\\nThe resulting 104 features are the 13 original features together with the 91 possible\\ncombinations of two features within those 13.5\\nWe will use these datasets to explain and illustrate the properties of the different\\nmachine learning algorithms. But for now, let’s get to the algorithms themselves.\\nFirst, we will revisit the k-nearest neighbors (k-NN) algorithm that we saw in the pre‐\\nvious chapter.\\n34 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 48, 'page_label': '35', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='k-Nearest Neighbors\\nThe k-NN algorithm is arguably the simplest machine learning algorithm. Building\\nthe model consists only of storing the training dataset. To make a prediction for a\\nnew data point, the algorithm finds the closest data points in the training dataset—its\\n“nearest neighbors. ”\\nk-Neighbors classification\\nIn its simplest version, the k-NN algorithm only considers exactly one nearest neigh‐\\nbor, which is the closest training data point to the point we want to make a prediction\\nfor. The prediction is then simply the known output for this training point. Figure 2-4\\nillustrates this for the case of classification on the forge dataset:\\nIn[10]:\\nmglearn.plots.plot_knn_classification(n_neighbors=1)\\nFigure 2-4. Predictions made by the one-nearest-neighbor model on the forge dataset\\nHere, we added three new data points, shown as stars. For each of them, we marked\\nthe closest point in the training set. The prediction of the one-nearest-neighbor algo‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 48, 'page_label': '35', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Here, we added three new data points, shown as stars. For each of them, we marked\\nthe closest point in the training set. The prediction of the one-nearest-neighbor algo‐\\nrithm is the label of that point (shown by the color of the cross).\\nSupervised Machine Learning Algorithms | 35'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 49, 'page_label': '36', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Instead of considering only the closest neighbor, we can also consider an arbitrary\\nnumber, k, of neighbors. This is where the name of the k-nearest neighbors algorithm\\ncomes from. When considering more than one neighbor, we use voting to assign a\\nlabel. This means that for each test point, we count how many neighbors belong to\\nclass 0 and how many neighbors belong to class 1. We then assign the class that is\\nmore frequent: in other words, the majority class among the k-nearest neighbors. The\\nfollowing example (Figure 2-5) uses the three closest neighbors:\\nIn[11]:\\nmglearn.plots.plot_knn_classification(n_neighbors=3)\\nFigure 2-5. Predictions made by the three-nearest-neighbors model on the forge dataset\\nAgain, the prediction is shown as the color of the cross. Y ou can see that the predic‐\\ntion for the new data point at the top left is not the same as the prediction when we\\nused only one neighbor.\\nWhile this illustration is for a binary classification problem, this method can be'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 49, 'page_label': '36', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='tion for the new data point at the top left is not the same as the prediction when we\\nused only one neighbor.\\nWhile this illustration is for a binary classification problem, this method can be\\napplied to datasets with any number of classes. For more classes, we count how many\\nneighbors belong to each class and again predict the most common class.\\nNow let’s look at how we can apply the k-nearest neighbors algorithm using scikit-\\nlearn. First, we split our data into a training and a test set so we can evaluate general‐\\nization performance, as discussed in Chapter 1:\\n36 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 50, 'page_label': '37', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[12]:\\nfrom sklearn.model_selection import train_test_split\\nX, y = mglearn.datasets.make_forge()\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\nNext, we import and instantiate the class. This is when we can set parameters, like the\\nnumber of neighbors to use. Here, we set it to 3:\\nIn[13]:\\nfrom sklearn.neighbors import KNeighborsClassifier\\nclf = KNeighborsClassifier(n_neighbors=3)\\nNow, we fit the classifier using the training set. For KNeighborsClassifier this\\nmeans storing the dataset, so we can compute neighbors during prediction:\\nIn[14]:\\nclf.fit(X_train, y_train)\\nTo make predictions on the test data, we call the predict method. For each data point\\nin the test set, this computes its nearest neighbors in the training set and finds the\\nmost common class among these:\\nIn[15]:\\nprint(\"Test set predictions: {}\".format(clf.predict(X_test)))\\nOut[15]:\\nTest set predictions: [1 0 1 0 1 0 0]'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 50, 'page_label': '37', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='most common class among these:\\nIn[15]:\\nprint(\"Test set predictions: {}\".format(clf.predict(X_test)))\\nOut[15]:\\nTest set predictions: [1 0 1 0 1 0 0]\\nTo evaluate how well our model generalizes, we can call the score method with the\\ntest data together with the test labels:\\nIn[16]:\\nprint(\"Test set accuracy: {:.2f}\".format(clf.score(X_test, y_test)))\\nOut[16]:\\nTest set accuracy: 0.86\\nWe see that our model is about 86% accurate, meaning the model predicted the class\\ncorrectly for 86% of the samples in the test dataset.\\nAnalyzing KNeighborsClassifier\\nFor two-dimensional datasets, we can also illustrate the prediction for all possible test\\npoints in the xy-plane. We color the plane according to the class that would be\\nassigned to a point in this region. This lets us view the decision boundary, which is the\\ndivide between where the algorithm assigns class 0 versus where it assigns class 1.\\nSupervised Machine Learning Algorithms | 37'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 51, 'page_label': '38', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='The following code produces the visualizations of the decision boundaries for one,\\nthree, and nine neighbors shown in Figure 2-6:\\nIn[17]:\\nfig, axes = plt.subplots(1, 3, figsize=(10, 3))\\nfor n_neighbors, ax in zip([1, 3, 9], axes):\\n    # the fit method returns the object self, so we can instantiate\\n    # and fit in one line\\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X, y)\\n    mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=.4)\\n    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\\n    ax.set_title(\"{} neighbor(s)\".format(n_neighbors))\\n    ax.set_xlabel(\"feature 0\")\\n    ax.set_ylabel(\"feature 1\")\\naxes[0].legend(loc=3)\\nFigure 2-6. Decision boundaries created by the nearest neighbors model for different val‐\\nues of n_neighbors\\nAs you can see on the left in the figure, using a single neighbor results in a decision\\nboundary that follows the training data closely. Considering more and more neigh‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 51, 'page_label': '38', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='ues of n_neighbors\\nAs you can see on the left in the figure, using a single neighbor results in a decision\\nboundary that follows the training data closely. Considering more and more neigh‐\\nbors leads to a smoother decision boundary. A smoother boundary corresponds to a\\nsimpler model. In other words, using few neighbors corresponds to high model com‐\\nplexity (as shown on the right side of Figure 2-1), and using many neighbors corre‐\\nsponds to low model complexity (as shown on the left side of Figure 2-1 ). If you\\nconsider the extreme case where the number of neighbors is the number of all data\\npoints in the training set, each test point would have exactly the same neighbors (all\\ntraining points) and all predictions would be the same: the class that is most frequent\\nin the training set.\\nLet’s investigate whether we can confirm the connection between model complexity\\nand generalization that we discussed earlier. We will do this on the real-world Breast'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 51, 'page_label': '38', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='in the training set.\\nLet’s investigate whether we can confirm the connection between model complexity\\nand generalization that we discussed earlier. We will do this on the real-world Breast\\nCancer dataset. We begin by splitting the dataset into a training and a test set. Then\\n38 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 52, 'page_label': '39', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='we evaluate training and test set performance with different numbers of neighbors.\\nThe results are shown in Figure 2-7:\\nIn[18]:\\nfrom sklearn.datasets import load_breast_cancer\\ncancer = load_breast_cancer()\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, stratify=cancer.target, random_state=66)\\ntraining_accuracy = []\\ntest_accuracy = []\\n# try n_neighbors from 1 to 10\\nneighbors_settings = range(1, 11)\\nfor n_neighbors in neighbors_settings:\\n    # build the model\\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\\n    clf.fit(X_train, y_train)\\n    # record training set accuracy\\n    training_accuracy.append(clf.score(X_train, y_train))\\n    # record generalization accuracy\\n    test_accuracy.append(clf.score(X_test, y_test))\\nplt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\\nplt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\\nplt.ylabel(\"Accuracy\")\\nplt.xlabel(\"n_neighbors\")\\nplt.legend()'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 52, 'page_label': '39', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\\nplt.ylabel(\"Accuracy\")\\nplt.xlabel(\"n_neighbors\")\\nplt.legend()\\nThe plot shows the training and test set accuracy on the y-axis against the setting of\\nn_neighbors on the x-axis. While real-world plots are rarely very smooth, we can still\\nrecognize some of the characteristics of overfitting and underfitting (note that\\nbecause considering fewer neighbors corresponds to a more complex model, the plot\\nis horizontally flipped relative to the illustration in Figure 2-1). Considering a single\\nnearest neighbor, the prediction on the training set is perfect. But when more neigh‐\\nbors are considered, the model becomes simpler and the training accuracy drops. The\\ntest set accuracy for using a single neighbor is lower than when using more neigh‐\\nbors, indicating that using the single nearest neighbor leads to a model that is too\\ncomplex. On the other hand, when considering 10 neighbors, the model is too simple'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 52, 'page_label': '39', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='bors, indicating that using the single nearest neighbor leads to a model that is too\\ncomplex. On the other hand, when considering 10 neighbors, the model is too simple\\nand performance is even worse. The best performance is somewhere in the middle,\\nusing around six neighbors. Still, it is good to keep the scale of the plot in mind. The\\nworst performance is around 88% accuracy, which might still be acceptable.\\nSupervised Machine Learning Algorithms | 39'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 53, 'page_label': '40', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-7. Comparison of training and test accuracy as a function of n_neighbors\\nk-neighbors regression\\nThere is also a regression variant of the k-nearest neighbors algorithm. Again, let’s\\nstart by using the single nearest neighbor, this time using the wave dataset. We’ve\\nadded three test data points as green stars on the x-axis. The prediction using a single\\nneighbor is just the target value of the nearest neighbor. These are shown as blue stars\\nin Figure 2-8:\\nIn[19]:\\nmglearn.plots.plot_knn_regression(n_neighbors=1)\\n40 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 54, 'page_label': '41', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-8. Predictions made by one-nearest-neighbor regression on the wave dataset\\nAgain, we can use more than the single closest neighbor for regression. When using\\nmultiple nearest neighbors, the prediction is the average, or mean, of the relevant\\nneighbors (Figure 2-9):\\nIn[20]:\\nmglearn.plots.plot_knn_regression(n_neighbors=3)\\nSupervised Machine Learning Algorithms | 41'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 55, 'page_label': '42', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-9. Predictions made by three-nearest-neighbors regression on the wave dataset\\nThe k-nearest neighbors algorithm for regression is implemented in the KNeighbors\\nRegressor class in scikit-learn. It’s used similarly to KNeighborsClassifier:\\nIn[21]:\\nfrom sklearn.neighbors import KNeighborsRegressor\\nX, y = mglearn.datasets.make_wave(n_samples=40)\\n# split the wave dataset into a training and a test set\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n# instantiate the model and set the number of neighbors to consider to 3\\nreg = KNeighborsRegressor(n_neighbors=3)\\n# fit the model using the training data and training targets\\nreg.fit(X_train, y_train)\\nNow we can make predictions on the test set:\\nIn[22]:\\nprint(\"Test set predictions:\\\\n{}\".format(reg.predict(X_test)))\\n42 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 56, 'page_label': '43', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Out[22]:\\nTest set predictions:\\n[-0.054  0.357  1.137 -1.894 -1.139 -1.631  0.357  0.912 -0.447 -1.139]\\nWe can also evaluate the model using the score method, which for regressors returns\\nthe R2 score. The R2 score, also known as the coefficient of determination, is a meas‐\\nure of goodness of a prediction for a regression model, and yields a score between 0\\nand 1. A value of 1 corresponds to a perfect prediction, and a value of 0 corresponds\\nto a constant model that just predicts the mean of the training set responses, y_train:\\nIn[23]:\\nprint(\"Test set R^2: {:.2f}\".format(reg.score(X_test, y_test)))\\nOut[23]:\\nTest set R^2: 0.83\\nHere, the score is 0.83, which indicates a relatively good model fit.\\nAnalyzing KNeighborsRegressor\\nFor our one-dimensional dataset, we can see what the predictions look like for all\\npossible feature values (Figure 2-10). To do this, we create a test dataset consisting of\\nmany points on the line:\\nIn[24]:\\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 56, 'page_label': '43', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='possible feature values (Figure 2-10). To do this, we create a test dataset consisting of\\nmany points on the line:\\nIn[24]:\\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\\n# create 1,000 data points, evenly spaced between -3 and 3\\nline = np.linspace(-3, 3, 1000).reshape(-1, 1)\\nfor n_neighbors, ax in zip([1, 3, 9], axes):\\n    # make predictions using 1, 3, or 9 neighbors\\n    reg = KNeighborsRegressor(n_neighbors=n_neighbors)\\n    reg.fit(X_train, y_train)\\n    ax.plot(line, reg.predict(line))\\n    ax.plot(X_train, y_train, \\'^\\', c=mglearn.cm2(0), markersize=8)\\n    ax.plot(X_test, y_test, \\'v\\', c=mglearn.cm2(1), markersize=8)\\n    ax.set_title(\\n        \"{} neighbor(s)\\\\n train score: {:.2f} test score: {:.2f}\".format(\\n            n_neighbors, reg.score(X_train, y_train),\\n            reg.score(X_test, y_test)))\\n    ax.set_xlabel(\"Feature\")\\n    ax.set_ylabel(\"Target\")\\naxes[0].legend([\"Model predictions\", \"Training data/target\",'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 56, 'page_label': '43', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='reg.score(X_test, y_test)))\\n    ax.set_xlabel(\"Feature\")\\n    ax.set_ylabel(\"Target\")\\naxes[0].legend([\"Model predictions\", \"Training data/target\",\\n                \"Test data/target\"], loc=\"best\")Supervised Machine Learning Algorithms | 43'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 57, 'page_label': '44', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-10. Comparing predictions made by nearest neighbors regression for different\\nvalues of n_neighbors\\nAs we can see from the plot, using only a single neighbor, each point in the training\\nset has an obvious influence on the predictions, and the predicted values go through\\nall of the data points. This leads to a very unsteady prediction. Considering more\\nneighbors leads to smoother predictions, but these do not fit the training data as well.\\nStrengths, weaknesses, and parameters\\nIn principle, there are two important parameters to the KNeighbors classifier: the\\nnumber of neighbors and how you measure distance between data points. In practice,\\nusing a small number of neighbors like three or five often works well, but you should\\ncertainly adjust this parameter. Choosing the right distance measure is somewhat\\nbeyond the scope of this book. By default, Euclidean distance is used, which works\\nwell in many settings.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 57, 'page_label': '44', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='certainly adjust this parameter. Choosing the right distance measure is somewhat\\nbeyond the scope of this book. By default, Euclidean distance is used, which works\\nwell in many settings.\\nOne of the strengths of k-NN is that the model is very easy to understand, and often\\ngives reasonable performance without a lot of adjustments. Using this algorithm is a\\ngood baseline method to try before considering more advanced techniques. Building\\nthe nearest neighbors model is usually very fast, but when your training set is very\\nlarge (either in number of features or in number of samples) prediction can be slow.\\nWhen using the k-NN algorithm, it’s important to preprocess your data (see Chap‐\\nter 3 ). This approach often does not perform well on datasets with many features\\n(hundreds or more), and it does particularly badly with datasets where most features\\nare 0 most of the time (so-called sparse datasets).\\nSo, while the nearest k-neighbors algorithm is easy to understand, it is not often used'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 57, 'page_label': '44', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='are 0 most of the time (so-called sparse datasets).\\nSo, while the nearest k-neighbors algorithm is easy to understand, it is not often used\\nin practice, due to prediction being slow and its inability to handle many features.\\nThe method we discuss next has neither of these drawbacks.\\n44 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 58, 'page_label': '45', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Linear Models\\nLinear models are a class of models that are widely used in practice and have been\\nstudied extensively in the last few decades, with roots going back over a hundred\\nyears. Linear models make a prediction using a linear function of the input features,\\nwhich we will explain shortly.\\nLinear models for regression\\nFor regression, the general prediction formula for a linear model looks as follows:\\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\\nHere, x[0] to x[p] denotes the features (in this example, the number of features is p)\\nof a single data point, w and b are parameters of the model that are learned, and ŷ is\\nthe prediction the model makes. For a dataset with a single feature, this is:\\nŷ = w[0] * x[0] + b\\nwhich you might remember from high school mathematics as the equation for a line.\\nHere, w[0] is the slope and b is the y-axis offset. For more features, w contains the\\nslopes along each feature axis. Alternatively, you can think of the predicted response'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 58, 'page_label': '45', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Here, w[0] is the slope and b is the y-axis offset. For more features, w contains the\\nslopes along each feature axis. Alternatively, you can think of the predicted response\\nas being a weighted sum of the input features, with weights (which can be negative)\\ngiven by the entries of w.\\nTrying to learn the parameters w[0] and b on our one-dimensional wave dataset\\nmight lead to the following line (see Figure 2-11):\\nIn[25]:\\nmglearn.plots.plot_linear_regression_wave()\\nOut[25]:\\nw[0]: 0.393906  b: -0.031804\\nSupervised Machine Learning Algorithms | 45'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 59, 'page_label': '46', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-11. Predictions of a linear model on the wave dataset\\nWe added a coordinate cross into the plot to make it easier to understand the line.\\nLooking at w[0] we see that the slope should be around 0.4, which we can confirm\\nvisually in the plot. The intercept is where the prediction line should cross the y-axis:\\nthis is slightly below zero, which you can also confirm in the image.\\nLinear models for regression can be characterized as regression models for which the\\nprediction is a line for a single feature, a plane when using two features, or a hyper‐\\nplane in higher dimensions (that is, when using more features).\\nIf you compare the predictions made by the straight line with those made by the\\nKNeighborsRegressor in Figure 2-10, using a straight line to make predictions seems\\nvery restrictive. It looks like all the fine details of the data are lost. In a sense, this is\\ntrue. It is a strong (and somewhat unrealistic) assumption that our target y is a linear'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 59, 'page_label': '46', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='very restrictive. It looks like all the fine details of the data are lost. In a sense, this is\\ntrue. It is a strong (and somewhat unrealistic) assumption that our target y is a linear\\n46 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 60, 'page_label': '47', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='6 This is easy to see if you know some linear algebra.\\ncombination of the features. But looking at one-dimensional data gives a somewhat\\nskewed perspective. For datasets with many features, linear models can be very pow‐\\nerful. In particular, if you have more features than training data points, any target y\\ncan be perfectly modeled (on the training set) as a linear function.6\\nThere are many different linear models for regression. The difference between these\\nmodels lies in how the model parameters w and b are learned from the training data,\\nand how model complexity can be controlled. We will now take a look at the most\\npopular linear models for regression.\\nLinear regression (aka ordinary least squares)\\nLinear regression, or ordinary least squares (OLS), is the simplest and most classic lin‐\\near method for regression. Linear regression finds the parameters w and b that mini‐\\nmize the mean squared error between predictions and the true regression targets, y,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 60, 'page_label': '47', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='ear method for regression. Linear regression finds the parameters w and b that mini‐\\nmize the mean squared error between predictions and the true regression targets, y,\\non the training set. The mean squared error is the sum of the squared differences\\nbetween the predictions and the true values. Linear regression has no parameters,\\nwhich is a benefit, but it also has no way to control model complexity.\\nHere is the code that produces the model you can see in Figure 2-11:\\nIn[26]:\\nfrom sklearn.linear_model import LinearRegression\\nX, y = mglearn.datasets.make_wave(n_samples=60)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\\nlr = LinearRegression().fit(X_train, y_train)\\nThe “slope” parameters (w), also called weights or coefficients, are stored in the coef_\\nattribute, while the offset or intercept (b) is stored in the intercept_ attribute:\\nIn[27]:\\nprint(\"lr.coef_: {}\".format(lr.coef_))\\nprint(\"lr.intercept_: {}\".format(lr.intercept_))\\nOut[27]:\\nlr.coef_: [ 0.394]'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 60, 'page_label': '47', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[27]:\\nprint(\"lr.coef_: {}\".format(lr.coef_))\\nprint(\"lr.intercept_: {}\".format(lr.intercept_))\\nOut[27]:\\nlr.coef_: [ 0.394]\\nlr.intercept_: -0.031804343026759746\\nSupervised Machine Learning Algorithms | 47'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 61, 'page_label': '48', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Y ou might notice the strange-looking trailing underscore at the end\\nof coef_ and intercept_. scikit-learn always stores anything\\nthat is derived from the training data in attributes that end with a\\ntrailing underscore. That is to separate them from parameters that\\nare set by the user.\\nThe intercept_ attribute is always a single float number, while the coef_ attribute is\\na NumPy array with one entry per input feature. As we only have a single input fea‐\\nture in the wave dataset, lr.coef_ only has a single entry.\\nLet’s look at the training set and test set performance:\\nIn[28]:\\nprint(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))\\nOut[28]:\\nTraining set score: 0.67\\nTest set score: 0.66\\nAn R2 of around 0.66 is not very good, but we can see that the scores on the training\\nand test sets are very close together. This means we are likely underfitting, not over‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 61, 'page_label': '48', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Test set score: 0.66\\nAn R2 of around 0.66 is not very good, but we can see that the scores on the training\\nand test sets are very close together. This means we are likely underfitting, not over‐\\nfitting. For this one-dimensional dataset, there is little danger of overfitting, as the\\nmodel is very simple (or restricted). However, with higher-dimensional datasets\\n(meaning datasets with a large number of features), linear models become more pow‐\\nerful, and there is a higher chance of overfitting. Let’s take a look at how LinearRe\\ngression performs on a more complex dataset, like the Boston Housing dataset.\\nRemember that this dataset has 506 samples and 105 derived features. First, we load\\nthe dataset and split it into a training and a test set. Then we build the linear regres‐\\nsion model as before:\\nIn[29]:\\nX, y = mglearn.datasets.load_extended_boston()\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\nlr = LinearRegression().fit(X_train, y_train)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 61, 'page_label': '48', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='sion model as before:\\nIn[29]:\\nX, y = mglearn.datasets.load_extended_boston()\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\nlr = LinearRegression().fit(X_train, y_train)\\nWhen comparing training set and test set scores, we find that we predict very accu‐\\nrately on the training set, but the R2 on the test set is much worse:\\nIn[30]:\\nprint(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))\\n48 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 62, 'page_label': '49', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='7 Mathematically, Ridge penalizes the L2 norm of the coefficients, or the Euclidean length of w.\\nOut[30]:\\nTraining set score: 0.95\\nTest set score: 0.61\\nThis discrepancy between performance on the training set and the test set is a clear\\nsign of overfitting, and therefore we should try to find a model that allows us to con‐\\ntrol complexity. One of the most commonly used alternatives to standard linear\\nregression is ridge regression, which we will look into next.\\nRidge regression\\nRidge regression is also a linear model for regression, so the formula it uses to make\\npredictions is the same one used for ordinary least squares. In ridge regression,\\nthough, the coefficients (w) are chosen not only so that they predict well on the train‐\\ning data, but also to fit an additional constraint. We also want the magnitude of coef‐\\nficients to be as small as possible; in other words, all entries of w should be close to'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 62, 'page_label': '49', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='ing data, but also to fit an additional constraint. We also want the magnitude of coef‐\\nficients to be as small as possible; in other words, all entries of w should be close to\\nzero. Intuitively, this means each feature should have as little effect on the outcome as\\npossible (which translates to having a small slope), while still predicting well. This\\nconstraint is an example of what is called regularization. Regularization means explic‐\\nitly restricting a model to avoid overfitting. The particular kind used by ridge regres‐\\nsion is known as L2 regularization.7\\nRidge regression is implemented in linear_model.Ridge. Let’s see how well it does\\non the extended Boston Housing dataset:\\nIn[31]:\\nfrom sklearn.linear_model import Ridge\\nridge = Ridge().fit(X_train, y_train)\\nprint(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))\\nOut[31]:\\nTraining set score: 0.89\\nTest set score: 0.75'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 62, 'page_label': '49', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='print(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))\\nOut[31]:\\nTraining set score: 0.89\\nTest set score: 0.75\\nAs you can see, the training set score of Ridge is lower than for LinearRegression,\\nwhile the test set score is higher. This is consistent with our expectation. With linear\\nregression, we were overfitting our data. Ridge is a more restricted model, so we are\\nless likely to overfit. A less complex model means worse performance on the training\\nset, but better generalization. As we are only interested in generalization perfor‐\\nmance, we should choose the Ridge model over the LinearRegression model.Supervised Machine Learning Algorithms | 49'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 63, 'page_label': '50', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='The Ridge model makes a trade-off between the simplicity of the model (near-zero\\ncoefficients) and its performance on the training set. How much importance the\\nmodel places on simplicity versus training set performance can be specified by the\\nuser, using the alpha parameter. In the previous example, we used the default param‐\\neter alpha=1.0. There is no reason why this will give us the best trade-off, though.\\nThe optimum setting of alpha depends on the particular dataset we are using.\\nIncreasing alpha forces coefficients to move more toward zero, which decreases\\ntraining set performance but might help generalization. For example:\\nIn[32]:\\nridge10 = Ridge(alpha=10).fit(X_train, y_train)\\nprint(\"Training set score: {:.2f}\".format(ridge10.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(ridge10.score(X_test, y_test)))\\nOut[32]:\\nTraining set score: 0.79\\nTest set score: 0.64\\nDecreasing alpha allows the coefficients to be less restricted, meaning we move right'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 63, 'page_label': '50', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Out[32]:\\nTraining set score: 0.79\\nTest set score: 0.64\\nDecreasing alpha allows the coefficients to be less restricted, meaning we move right\\nin Figure 2-1. For very small values of alpha, coefficients are barely restricted at all,\\nand we end up with a model that resembles LinearRegression:\\nIn[33]:\\nridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\\nprint(\"Training set score: {:.2f}\".format(ridge01.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(ridge01.score(X_test, y_test)))\\nOut[33]:\\nTraining set score: 0.93\\nTest set score: 0.77\\nHere, alpha=0.1 seems to be working well. We could try decreasing alpha even more\\nto improve generalization. For now, notice how the parameter alpha corresponds to\\nthe model complexity as shown in Figure 2-1. We will discuss methods to properly\\nselect parameters in Chapter 5.\\nWe can also get a more qualitative insight into how the alpha parameter changes the\\nmodel by inspecting the coef_ attribute of models with different values of alpha. A'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 63, 'page_label': '50', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='select parameters in Chapter 5.\\nWe can also get a more qualitative insight into how the alpha parameter changes the\\nmodel by inspecting the coef_ attribute of models with different values of alpha. A\\nhigher alpha means a more restricted model, so we expect the entries of coef_ to\\nhave smaller magnitude for a high value of alpha than for a low value of alpha. This\\nis confirmed in the plot in Figure 2-12:\\n50 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 64, 'page_label': '51', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[34]:\\nplt.plot(ridge.coef_, \\'s\\', label=\"Ridge alpha=1\")\\nplt.plot(ridge10.coef_, \\'^\\', label=\"Ridge alpha=10\")\\nplt.plot(ridge01.coef_, \\'v\\', label=\"Ridge alpha=0.1\")\\nplt.plot(lr.coef_, \\'o\\', label=\"LinearRegression\")\\nplt.xlabel(\"Coefficient index\")\\nplt.ylabel(\"Coefficient magnitude\")\\nplt.hlines(0, 0, len(lr.coef_))\\nplt.ylim(-25, 25)\\nplt.legend()\\nFigure 2-12. Comparing coefficient magnitudes for ridge regression with different values\\nof alpha and linear regression\\nHere, the x-axis enumerates the entries of coef_: x=0 shows the coefficient associated\\nwith the first feature, x=1 the coefficient associated with the second feature, and so on\\nup to x=100. The y-axis shows the numeric values of the corresponding values of the\\ncoefficients. The main takeaway here is that for alpha=10, the coefficients are mostly\\nbetween around –3 and 3. The coefficients for the Ridge model with alpha=1 are\\nsomewhat larger. The dots corresponding to alpha=0.1 have larger magnitude still,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 64, 'page_label': '51', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='between around –3 and 3. The coefficients for the Ridge model with alpha=1 are\\nsomewhat larger. The dots corresponding to alpha=0.1 have larger magnitude still,\\nand many of the dots corresponding to linear regression without any regularization\\n(which would be alpha=0) are so large they are outside of the chart.\\nSupervised Machine Learning Algorithms | 51'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 65, 'page_label': '52', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Another way to understand the influence of regularization is to fix a value of alpha\\nbut vary the amount of training data available. For Figure 2-13, we subsampled the\\nBoston Housing dataset and evaluated LinearRegression and Ridge(alpha=1) on\\nsubsets of increasing size (plots that show model performance as a function of dataset\\nsize are called learning curves):\\nIn[35]:\\nmglearn.plots.plot_ridge_n_samples()\\nFigure 2-13. Learning curves for ridge regression and linear regression on the Boston\\nHousing dataset\\nAs one would expect, the training score is higher than the test score for all dataset\\nsizes, for both ridge and linear regression. Because ridge is regularized, the training\\nscore of ridge is lower than the training score for linear regression across the board.\\nHowever, the test score for ridge is better, particularly for small subsets of the data.\\nFor less than 400 data points, linear regression is not able to learn anything. As more'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 65, 'page_label': '52', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='However, the test score for ridge is better, particularly for small subsets of the data.\\nFor less than 400 data points, linear regression is not able to learn anything. As more\\nand more data becomes available to the model, both models improve, and linear\\nregression catches up with ridge in the end. The lesson here is that with enough train‐\\ning data, regularization becomes less important, and given enough data, ridge and\\n52 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 66, 'page_label': '53', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='8 The lasso penalizes the L1 norm of the coefficient vector—or in other words, the sum of the absolute values of\\nthe coefficients.\\nlinear regression will have the same performance (the fact that this happens here\\nwhen using the full dataset is just by chance). Another interesting aspect of\\nFigure 2-13 is the decrease in training performance for linear regression. If more data\\nis added, it becomes harder for a model to overfit, or memorize the data.\\nLasso\\nAn alternative to Ridge for regularizing linear regression is Lasso. As with ridge\\nregression, using the lasso also restricts coefficients to be close to zero, but in a\\nslightly different way, called L1 regularization.8 The consequence of L1 regularization\\nis that when using the lasso, some coefficients are exactly zero. This means some fea‐\\ntures are entirely ignored by the model. This can be seen as a form of automatic fea‐\\nture selection. Having some coefficients be exactly zero often makes a model easier to'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 66, 'page_label': '53', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='tures are entirely ignored by the model. This can be seen as a form of automatic fea‐\\nture selection. Having some coefficients be exactly zero often makes a model easier to\\ninterpret, and can reveal the most important features of your model.\\nLet’s apply the lasso to the extended Boston Housing dataset:\\nIn[36]:\\nfrom sklearn.linear_model import Lasso\\nlasso = Lasso().fit(X_train, y_train)\\nprint(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\\nprint(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))\\nOut[36]:\\nTraining set score: 0.29\\nTest set score: 0.21\\nNumber of features used: 4\\nAs you can see, Lasso does quite badly, both on the training and the test set. This\\nindicates that we are underfitting, and we find that it used only 4 of the 105 features.\\nSimilarly to Ridge, the Lasso also has a regularization parameter, alpha, that controls'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 66, 'page_label': '53', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='indicates that we are underfitting, and we find that it used only 4 of the 105 features.\\nSimilarly to Ridge, the Lasso also has a regularization parameter, alpha, that controls\\nhow strongly coefficients are pushed toward zero. In the previous example, we used\\nthe default of alpha=1.0. To reduce underfitting, let’s try decreasing alpha. When we\\ndo this, we also need to increase the default setting of max_iter (the maximum num‐\\nber of iterations to run):\\nSupervised Machine Learning Algorithms | 53'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 67, 'page_label': '54', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[37]:\\n# we increase the default setting of \"max_iter\",\\n# otherwise the model would warn us that we should increase max_iter.\\nlasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)\\nprint(\"Training set score: {:.2f}\".format(lasso001.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(lasso001.score(X_test, y_test)))\\nprint(\"Number of features used: {}\".format(np.sum(lasso001.coef_ != 0)))\\nOut[37]:\\nTraining set score: 0.90\\nTest set score: 0.77\\nNumber of features used: 33\\nA lower alpha allowed us to fit a more complex model, which worked better on the\\ntraining and test data. The performance is slightly better than using Ridge, and we are\\nusing only 33 of the 105 features. This makes this model potentially easier to under‐\\nstand.\\nIf we set alpha too low, however, we again remove the effect of regularization and end\\nup overfitting, with a result similar to LinearRegression:\\nIn[38]:\\nlasso00001 = Lasso(alpha=0.0001, max_iter=100000).fit(X_train, y_train)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 67, 'page_label': '54', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='up overfitting, with a result similar to LinearRegression:\\nIn[38]:\\nlasso00001 = Lasso(alpha=0.0001, max_iter=100000).fit(X_train, y_train)\\nprint(\"Training set score: {:.2f}\".format(lasso00001.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(lasso00001.score(X_test, y_test)))\\nprint(\"Number of features used: {}\".format(np.sum(lasso00001.coef_ != 0)))\\nOut[38]:\\nTraining set score: 0.95\\nTest set score: 0.64\\nNumber of features used: 94\\nAgain, we can plot the coefficients of the different models, similarly to Figure 2-12.\\nThe result is shown in Figure 2-14:\\nIn[39]:\\nplt.plot(lasso.coef_, \\'s\\', label=\"Lasso alpha=1\")\\nplt.plot(lasso001.coef_, \\'^\\', label=\"Lasso alpha=0.01\")\\nplt.plot(lasso00001.coef_, \\'v\\', label=\"Lasso alpha=0.0001\")\\nplt.plot(ridge01.coef_, \\'o\\', label=\"Ridge alpha=0.1\")\\nplt.legend(ncol=2, loc=(0, 1.05))\\nplt.ylim(-25, 25)\\nplt.xlabel(\"Coefficient index\")\\nplt.ylabel(\"Coefficient magnitude\")\\n54 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 68, 'page_label': '55', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-14. Comparing coefficient magnitudes for lasso regression with different values\\nof alpha and ridge regression\\nFor alpha=1, we not only see that most of the coefficients are zero (which we already\\nknew), but that the remaining coefficients are also small in magnitude. Decreasing\\nalpha to 0.01, we obtain the solution shown as the green dots, which causes most\\nfeatures to be exactly zero. Using alpha=0.00001, we get a model that is quite unregu‐\\nlarized, with most coefficients nonzero and of large magnitude. For comparison, the\\nbest Ridge solution is shown in teal. The Ridge model with alpha=0.1 has similar\\npredictive performance as the lasso model with alpha=0.01, but using Ridge, all coef‐\\nficients are nonzero.\\nIn practice, ridge regression is usually the first choice between these two models.\\nHowever, if you have a large amount of features and expect only a few of them to be\\nimportant, Lasso might be a better choice. Similarly, if you would like to have a'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 68, 'page_label': '55', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='However, if you have a large amount of features and expect only a few of them to be\\nimportant, Lasso might be a better choice. Similarly, if you would like to have a\\nmodel that is easy to interpret, Lasso will provide a model that is easier to under‐\\nstand, as it will select only a subset of the input features. scikit-learn also provides\\nthe ElasticNet class, which combines the penalties of Lasso and Ridge. In practice,\\nthis combination works best, though at the price of having two parameters to adjust:\\none for the L1 regularization, and one for the L2 regularization.\\nSupervised Machine Learning Algorithms | 55'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 69, 'page_label': '56', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Linear models for classification\\nLinear models are also extensively used for classification. Let’s look at binary classifi‐\\ncation first. In this case, a prediction is made using the following formula:\\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0\\nThe formula looks very similar to the one for linear regression, but instead of just\\nreturning the weighted sum of the features, we threshold the predicted value at zero.\\nIf the function is smaller than zero, we predict the class –1; if it is larger than zero, we\\npredict the class +1. This prediction rule is common to all linear models for classifica‐\\ntion. Again, there are many different ways to find the coefficients ( w) and the inter‐\\ncept (b).\\nFor linear models for regression, the output, ŷ, is a linear function of the features: a\\nline, plane, or hyperplane (in higher dimensions). For linear models for classification,\\nthe decision boundary is a linear function of the input. In other words, a (binary) lin‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 69, 'page_label': '56', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='line, plane, or hyperplane (in higher dimensions). For linear models for classification,\\nthe decision boundary is a linear function of the input. In other words, a (binary) lin‐\\near classifier is a classifier that separates two classes using a line, a plane, or a hyper‐\\nplane. We will see examples of that in this section.\\nThere are many algorithms for learning linear models. These algorithms all differ in\\nthe following two ways:\\n• The way in which they measure how well a particular combination of coefficients\\nand intercept fits the training data\\n• If and what kind of regularization they use\\nDifferent algorithms choose different ways to measure what “fitting the training set\\nwell” means. For technical mathematical reasons, it is not possible to adjust w and b\\nto minimize the number of misclassifications the algorithms produce, as one might\\nhope. For our purposes, and many applications, the different choices for item 1 in the'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 69, 'page_label': '56', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='to minimize the number of misclassifications the algorithms produce, as one might\\nhope. For our purposes, and many applications, the different choices for item 1 in the\\npreceding list (called loss functions) are of little significance.\\nThe two most common linear classification algorithms are logistic regression, imple‐\\nmented in linear_model.LogisticRegression, and linear support vector machines\\n(linear SVMs), implemented in svm.LinearSVC (SVC stands for support vector classi‐\\nfier). Despite its name, LogisticRegression is a classification algorithm and not a\\nregression algorithm, and it should not be confused with LinearRegression.\\nWe can apply the LogisticRegression and LinearSVC models to the forge dataset,\\nand visualize the decision boundary as found by the linear models (Figure 2-15):\\n56 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 70, 'page_label': '57', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[40]:\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import LinearSVC\\nX, y = mglearn.datasets.make_forge()\\nfig, axes = plt.subplots(1, 2, figsize=(10, 3))\\nfor model, ax in zip([LinearSVC(), LogisticRegression()], axes):\\n    clf = model.fit(X, y)\\n    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,\\n                                    ax=ax, alpha=.7)\\n    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\\n    ax.set_title(\"{}\".format(clf.__class__.__name__))\\n    ax.set_xlabel(\"Feature 0\")\\n    ax.set_ylabel(\"Feature 1\")\\naxes[0].legend()\\nFigure 2-15. Decision boundaries of a linear SVM and logistic regression on the forge\\ndataset with the default parameters\\nIn this figure, we have the first feature of the forge dataset on the x-axis and the sec‐\\nond feature on the y-axis, as before. We display the decision boundaries found by\\nLinearSVC and LogisticRegression respectively as straight lines, separating the area'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 70, 'page_label': '57', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='ond feature on the y-axis, as before. We display the decision boundaries found by\\nLinearSVC and LogisticRegression respectively as straight lines, separating the area\\nclassified as class 1 on the top from the area classified as class 0 on the bottom. In\\nother words, any new data point that lies above the black line will be classified as class\\n1 by the respective classifier, while any point that lies below the black line will be clas‐\\nsified as class 0.\\nThe two models come up with similar decision boundaries. Note that both misclas‐\\nsify two of the points. By default, both models apply an L2 regularization, in the same\\nway that Ridge does for regression.\\nFor LogisticRegression and LinearSVC the trade-off parameter that determines the\\nstrength of the regularization is called C, and higher values of C correspond to less\\nSupervised Machine Learning Algorithms | 57'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 71, 'page_label': '58', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='regularization. In other words, when you use a high value for the parameter C, Logis\\nticRegression and LinearSVC try to fit the training set as best as possible, while with\\nlow values of the parameter C, the models put more emphasis on finding a coefficient\\nvector (w) that is close to zero.\\nThere is another interesting aspect of how the parameter C acts. Using low values of C\\nwill cause the algorithms to try to adjust to the “majority” of data points, while using\\na higher value of C stresses the importance that each individual data point be classi‐\\nfied correctly. Here is an illustration using LinearSVC (Figure 2-16):\\nIn[41]:\\nmglearn.plots.plot_linear_svc_regularization()\\nFigure 2-16. Decision boundaries of a linear SVM on the forge dataset for different\\nvalues of C\\nOn the lefthand side, we have a very small C corresponding to a lot of regularization.\\nMost of the points in class 0 are at the top, and most of the points in class 1 are at the'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 71, 'page_label': '58', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='values of C\\nOn the lefthand side, we have a very small C corresponding to a lot of regularization.\\nMost of the points in class 0 are at the top, and most of the points in class 1 are at the\\nbottom. The strongly regularized model chooses a relatively horizontal line, misclas‐\\nsifying two points. In the center plot, C is slightly higher, and the model focuses more\\non the two misclassified samples, tilting the decision boundary. Finally, on the right‐\\nhand side, the very high value of C in the model tilts the decision boundary a lot, now\\ncorrectly classifying all points in class 0. One of the points in class 1 is still misclassi‐\\nfied, as it is not possible to correctly classify all points in this dataset using a straight\\nline. The model illustrated on the righthand side tries hard to correctly classify all\\npoints, but might not capture the overall layout of the classes well. In other words,\\nthis model is likely overfitting.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 71, 'page_label': '58', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='points, but might not capture the overall layout of the classes well. In other words,\\nthis model is likely overfitting.\\nSimilarly to the case of regression, linear models for classification might seem very\\nrestrictive in low-dimensional spaces, only allowing for decision boundaries that are\\nstraight lines or planes. Again, in high dimensions, linear models for classification\\n58 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 72, 'page_label': '59', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='become very powerful, and guarding against overfitting becomes increasingly impor‐\\ntant when considering more features.\\nLet’s analyze LinearLogistic in more detail on the Breast Cancer dataset:\\nIn[42]:\\nfrom sklearn.datasets import load_breast_cancer\\ncancer = load_breast_cancer()\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\\nlogreg = LogisticRegression().fit(X_train, y_train)\\nprint(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\\nprint(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\\nOut[42]:\\nTraining set score: 0.953\\nTest set score: 0.958\\nThe default value of C=1 provides quite good performance, with 95% accuracy on\\nboth the training and the test set. But as training and test set performance are very\\nclose, it is likely that we are underfitting. Let’s try to increase C to fit a more flexible\\nmodel:\\nIn[43]:\\nlogreg100 = LogisticRegression(C=100).fit(X_train, y_train)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 72, 'page_label': '59', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='close, it is likely that we are underfitting. Let’s try to increase C to fit a more flexible\\nmodel:\\nIn[43]:\\nlogreg100 = LogisticRegression(C=100).fit(X_train, y_train)\\nprint(\"Training set score: {:.3f}\".format(logreg100.score(X_train, y_train)))\\nprint(\"Test set score: {:.3f}\".format(logreg100.score(X_test, y_test)))\\nOut[43]:\\nTraining set score: 0.972\\nTest set score: 0.965\\nUsing C=100 results in higher training set accuracy, and also a slightly increased test\\nset accuracy, confirming our intuition that a more complex model should perform\\nbetter.\\nWe can also investigate what happens if we use an even more regularized model than\\nthe default of C=1, by setting C=0.01:\\nIn[44]:\\nlogreg001 = LogisticRegression(C=0.01).fit(X_train, y_train)\\nprint(\"Training set score: {:.3f}\".format(logreg001.score(X_train, y_train)))\\nprint(\"Test set score: {:.3f}\".format(logreg001.score(X_test, y_test)))\\nOut[44]:\\nTraining set score: 0.934\\nTest set score: 0.930\\nSupervised Machine Learning Algorithms | 59'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 73, 'page_label': '60', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='As expected, when moving more to the left along the scale shown in Figure 2-1 from\\nan already underfit model, both training and test set accuracy decrease relative to the\\ndefault parameters.\\nFinally, let’s look at the coefficients learned by the models with the three different set‐\\ntings of the regularization parameter C (Figure 2-17):\\nIn[45]:\\nplt.plot(logreg.coef_.T, \\'o\\', label=\"C=1\")\\nplt.plot(logreg100.coef_.T, \\'^\\', label=\"C=100\")\\nplt.plot(logreg001.coef_.T, \\'v\\', label=\"C=0.001\")\\nplt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\\nplt.hlines(0, 0, cancer.data.shape[1])\\nplt.ylim(-5, 5)\\nplt.xlabel(\"Coefficient index\")\\nplt.ylabel(\"Coefficient magnitude\")\\nplt.legend()\\nAs LogisticRegression applies an L2 regularization by default,\\nthe result looks similar to that produced by Ridge in Figure 2-12.\\nStronger regularization pushes coefficients more and more toward\\nzero, though coefficients never become exactly zero. Inspecting the'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 73, 'page_label': '60', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the result looks similar to that produced by Ridge in Figure 2-12.\\nStronger regularization pushes coefficients more and more toward\\nzero, though coefficients never become exactly zero. Inspecting the\\nplot more closely, we can also see an interesting effect in the third\\ncoefficient, for “mean perimeter. ” For C=100 and C=1, the coefficient\\nis negative, while for C=0.001, the coefficient is positive, with a\\nmagnitude that is even larger than for C=1. Interpreting a model\\nlike this, one might think the coefficient tells us which class a fea‐\\nture is associated with. For example, one might think that a high\\n“texture error” feature is related to a sample being “malignant. ”\\nHowever, the change of sign in the coefficient for “mean perimeter”\\nmeans that depending on which model we look at, a high “mean\\nperimeter” could be taken as being either indicative of “benign” or\\nindicative of “malignant. ” This illustrates that interpretations of'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 73, 'page_label': '60', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='means that depending on which model we look at, a high “mean\\nperimeter” could be taken as being either indicative of “benign” or\\nindicative of “malignant. ” This illustrates that interpretations of\\ncoefficients of linear models should always be taken with a grain of\\nsalt.\\n60 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 74, 'page_label': '61', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-17. Coefficients learned by logistic regression on the Breast Cancer dataset for\\ndifferent values of C\\nSupervised Machine Learning Algorithms | 61'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 75, 'page_label': '62', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='If we desire a more interpretable model, using L1 regularization might help, as it lim‐\\nits the model to using only a few features. Here is the coefficient plot and classifica‐\\ntion accuracies for L1 regularization (Figure 2-18):\\nIn[46]:\\nfor C, marker in zip([0.001, 1, 100], [\\'o\\', \\'^\\', \\'v\\']):\\n    lr_l1 = LogisticRegression(C=C, penalty=\"l1\").fit(X_train, y_train)\\n    print(\"Training accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\\n          C, lr_l1.score(X_train, y_train)))\\n    print(\"Test accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\\n          C, lr_l1.score(X_test, y_test)))\\n    plt.plot(lr_l1.coef_.T, marker, label=\"C={:.3f}\".format(C))\\nplt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\\nplt.hlines(0, 0, cancer.data.shape[1])\\nplt.xlabel(\"Coefficient index\")\\nplt.ylabel(\"Coefficient magnitude\")\\nplt.ylim(-5, 5)\\nplt.legend(loc=3)\\nOut[46]:\\nTraining accuracy of l1 logreg with C=0.001: 0.91\\nTest accuracy of l1 logreg with C=0.001: 0.92'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 75, 'page_label': '62', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='plt.ylabel(\"Coefficient magnitude\")\\nplt.ylim(-5, 5)\\nplt.legend(loc=3)\\nOut[46]:\\nTraining accuracy of l1 logreg with C=0.001: 0.91\\nTest accuracy of l1 logreg with C=0.001: 0.92\\nTraining accuracy of l1 logreg with C=1.000: 0.96\\nTest accuracy of l1 logreg with C=1.000: 0.96\\nTraining accuracy of l1 logreg with C=100.000: 0.99\\nTest accuracy of l1 logreg with C=100.000: 0.98\\nAs you can see, there are many parallels between linear models for binary classifica‐\\ntion and linear models for regression. As in regression, the main difference between\\nthe models is the penalty parameter, which influences the regularization and\\nwhether the model will use all available features or select only a subset.\\n62 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 76, 'page_label': '63', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-18. Coefficients learned by logistic regression with L1 penalty on the Breast\\nCancer dataset for different values of C\\nLinear models for multiclass classification\\nMany linear classification models are for binary classification only, and don’t extend\\nnaturally to the multiclass case (with the exception of logistic regression). A common\\ntechnique to extend a binary classification algorithm to a multiclass classification\\nalgorithm is the one-vs.-rest approach. In the one-vs.-rest approach, a binary model is\\nlearned for each class that tries to separate that class from all of the other classes,\\nresulting in as many binary models as there are classes. To make a prediction, all\\nbinary classifiers are run on a test point. The classifier that has the highest score on its\\nsingle class “wins, ” and this class label is returned as the prediction.\\nSupervised Machine Learning Algorithms | 63'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 77, 'page_label': '64', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Having one binary classifier per class results in having one vector of coefficients ( w)\\nand one intercept (b) for each class. The class for which the result of the classification\\nconfidence formula given here is highest is the assigned class label:\\nw[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\\nThe mathematics behind multiclass logistic regression differ somewhat from the one-\\nvs.-rest approach, but they also result in one coefficient vector and intercept per class,\\nand the same method of making a prediction is applied.\\nLet’s apply the one-vs.-rest method to a simple three-class classification dataset. We\\nuse a two-dimensional dataset, where each class is given by data sampled from a\\nGaussian distribution (see Figure 2-19):\\nIn[47]:\\nfrom sklearn.datasets import make_blobs\\nX, y = make_blobs(random_state=42)\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nplt.legend([\"Class 0\", \"Class 1\", \"Class 2\"])'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 77, 'page_label': '64', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='X, y = make_blobs(random_state=42)\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nplt.legend([\"Class 0\", \"Class 1\", \"Class 2\"])\\nFigure 2-19. Two-dimensional toy dataset containing three classes\\n64 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 78, 'page_label': '65', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Now, we train a LinearSVC classifier on the dataset:\\nIn[48]:\\nlinear_svm = LinearSVC().fit(X, y)\\nprint(\"Coefficient shape: \", linear_svm.coef_.shape)\\nprint(\"Intercept shape: \", linear_svm.intercept_.shape)\\nOut[48]:\\nCoefficient shape:  (3, 2)\\nIntercept shape:  (3,)\\nWe see that the shape of the coef_ is (3, 2), meaning that each row of coef_ con‐\\ntains the coefficient vector for one of the three classes and each column holds the\\ncoefficient value for a specific feature (there are two in this dataset). The intercept_\\nis now a one-dimensional array, storing the intercepts for each class.\\nLet’s visualize the lines given by the three binary classifiers ( Figure 2-20):\\nIn[49]:\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\nline = np.linspace(-15, 15)\\nfor coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\\n                                  [\\'b\\', \\'r\\', \\'g\\']):\\n    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\\nplt.ylim(-10, 15)\\nplt.xlim(-10, 8)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 78, 'page_label': '65', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='[\\'b\\', \\'r\\', \\'g\\']):\\n    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\\nplt.ylim(-10, 15)\\nplt.xlim(-10, 8)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nplt.legend([\\'Class 0\\', \\'Class 1\\', \\'Class 2\\', \\'Line class 0\\', \\'Line class 1\\',\\n            \\'Line class 2\\'], loc=(1.01, 0.3))\\nY ou can see that all the points belonging to class 0 in the training data are above the\\nline corresponding to class 0, which means they are on the “class 0” side of this binary\\nclassifier. The points in class 0 are above the line corresponding to class 2, which\\nmeans they are classified as “rest” by the binary classifier for class 2. The points\\nbelonging to class 0 are to the left of the line corresponding to class 1, which means\\nthe binary classifier for class 1 also classifies them as “rest. ” Therefore, any point in\\nthis area will be classified as class 0 by the final classifier (the result of the classifica‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 78, 'page_label': '65', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the binary classifier for class 1 also classifies them as “rest. ” Therefore, any point in\\nthis area will be classified as class 0 by the final classifier (the result of the classifica‐\\ntion confidence formula for classifier 0 is greater than zero, while it is smaller than\\nzero for the other two classes).\\nBut what about the triangle in the middle of the plot? All three binary classifiers clas‐\\nsify points there as “rest. ” Which class would a point there be assigned to? The answer\\nis the one with the highest value for the classification formula: the class of the closest\\nline.\\nSupervised Machine Learning Algorithms | 65'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 79, 'page_label': '66', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-20. Decision boundaries learned by the three one-vs.-rest classifiers\\nThe following example ( Figure 2-21) shows the predictions for all regions of the 2D\\nspace:\\nIn[50]:\\nmglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7)\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\nline = np.linspace(-15, 15)\\nfor coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\\n                                  [\\'b\\', \\'r\\', \\'g\\']):\\n    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\\nplt.legend([\\'Class 0\\', \\'Class 1\\', \\'Class 2\\', \\'Line class 0\\', \\'Line class 1\\',\\n            \\'Line class 2\\'], loc=(1.01, 0.3))\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\n66 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 80, 'page_label': '67', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-21. Multiclass decision boundaries derived from the three one-vs.-rest classifiers\\nStrengths, weaknesses, and parameters\\nThe main parameter of linear models is the regularization parameter, called alpha in\\nthe regression models and C in LinearSVC and LogisticRegression. Large values for\\nalpha or small values for C mean simple models. In particular for the regression mod‐\\nels, tuning these parameters is quite important. Usually C and alpha are searched for\\non a logarithmic scale. The other decision you have to make is whether you want to\\nuse L1 regularization or L2 regularization. If you assume that only a few of your fea‐\\ntures are actually important, you should use L1. Otherwise, you should default to L2.\\nL1 can also be useful if interpretability of the model is important. As L1 will use only\\na few features, it is easier to explain which features are important to the model, and\\nwhat the effects of these features are.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 80, 'page_label': '67', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"a few features, it is easier to explain which features are important to the model, and\\nwhat the effects of these features are.\\nLinear models are very fast to train, and also fast to predict. They scale to very large\\ndatasets and work well with sparse data. If your data consists of hundreds of thou‐\\nsands or millions of samples, you might want to investigate using the solver='sag'\\noption in LogisticRegression and Ridge, which can be faster than the default on\\nlarge datasets. Other options are the SGDClassifier class and the SGDRegressor\\nclass, which implement even more scalable versions of the linear models described\\nhere.\\nAnother strength of linear models is that they make it relatively easy to understand\\nhow a prediction is made, using the formulas we saw earlier for regression and classi‐\\nfication. Unfortunately, it is often not entirely clear why coefficients are the way they\\nare. This is particularly true if your dataset has highly correlated features; in these\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 80, 'page_label': '67', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='fication. Unfortunately, it is often not entirely clear why coefficients are the way they\\nare. This is particularly true if your dataset has highly correlated features; in these\\ncases, the coefficients might be hard to interpret.\\nSupervised Machine Learning Algorithms | 67'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 81, 'page_label': '68', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Linear models often perform well when the number of features is large compared to\\nthe number of samples. They are also often used on very large datasets, simply\\nbecause it’s not feasible to train other models. However, in lower-dimensional spaces,\\nother models might yield better generalization performance. We will look at some\\nexamples in which linear models fail in “Kernelized Support Vector Machines” on\\npage 92.\\nMethod Chaining\\nThe fit method of all scikit-learn models returns self. This allows you to write\\ncode like the following, which we’ve already used extensively in this chapter:\\nIn[51]:\\n# instantiate model and fit it in one line\\nlogreg = LogisticRegression().fit(X_train, y_train)\\nHere, we used the return value of fit (which is self) to assign the trained model to\\nthe variable logreg. This concatenation of method calls (here __init__ and then fit)\\nis known as method chaining. Another common application of method chaining in\\nscikit-learn is to fit and predict in one line:\\nIn[52]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 81, 'page_label': '68', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='is known as method chaining. Another common application of method chaining in\\nscikit-learn is to fit and predict in one line:\\nIn[52]:\\nlogreg = LogisticRegression()\\ny_pred = logreg.fit(X_train, y_train).predict(X_test)\\nFinally, you can even do model instantiation, fitting, and predicting in one line:\\nIn[53]:\\ny_pred = LogisticRegression().fit(X_train, y_train).predict(X_test)\\nThis very short variant is not ideal, though. A lot is happening in a single line, which\\nmight make the code hard to read. Additionally, the fitted logistic regression model\\nisn’t stored in any variable, so we can’t inspect it or use it to predict on any other data.\\nNaive Bayes \\nClassifiers\\nNaive Bayes classifiers are a family of classifiers that are quite similar to the linear\\nmodels discussed in the previous section. However, they tend to be even faster in\\ntraining. The price paid for this efficiency is that naive Bayes models often provide'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 81, 'page_label': '68', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='models discussed in the previous section. However, they tend to be even faster in\\ntraining. The price paid for this efficiency is that naive Bayes models often provide\\ngeneralization performance that is slightly worse than that of linear classifiers like\\nLogisticRegression and LinearSVC.\\nThe reason that naive Bayes models are so efficient is that they learn parameters by\\nlooking at each feature individually and collect simple per-class statistics from each\\nfeature. There are three kinds of naive Bayes classifiers implemented in scikit-\\n68 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 82, 'page_label': '69', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='learn: GaussianNB, BernoulliNB, and MultinomialNB. GaussianNB can be applied to\\nany continuous data, while BernoulliNB assumes binary data and MultinomialNB\\nassumes count data (that is, that each feature represents an integer count of some‐\\nthing, like how often a word appears in a sentence). BernoulliNB and MultinomialNB\\nare mostly used in text data classification.\\nThe BernoulliNB classifier counts how often every feature of each class is not zero.\\nThis is most easily understood with an example:\\nIn[54]:\\nX = np.array([[0, 1, 0, 1],\\n              [1, 0, 1, 1],\\n              [0, 0, 0, 1],\\n              [1, 0, 1, 0]])\\ny = np.array([0, 1, 0, 1])\\nHere, we have four data points, with four binary features each. There are two classes,\\n0 and 1. For class 0 (the first and third data points), the first feature is zero two times\\nand nonzero zero times, the second feature is zero one time and nonzero one time,\\nand so on. These same counts are then calculated for the data points in the second'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 82, 'page_label': '69', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='and nonzero zero times, the second feature is zero one time and nonzero one time,\\nand so on. These same counts are then calculated for the data points in the second\\nclass. Counting the nonzero entries per class in essence looks like this:\\nIn[55]:\\ncounts = {}\\nfor label in np.unique(y):\\n    # iterate over each class\\n    # count (sum) entries of 1 per feature\\n    counts[label] = X[y == label].sum(axis=0)\\nprint(\"Feature counts:\\\\n{}\".format(counts))\\nOut[55]:\\nFeature counts:\\n{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\\nThe other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐\\nferent in what kinds of statistics they compute. MultinomialNB takes into account the\\naverage value of each feature for each class, while GaussianNB stores the average value\\nas well as the standard deviation of each feature for each class.\\nTo make a prediction, a data point is compared to the statistics for each of the classes,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 82, 'page_label': '69', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='as well as the standard deviation of each feature for each class.\\nTo make a prediction, a data point is compared to the statistics for each of the classes,\\nand the best matching class is predicted. Interestingly, for both MultinomialNB and\\nBernoulliNB, this leads to a prediction formula that is of the same form as in the lin‐\\near models (see “Linear models for classification” on page 56). Unfortunately, coef_\\nfor the naive Bayes models has a somewhat different meaning than in the linear mod‐\\nels, in that coef_ is not the same as w.\\nSupervised Machine Learning Algorithms | 69'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 83, 'page_label': '70', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Strengths, weaknesses, and parameters\\nMultinomialNB and BernoulliNB have a single parameter, alpha, which controls\\nmodel complexity. The way alpha works is that the algorithm adds to the data alpha\\nmany virtual data points that have positive values for all the features. This results in a\\n“smoothing” of the statistics. A large alpha means more smoothing, resulting in less\\ncomplex models. The algorithm’s performance is relatively robust to the setting of\\nalpha, meaning that setting alpha is not critical for good performance. However,\\ntuning it usually improves accuracy somewhat.\\nGaussianNB is mostly used on very high-dimensional data, while the other two var‐\\niants of naive Bayes are widely used for sparse count data such as text. MultinomialNB\\nusually performs better than BinaryNB, particularly on datasets with a relatively large\\nnumber of nonzero features (i.e., large documents).\\nThe naive Bayes models share many of the strengths and weaknesses of the linear'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 83, 'page_label': '70', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='number of nonzero features (i.e., large documents).\\nThe naive Bayes models share many of the strengths and weaknesses of the linear\\nmodels. They are very fast to train and to predict, and the training procedure is easy\\nto understand. The models work very well with high-dimensional sparse data and are\\nrelatively robust to the parameters. Naive Bayes models are great baseline models and\\nare often used on very large datasets, where training even a linear model might take\\ntoo long.\\nDecision Trees\\nDecision trees are widely used models for classification and regression tasks. Essen‐\\ntially, they learn a hierarchy of if/else questions, leading to a decision.\\nThese questions are similar to the questions you might ask in a game of 20 Questions.\\nImagine you want to distinguish between the following four animals: bears, hawks,\\npenguins, and dolphins. Y our goal is to get to the right answer by asking as few if/else'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 83, 'page_label': '70', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Imagine you want to distinguish between the following four animals: bears, hawks,\\npenguins, and dolphins. Y our goal is to get to the right answer by asking as few if/else\\nquestions as possible. Y ou might start off by asking whether the animal has feathers, a\\nquestion that narrows down your possible animals to just two. If the answer is “yes, ”\\nyou can ask another question that could help you distinguish between hawks and\\npenguins. For example, you could ask whether the animal can fly. If the animal\\ndoesn’t have feathers, your possible animal choices are dolphins and bears, and you\\nwill need to ask a question to distinguish between these two animals—for example,\\nasking whether the animal has fins.\\nThis series of questions can be expressed as a decision tree, as shown in Figure 2-22.\\nIn[56]:\\nmglearn.plots.plot_animal_tree()\\n70 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 84, 'page_label': '71', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-22. A decision tree to distinguish among several animals\\nIn this illustration, each node in the tree either represents a question or a terminal\\nnode (also called a leaf) that contains the answer. The edges connect the answers to a\\nquestion with the next question you would ask.\\nIn machine learning parlance, we built a model to distinguish between four classes of\\nanimals (hawks, penguins, dolphins, and bears) using the three features “has feath‐\\ners, ” “can fly, ” and “has fins. ” Instead of building these models by hand, we can learn\\nthem from data using supervised learning.\\nBuilding decision trees\\nLet’s go through the process of building a decision tree for the 2D classification data‐\\nset shown in Figure 2-23 . The dataset consists of two half-moon shapes, with each\\nclass consisting of 75 data points. We will refer to this dataset as two_moons.\\nLearning a decision tree means learning the sequence of if/else questions that gets us'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 84, 'page_label': '71', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='class consisting of 75 data points. We will refer to this dataset as two_moons.\\nLearning a decision tree means learning the sequence of if/else questions that gets us\\nto the true answer most quickly. In the machine learning setting, these questions are\\ncalled tests (not to be confused with the test set, which is the data we use to test to see\\nhow generalizable our model is). Usually data does not come in the form of binary\\nyes/no features as in the animal example, but is instead represented as continuous\\nfeatures such as in the 2D dataset shown in Figure 2-23. The tests that are used on\\ncontinuous data are of the form “Is feature i larger than value a?”\\nSupervised Machine Learning Algorithms | 71'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 85, 'page_label': '72', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-23. Two-moons dataset on which the decision tree will be built\\nTo build a tree, the algorithm searches over all possible tests and finds the one that is\\nmost informative about the target variable. Figure 2-24  shows the first test that is\\npicked. Splitting the dataset vertically at x[1]=0.0596 yields the most information; it\\nbest separates the points in class 1 from the points in class 2. The top node, also called\\nthe root, represents the whole dataset, consisting of 75 points belonging to class 0 and\\n75 points belonging to class 1. The split is done by testing whether x[1] <= 0.0596,\\nindicated by a black line. If the test is true, a point is assigned to the left node, which\\ncontains 2 points belonging to class 0 and 32 points belonging to class 1. Otherwise\\nthe point is assigned to the right node, which contains 48 points belonging to class 0\\nand 18 points belonging to class 1. These two nodes correspond to the top and bot‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 85, 'page_label': '72', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the point is assigned to the right node, which contains 48 points belonging to class 0\\nand 18 points belonging to class 1. These two nodes correspond to the top and bot‐\\ntom regions shown in Figure 2-24. Even though the first split did a good job of sepa‐\\nrating the two classes, the bottom region still contains points belonging to class 0, and\\nthe top region still contains points belonging to class 1. We can build a more accurate\\nmodel by repeating the process of looking for the best test in both regions.\\nFigure 2-25 shows that the most informative next split for the left and the right region\\nis based on x[0].\\n72 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 86, 'page_label': '73', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-24. Decision boundary of tree with depth 1 (left) and corresponding tree (right)\\nFigure 2-25. Decision boundary of tree with depth 2 (left) and corresponding decision\\ntree (right)\\nThis recursive process yields a binary tree of decisions, with each node containing a\\ntest. Alternatively, you can think of each test as splitting the part of the data that is\\ncurrently being considered along one axis. This yields a view of the algorithm as\\nbuilding a hierarchical partition. As each test concerns only a single feature, the\\nregions in the resulting partition always have axis-parallel boundaries.\\nThe recursive partitioning of the data is repeated until each region in the partition\\n(each leaf in the decision tree) only contains a single target value (a single class or a\\nsingle regression value). A leaf of the tree that contains data points that all share the\\nsame target value is called pure. The final partitioning for this dataset is shown in\\nFigure 2-26.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 86, 'page_label': '73', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='single regression value). A leaf of the tree that contains data points that all share the\\nsame target value is called pure. The final partitioning for this dataset is shown in\\nFigure 2-26.\\nSupervised Machine Learning Algorithms | 73'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 87, 'page_label': '74', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-26. Decision boundary of tree with depth 9 (left) and part of the corresponding\\ntree (right); the full tree is quite large and hard to visualize\\nA prediction on a new data point is made by checking which region of the partition\\nof the feature space the point lies in, and then predicting the majority target (or the\\nsingle target in the case of pure leaves) in that region. The region can be found by\\ntraversing the tree from the root and going left or right, depending on whether the\\ntest is fulfilled or not.\\nIt is also possible to use trees for regression tasks, using exactly the same technique.\\nTo make a prediction, we traverse the tree based on the tests in each node and find\\nthe leaf the new data point falls into. The output for this data point is the mean target\\nof the training points in this leaf.\\nControlling complexity of decision trees\\nTypically, building a tree as described here and continuing until all leaves are pure'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 87, 'page_label': '74', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='of the training points in this leaf.\\nControlling complexity of decision trees\\nTypically, building a tree as described here and continuing until all leaves are pure\\nleads to models that are very complex and highly overfit to the training data. The\\npresence of pure leaves mean that a tree is 100% accurate on the training set; each\\ndata point in the training set is in a leaf that has the correct majority class. The over‐\\nfitting can be seen on the left of Figure 2-26. Y ou can see the regions determined to\\nbelong to class 1 in the middle of all the points belonging to class 0. On the other\\nhand, there is a small strip predicted as class 0 around the point belonging to class 0\\nto the very right. This is not how one would imagine the decision boundary to look,\\nand the decision boundary focuses a lot on single outlier points that are far away\\nfrom the other points in that class.\\nThere are two common strategies to prevent overfitting: stopping the creation of the'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 87, 'page_label': '74', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='from the other points in that class.\\nThere are two common strategies to prevent overfitting: stopping the creation of the\\ntree early (also called pre-pruning), or building the tree but then removing or collaps‐\\ning nodes that contain little information (also called post-pruning or just pruning).\\nPossible criteria for pre-pruning include limiting the maximum depth of the tree,\\nlimiting the maximum number of leaves, or requiring a minimum number of points\\nin a node to keep splitting it.\\n74 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 88, 'page_label': '75', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Decision trees in scikit-learn are implemented in the DecisionTreeRegressor and\\nDecisionTreeClassifier classes. scikit-learn only implements pre-pruning, not\\npost-pruning.\\nLet’s look at the effect of pre-pruning in more detail on the Breast Cancer dataset. As\\nalways, we import the dataset and split it into a training and a test part. Then we build\\na model using the default setting of fully developing the tree (growing the tree until\\nall leaves are pure). We fix the random_state in the tree, which is used for tie-\\nbreaking internally:\\nIn[58]:\\nfrom sklearn.tree import DecisionTreeClassifier\\ncancer = load_breast_cancer()\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\\ntree = DecisionTreeClassifier(random_state=0)\\ntree.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\\nOut[58]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 88, 'page_label': '75', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='tree.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\\nOut[58]:\\nAccuracy on training set: 1.000\\nAccuracy on test set: 0.937\\nAs expected, the accuracy on the training set is 100%—because the leaves are pure,\\nthe tree was grown deep enough that it could perfectly memorize all the labels on the\\ntraining data. The test set accuracy is slightly worse than for the linear models we\\nlooked at previously, which had around 95% accuracy.\\nIf we don’t restrict the depth of a decision tree, the tree can become arbitrarily deep\\nand complex. Unpruned trees are therefore prone to overfitting and not generalizing\\nwell to new data. Now let’s apply pre-pruning to the tree, which will stop developing\\nthe tree before we perfectly fit to the training data. One option is to stop building the\\ntree after a certain depth has been reached. Here we set max_depth=4, meaning only'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 88, 'page_label': '75', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the tree before we perfectly fit to the training data. One option is to stop building the\\ntree after a certain depth has been reached. Here we set max_depth=4, meaning only\\nfour consecutive questions can be asked (cf. Figures 2-24 and 2-26). Limiting the\\ndepth of the tree decreases overfitting. This leads to a lower accuracy on the training\\nset, but an improvement on the test set:\\nIn[59]:\\ntree = DecisionTreeClassifier(max_depth=4, random_state=0)\\ntree.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\\nSupervised Machine Learning Algorithms | 75'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 89, 'page_label': '76', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Out[59]:\\nAccuracy on training set: 0.988\\nAccuracy on test set: 0.951\\nAnalyzing decision trees\\nWe can visualize the tree using the export_graphviz function from the tree module.\\nThis writes a file in the .dot file format, which is a text file format for storing graphs.\\nWe set an option to color the nodes to reflect the majority class in each node and pass\\nthe class and features names so the tree can be properly labeled:\\nIn[61]:\\nfrom sklearn.tree import export_graphviz\\nexport_graphviz(tree, out_file=\"tree.dot\", class_names=[\"malignant\", \"benign\"],\\n                feature_names=cancer.feature_names, impurity=False, filled=True)\\nWe can read this file and visualize it, as seen in Figure 2-27, using the graphviz mod‐\\nule (or you can use any program that can read .dot files):\\nIn[61]:\\nimport graphviz\\nwith open(\"tree.dot\") as f:\\n    dot_graph = f.read()\\ngraphviz.Source(dot_graph)\\nFigure 2-27. Visualization of the decision tree built on the Breast Cancer dataset'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 89, 'page_label': '76', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[61]:\\nimport graphviz\\nwith open(\"tree.dot\") as f:\\n    dot_graph = f.read()\\ngraphviz.Source(dot_graph)\\nFigure 2-27. Visualization of the decision tree built on the Breast Cancer dataset\\n76 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 90, 'page_label': '77', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='The visualization of the tree provides a great in-depth view of how the algorithm\\nmakes predictions, and is a good example of a machine learning algorithm that is\\neasily explained to nonexperts. However, even with a tree of depth four, as seen here,\\nthe tree can become a bit overwhelming. Deeper trees (a depth of 10 is not uncom‐\\nmon) are even harder to grasp. One method of inspecting the tree that may be helpful\\nis to find out which path most of the data actually takes. The n_samples shown in\\neach node in Figure 2-27 gives the number of samples in that node, while value pro‐\\nvides the number of samples per class. Following the branches to the right, we see\\nthat worst radius <= 16.795  creates a node that contains only 8 benign but 134\\nmalignant samples. The rest of this side of the tree then uses some finer distinctions\\nto split off these 8 remaining benign samples. Of the 142 samples that went to the'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 90, 'page_label': '77', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='malignant samples. The rest of this side of the tree then uses some finer distinctions\\nto split off these 8 remaining benign samples. Of the 142 samples that went to the\\nright in the initial split, nearly all of them (132) end up in the leaf to the very right.\\nTaking a left at the root, for worst radius > 16.795  we end up with 25 malignant\\nand 259 benign samples. Nearly all of the benign samples end up in the second leaf\\nfrom the right, with most of the other leaves containing very few samples.\\nFeature importance in trees\\nInstead of looking at the whole tree, which can be taxing, there are some useful prop‐\\nerties that we can derive to summarize the workings of the tree. The most commonly\\nused summary is feature importance, which rates how important each feature is for\\nthe decision a tree makes. It is a number between 0 and 1 for each feature, where 0\\nmeans “not used at all” and 1 means “perfectly predicts the target. ” The feature\\nimportances always sum to 1:\\nIn[62]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 90, 'page_label': '77', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='means “not used at all” and 1 means “perfectly predicts the target. ” The feature\\nimportances always sum to 1:\\nIn[62]:\\nprint(\"Feature importances:\\\\n{}\".format(tree.feature_importances_))\\nOut[62]:\\nFeature importances:\\n[ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.01\\n  0.048  0.     0.     0.002  0.     0.     0.     0.     0.     0.727  0.046\\n  0.     0.     0.014  0.     0.018  0.122  0.012  0.   ]\\nWe can visualize the feature importances in a way that is similar to the way we visual‐\\nize the coefficients in the linear model (Figure 2-28):\\nIn[63]:\\ndef plot_feature_importances_cancer(model):\\n    n_features = cancer.data.shape[1]\\n    plt.barh(range(n_features), model.feature_importances_, align=\\'center\\')\\n    plt.yticks(np.arange(n_features), cancer.feature_names)\\n    plt.xlabel(\"Feature importance\")\\n    plt.ylabel(\"Feature\")\\nplot_feature_importances_cancer(tree)\\nSupervised Machine Learning Algorithms | 77'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 91, 'page_label': '78', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-28. Feature importances computed from a decision tree learned on the Breast\\nCancer dataset\\nHere we see that the feature used in the top split (“worst radius”) is by far the most\\nimportant feature. This confirms our observation in analyzing the tree that the first\\nlevel already separates the two classes fairly well.\\nHowever, if a feature has a low feature_importance, it doesn’t mean that this feature\\nis uninformative. It only means that the feature was not picked by the tree, likely\\nbecause another feature encodes the same information.\\nIn contrast to the coefficients in linear models, feature importances are always posi‐\\ntive, and don’t encode which class a feature is indicative of. The feature importances\\ntell us that “worst radius” is important, but not whether a high radius is indicative of a\\nsample being benign or malignant. In fact, there might not be such a simple relation‐\\nship between features and class, as you can see in the following example (Figures 2-29\\nand 2-30):'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 91, 'page_label': '78', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='sample being benign or malignant. In fact, there might not be such a simple relation‐\\nship between features and class, as you can see in the following example (Figures 2-29\\nand 2-30):\\nIn[64]:\\ntree = mglearn.plots.plot_tree_not_monotone()\\ndisplay(tree)\\nOut[64]:\\nFeature importances: [ 0.  1.]\\n78 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 92, 'page_label': '79', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-29. A two-dimensional dataset in which the feature on the y-axis has a nonmo‐\\nnotonous relationship with the class label, and the decision boundaries found by a deci‐\\nsion tree\\nFigure 2-30. Decision tree learned on the data shown in Figure 2-29\\nThe plot shows a dataset with two features and two classes. Here, all the information\\nis contained in X[1], and X[0] is not used at all. But the relation between X[1] and\\nSupervised Machine Learning Algorithms | 79'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 93, 'page_label': '80', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the output class is not monotonous, meaning we cannot say “a high value of X[0]\\nmeans class 0, and a low value means class 1” (or vice versa).\\nWhile we focused our discussion here on decision trees for classification, all that was\\nsaid is similarly true for decision trees for regression, as implemented in Decision\\nTreeRegressor. The usage and analysis of regression trees is very similar to that of\\nclassification trees. There is one particular property of using tree-based models for\\nregression that we want to point out, though. The DecisionTreeRegressor (and all\\nother tree-based regression models) is not able to extrapolate, or make predictions\\noutside of the range of the training data.\\nLet’s look into this in more detail, using a dataset of historical computer memory\\n(RAM) prices. Figure 2-31 shows the dataset, with the date on the x-axis and the price\\nof one megabyte of RAM in that year on the y-axis:\\nIn[65]:\\nimport pandas as pd\\nram_prices = pd.read_csv(\"data/ram_price.csv\")'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 93, 'page_label': '80', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='of one megabyte of RAM in that year on the y-axis:\\nIn[65]:\\nimport pandas as pd\\nram_prices = pd.read_csv(\"data/ram_price.csv\")\\nplt.semilogy(ram_prices.date, ram_prices.price)\\nplt.xlabel(\"Year\")\\nplt.ylabel(\"Price in $/Mbyte\")\\nFigure 2-31. Historical development of the price of RAM, plotted on a log scale\\n80 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 94, 'page_label': '81', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Note the logarithmic scale of the y-axis. When plotting logarithmically, the relation\\nseems to be quite linear and so should be relatively easy to predict, apart from some\\nbumps.\\nWe will make a forecast for the years after 2000 using the historical data up to that\\npoint, with the date as our only feature. We will compare two simple models: a\\nDecisionTreeRegressor and LinearRegression. We rescale the prices using a loga‐\\nrithm, so that the relationship is relatively linear. This doesn’t make a difference for\\nthe DecisionTreeRegressor, but it makes a big difference for LinearRegression (we\\nwill discuss this in more depth in Chapter 4). After training the models and making\\npredictions, we apply the exponential map to undo the logarithm transform. We\\nmake predictions on the whole dataset for visualization purposes here, but for a\\nquantitative evaluation we would only consider the test dataset:\\nIn[66]:\\nfrom sklearn.tree import DecisionTreeRegressor'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 94, 'page_label': '81', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='quantitative evaluation we would only consider the test dataset:\\nIn[66]:\\nfrom sklearn.tree import DecisionTreeRegressor\\n# use historical data to forecast prices after the year 2000\\ndata_train = ram_prices[ram_prices.date < 2000]\\ndata_test = ram_prices[ram_prices.date >= 2000]\\n# predict prices based on date\\nX_train = data_train.date[:, np.newaxis]\\n# we use a log-transform to get a simpler relationship of data to target\\ny_train = np.log(data_train.price)\\ntree = DecisionTreeRegressor().fit(X_train, y_train)\\nlinear_reg = LinearRegression().fit(X_train, y_train)\\n# predict on all data\\nX_all = ram_prices.date[:, np.newaxis]\\npred_tree = tree.predict(X_all)\\npred_lr = linear_reg.predict(X_all)\\n# undo log-transform\\nprice_tree = np.exp(pred_tree)\\nprice_lr = np.exp(pred_lr)\\nFigure 2-32, created here, compares the predictions of the decision tree and the linear\\nregression model with the ground truth:\\nIn[67]:\\nplt.semilogy(data_train.date, data_train.price, label=\"Training data\")'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 94, 'page_label': '81', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='regression model with the ground truth:\\nIn[67]:\\nplt.semilogy(data_train.date, data_train.price, label=\"Training data\")\\nplt.semilogy(data_test.date, data_test.price, label=\"Test data\")\\nplt.semilogy(ram_prices.date, price_tree, label=\"Tree prediction\")\\nplt.semilogy(ram_prices.date, price_lr, label=\"Linear prediction\")\\nplt.legend()\\nSupervised Machine Learning Algorithms | 81'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 95, 'page_label': '82', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='9 It is actually possible to make very good forecasts with tree-based models (for example, when trying to predict\\nwhether a price will go up or down). The point of this example was not to show that trees are a bad model for\\ntime series, but to illustrate a particular property of how trees make predictions.\\nFigure 2-32. Comparison of predictions made by a linear model and predictions made\\nby a regression tree on the RAM price data\\nThe difference between the models is quite striking. The linear model approximates\\nthe data with a line, as we knew it would. This line provides quite a good forecast for\\nthe test data (the years after 2000), while glossing over some of the finer variations in\\nboth the training and the test data. The tree model, on the other hand, makes perfect\\npredictions on the training data; we did not restrict the complexity of the tree, so it\\nlearned the whole dataset by heart. However, once we leave the data range for which'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 95, 'page_label': '82', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='predictions on the training data; we did not restrict the complexity of the tree, so it\\nlearned the whole dataset by heart. However, once we leave the data range for which\\nthe model has data, the model simply keeps predicting the last known point. The tree\\nhas no ability to generate “new” responses, outside of what was seen in the training\\ndata. This shortcoming applies to all models based on trees.9\\nStrengths, weaknesses, and parameters\\nAs discussed earlier, the parameters that control model complexity in decision trees\\nare the pre-pruning parameters that stop the building of the tree before it is fully\\ndeveloped. Usually, picking one of the pre-pruning strategies—setting either\\n82 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 96, 'page_label': '83', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='max_depth, max_leaf_nodes, or min_samples_leaf—is sufficient to prevent overfit‐\\nting.\\nDecision trees have two advantages over many of the algorithms we’ve discussed so\\nfar: the resulting model can easily be visualized and understood by nonexperts (at\\nleast for smaller trees), and the algorithms are completely invariant to scaling of the\\ndata. As each feature is processed separately, and the possible splits of the data don’t\\ndepend on scaling, no preprocessing like normalization or standardization of features\\nis needed for decision tree algorithms. In particular, decision trees work well when\\nyou have features that are on completely different scales, or a mix of binary and con‐\\ntinuous features.\\nThe main downside of decision trees is that even with the use of pre-pruning, they\\ntend to overfit and provide poor generalization performance. Therefore, in most\\napplications, the ensemble methods we discuss next are usually used in place of a sin‐\\ngle decision tree.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 96, 'page_label': '83', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='tend to overfit and provide poor generalization performance. Therefore, in most\\napplications, the ensemble methods we discuss next are usually used in place of a sin‐\\ngle decision tree.\\nEnsembles of Decision Trees\\nEnsembles are methods that combine multiple machine learning models to create\\nmore powerful models. There are many models in the machine learning literature\\nthat belong to this category, but there are two ensemble models that have proven to\\nbe effective on a wide range of datasets for classification and regression, both of\\nwhich use decision trees as their building blocks: random forests and gradient boos‐\\nted decision trees.\\nRandom forests\\nAs we just observed, a main drawback of decision trees is that they tend to overfit the\\ntraining data. Random forests are one way to address this problem. A random forest\\nis essentially a collection of decision trees, where each tree is slightly different from'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 96, 'page_label': '83', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='training data. Random forests are one way to address this problem. A random forest\\nis essentially a collection of decision trees, where each tree is slightly different from\\nthe others. The idea behind random forests is that each tree might do a relatively\\ngood job of predicting, but will likely overfit on part of the data. If we build many\\ntrees, all of which work well and overfit in different ways, we can reduce the amount\\nof overfitting by averaging their results. This reduction in overfitting, while retaining\\nthe predictive power of the trees, can be shown using rigorous mathematics.\\nTo implement this strategy, we need to build many decision trees. Each tree should do\\nan acceptable job of predicting the target, and should also be different from the other\\ntrees. Random forests get their name from injecting randomness into the tree build‐\\ning to ensure each tree is different. There are two ways in which the trees in a random'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 96, 'page_label': '83', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='trees. Random forests get their name from injecting randomness into the tree build‐\\ning to ensure each tree is different. There are two ways in which the trees in a random\\nforest are randomized: by selecting the data points used to build a tree and by select‐\\ning the features in each split test. Let’s go into this process in more detail.\\nSupervised Machine Learning Algorithms | 83'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 97, 'page_label': '84', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"Building random forests.    To build a random forest model, you need to decide on the\\nnumber of trees to build (the n_estimators parameter of RandomForestRegressor or\\nRandomForestClassifier). Let’s say we want to build 10 trees. These trees will be\\nbuilt completely independently from each other, and the algorithm will make differ‐\\nent random choices for each tree to make sure the trees are distinct. To build a tree,\\nwe first take what is called a bootstrap sample of our data. That is, from our n_samples\\ndata points, we repeatedly draw an example randomly with replacement (meaning the\\nsame sample can be picked multiple times), n_samples times. This will create a data‐\\nset that is as big as the original dataset, but some data points will be missing from it\\n(approximately one third), and some will be repeated.\\nTo illustrate, let’s say we want to create a bootstrap sample of the list ['a', 'b',\\n'c', 'd']. A possible bootstrap sample would be ['b', 'd', 'd', 'c']. Another\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 97, 'page_label': '84', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"To illustrate, let’s say we want to create a bootstrap sample of the list ['a', 'b',\\n'c', 'd']. A possible bootstrap sample would be ['b', 'd', 'd', 'c']. Another\\npossible sample would be ['d', 'a', 'd', 'a'].\\nNext, a decision tree is built based on this newly created dataset. However, the algo‐\\nrithm we described for the decision tree is slightly modified. Instead of looking for\\nthe best test for each node, in each node the algorithm randomly selects a subset of\\nthe features, and it looks for the best possible test involving one of these features. The\\nnumber of features that are selected is controlled by the max_features parameter.\\nThis selection of a subset of features is repeated separately in each node, so that each\\nnode in a tree can make a decision using a different subset of the features.\\nThe bootstrap sampling leads to each decision tree in the random forest being built\\non a slightly different dataset. Because of the selection of features in each node, each\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 97, 'page_label': '84', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='The bootstrap sampling leads to each decision tree in the random forest being built\\non a slightly different dataset. Because of the selection of features in each node, each\\nsplit in each tree operates on a different subset of features. Together, these two mech‐\\nanisms ensure that all the trees in the random forest are different.\\nA critical parameter in this process is max_features. If we set max_features to n_fea\\ntures, that means that each split can look at all features in the dataset, and no ran‐\\ndomness will be injected in the feature selection (the randomness due to the\\nbootstrapping remains, though). If we set max_features to 1, that means that the\\nsplits have no choice at all on which feature to test, and can only search over different\\nthresholds for the feature that was selected randomly. Therefore, a high max_fea\\ntures means that the trees in the random forest will be quite similar, and they will be'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 97, 'page_label': '84', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='thresholds for the feature that was selected randomly. Therefore, a high max_fea\\ntures means that the trees in the random forest will be quite similar, and they will be\\nable to fit the data easily, using the most distinctive features. A low max_features\\nmeans that the trees in the random forest will be quite different, and that each tree\\nmight need to be very deep in order to fit the data well.\\nTo make a prediction using the random forest, the algorithm first makes a prediction\\nfor every tree in the forest. For regression, we can average these results to get our final\\nprediction. For classification, a “soft voting” strategy is used. This means each algo‐\\nrithm makes a “soft” prediction, providing a probability for each possible output\\n84 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 98, 'page_label': '85', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='label. The probabilities predicted by all the trees are averaged, and the class with the\\nhighest probability is predicted.\\nAnalyzing random forests.    Let’s apply a random forest consisting of five trees to the\\ntwo_moons dataset we studied earlier:\\nIn[68]:\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.datasets import make_moons\\nX, y = make_moons(n_samples=100, noise=0.25, random_state=3)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\\n                                                    random_state=42)\\nforest = RandomForestClassifier(n_estimators=5, random_state=2)\\nforest.fit(X_train, y_train)\\nThe trees that are built as part of the random forest are stored in the estimator_\\nattribute. Let’s visualize the decision boundaries learned by each tree, together with\\ntheir aggregate prediction as made by the forest (Figure 2-33):\\nIn[69]:\\nfig, axes = plt.subplots(2, 3, figsize=(20, 10))'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 98, 'page_label': '85', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='their aggregate prediction as made by the forest (Figure 2-33):\\nIn[69]:\\nfig, axes = plt.subplots(2, 3, figsize=(20, 10))\\nfor i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):\\n    ax.set_title(\"Tree {}\".format(i))\\n    mglearn.plots.plot_tree_partition(X_train, y_train, tree, ax=ax)\\nmglearn.plots.plot_2d_separator(forest, X_train, fill=True, ax=axes[-1, -1],\\n                                alpha=.4)\\naxes[-1, -1].set_title(\"Random Forest\")\\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\\nY ou can clearly see that the decision boundaries learned by the five trees are quite dif‐\\nferent. Each of them makes some mistakes, as some of the training points that are\\nplotted here were not actually included in the training sets of the trees, due to the\\nbootstrap sampling.\\nThe random forest overfits less than any of the trees individually, and provides a\\nmuch more intuitive decision boundary. In any real application, we would use many'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 98, 'page_label': '85', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='bootstrap sampling.\\nThe random forest overfits less than any of the trees individually, and provides a\\nmuch more intuitive decision boundary. In any real application, we would use many\\nmore trees (often hundreds or thousands), leading to even smoother boundaries.\\nSupervised Machine Learning Algorithms | 85'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 99, 'page_label': '86', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-33. Decision boundaries found by five randomized decision trees and the deci‐\\nsion boundary obtained by averaging their predicted probabilities\\nAs another example, let’s apply a random forest consisting of 100 trees on the Breast\\nCancer dataset:\\nIn[70]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, random_state=0)\\nforest = RandomForestClassifier(n_estimators=100, random_state=0)\\nforest.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))\\nOut[70]:\\nAccuracy on training set: 1.000\\nAccuracy on test set: 0.972\\nThe random forest gives us an accuracy of 97%, better than the linear models or a\\nsingle decision tree, without tuning any parameters. We could adjust the max_fea\\ntures setting, or apply pre-pruning as we did for the single decision tree. However,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 99, 'page_label': '86', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='single decision tree, without tuning any parameters. We could adjust the max_fea\\ntures setting, or apply pre-pruning as we did for the single decision tree. However,\\noften the default parameters of the random forest already work quite well.\\nSimilarly to the decision tree, the random forest provides feature importances, which\\nare computed by aggregating the feature importances over the trees in the forest. Typ‐\\nically, the feature importances provided by the random forest are more reliable than\\nthe ones provided by a single tree. Take a look at Figure 2-34.\\n86 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 100, 'page_label': '87', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[71]:\\nplot_feature_importances_cancer(forest)\\nFigure 2-34. Feature importances computed from a random forest that was fit to the\\nBreast Cancer dataset\\nAs you can see, the random forest gives nonzero importance to many more features\\nthan the single tree. Similarly to the single decision tree, the random forest also gives\\na lot of importance to the “worst radius” feature, but it actually chooses “worst perim‐\\neter” to be the most informative feature overall. The randomness in building the ran‐\\ndom forest forces the algorithm to consider many possible explanations, the result\\nbeing that the random forest captures a much broader picture of the data than a sin‐\\ngle tree.\\nStrengths, weaknesses, and parameters.    Random forests for regression and classifica‐\\ntion are currently among the most widely used machine learning methods. They are\\nvery powerful, often work well without heavy tuning of the parameters, and don’t\\nrequire scaling of the data.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 100, 'page_label': '87', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='tion are currently among the most widely used machine learning methods. They are\\nvery powerful, often work well without heavy tuning of the parameters, and don’t\\nrequire scaling of the data.\\nEssentially, random forests share all of the benefits of decision trees, while making up\\nfor some of their deficiencies. One reason to still use decision trees is if you need a\\ncompact representation of the decision-making process. It is basically impossible to\\ninterpret tens or hundreds of trees in detail, and trees in random forests tend to be\\ndeeper than decision trees (because of the use of feature subsets). Therefore, if you\\nneed to summarize the prediction making in a visual way to nonexperts, a single\\ndecision tree might be a better choice. While building random forests on large data‐\\nsets might be somewhat time consuming, it can be parallelized across multiple CPU\\nSupervised Machine Learning Algorithms | 87'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 101, 'page_label': '88', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='cores within a computer easily. If you are using a multi-core processor (as nearly all\\nmodern computers do), you can use the n_jobs parameter to adjust the number of\\ncores to use. Using more CPU cores will result in linear speed-ups (using two cores,\\nthe training of the random forest will be twice as fast), but specifying n_jobs larger\\nthan the number of cores will not help. Y ou can set n_jobs=-1 to use all the cores in\\nyour computer.\\nY ou should keep in mind that random forests, by their nature, are random, and set‐\\nting different random states (or not setting the random_state at all) can drastically\\nchange the model that is built. The more trees there are in the forest, the more robust\\nit will be against the choice of random state. If you want to have reproducible results,\\nit is important to fix the random_state.\\nRandom forests don’t tend to perform well on very high dimensional, sparse data,\\nsuch as text data. For this kind of data, linear models might be more appropriate.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 101, 'page_label': '88', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Random forests don’t tend to perform well on very high dimensional, sparse data,\\nsuch as text data. For this kind of data, linear models might be more appropriate.\\nRandom forests usually work well even on very large datasets, and training can easily\\nbe parallelized over many CPU cores within a powerful computer. However, random\\nforests require more memory and are slower to train and to predict than linear mod‐\\nels. If time and memory are important in an application, it might make sense to use a\\nlinear model instead.\\nThe important parameters to adjust are n_estimators, max_features, and possibly\\npre-pruning options like max_depth. For n_estimators, larger is always better. Aver‐\\naging more trees will yield a more robust ensemble by reducing overfitting. However,\\nthere are diminishing returns, and more trees need more memory and more time to\\ntrain. A common rule of thumb is to build “as many as you have time/memory for. ”'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 101, 'page_label': '88', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='there are diminishing returns, and more trees need more memory and more time to\\ntrain. A common rule of thumb is to build “as many as you have time/memory for. ”\\nAs described earlier, max_features determines how random each tree is, and a\\nsmaller max_features reduces overfitting. In general, it’s a good rule of thumb to use\\nthe default values: max_features=sqrt(n_features) for classification and max_fea\\ntures=log2(n_features) for regression. Adding max_features or max_leaf_nodes\\nmight sometimes improve performance. It can also drastically reduce space and time\\nrequirements for training and prediction.\\nGradient boosted regression trees (gradient boosting machines)\\nThe gradient boosted regression tree is another ensemble method that combines mul‐\\ntiple decision trees to create a more powerful model. Despite the “regression” in the\\nname, these models can be used for regression and classification. In contrast to the'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 101, 'page_label': '88', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='tiple decision trees to create a more powerful model. Despite the “regression” in the\\nname, these models can be used for regression and classification. In contrast to the\\nrandom forest approach, gradient boosting works by building trees in a serial man‐\\nner, where each tree tries to correct the mistakes of the previous one. By default, there\\nis no randomization in gradient boosted regression trees; instead, strong pre-pruning\\nis used. Gradient boosted trees often use very shallow trees, of depth one to five,\\nwhich makes the model smaller in terms of memory and makes predictions faster.\\n88 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 102, 'page_label': '89', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='The main idea behind gradient boosting is to combine many simple models (in this\\ncontext known as weak learners), like shallow trees. Each tree can only provide good\\npredictions on part of the data, and so more and more trees are added to iteratively\\nimprove performance.\\nGradient boosted trees are frequently the winning entries in machine learning com‐\\npetitions, and are widely used in industry. They are generally a bit more sensitive to\\nparameter settings than random forests, but can provide better accuracy if the param‐\\neters are set correctly.\\nApart from the pre-pruning and the number of trees in the ensemble, another impor‐\\ntant parameter of gradient boosting is the learning_rate, which controls how\\nstrongly each tree tries to correct the mistakes of the previous trees. A higher learning\\nrate means each tree can make stronger corrections, allowing for more complex mod‐\\nels. Adding more trees to the ensemble, which can be accomplished by increasing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 102, 'page_label': '89', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='rate means each tree can make stronger corrections, allowing for more complex mod‐\\nels. Adding more trees to the ensemble, which can be accomplished by increasing\\nn_estimators, also increases the model complexity, as the model has more chances\\nto correct mistakes on the training set.\\nHere is an example of using GradientBoostingClassifier on the Breast Cancer\\ndataset. By default, 100 trees of maximum depth 3 and a learning rate of 0.1 are used:\\nIn[72]:\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, random_state=0)\\ngbrt = GradientBoostingClassifier(random_state=0)\\ngbrt.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\\nOut[72]:\\nAccuracy on training set: 1.000\\nAccuracy on test set: 0.958'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 102, 'page_label': '89', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\\nOut[72]:\\nAccuracy on training set: 1.000\\nAccuracy on test set: 0.958\\nAs the training set accuracy is 100%, we are likely to be overfitting. To reduce overfit‐\\nting, we could either apply stronger pre-pruning by limiting the maximum depth or\\nlower the learning rate:\\nSupervised Machine Learning Algorithms | 89'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 103, 'page_label': '90', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[73]:\\ngbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\\ngbrt.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\\nOut[73]:\\nAccuracy on training set: 0.991\\nAccuracy on test set: 0.972\\nIn[74]:\\ngbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)\\ngbrt.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\\nOut[74]:\\nAccuracy on training set: 0.988\\nAccuracy on test set: 0.965\\nBoth methods of decreasing the model complexity reduced the training set accuracy,\\nas expected. In this case, lowering the maximum depth of the trees provided a signifi‐\\ncant improvement of the model, while lowering the learning rate only increased the\\ngeneralization performance slightly.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 103, 'page_label': '90', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='cant improvement of the model, while lowering the learning rate only increased the\\ngeneralization performance slightly.\\nAs for the other decision tree–based models, we can again visualize the feature\\nimportances to get more insight into our model (Figure 2-35). As we used 100 trees, it\\nis impractical to inspect them all, even if they are all of depth 1:\\nIn[75]:\\ngbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\\ngbrt.fit(X_train, y_train)\\nplot_feature_importances_cancer(gbrt)\\n90 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 104, 'page_label': '91', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-35. Feature importances computed from a gradient boosting classifier that was\\nfit to the Breast Cancer dataset\\nWe can see that the feature importances of the gradient boosted trees are somewhat\\nsimilar to the feature importances of the random forests, though the gradient boost‐\\ning completely ignored some of the features.\\nAs both gradient boosting and random forests perform well on similar kinds of data,\\na common approach is to first try random forests, which work quite robustly. If ran‐\\ndom forests work well but prediction time is at a premium, or it is important to\\nsqueeze out the last percentage of accuracy from the machine learning model, mov‐\\ning to gradient boosting often helps.\\nIf you want to apply gradient boosting to a large-scale problem, it might be worth\\nlooking into the xgboost package and its Python interface, which at the time of writ‐\\ning is faster (and sometimes easier to tune) than the scikit-learn implementation of\\ngradient boosting on many datasets.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 104, 'page_label': '91', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='ing is faster (and sometimes easier to tune) than the scikit-learn implementation of\\ngradient boosting on many datasets.\\nStrengths, weaknesses, and parameters.    Gradient boosted decision trees are among the\\nmost powerful and widely used models for supervised learning. Their main drawback\\nis that they require careful tuning of the parameters and may take a long time to\\ntrain. Similarly to other tree-based models, the algorithm works well without scaling\\nand on a mixture of binary and continuous features. As with other tree-based models,\\nit also often does not work well on high-dimensional sparse data.\\nThe main parameters of gradient boosted tree models are the number of trees, n_esti\\nmators, and the learning_rate, which controls the degree to which each tree is\\nallowed to correct the mistakes of the previous trees. These two parameters are highly\\nSupervised Machine Learning Algorithms | 91'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 105, 'page_label': '92', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='interconnected, as a lower learning_rate means that more trees are needed to build\\na model of similar complexity. In contrast to random forests, where a higher n_esti\\nmators value is always better, increasing n_estimators in gradient boosting leads to a\\nmore complex model, which may lead to overfitting. A common practice is to fit\\nn_estimators depending on the time and memory budget, and then search over dif‐\\nferent learning_rates.\\nAnother important parameter is max_depth (or alternatively max_leaf_nodes), to\\nreduce the complexity of each tree. Usually max_depth is set very low for gradient\\nboosted models, often not deeper than five splits.\\nKernelized Support Vector Machines\\nThe next type of supervised model we will discuss is kernelized support vector\\nmachines. We explored the use of linear support vector machines for classification in\\n“Linear models for classification” on page 56. Kernelized support vector machines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 105, 'page_label': '92', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='machines. We explored the use of linear support vector machines for classification in\\n“Linear models for classification” on page 56. Kernelized support vector machines\\n(often just referred to as SVMs) are an extension that allows for more complex mod‐\\nels that are not defined simply by hyperplanes in the input space. While there are sup‐\\nport vector machines for classification and regression, we will restrict ourselves to the\\nclassification case, as implemented in SVC. Similar concepts apply to support vector\\nregression, as implemented in SVR.\\nThe math behind kernelized support vector machines is a bit involved, and is beyond\\nthe scope of this book. Y ou can find the details in Chapter 1 of Hastie, Tibshirani, and\\nFriedman’s The Elements of Statistical Learning. However, we will try to give you some\\nsense of the idea behind the method.\\nLinear models and nonlinear features\\nAs you saw in Figure 2-15 , linear models can be quite limiting in low-dimensional'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 105, 'page_label': '92', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='sense of the idea behind the method.\\nLinear models and nonlinear features\\nAs you saw in Figure 2-15 , linear models can be quite limiting in low-dimensional\\nspaces, as lines and hyperplanes have limited flexibility. One way to make a linear\\nmodel more flexible is by adding more features—for example, by adding interactions\\nor polynomials of the input features.\\nLet’s look at the synthetic dataset we used in “Feature importance in trees” on page 77\\n(see Figure 2-29):\\nIn[76]:\\nX, y = make_blobs(centers=4, random_state=8)\\ny = y % 2\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\n92 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 106, 'page_label': '93', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='10 We picked this particular feature to add for illustration purposes. The choice is not particularly important.\\nFigure 2-36. Two-class classification dataset in which classes are not linearly separable\\nA linear model for classification can only separate points using a line, and will not be\\nable to do a very good job on this dataset (see Figure 2-37):\\nIn[77]:\\nfrom sklearn.svm import LinearSVC\\nlinear_svm = LinearSVC().fit(X, y)\\nmglearn.plots.plot_2d_separator(linear_svm, X)\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nNow let’s expand the set of input features, say by also adding feature1 ** 2 , the\\nsquare of the second feature, as a new feature. Instead of representing each data point\\nas a two-dimensional point, (feature0, feature1), we now represent it as a three-\\ndimensional point, (feature0, feature1, feature1 ** 2) .10 This new representa‐\\ntion is illustrated in Figure 2-38 in a three-dimensional scatter plot:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 106, 'page_label': '93', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='dimensional point, (feature0, feature1, feature1 ** 2) .10 This new representa‐\\ntion is illustrated in Figure 2-38 in a three-dimensional scatter plot:\\nSupervised Machine Learning Algorithms | 93'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 107, 'page_label': '94', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-37. Decision boundary found by a linear SVM\\nIn[78]:\\n# add the squared first feature\\nX_new = np.hstack([X, X[:, 1:] ** 2])\\nfrom mpl_toolkits.mplot3d import Axes3D, axes3d\\nfigure = plt.figure()\\n# visualize in 3D\\nax = Axes3D(figure, elev=-152, azim=-26)\\n# plot first all the points with y == 0, then all with y == 1\\nmask = y == 0\\nax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c=\\'b\\',\\n           cmap=mglearn.cm2, s=60)\\nax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c=\\'r\\', marker=\\'^\\',\\n           cmap=mglearn.cm2, s=60)\\nax.set_xlabel(\"feature0\")\\nax.set_ylabel(\"feature1\")\\nax.set_zlabel(\"feature1 ** 2\")\\n94 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 108, 'page_label': '95', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"Figure 2-38. Expansion of the dataset shown in Figure 2-37, created by adding a third\\nfeature derived from feature1\\nIn the new representation of the data, it is now indeed possible to separate the two\\nclasses using a linear model, a plane in three dimensions. We can confirm this by fit‐\\nting a linear model to the augmented data (see Figure 2-39):\\nIn[79]:\\nlinear_svm_3d = LinearSVC().fit(X_new, y)\\ncoef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_\\n# show linear decision boundary\\nfigure = plt.figure()\\nax = Axes3D(figure, elev=-152, azim=-26)\\nxx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)\\nyy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)\\nXX, YY = np.meshgrid(xx, yy)\\nZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]\\nax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)\\nax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\\n           cmap=mglearn.cm2, s=60)\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 108, 'page_label': '95', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='ax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)\\nax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c=\\'b\\',\\n           cmap=mglearn.cm2, s=60)\\nax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c=\\'r\\', marker=\\'^\\',\\n           cmap=mglearn.cm2, s=60)\\nax.set_xlabel(\"feature0\")\\nax.set_ylabel(\"feature1\")\\nax.set_zlabel(\"feature0 ** 2\")\\nSupervised Machine Learning Algorithms | 95'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 109, 'page_label': '96', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-39. Decision boundary found by a linear SVM on the expanded three-\\ndimensional dataset\\nAs a function of the original features, the linear SVM model is not actually linear any‐\\nmore. It is not a line, but more of an ellipse, as you can see from the plot created here\\n(Figure 2-40):\\nIn[80]:\\nZZ = YY ** 2\\ndec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])\\nplt.contourf(XX, YY, dec.reshape(XX.shape), levels=[dec.min(), 0, dec.max()],\\n             cmap=mglearn.cm2, alpha=0.5)\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\n96 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 110, 'page_label': '97', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-40. The decision boundary from Figure 2-39 as a function of the original two\\nfeatures\\nThe kernel trick\\nThe lesson here is that adding nonlinear features to the representation of our data can\\nmake linear models much more powerful. However, often we don’t know which fea‐\\ntures to add, and adding many features (like all possible interactions in a 100-\\ndimensional feature space) might make computation very expensive. Luckily, there is\\na clever mathematical trick that allows us to learn a classifier in a higher-dimensional\\nspace without actually computing the new, possibly very large representation. This is\\nknown as the kernel trick, and it works by directly computing the distance (more pre‐\\ncisely, the scalar products) of the data points for the expanded feature representation,\\nwithout ever actually computing the expansion.\\nThere are two ways to map your data into a higher-dimensional space that are com‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 110, 'page_label': '97', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='without ever actually computing the expansion.\\nThere are two ways to map your data into a higher-dimensional space that are com‐\\nmonly used with support vector machines: the polynomial kernel, which computes all\\npossible polynomials up to a certain degree of the original features (like feature1 **\\n2 * feature2 ** 5 ); and the radial basis function (RBF) kernel, also known as the\\nGaussian kernel. The Gaussian kernel is a bit harder to explain, as it corresponds to\\nan infinite-dimensional feature space. One way to explain the Gaussian kernel is that\\nSupervised Machine Learning Algorithms | 97'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 111, 'page_label': '98', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='11 This follows from the Taylor expansion of the exponential map.\\nit considers all possible polynomials of all degrees, but the importance of the features\\ndecreases for higher degrees.11\\nIn practice, the mathematical details behind the kernel SVM are not that important,\\nthough, and how an SVM with an RBF kernel makes a decision can be summarized\\nquite easily—we’ll do so in the next section.\\nUnderstanding SVMs\\nDuring training, the SVM learns how important each of the training data points is to\\nrepresent the decision boundary between the two classes. Typically only a subset of\\nthe training points matter for defining the decision boundary: the ones that lie on the\\nborder between the classes. These are called support vectors and give the support vec‐\\ntor machine its name.\\nTo make a prediction for a new point, the distance to each of the support vectors is\\nmeasured. A classification decision is made based on the distances to the support vec‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 111, 'page_label': '98', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"tor machine its name.\\nTo make a prediction for a new point, the distance to each of the support vectors is\\nmeasured. A classification decision is made based on the distances to the support vec‐\\ntor, and the importance of the support vectors that was learned during training\\n(stored in the dual_coef_ attribute of SVC).\\nThe distance between data points is measured by the Gaussian kernel:\\nkrbf(x1, x2) = exp (ɣǁx1 - x2ǁ2)\\nHere, x1 and x2 are data points, ǁ x1 - x2 ǁ denotes Euclidean distance, and ɣ (gamma)\\nis a parameter that controls the width of the Gaussian kernel.\\nFigure 2-41  shows the result of training a support vector machine on a two-\\ndimensional two-class dataset. The decision boundary is shown in black, and the sup‐\\nport vectors are larger points with the wide outline. The following code creates this\\nplot by training an SVM on the forge dataset:\\nIn[81]:\\nfrom sklearn.svm import SVC\\nX, y = mglearn.tools.make_handcrafted_dataset()\\nsvm = SVC(kernel='rbf', C=10, gamma=0.1).fit(X, y)\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 111, 'page_label': '98', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='plot by training an SVM on the forge dataset:\\nIn[81]:\\nfrom sklearn.svm import SVC\\nX, y = mglearn.tools.make_handcrafted_dataset()\\nsvm = SVC(kernel=\\'rbf\\', C=10, gamma=0.1).fit(X, y)\\nmglearn.plots.plot_2d_separator(svm, X, eps=.5)\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\n# plot support vectors\\nsv = svm.support_vectors_\\n# class labels of support vectors are given by the sign of the dual coefficients\\nsv_labels = svm.dual_coef_.ravel() > 0\\nmglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\n98 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 112, 'page_label': '99', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-41. Decision boundary and support vectors found by an SVM with RBF kernel\\nIn this case, the SVM yields a very smooth and nonlinear (not a straight line) bound‐\\nary. We adjusted two parameters here: the C parameter and the gamma parameter,\\nwhich we will now discuss in detail.\\nTuning SVM parameters\\nThe gamma parameter is the one shown in the formula given in the previous section,\\nwhich controls the width of the Gaussian kernel. It determines the scale of what it\\nmeans for points to be close together. The C parameter is a regularization parameter,\\nsimilar to that used in the linear models. It limits the importance of each point (or\\nmore precisely, their dual_coef_).\\nLet’s have a look at what happens when we vary these parameters (Figure 2-42):\\nIn[82]:\\nfig, axes = plt.subplots(3, 3, figsize=(15, 10))\\nfor ax, C in zip(axes, [-1, 0, 3]):\\n    for a, gamma in zip(ax, range(-1, 2)):\\n        mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 112, 'page_label': '99', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[82]:\\nfig, axes = plt.subplots(3, 3, figsize=(15, 10))\\nfor ax, C in zip(axes, [-1, 0, 3]):\\n    for a, gamma in zip(ax, range(-1, 2)):\\n        mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)\\naxes[0, 0].legend([\"class 0\", \"class 1\", \"sv class 0\", \"sv class 1\"],\\n                  ncol=4, loc=(.9, 1.2))\\nSupervised Machine Learning Algorithms | 99'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 113, 'page_label': '100', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-42. Decision boundaries and support vectors for different settings of the param‐\\neters C and gamma\\nGoing from left to right, we increase the value of the parameter gamma from 0.1 to 10.\\nA small gamma means a large radius for the Gaussian kernel, which means that many\\npoints are considered close by. This is reflected in very smooth decision boundaries\\non the left, and boundaries that focus more on single points further to the right. A\\nlow value of gamma means that the decision boundary will vary slowly, which yields a\\nmodel of low complexity, while a high value of gamma yields a more complex model.\\nGoing from top to bottom, we increase the C parameter from 0.1 to 1000. As with the\\nlinear models, a small C means a very restricted model, where each data point can\\nonly have very limited influence. Y ou can see that at the top left the decision bound‐\\nary looks nearly linear, with the misclassified points barely having any influence on'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 113, 'page_label': '100', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='only have very limited influence. Y ou can see that at the top left the decision bound‐\\nary looks nearly linear, with the misclassified points barely having any influence on\\nthe line. Increasing C, as shown on the bottom right, allows these points to have a\\nstronger influence on the model and makes the decision boundary bend to correctly\\nclassify them.\\n100 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 114, 'page_label': '101', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Let’s apply the RBF kernel SVM to the Breast Cancer dataset. By default, C=1 and\\ngamma=1/n_features:\\nIn[83]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, random_state=0)\\nsvc = SVC()\\nsvc.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.2f}\".format(svc.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.2f}\".format(svc.score(X_test, y_test)))\\nOut[83]:\\nAccuracy on training set: 1.00\\nAccuracy on test set: 0.63\\nThe model overfits quite substantially, with a perfect score on the training set and\\nonly 63% accuracy on the test set. While SVMs often perform quite well, they are\\nvery sensitive to the settings of the parameters and to the scaling of the data. In par‐\\nticular, they require all the features to vary on a similar scale. Let’s look at the mini‐\\nmum and maximum values for each feature, plotted in log-space (Figure 2-43):\\nIn[84]:\\nplt.plot(X_train.min(axis=0), \\'o\\', label=\"min\")\\nplt.plot(X_train.max(axis=0), \\'^\\', label=\"max\")'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 114, 'page_label': '101', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='mum and maximum values for each feature, plotted in log-space (Figure 2-43):\\nIn[84]:\\nplt.plot(X_train.min(axis=0), \\'o\\', label=\"min\")\\nplt.plot(X_train.max(axis=0), \\'^\\', label=\"max\")\\nplt.legend(loc=4)\\nplt.xlabel(\"Feature index\")\\nplt.ylabel(\"Feature magnitude\")\\nplt.yscale(\"log\")\\nFrom this plot we can determine that features in the Breast Cancer dataset are of\\ncompletely different orders of magnitude. This can be somewhat of a problem for\\nother models (like linear models), but it has devastating effects for the kernel SVM.\\nLet’s examine some ways to deal with this issue.\\nSupervised Machine Learning Algorithms | 101'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 115, 'page_label': '102', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-43. Feature ranges for the Breast Cancer dataset (note that the y axis has a log‐\\narithmic scale)\\nPreprocessing data for SVMs\\nOne way to resolve this problem is by rescaling each feature so that they are all\\napproximately on the same scale. A common rescaling method for kernel SVMs is to\\nscale the data such that all features are between 0 and 1. We will see how to do this\\nusing the MinMaxScaler preprocessing method in Chapter 3, where we’ll give more\\ndetails. For now, let’s do this “by hand”:\\nIn[85]:\\n# compute the minimum value per feature on the training set\\nmin_on_training = X_train.min(axis=0)\\n# compute the range of each feature (max - min) on the training set\\nrange_on_training = (X_train - min_on_training).max(axis=0)\\n# subtract the min, and divide by range\\n# afterward, min=0 and max=1 for each feature\\nX_train_scaled = (X_train - min_on_training) / range_on_training\\nprint(\"Minimum for each feature\\\\n{}\".format(X_train_scaled.min(axis=0)))'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 115, 'page_label': '102', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='# afterward, min=0 and max=1 for each feature\\nX_train_scaled = (X_train - min_on_training) / range_on_training\\nprint(\"Minimum for each feature\\\\n{}\".format(X_train_scaled.min(axis=0)))\\nprint(\"Maximum for each feature\\\\n {}\".format(X_train_scaled.max(axis=0)))\\n102 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 116, 'page_label': '103', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Out[85]:\\nMinimum for each feature\\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\\nMaximum for each feature\\n [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\\n   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\\nIn[86]:\\n# use THE SAME transformation on the test set,\\n# using min and range of the training set (see Chapter 3 for details)\\nX_test_scaled = (X_test - min_on_training) / range_on_training\\nIn[87]:\\nsvc = SVC()\\nsvc.fit(X_train_scaled, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(\\n    svc.score(X_train_scaled, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))\\nOut[87]:\\nAccuracy on training set: 0.948\\nAccuracy on test set: 0.951\\nScaling the data made a huge difference! Now we are actually in an underfitting\\nregime, where training and test set performance are quite similar but less close to'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 116, 'page_label': '103', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Accuracy on test set: 0.951\\nScaling the data made a huge difference! Now we are actually in an underfitting\\nregime, where training and test set performance are quite similar but less close to\\n100% accuracy. From here, we can try increasing either C or gamma to fit a more com‐\\nplex model. For example:\\nIn[88]:\\nsvc = SVC(C=1000)\\nsvc.fit(X_train_scaled, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(\\n    svc.score(X_train_scaled, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))\\nOut[88]:\\nAccuracy on training set: 0.988\\nAccuracy on test set: 0.972\\nHere, increasing C allows us to improve the model significantly, resulting in 97.2%\\naccuracy.\\nSupervised Machine Learning Algorithms | 103'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 117, 'page_label': '104', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Strengths, weaknesses, and parameters\\nKernelized support vector machines are powerful models and perform well on a vari‐\\nety of datasets. SVMs allow for complex decision boundaries, even if the data has only\\na few features. They work well on low-dimensional and high-dimensional data (i.e.,\\nfew and many features), but don’t scale very well with the number of samples. Run‐\\nning an SVM on data with up to 10,000 samples might work well, but working with\\ndatasets of size 100,000 or more can become challenging in terms of runtime and\\nmemory usage.\\nAnother downside of SVMs is that they require careful preprocessing of the data and\\ntuning of the parameters. This is why, these days, most people instead use tree-based\\nmodels such as random forests or gradient boosting (which require little or no pre‐\\nprocessing) in many applications. Furthermore, SVM models are hard to inspect; it\\ncan be difficult to understand why a particular prediction was made, and it might be'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 117, 'page_label': '104', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='processing) in many applications. Furthermore, SVM models are hard to inspect; it\\ncan be difficult to understand why a particular prediction was made, and it might be\\ntricky to explain the model to a nonexpert.\\nStill, it might be worth trying SVMs, particularly if all of your features represent\\nmeasurements in similar units (e.g., all are pixel intensities) and they are on similar\\nscales.\\nThe important parameters in kernel SVMs are the regularization parameter C, the\\nchoice of the kernel, and the kernel-specific parameters. Although we primarily\\nfocused on the RBF kernel, other choices are available in scikit-learn. The RBF\\nkernel has only one parameter, gamma, which is the inverse of the width of the Gaus‐\\nsian kernel. gamma and C both control the complexity of the model, with large values\\nin either resulting in a more complex model. Therefore, good settings for the two\\nparameters are usually strongly correlated, and C and gamma should be adjusted\\ntogether.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 117, 'page_label': '104', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='in either resulting in a more complex model. Therefore, good settings for the two\\nparameters are usually strongly correlated, and C and gamma should be adjusted\\ntogether.\\nNeural Networks (Deep Learning)\\nA family of algorithms known as neural networks has recently seen a revival under\\nthe name “deep learning. ” While deep learning shows great promise in many machine\\nlearning applications, deep learning algorithms are often tailored very carefully to a\\nspecific use case. Here, we will only discuss some relatively simple methods, namely\\nmultilayer perceptrons for classification and regression, that can serve as a starting\\npoint for more involved deep learning methods. Multilayer perceptrons (MLPs) are\\nalso known as (vanilla) feed-forward neural networks, or sometimes just neural\\nnetworks.\\nThe neural network model\\nMLPs can be viewed as generalizations of linear models that perform multiple stages\\nof processing to come to a decision.\\n104 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 118, 'page_label': '105', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Remember that the prediction by a linear regressor is given as:\\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\\nIn plain English, ŷ is a weighted sum of the input features x[0] to x[p], weighted by\\nthe learned coefficients w[0] to w[p]. We could visualize this graphically as shown in\\nFigure 2-44:\\nIn[89]:\\ndisplay(mglearn.plots.plot_logistic_regression_graph())\\nFigure 2-44. Visualization of logistic regression, where input features and predictions are\\nshown as nodes, and the coefficients are connections between the nodes\\nHere, each node on the left represents an input feature, the connecting lines represent\\nthe learned coefficients, and the node on the right represents the output, which is a\\nweighted sum of the inputs.\\nIn an MLP this process of computing weighted sums is repeated multiple times, first\\ncomputing hidden units that represent an intermediate processing step, which are\\nagain combined using weighted sums to yield the final result (Figure 2-45):\\nIn[90]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 118, 'page_label': '105', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='computing hidden units that represent an intermediate processing step, which are\\nagain combined using weighted sums to yield the final result (Figure 2-45):\\nIn[90]:\\ndisplay(mglearn.plots.plot_single_hidden_layer_graph())\\nSupervised Machine Learning Algorithms | 105'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 119, 'page_label': '106', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-45. Illustration of a multilayer perceptron with a single hidden layer\\nThis model has a lot more coefficients (also called weights) to learn: there is one\\nbetween every input and every hidden unit (which make up the hidden layer), and\\none between every unit in the hidden layer and the output.\\nComputing a series of weighted sums is mathematically the same as computing just\\none weighted sum, so to make this model truly more powerful than a linear model,\\nwe need one extra trick. After computing a weighted sum for each hidden unit, a\\nnonlinear function is applied to the result—usually the rectifying nonlinearity (also\\nknown as rectified linear unit or relu) or the tangens hyperbolicus (tanh). The result of\\nthis function is then used in the weighted sum that computes the output, ŷ. The two\\nfunctions are visualized in Figure 2-46. The relu cuts off values below zero, while tanh\\nsaturates to –1 for low input values and +1 for high input values. Either nonlinear'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 119, 'page_label': '106', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='functions are visualized in Figure 2-46. The relu cuts off values below zero, while tanh\\nsaturates to –1 for low input values and +1 for high input values. Either nonlinear\\nfunction allows the neural network to learn much more complicated functions than a\\nlinear model could:\\nIn[91]:\\nline = np.linspace(-3, 3, 100)\\nplt.plot(line, np.tanh(line), label=\"tanh\")\\nplt.plot(line, np.maximum(line, 0), label=\"relu\")\\nplt.legend(loc=\"best\")\\nplt.xlabel(\"x\")\\nplt.ylabel(\"relu(x), tanh(x)\")\\n106 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 120, 'page_label': '107', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-46. The hyperbolic tangent activation function and the rectified linear activa‐\\ntion function\\nFor the small neural network pictured in Figure 2-45, the full formula for computing\\nŷ in the case of regression would be (when using a tanh nonlinearity):\\nh[0] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\\nh[1] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\\nh[2] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\\nŷ = v[0] * h[0] + v[1] * h[1] + v[2] * h[2]\\nHere, w are the weights between the input x and the hidden layer h, and v are the\\nweights between the hidden layer h and the output ŷ. The weights v and w are learned\\nfrom data, x are the input features, ŷ is the computed output, and h are intermediate\\ncomputations. An important parameter that needs to be set by the user is the number\\nof nodes in the hidden layer. This can be as small as 10 for very small or simple data‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 120, 'page_label': '107', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='computations. An important parameter that needs to be set by the user is the number\\nof nodes in the hidden layer. This can be as small as 10 for very small or simple data‐\\nsets and as big as 10,000 for very complex data. It is also possible to add additional\\nhidden layers, as shown in Figure 2-47:\\nSupervised Machine Learning Algorithms | 107'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 121, 'page_label': '108', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[92]:\\nmglearn.plots.plot_two_hidden_layer_graph()\\nFigure 2-47. A multilayer perceptron with two hidden layers\\nHaving large neural networks made up of many of these layers of computation is\\nwhat inspired the term “deep learning. ”\\nTuning neural networks\\nLet’s look into the workings of the MLP by applying the MLPClassifier to the\\ntwo_moons dataset we used earlier in this chapter. The results are shown in\\nFigure 2-48:\\nIn[93]:\\nfrom sklearn.neural_network import MLPClassifier\\nfrom sklearn.datasets import make_moons\\nX, y = make_moons(n_samples=100, noise=0.25, random_state=3)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\\n                                                    random_state=42)\\nmlp = MLPClassifier(algorithm=\\'l-bfgs\\', random_state=0).fit(X_train, y_train)\\nmglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 121, 'page_label': '108', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\n108 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 122, 'page_label': '109', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-48. Decision boundary learned by a neural network with 100 hidden units on\\nthe two_moons dataset\\nAs you can see, the neural network learned a very nonlinear but relatively smooth\\ndecision boundary. We used algorithm=\\'l-bfgs\\', which we will discuss later.\\nBy default, the MLP uses 100 hidden nodes, which is quite a lot for this small dataset.\\nWe can reduce the number (which reduces the complexity of the model) and still get\\na good result (Figure 2-49):\\nIn[94]:\\nmlp = MLPClassifier(algorithm=\\'l-bfgs\\', random_state=0, hidden_layer_sizes=[10])\\nmlp.fit(X_train, y_train)\\nmglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nSupervised Machine Learning Algorithms | 109'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 123, 'page_label': '110', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-49. Decision boundary learned by a neural network with 10 hidden units on\\nthe two_moons dataset\\nWith only 10 hidden units, the decision boundary looks somewhat more ragged. The\\ndefault nonlinearity is relu, shown in Figure 2-46 . With a single hidden layer, this\\nmeans the decision function will be made up of 10 straight line segments. If we want\\na smoother decision boundary, we could add more hidden units (as in Figure 2-49),\\nadd a second hidden layer (Figure 2-50), or use the tanh nonlinearity (Figure 2-51):\\nIn[95]:\\n# using two hidden layers, with 10 units each\\nmlp = MLPClassifier(algorithm=\\'l-bfgs\\', random_state=0,\\n                    hidden_layer_sizes=[10, 10])\\nmlp.fit(X_train, y_train)\\nmglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\n110 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 124, 'page_label': '111', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[96]:\\n# using two hidden layers, with 10 units each, now with tanh nonlinearity\\nmlp = MLPClassifier(algorithm=\\'l-bfgs\\', activation=\\'tanh\\',\\n                    random_state=0, hidden_layer_sizes=[10, 10])\\nmlp.fit(X_train, y_train)\\nmglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nFigure 2-50. Decision boundary learned using 2 hidden layers with 10 hidden units\\neach, with rect activation function\\nSupervised Machine Learning Algorithms | 111'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 125, 'page_label': '112', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"Figure 2-51. Decision boundary learned using 2 hidden layers with 10 hidden units\\neach, with tanh activation function\\nFinally, we can also control the complexity of a neural network by using an l2 penalty\\nto shrink the weights toward zero, as we did in ridge regression and the linear classifi‐\\ners. The parameter for this in the MLPClassifier is alpha (as in the linear regression\\nmodels), and it’s set to a very low value (little regularization) by default. Figure 2-52\\nshows the effect of different values of alpha on the two_moons dataset, using two hid‐\\nden layers of 10 or 100 units each:\\nIn[97]:\\nfig, axes = plt.subplots(2, 4, figsize=(20, 8))\\nfor axx, n_hidden_nodes in zip(axes, [10, 100]):\\n    for ax, alpha in zip(axx, [0.0001, 0.01, 0.1, 1]):\\n        mlp = MLPClassifier(algorithm='l-bfgs', random_state=0,\\n                            hidden_layer_sizes=[n_hidden_nodes, n_hidden_nodes],\\n                            alpha=alpha)\\n        mlp.fit(X_train, y_train)\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 125, 'page_label': '112', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='hidden_layer_sizes=[n_hidden_nodes, n_hidden_nodes],\\n                            alpha=alpha)\\n        mlp.fit(X_train, y_train)\\n        mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax)\\n        mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)\\n        ax.set_title(\"n_hidden=[{}, {}]\\\\nalpha={:.4f}\".format(\\n                      n_hidden_nodes, n_hidden_nodes, alpha))\\n112 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 126, 'page_label': '113', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-52. Decision functions for different numbers of hidden units and different set‐\\ntings of the alpha parameter\\nAs you probably have realized by now, there are many ways to control the complexity\\nof a neural network: the number of hidden layers, the number of units in each hidden\\nlayer, and the regularization ( alpha). There are actually even more, which we won’t\\ngo into here.\\nAn important property of neural networks is that their weights are set randomly\\nbefore learning is started, and this random initialization affects the model that is\\nlearned. That means that even when using exactly the same parameters, we can\\nobtain very different models when using different random seeds. If the networks are\\nlarge, and their complexity is chosen properly, this should not affect accuracy too\\nmuch, but it is worth keeping in mind (particularly for smaller networks).\\nFigure 2-53 shows plots of several models, all learned with the same settings of the\\nparameters:\\nIn[98]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 126, 'page_label': '113', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"much, but it is worth keeping in mind (particularly for smaller networks).\\nFigure 2-53 shows plots of several models, all learned with the same settings of the\\nparameters:\\nIn[98]:\\nfig, axes = plt.subplots(2, 4, figsize=(20, 8))\\nfor i, ax in enumerate(axes.ravel()):\\n    mlp = MLPClassifier(algorithm='l-bfgs', random_state=i,\\n                        hidden_layer_sizes=[100, 100])\\n    mlp.fit(X_train, y_train)\\n    mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax)\\n    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)\\nSupervised Machine Learning Algorithms | 113\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 127, 'page_label': '114', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-53. Decision functions learned with the same parameters but different random\\ninitializations\\nTo get a better understanding of neural networks on real-world data, let’s apply the\\nMLPClassifier to the Breast Cancer dataset. We start with the default parameters:\\nIn[99]:\\nprint(\"Cancer data per-feature maxima:\\\\n{}\".format(cancer.data.max(axis=0)))\\nOut[99]:\\nCancer data per-feature maxima:\\n[   28.110    39.280   188.500  2501.000     0.163     0.345     0.427\\n     0.201     0.304     0.097     2.873     4.885    21.980   542.200\\n     0.031     0.135     0.396     0.053     0.079     0.030    36.040\\n    49.540   251.200  4254.000     0.223     1.058     1.252     0.291\\n     0.664     0.207]\\nIn[100]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, random_state=0)\\nmlp = MLPClassifier(random_state=42)\\nmlp.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.2f}\".format(mlp.score(X_train, y_train)))'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 127, 'page_label': '114', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='cancer.data, cancer.target, random_state=0)\\nmlp = MLPClassifier(random_state=42)\\nmlp.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.2f}\".format(mlp.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.2f}\".format(mlp.score(X_test, y_test)))\\nOut[100]:\\nAccuracy on training set: 0.92\\nAccuracy on test set: 0.90\\nThe accuracy of the MLP is quite good, but not as good as the other models. As in the\\nearlier SVC example, this is likely due to scaling of the data. Neural networks also\\nexpect all input features to vary in a similar way, and ideally to have a mean of 0, and\\n114 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 128, 'page_label': '115', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='a variance of 1. We must rescale our data so that it fulfills these requirements. Again,\\nwe will do this by hand here, but we’ll introduce the StandardScaler to do this auto‐\\nmatically in Chapter 3:\\nIn[101]:\\n# compute the mean value per feature on the training set\\nmean_on_train = X_train.mean(axis=0)\\n# compute the standard deviation of each feature on the training set\\nstd_on_train = X_train.std(axis=0)\\n# subtract the mean, and scale by inverse standard deviation\\n# afterward, mean=0 and std=1\\nX_train_scaled = (X_train - mean_on_train) / std_on_train\\n# use THE SAME transformation (using training mean and std) on the test set\\nX_test_scaled = (X_test - mean_on_train) / std_on_train\\nmlp = MLPClassifier(random_state=0)\\nmlp.fit(X_train_scaled, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(\\n    mlp.score(X_train_scaled, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\\nOut[101]:\\nAccuracy on training set: 0.991\\nAccuracy on test set: 0.965'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 128, 'page_label': '115', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='mlp.score(X_train_scaled, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\\nOut[101]:\\nAccuracy on training set: 0.991\\nAccuracy on test set: 0.965\\nConvergenceWarning:\\n    Stochastic Optimizer: Maximum iterations reached and the optimization\\n    hasn\\'t converged yet.\\nThe results are much better after scaling, and already quite competitive. We got a\\nwarning from the model, though, that tells us that the maximum number of iterations\\nhas been reached. This is part of the adam algorithm for learning the model, and tells\\nus that we should increase the number of iterations:\\nIn[102]:\\nmlp = MLPClassifier(max_iter=1000, random_state=0)\\nmlp.fit(X_train_scaled, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(\\n    mlp.score(X_train_scaled, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\\nOut[102]:\\nAccuracy on training set: 0.995\\nAccuracy on test set: 0.965\\nSupervised Machine Learning Algorithms | 115'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 129, 'page_label': '116', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='12 Y ou might have noticed at this point that many of the well-performing models achieved exactly the same\\naccuracy of 0.972. This means that all of the models make exactly the same number of mistakes, which is four.\\nIf you compare the actual predictions, you can even see that they make exactly the same mistakes! This might\\nbe a consequence of the dataset being very small, or it may be because these points are really different from\\nthe rest.\\nIncreasing the number of iterations only increased the training set performance, not\\nthe generalization performance. Still, the model is performing quite well. As there is\\nsome gap between the training and the test performance, we might try to decrease the\\nmodel’s complexity to get better generalization performance. Here, we choose to\\nincrease the alpha parameter (quite aggressively, from 0.0001 to 1) to add stronger\\nregularization of the weights:\\nIn[103]:\\nmlp = MLPClassifier(max_iter=1000, alpha=1, random_state=0)\\nmlp.fit(X_train_scaled, y_train)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 129, 'page_label': '116', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='regularization of the weights:\\nIn[103]:\\nmlp = MLPClassifier(max_iter=1000, alpha=1, random_state=0)\\nmlp.fit(X_train_scaled, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(\\n    mlp.score(X_train_scaled, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\\nOut[103]:\\nAccuracy on training set: 0.988\\nAccuracy on test set: 0.972\\nThis leads to a performance on par with the best models so far.12\\nWhile it is possible to analyze what a neural network has learned, this is usually much\\ntrickier than analyzing a linear model or a tree-based model. One way to introspect\\nwhat was learned is to look at the weights in the model. Y ou can see an example of\\nthis in the scikit-learn example gallery . For the Breast Cancer dataset, this might\\nbe a bit hard to understand. The following plot ( Figure 2-54) shows the weights that\\nwere learned connecting the input to the first hidden layer. The rows in this plot cor‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 129, 'page_label': '116', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='be a bit hard to understand. The following plot ( Figure 2-54) shows the weights that\\nwere learned connecting the input to the first hidden layer. The rows in this plot cor‐\\nrespond to the 30 input features, while the columns correspond to the 100 hidden\\nunits. Light colors represent large positive values, while dark colors represent nega‐\\ntive values:\\nIn[104]:\\nplt.figure(figsize=(20, 5))\\nplt.imshow(mlp.coefs_[0], interpolation=\\'none\\', cmap=\\'viridis\\')\\nplt.yticks(range(30), cancer.feature_names)\\nplt.xlabel(\"Columns in weight matrix\")\\nplt.ylabel(\"Input feature\")\\nplt.colorbar()\\n116 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 130, 'page_label': '117', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-54. Heat map of the first layer weights in a neural network learned on the\\nBreast Cancer dataset\\nOne possible inference we can make is that features that have very small weights for\\nall of the hidden units are “less important” to the model. We can see that “mean\\nsmoothness” and “mean compactness, ” in addition to the features found between\\n“smoothness error” and “fractal dimension error, ” have relatively low weights com‐\\npared to other features. This could mean that these are less important features or pos‐\\nsibly that we didn’t represent them in a way that the neural network could use.\\nWe could also visualize the weights connecting the hidden layer to the output layer,\\nbut those are even harder to interpret.\\nWhile the MLPClassifier and MLPRegressor provide easy-to-use interfaces for the\\nmost common neural network architectures, they only capture a small subset of what\\nis possible with neural networks. If you are interested in working with more flexible'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 130, 'page_label': '117', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='most common neural network architectures, they only capture a small subset of what\\nis possible with neural networks. If you are interested in working with more flexible\\nor larger models, we encourage you to look beyond scikit-learn into the fantastic\\ndeep learning libraries that are out there. For Python users, the most well-established\\nare keras, lasagna, and tensor-flow. lasagna builds on the theano library, while\\nkeras can use either tensor-flow or theano. These libraries provide a much more\\nflexible interface to build neural networks and track the rapid progress in deep learn‐\\ning research. All of the popular deep learning libraries also allow the use of high-\\nperformance graphics processing units (GPUs), which scikit-learn does not\\nsupport. Using GPUs allows us to accelerate computations by factors of 10x to 100x,\\nand they are essential for applying deep learning methods to large-scale datasets.\\nStrengths, weaknesses, and parameters'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 130, 'page_label': '117', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='and they are essential for applying deep learning methods to large-scale datasets.\\nStrengths, weaknesses, and parameters\\nNeural networks have reemerged as state-of-the-art models in many applications of\\nmachine learning. One of their main advantages is that they are able to capture infor‐\\nmation contained in large amounts of data and build incredibly complex models.\\nGiven enough computation time, data, and careful tuning of the parameters, neural\\nnetworks often beat other machine learning algorithms (for classification and regres‐\\nsion tasks).\\nSupervised Machine Learning Algorithms | 117'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 131, 'page_label': '118', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='This brings us to the downsides. Neural networks—particularly the large and power‐\\nful ones—often take a long time to train. They also require careful preprocessing of\\nthe data, as we saw here. Similarly to SVMs, they work best with “homogeneous”\\ndata, where all the features have similar meanings. For data that has very different\\nkinds of features, tree-based models might work better. Tuning neural network\\nparameters is also an art unto itself. In our experiments, we barely scratched the sur‐\\nface of possible ways to adjust neural network models and how to train them.\\nEstimating complexity in neural networks.    The most important parameters are the num‐\\nber of layers and the number of hidden units per layer. Y ou should start with one or\\ntwo hidden layers, and possibly expand from there. The number of nodes per hidden\\nlayer is often similar to the number of input features, but rarely higher than in the low\\nto mid-thousands.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 131, 'page_label': '118', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='two hidden layers, and possibly expand from there. The number of nodes per hidden\\nlayer is often similar to the number of input features, but rarely higher than in the low\\nto mid-thousands.\\nA helpful measure when thinking about the model complexity of a neural network is\\nthe number of weights or coefficients that are learned. If you have a binary classifica‐\\ntion dataset with 100 features, and you have 100 hidden units, then there are 100 *\\n100 = 10,000 weights between the input and the first hidden layer. There are also\\n100 * 1 = 100 weights between the hidden layer and the output layer, for a total of\\naround 10,100 weights. If you add a second hidden layer with 100 hidden units, there\\nwill be another 100 * 100 = 10,000 weights from the first hidden layer to the second\\nhidden layer, resulting in a total of 20,100 weights. If instead you use one layer with\\n1,000 hidden units, you are learning 100 * 1,000 = 100,000 weights from the input to'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 131, 'page_label': '118', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='hidden layer, resulting in a total of 20,100 weights. If instead you use one layer with\\n1,000 hidden units, you are learning 100 * 1,000 = 100,000 weights from the input to\\nthe hidden layer and 1,000 x 1 weights from the hidden layer to the output layer, for a\\ntotal of 101,000. If you add a second hidden layer you add 1,000 * 1,000 = 1,000,000\\nweights, for a whopping total of 1,101,000—50 times larger than the model with two\\nhidden layers of size 100.\\nA common way to adjust parameters in a neural network is to first create a network\\nthat is large enough to overfit, making sure that the task can actually be learned by\\nthe network. Then, once you know the training data can be learned, either shrink the\\nnetwork or increase alpha to add regularization, which will improve generalization\\nperformance.\\nIn our experiments, we focused mostly on the definition of the model: the number of\\nlayers and nodes per layer, the regularization, and the nonlinearity. These define the'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 131, 'page_label': '118', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"performance.\\nIn our experiments, we focused mostly on the definition of the model: the number of\\nlayers and nodes per layer, the regularization, and the nonlinearity. These define the\\nmodel we want to learn. There is also the question of how to learn the model, or the\\nalgorithm that is used for learning the parameters, which is set using the algorithm\\nparameter. There are two easy-to-use choices for algorithm. The default is 'adam',\\nwhich works well in most situations but is quite sensitive to the scaling of the data (so\\nit is important to always scale your data to 0 mean and unit variance). The other one\\nis 'l-bfgs', which is quite robust but might take a long time on larger models or\\nlarger datasets. There is also the more advanced 'sgd' option, which is what many\\ndeep learning researchers use. The 'sgd' option comes with many additional param‐\\n118 | Chapter 2: Supervised Learning\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 132, 'page_label': '119', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"eters that need to be tuned for best results. Y ou can find all of these parameters and\\ntheir definitions in the user guide. When starting to work with MLPs, we recommend\\nsticking to 'adam' and 'l-bfgs'.\\nfit  Resets a Model\\nAn important property of scikit-learn models is that calling fit\\nwill always reset everything a model previously learned. So if you\\nbuild a model on one dataset, and then call fit again on a different\\ndataset, the model will “forget” everything it learned from the first\\ndataset. Y ou can call fit as often as you like on a model, and the\\noutcome will be the same as calling fit on a “new” model.\\nUncertainty Estimates from Classifiers\\nAnother useful part of the scikit-learn interface that we haven’t talked about yet is\\nthe ability of classifiers to provide uncertainty estimates of predictions. Often, you are\\nnot only interested in which class a classifier predicts for a certain test point, but also\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 132, 'page_label': '119', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the ability of classifiers to provide uncertainty estimates of predictions. Often, you are\\nnot only interested in which class a classifier predicts for a certain test point, but also\\nhow certain it is that this is the right class. In practice, different kinds of mistakes lead\\nto very different outcomes in real-world applications. Imagine a medical application\\ntesting for cancer. Making a false positive prediction might lead to a patient undergo‐\\ning additional tests, while a false negative prediction might lead to a serious disease\\nnot being treated. We will go into this topic in more detail in Chapter 6.\\nThere are two different functions in scikit-learn that can be used to obtain uncer‐\\ntainty estimates from classifiers: decision_function and predict_proba. Most (but\\nnot all) classifiers have at least one of them, and many classifiers have both. Let’s look\\nat what these two functions do on a synthetic two-dimensional dataset, when build‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 132, 'page_label': '119', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='not all) classifiers have at least one of them, and many classifiers have both. Let’s look\\nat what these two functions do on a synthetic two-dimensional dataset, when build‐\\ning a GradientBoostingClassifier classifier, which has both a decision_function\\nand a predict_proba method:\\nIn[105]:\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.datasets import make_blobs, make_circles\\nX, y = make_circles(noise=0.25, factor=0.5, random_state=1)\\n# we rename the classes \"blue\" and \"red\" for illustration purposes\\ny_named = np.array([\"blue\", \"red\"])[y]\\n# we can call train_test_split with arbitrarily many arrays;\\n# all will be split in a consistent manner\\nX_train, X_test, y_train_named, y_test_named, y_train, y_test = \\\\\\n    train_test_split(X, y_named, y, random_state=0)\\n# build the gradient boosting model\\ngbrt = GradientBoostingClassifier(random_state=0)\\ngbrt.fit(X_train, y_train_named)\\nUncertainty Estimates from Classifiers  | 119'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 133, 'page_label': '120', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='The Decision Function\\nIn the binary classification case, the return value of decision_function is of shape\\n(n_samples,), and it returns one floating-point number for each sample:\\nIn[106]:\\nprint(\"X_test.shape: {}\".format(X_test.shape))\\nprint(\"Decision function shape: {}\".format(\\n    gbrt.decision_function(X_test).shape))\\nOut[106]:\\nX_test.shape: (25, 2)\\nDecision function shape: (25,)\\nThis value encodes how strongly the model believes a data point to belong to the\\n“positive” class, in this case class 1. Positive values indicate a preference for the posi‐\\ntive class, and negative values indicate a preference for the “negative” (other) class:\\nIn[107]:\\n# show the first few entries of decision_function\\nprint(\"Decision function:\\\\n{}\".format(gbrt.decision_function(X_test)[:6]))\\nOut[107]:\\nDecision function:\\n[ 4.136 -1.683 -3.951 -3.626  4.29   3.662]\\nWe can recover the prediction by looking only at the sign of the decision function:\\nIn[108]:\\nprint(\"Thresholded decision function:\\\\n{}\".format('),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 133, 'page_label': '120', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='[ 4.136 -1.683 -3.951 -3.626  4.29   3.662]\\nWe can recover the prediction by looking only at the sign of the decision function:\\nIn[108]:\\nprint(\"Thresholded decision function:\\\\n{}\".format(\\n    gbrt.decision_function(X_test) > 0))\\nprint(\"Predictions:\\\\n{}\".format(gbrt.predict(X_test)))\\nOut[108]:\\nThresholded decision function:\\n[ True False False False  True  True False  True  True  True False  True\\n  True False  True False False False  True  True  True  True  True False\\n  False]\\nPredictions:\\n[\\'red\\' \\'blue\\' \\'blue\\' \\'blue\\' \\'red\\' \\'red\\' \\'blue\\' \\'red\\' \\'red\\' \\'red\\' \\'blue\\'\\n \\'red\\' \\'red\\' \\'blue\\' \\'red\\' \\'blue\\' \\'blue\\' \\'blue\\' \\'red\\' \\'red\\' \\'red\\' \\'red\\'\\n \\'red\\' \\'blue\\' \\'blue\\']\\nFor binary classification, the “negative” class is always the first entry of the classes_\\nattribute, and the “positive” class is the second entry of classes_. So if you want to\\nfully recover the output of predict, you need to make use of the classes_ attribute:\\n120 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 134, 'page_label': '121', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[109]:\\n# make the boolean True/False into 0 and 1\\ngreater_zero = (gbrt.decision_function(X_test) > 0).astype(int)\\n# use 0 and 1 as indices into classes_\\npred = gbrt.classes_[greater_zero]\\n# pred is the same as the output of gbrt.predict\\nprint(\"pred is equal to predictions: {}\".format(\\n    np.all(pred == gbrt.predict(X_test))))\\nOut[109]:\\npred is equal to predictions: True\\nThe range of decision_function can be arbitrary, and depends on the data and the\\nmodel parameters:\\nIn[110]:\\ndecision_function = gbrt.decision_function(X_test)\\nprint(\"Decision function minimum: {:.2f} maximum: {:.2f}\".format(\\n    np.min(decision_function), np.max(decision_function)))\\nOut[110]:\\nDecision function minimum: -7.69 maximum: 4.29\\nThis arbitrary scaling makes the output of decision_function often hard to\\ninterpret.\\nIn the following example we plot the decision_function for all points in the 2D\\nplane using a color coding, next to a visualization of the decision boundary, as we saw'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 134, 'page_label': '121', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='interpret.\\nIn the following example we plot the decision_function for all points in the 2D\\nplane using a color coding, next to a visualization of the decision boundary, as we saw\\nearlier. We show training points as circles and test data as triangles (Figure 2-55):\\nIn[111]:\\nfig, axes = plt.subplots(1, 2, figsize=(13, 5))\\nmglearn.tools.plot_2d_separator(gbrt, X, ax=axes[0], alpha=.4,\\n                                fill=True, cm=mglearn.cm2)\\nscores_image = mglearn.tools.plot_2d_scores(gbrt, X, ax=axes[1],\\n                                            alpha=.4, cm=mglearn.ReBl)\\nfor ax in axes:\\n    # plot training and test points\\n    mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test,\\n                             markers=\\'^\\', ax=ax)\\n    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train,\\n                             markers=\\'o\\', ax=ax)\\n    ax.set_xlabel(\"Feature 0\")\\n    ax.set_ylabel(\"Feature 1\")\\ncbar = plt.colorbar(scores_image, ax=axes.tolist())'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 134, 'page_label': '121', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='markers=\\'o\\', ax=ax)\\n    ax.set_xlabel(\"Feature 0\")\\n    ax.set_ylabel(\"Feature 1\")\\ncbar = plt.colorbar(scores_image, ax=axes.tolist())\\naxes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\",\\n                \"Train class 1\"], ncol=4, loc=(.1, 1.1))\\nUncertainty Estimates from Classifiers  | 121'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 135, 'page_label': '122', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-55. Decision boundary (left) and decision function (right) for a gradient boost‐\\ning model on a two-dimensional toy dataset\\nEncoding not only the predicted outcome but also how certain the classifier is pro‐\\nvides additional information. However, in this visualization, it is hard to make out the\\nboundary between the two classes.\\nPredicting Probabilities\\nThe output of predict_proba is a probability for each class, and is often more easily\\nunderstood than the output of decision_function. It is always of shape (n_samples,\\n2) for binary classification:\\nIn[112]:\\nprint(\"Shape of probabilities: {}\".format(gbrt.predict_proba(X_test).shape))\\nOut[112]:\\nShape of probabilities: (25, 2)\\nThe first entry in each row is the estimated probability of the first class, and the sec‐\\nond entry is the estimated probability of the second class. Because it is a probability,\\nthe output of predict_proba is always between 0 and 1, and the sum of the entries\\nfor both classes is always 1:\\nIn[113]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 135, 'page_label': '122', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the output of predict_proba is always between 0 and 1, and the sum of the entries\\nfor both classes is always 1:\\nIn[113]:\\n# show the first few entries of predict_proba\\nprint(\"Predicted probabilities:\\\\n{}\".format(\\n    gbrt.predict_proba(X_test[:6])))\\n122 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 136, 'page_label': '123', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='13 Because the probabilities are floating-point numbers, it is unlikely that they will both be exactly 0.500. How‐\\never, if that happens, the prediction is made at random.\\nOut[113]:\\nPredicted probabilities:\\n[[ 0.016  0.984]\\n [ 0.843  0.157]\\n [ 0.981  0.019]\\n [ 0.974  0.026]\\n [ 0.014  0.986]\\n [ 0.025  0.975]]\\nBecause the probabilities for the two classes sum to 1, exactly one of the classes will\\nbe above 50% certainty. That class is the one that is predicted.13\\nY ou can see in the previous output that the classifier is relatively certain for most\\npoints. How well the uncertainty actually reflects uncertainty in the data depends on\\nthe model and the parameters. A model that is more overfitted tends to make more\\ncertain predictions, even if they might be wrong. A model with less complexity usu‐\\nally has more uncertainty in its predictions. A model is called calibrated if the\\nreported uncertainty actually matches how correct it is—in a calibrated model, a pre‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 136, 'page_label': '123', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='ally has more uncertainty in its predictions. A model is called calibrated if the\\nreported uncertainty actually matches how correct it is—in a calibrated model, a pre‐\\ndiction made with 70% certainty would be correct 70% of the time.\\nIn the following example ( Figure 2-56) we again show the decision boundary on the\\ndataset, next to the class probabilities for the class 1:\\nIn[114]:\\nfig, axes = plt.subplots(1, 2, figsize=(13, 5))\\nmglearn.tools.plot_2d_separator(\\n    gbrt, X, ax=axes[0], alpha=.4, fill=True, cm=mglearn.cm2)\\nscores_image = mglearn.tools.plot_2d_scores(\\n    gbrt, X, ax=axes[1], alpha=.5, cm=mglearn.ReBl, function=\\'predict_proba\\')\\nfor ax in axes:\\n    # plot training and test points\\n    mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test,\\n                             markers=\\'^\\', ax=ax)\\n    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train,\\n                             markers=\\'o\\', ax=ax)\\n    ax.set_xlabel(\"Feature 0\")\\n    ax.set_ylabel(\"Feature 1\")'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 136, 'page_label': '123', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train,\\n                             markers=\\'o\\', ax=ax)\\n    ax.set_xlabel(\"Feature 0\")\\n    ax.set_ylabel(\"Feature 1\")\\ncbar = plt.colorbar(scores_image, ax=axes.tolist())\\naxes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\",\\n                \"Train class 1\"], ncol=4, loc=(.1, 1.1))\\nUncertainty Estimates from Classifiers  | 123'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 137, 'page_label': '124', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 2-56. Decision boundary (left) and predicted probabilities for the gradient boost‐\\ning model shown in Figure 2-55\\nThe boundaries in this plot are much more well-defined, and the small areas of\\nuncertainty are clearly visible.\\nThe scikit-learn website has a great comparison of many models and what their\\nuncertainty estimates look like. We’ve reproduced this in Figure 2-57, and we encour‐\\nage you to go though the example there.\\nFigure 2-57. Comparison of several classifiers in scikit-learn on synthetic datasets (image\\ncourtesy http://scikit-learn.org)\\nUncertainty in Multiclass Classification\\nSo far, we’ve only talked about uncertainty estimates in binary classification. But the\\ndecision_function and predict_proba methods also work in the multiclass setting.\\nLet’s apply them on the Iris dataset, which is a three-class classification dataset:\\n124 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 138, 'page_label': '125', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[115]:\\nfrom sklearn.datasets import load_iris\\niris = load_iris()\\nX_train, X_test, y_train, y_test = train_test_split(\\n    iris.data, iris.target, random_state=42)\\ngbrt = GradientBoostingClassifier(learning_rate=0.01, random_state=0)\\ngbrt.fit(X_train, y_train)\\nIn[116]:\\nprint(\"Decision function shape: {}\".format(gbrt.decision_function(X_test).shape))\\n# plot the first few entries of the decision function\\nprint(\"Decision function:\\\\n{}\".format(gbrt.decision_function(X_test)[:6, :]))\\nOut[116]:\\nDecision function shape: (38, 3)\\nDecision function:\\n[[-0.529  1.466 -0.504]\\n [ 1.512 -0.496 -0.503]\\n [-0.524 -0.468  1.52 ]\\n [-0.529  1.466 -0.504]\\n [-0.531  1.282  0.215]\\n [ 1.512 -0.496 -0.503]]\\nIn the multiclass case, the decision_function has the shape (n_samples,\\nn_classes) and each column provides a “certainty score” for each class, where a large\\nscore means that a class is more likely and a small score means the class is less likely.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 138, 'page_label': '125', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='n_classes) and each column provides a “certainty score” for each class, where a large\\nscore means that a class is more likely and a small score means the class is less likely.\\nY ou can recover the predictions from these scores by finding the maximum entry for\\neach data point:\\nIn[117]:\\nprint(\"Argmax of decision function:\\\\n{}\".format(\\n      np.argmax(gbrt.decision_function(X_test), axis=1)))\\nprint(\"Predictions:\\\\n{}\".format(gbrt.predict(X_test)))\\nOut[117]:\\nArgmax of decision function:\\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\\nPredictions:\\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\\nThe output of predict_proba has the same shape, (n_samples, n_classes). Again,\\nthe probabilities for the possible classes for each data point sum to 1:\\nUncertainty Estimates from Classifiers  | 125'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 139, 'page_label': '126', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[118]:\\n# show the first few entries of predict_proba\\nprint(\"Predicted probabilities:\\\\n{}\".format(gbrt.predict_proba(X_test)[:6]))\\n# show that sums across rows are one\\nprint(\"Sums: {}\".format(gbrt.predict_proba(X_test)[:6].sum(axis=1)))\\nOut[118]:\\nPredicted probabilities:\\n[[ 0.107  0.784  0.109]\\n [ 0.789  0.106  0.105]\\n [ 0.102  0.108  0.789]\\n [ 0.107  0.784  0.109]\\n [ 0.108  0.663  0.228]\\n [ 0.789  0.106  0.105]]\\nSums: [ 1.  1.  1.  1.  1.  1.]\\nWe can again recover the predictions by computing the argmax of predict_proba:\\nIn[119]:\\nprint(\"Argmax of predicted probabilities:\\\\n{}\".format(\\n    np.argmax(gbrt.predict_proba(X_test), axis=1)))\\nprint(\"Predictions:\\\\n{}\".format(gbrt.predict(X_test)))\\nOut[119]:\\nArgmax of predicted probabilities:\\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\\nPredictions:\\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\\nTo summarize, predict_proba and decision_function always have shape (n_sam'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 139, 'page_label': '126', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Predictions:\\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\\nTo summarize, predict_proba and decision_function always have shape (n_sam\\nples, n_classes)—apart from decision_function in the special binary case. In the\\nbinary case, decision_function only has one column, corresponding to the “posi‐\\ntive” class classes_[1]. This is mostly for historical reasons.\\nY ou can recover the prediction when there are n_classes many columns by comput‐\\ning the argmax across columns. Be careful, though, if your classes are strings, or you\\nuse integers but they are not consecutive and starting from 0. If you want to compare\\nresults obtained with predict to results obtained via decision_function or pre\\ndict_proba, make sure to use the classes_ attribute of the classifier to get the actual\\nclass names:\\n126 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 140, 'page_label': '127', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[120]:\\nlogreg = LogisticRegression()\\n# represent each target by its class name in the iris dataset\\nnamed_target = iris.target_names[y_train]\\nlogreg.fit(X_train, named_target)\\nprint(\"unique classes in training data: {}\".format(logreg.classes_))\\nprint(\"predictions: {}\".format(logreg.predict(X_test)[:10]))\\nargmax_dec_func = np.argmax(logreg.decision_function(X_test), axis=1)\\nprint(\"argmax of decision function: {}\".format(argmax_dec_func[:10]))\\nprint(\"argmax combined with classes_: {}\".format(\\n        logreg.classes_[argmax_dec_func][:10]))\\nOut[120]:\\nunique classes in training data: [\\'setosa\\' \\'versicolor\\' \\'virginica\\']\\npredictions: [\\'versicolor\\' \\'setosa\\' \\'virginica\\' \\'versicolor\\' \\'versicolor\\'\\n \\'setosa\\' \\'versicolor\\' \\'virginica\\' \\'versicolor\\' \\'versicolor\\']\\nargmax of decision function: [1 0 2 1 1 0 1 2 1 1]\\nargmax combined with classes_: [\\'versicolor\\' \\'setosa\\' \\'virginica\\' \\'versicolor\\'\\n \\'versicolor\\' \\'setosa\\' \\'versicolor\\' \\'virginica\\' \\'versicolor\\' \\'versicolor\\']\\nSummary and Outlook'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 140, 'page_label': '127', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"argmax combined with classes_: ['versicolor' 'setosa' 'virginica' 'versicolor'\\n 'versicolor' 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\\nSummary and Outlook\\nWe started this chapter with a discussion of model complexity, then discussed gener‐\\nalization, or learning a model that is able to perform well on new, previously unseen\\ndata. This led us to the concepts of underfitting, which describes a model that cannot\\ncapture the variations present in the training data, and overfitting, which describes a\\nmodel that focuses too much on the training data and is not able to generalize to new\\ndata very well.\\nWe then discussed a wide array of machine learning models for classification and\\nregression, what their advantages and disadvantages are, and how to control model\\ncomplexity for each of them. We saw that for many of the algorithms, setting the right\\nparameters is important for good performance. Some of the algorithms are also sensi‐\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 140, 'page_label': '127', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='complexity for each of them. We saw that for many of the algorithms, setting the right\\nparameters is important for good performance. Some of the algorithms are also sensi‐\\ntive to how we represent the input data, and in particular to how the features are\\nscaled. Therefore, blindly applying an algorithm to a dataset without understanding\\nthe assumptions the model makes and the meanings of the parameter settings will\\nrarely lead to an accurate model.\\nThis chapter contains a lot of information about the algorithms, and it is not neces‐\\nsary for you to remember all of these details for the following chapters. However,\\nsome knowledge of the models described here—and which to use in a specific situa‐\\ntion—is important for successfully applying machine learning in practice. Here is a\\nquick summary of when to use each model:\\nSummary and Outlook | 127'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 141, 'page_label': '128', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Nearest neighbors\\nFor small datasets, good as a baseline, easy to explain.\\nLinear models\\nGo-to as a first algorithm to try, good for very large datasets, good for very high-\\ndimensional data.\\nNaive Bayes\\nOnly for classification. Even faster than linear models, good for very large data‐\\nsets and high-dimensional data. Often less accurate than linear models.\\nDecision trees\\nVery fast, don’t need scaling of the data, can be visualized and easily explained.\\nRandom forests\\nNearly always perform better than a single decision tree, very robust and power‐\\nful. Don’t need scaling of data. Not good for very high-dimensional sparse data.\\nGradient boosted decision trees\\nOften slightly more accurate than random forests. Slower to train but faster to\\npredict than random forests, and smaller in memory. Need more parameter tun‐\\ning than random forests.\\nSupport vector machines\\nPowerful for medium-sized datasets of features with similar meaning. Require\\nscaling of data, sensitive to parameters.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 141, 'page_label': '128', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='ing than random forests.\\nSupport vector machines\\nPowerful for medium-sized datasets of features with similar meaning. Require\\nscaling of data, sensitive to parameters.\\nNeural networks\\nCan build very complex models, particularly for large datasets. Sensitive to scal‐\\ning of the data and to the choice of parameters. Large models need a long time to\\ntrain.\\nWhen working with a new dataset, it is in general a good idea to start with a simple\\nmodel, such as a linear model or a naive Bayes or nearest neighbors classifier, and see\\nhow far you can get. After understanding more about the data, you can consider\\nmoving to an algorithm that can build more complex models, such as random forests,\\ngradient boosted decision trees, SVMs, or neural networks.\\nY ou should now be in a position where you have some idea of how to apply, tune, and\\nanalyze the models we discussed here. In this chapter, we focused on the binary clas‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 141, 'page_label': '128', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Y ou should now be in a position where you have some idea of how to apply, tune, and\\nanalyze the models we discussed here. In this chapter, we focused on the binary clas‐\\nsification case, as this is usually easiest to understand. Most of the algorithms presen‐\\nted have classification and regression variants, however, and all of the classification\\nalgorithms support both binary and multiclass classification. Try applying any of\\nthese algorithms to the built-in datasets in scikit-learn, like the boston_housing or\\ndiabetes datasets for regression, or the digits dataset for multiclass classification.\\nPlaying around with the algorithms on different datasets will give you a better feel for\\n128 | Chapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 142, 'page_label': '129', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='how long they need to train, how easy it is to analyze the models, and how sensitive\\nthey are to the representation of the data.\\nWhile we analyzed the consequences of different parameter settings for the algo‐\\nrithms we investigated, building a model that actually generalizes well to new data in\\nproduction is a bit trickier than that. We will see how to properly adjust parameters\\nand how to find good parameters automatically in Chapter 6.\\nFirst, though, we will dive in more detail into unsupervised learning and preprocess‐\\ning in the next chapter.\\nSummary and Outlook | 129'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 144, 'page_label': '131', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 3\\nUnsupervised Learning and Preprocessing\\nThe second family of machine learning algorithms that we will discuss is unsuper‐\\nvised learning algorithms. Unsupervised learning subsumes all kinds of machine\\nlearning where there is no known output, no teacher to instruct the learning algo‐\\nrithm. In unsupervised learning, the learning algorithm is just shown the input data\\nand asked to extract knowledge from this data.\\nTypes of Unsupervised Learning\\nWe will look into two kinds of unsupervised learning in this chapter: transformations\\nof the dataset and clustering.\\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\\ntion of the data which might be easier for humans or other machine learning algo‐\\nrithms to understand compared to the original representation of the data. A common\\napplication of unsupervised transformations is dimensionality reduction, which takes\\na high-dimensional representation of the data, consisting of many features, and finds'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 144, 'page_label': '131', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='application of unsupervised transformations is dimensionality reduction, which takes\\na high-dimensional representation of the data, consisting of many features, and finds\\na new way to represent this data that summarizes the essential characteristics with\\nfewer features. A common application for dimensionality reduction is reduction to\\ntwo dimensions for visualization purposes.\\nAnother application for unsupervised transformations is finding the parts or compo‐\\nnents that “make up” the data. An example of this is topic extraction on collections of\\ntext documents. Here, the task is to find the unknown topics that are talked about in\\neach document, and to learn what topics appear in each document. This can be useful\\nfor tracking the discussion of themes like elections, gun control, or pop stars on social\\nmedia.\\nClustering algorithms, on the other hand, partition data into distinct groups of similar\\nitems. Consider the example of uploading photos to a social media site. To allow you\\n131'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 145, 'page_label': '132', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='to organize your pictures, the site might want to group together pictures that show\\nthe same person. However, the site doesn’t know which pictures show whom, and it\\ndoesn’t know how many different people appear in your photo collection. A sensible\\napproach would be to extract all the faces and divide them into groups of faces that\\nlook similar. Hopefully, these correspond to the same person, and the images can be\\ngrouped together for you.\\nChallenges in Unsupervised Learning\\nA major challenge in unsupervised learning is evaluating whether the algorithm\\nlearned something useful. Unsupervised learning algorithms are usually applied to\\ndata that does not contain any label information, so we don’t know what the right\\noutput should be. Therefore, it is very hard to say whether a model “did well. ” For\\nexample, our hypothetical clustering algorithm could have grouped together all the\\npictures that show faces in profile and all the full-face pictures. This would certainly'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 145, 'page_label': '132', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='example, our hypothetical clustering algorithm could have grouped together all the\\npictures that show faces in profile and all the full-face pictures. This would certainly\\nbe a possible way to divide a collection of pictures of people’s faces, but it’s not the one\\nwe were looking for. However, there is no way for us to “tell” the algorithm what we\\nare looking for, and often the only way to evaluate the result of an unsupervised algo‐\\nrithm is to inspect it manually.\\nAs a consequence, unsupervised algorithms are used often in an exploratory setting,\\nwhen a data scientist wants to understand the data better, rather than as part of a\\nlarger automatic system. Another common application for unsupervised algorithms\\nis as a preprocessing step for supervised algorithms. Learning a new representation of\\nthe data can sometimes improve the accuracy of supervised algorithms, or can lead to\\nreduced memory and time consumption.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 145, 'page_label': '132', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the data can sometimes improve the accuracy of supervised algorithms, or can lead to\\nreduced memory and time consumption.\\nBefore we start with “real” unsupervised algorithms, we will briefly discuss some sim‐\\nple preprocessing methods that often come in handy. Even though preprocessing and\\nscaling are often used in tandem with supervised learning algorithms, scaling meth‐\\nods don’t make use of the supervised information, making them unsupervised.\\nPreprocessing and Scaling\\nIn the previous chapter we saw that some algorithms, like neural networks and SVMs,\\nare very sensitive to the scaling of the data. Therefore, a common practice is to adjust\\nthe features so that the data representation is more suitable for these algorithms.\\nOften, this is a simple per-feature rescaling and shift of the data. The following code\\n(Figure 3-1) shows a simple example:\\nIn[2]:\\nmglearn.plots.plot_scaling()\\n132 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 146, 'page_label': '133', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='1 The median of a set of numbers is the number x such that half of the numbers are smaller than x and half of\\nthe numbers are larger than x. The lower quartile is the number x such that one-fourth of the numbers are\\nsmaller than x, and the upper quartile is the number x such that one-fourth of the numbers are larger than x.\\nFigure 3-1. Different ways to rescale and preprocess a dataset\\nDifferent  Kinds of Preprocessing\\nThe first plot in Figure 3-1 shows a synthetic two-class classification dataset with two\\nfeatures. The first feature (the x-axis value) is between 10 and 15. The second feature\\n(the y-axis value) is between around 1 and 9.\\nThe following four plots show four different ways to transform the data that yield\\nmore standard ranges. The StandardScaler in scikit-learn ensures that for each\\nfeature the mean is 0 and the variance is 1, bringing all features to the same magni‐\\ntude. However, this scaling does not ensure any particular minimum and maximum'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 146, 'page_label': '133', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='feature the mean is 0 and the variance is 1, bringing all features to the same magni‐\\ntude. However, this scaling does not ensure any particular minimum and maximum\\nvalues for the features. The RobustScaler works similarly to the StandardScaler in\\nthat it ensures statistical properties for each feature that guarantee that they are on the\\nsame scale. However, the RobustScaler uses the median and quartiles, 1 instead of\\nmean and variance. This makes the RobustScaler ignore data points that are very\\ndifferent from the rest (like measurement errors). These odd data points are also\\ncalled outliers, and can lead to trouble for other scaling techniques.\\nThe MinMaxScaler, on the other hand, shifts the data such that all features are exactly\\nbetween 0 and 1. For the two-dimensional dataset this means all of the data is con‐\\nPreprocessing and Scaling | 133'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 147, 'page_label': '134', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='tained within the rectangle created by the x-axis between 0 and 1 and the y-axis\\nbetween 0 and 1.\\nFinally, the Normalizer does a very different kind of rescaling. It scales each data\\npoint such that the feature vector has a Euclidean length of 1. In other words, it\\nprojects a data point on the circle (or sphere, in the case of higher dimensions) with a\\nradius of 1. This means every data point is scaled by a different number (by the\\ninverse of its length). This normalization is often used when only the direction (or\\nangle) of the data matters, not the length of the feature vector.\\nApplying Data Transformations\\nNow that we’ve seen what the different kinds of transformations do, let’s apply them\\nusing scikit-learn. We will use the cancer dataset that we saw in Chapter 2. Pre‐\\nprocessing methods like the scalers are usually applied before applying a supervised\\nmachine learning algorithm. As an example, say we want to apply the kernel SVM'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 147, 'page_label': '134', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='processing methods like the scalers are usually applied before applying a supervised\\nmachine learning algorithm. As an example, say we want to apply the kernel SVM\\n(SVC) to the cancer dataset, and use MinMaxScaler for preprocessing the data. We\\nstart by loading our dataset and splitting it into a training set and a test set (we need\\nseparate training and test sets to evaluate the supervised model we will build after the\\npreprocessing):\\nIn[3]:\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\ncancer = load_breast_cancer()\\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\\n                                                    random_state=1)\\nprint(X_train.shape)\\nprint(X_test.shape)\\nOut[3]:\\n(426, 30)\\n(143, 30)\\nAs a reminder, the dataset contains 569 data points, each represented by 30 measure‐\\nments. We split the dataset into 426 samples for the training set and 143 samples for\\nthe test set.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 147, 'page_label': '134', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='(143, 30)\\nAs a reminder, the dataset contains 569 data points, each represented by 30 measure‐\\nments. We split the dataset into 426 samples for the training set and 143 samples for\\nthe test set.\\nAs with the supervised models we built earlier, we first import the class that imple‐\\nments the preprocessing, and then instantiate it:\\nIn[4]:\\nfrom sklearn.preprocessing import MinMaxScaler\\nscaler = MinMaxScaler()\\n134 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 148, 'page_label': '135', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='We then fit the scaler using the fit method, applied to the training data. For the Min\\nMaxScaler, the fit method computes the minimum and maximum value of each fea‐\\nture on the training set. In contrast to the classifiers and regressors of Chapter 2, the\\nscaler is only provided with the data (X_train) when fit is called, and y_train is not\\nused:\\nIn[5]:\\nscaler.fit(X_train)\\nOut[5]:\\nMinMaxScaler(copy=True, feature_range=(0, 1))\\nTo apply the transformation that we just learned—that is, to actually scale the training\\ndata—we use the transform method of the scaler. The transform method is used in\\nscikit-learn whenever a model returns a new representation of the data:\\nIn[6]:\\n# transform data\\nX_train_scaled = scaler.transform(X_train)\\n# print dataset properties before and after scaling\\nprint(\"transformed shape: {}\".format(X_train_scaled.shape))\\nprint(\"per-feature minimum before scaling:\\\\n {}\".format(X_train.min(axis=0)))'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 148, 'page_label': '135', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='# print dataset properties before and after scaling\\nprint(\"transformed shape: {}\".format(X_train_scaled.shape))\\nprint(\"per-feature minimum before scaling:\\\\n {}\".format(X_train.min(axis=0)))\\nprint(\"per-feature maximum before scaling:\\\\n {}\".format(X_train.max(axis=0)))\\nprint(\"per-feature minimum after scaling:\\\\n {}\".format(\\n    X_train_scaled.min(axis=0)))\\nprint(\"per-feature maximum after scaling:\\\\n {}\".format(\\n    X_train_scaled.max(axis=0)))\\nOut[6]:\\ntransformed shape: (426, 30)\\nper-feature minimum before scaling:\\n [   6.98    9.71   43.79  143.50    0.05    0.02    0.      0.      0.11\\n     0.05    0.12    0.36    0.76    6.80    0.      0.      0.      0.\\n     0.01    0.      7.93   12.02   50.41  185.20    0.07    0.03    0.\\n     0.      0.16    0.06]\\nper-feature maximum before scaling:\\n [   28.11    39.28   188.5   2501.0     0.16     0.29     0.43     0.2\\n     0.300    0.100    2.87     4.88    21.98   542.20     0.03     0.14'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 148, 'page_label': '135', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='per-feature maximum before scaling:\\n [   28.11    39.28   188.5   2501.0     0.16     0.29     0.43     0.2\\n     0.300    0.100    2.87     4.88    21.98   542.20     0.03     0.14\\n     0.400    0.050    0.06     0.03    36.04    49.54   251.20  4254.00\\n     0.220    0.940    1.17     0.29     0.58     0.15]\\nper-feature minimum after scaling:\\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\\n   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\\nper-feature maximum after scaling:\\n [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\\n   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\\nPreprocessing and Scaling | 135'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 149, 'page_label': '136', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='The transformed data has the same shape as the original data—the features are simply\\nshifted and scaled. Y ou can see that all of the features are now between 0 and 1, as\\ndesired.\\nTo apply the SVM to the scaled data, we also need to transform the test set. This is\\nagain done by calling the transform method, this time on X_test:\\nIn[7]:\\n# transform test data\\nX_test_scaled = scaler.transform(X_test)\\n# print test data properties after scaling\\nprint(\"per-feature minimum after scaling:\\\\n{}\".format(X_test_scaled.min(axis=0)))\\nprint(\"per-feature maximum after scaling:\\\\n{}\".format(X_test_scaled.max(axis=0)))\\nOut[7]:\\nper-feature minimum after scaling:\\n[ 0.034  0.023  0.031  0.011  0.141  0.044  0.     0.     0.154 -0.006\\n -0.001  0.006  0.004  0.001  0.039  0.011  0.     0.    -0.032  0.007\\n  0.027  0.058  0.02   0.009  0.109  0.026  0.     0.    -0.    -0.002]\\nper-feature maximum after scaling:\\n[ 0.958  0.815  0.956  0.894  0.811  1.22   0.88   0.933  0.932  1.037'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 149, 'page_label': '136', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='0.027  0.058  0.02   0.009  0.109  0.026  0.     0.    -0.    -0.002]\\nper-feature maximum after scaling:\\n[ 0.958  0.815  0.956  0.894  0.811  1.22   0.88   0.933  0.932  1.037\\n  0.427  0.498  0.441  0.284  0.487  0.739  0.767  0.629  1.337  0.391\\n  0.896  0.793  0.849  0.745  0.915  1.132  1.07   0.924  1.205  1.631]\\nMaybe somewhat surprisingly, you can see that for the test set, after scaling, the mini‐\\nmum and maximum are not 0 and 1. Some of the features are even outside the 0–1\\nrange! The explanation is that the MinMaxScaler (and all the other scalers) always\\napplies exactly the same transformation to the training and the test set. This means\\nthe transform method always subtracts the training set minimum and divides by the\\ntraining set range, which might be different from the minimum and range for the test\\nset.\\nScaling Training and Test Data the Same Way\\nIt is important to apply exactly the same transformation to the training set and the'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 149, 'page_label': '136', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='set.\\nScaling Training and Test Data the Same Way\\nIt is important to apply exactly the same transformation to the training set and the\\ntest set for the supervised model to work on the test set. The following example\\n(Figure 3-2) illustrates what would happen if we were to use the minimum and range\\nof the test set instead:\\nIn[8]:\\nfrom sklearn.datasets import make_blobs\\n# make synthetic data\\nX, _ = make_blobs(n_samples=50, centers=5, random_state=4, cluster_std=2)\\n# split it into training and test sets\\nX_train, X_test = train_test_split(X, random_state=5, test_size=.1)\\n# plot the training and test sets\\nfig, axes = plt.subplots(1, 3, figsize=(13, 4))\\n136 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 150, 'page_label': '137', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='axes[0].scatter(X_train[:, 0], X_train[:, 1],\\n                c=mglearn.cm2(0), label=\"Training set\", s=60)\\naxes[0].scatter(X_test[:, 0], X_test[:, 1], marker=\\'^\\',\\n                c=mglearn.cm2(1), label=\"Test set\", s=60)\\naxes[0].legend(loc=\\'upper left\\')\\naxes[0].set_title(\"Original Data\")\\n# scale the data using MinMaxScaler\\nscaler = MinMaxScaler()\\nscaler.fit(X_train)\\nX_train_scaled = scaler.transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n# visualize the properly scaled data\\naxes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\\n                c=mglearn.cm2(0), label=\"Training set\", s=60)\\naxes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker=\\'^\\',\\n                c=mglearn.cm2(1), label=\"Test set\", s=60)\\naxes[1].set_title(\"Scaled Data\")\\n# rescale the test set separately\\n# so test set min is 0 and test set max is 1\\n# DO NOT DO THIS! For illustration purposes only.\\ntest_scaler = MinMaxScaler()\\ntest_scaler.fit(X_test)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 150, 'page_label': '137', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='# rescale the test set separately\\n# so test set min is 0 and test set max is 1\\n# DO NOT DO THIS! For illustration purposes only.\\ntest_scaler = MinMaxScaler()\\ntest_scaler.fit(X_test)\\nX_test_scaled_badly = test_scaler.transform(X_test)\\n# visualize wrongly scaled data\\naxes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\\n                c=mglearn.cm2(0), label=\"training set\", s=60)\\naxes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1],\\n                marker=\\'^\\', c=mglearn.cm2(1), label=\"test set\", s=60)\\naxes[2].set_title(\"Improperly Scaled Data\")\\nfor ax in axes:\\n    ax.set_xlabel(\"Feature 0\")\\n    ax.set_ylabel(\"Feature 1\")\\nFigure 3-2. Effect of scaling training and test data shown on the left together (center) and\\nseparately (right)\\nPreprocessing and Scaling | 137'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 151, 'page_label': '138', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='The first panel is an unscaled two-dimensional dataset, with the training set shown as\\ncircles and the test set shown as triangles. The second panel is the same data, but\\nscaled using the MinMaxScaler. Here, we called fit on the training set, and then\\ncalled transform on the training and test sets. Y ou can see that the dataset in the sec‐\\nond panel looks identical to the first; only the ticks on the axes have changed. Now all\\nthe features are between 0 and 1. Y ou can also see that the minimum and maximum\\nfeature values for the test data (the triangles) are not 0 and 1.\\nThe third panel shows what would happen if we scaled the training set and test set\\nseparately. In this case, the minimum and maximum feature values for both the train‐\\ning and the test set are 0 and 1. But now the dataset looks different. The test points\\nmoved incongruously to the training set, as they were scaled differently. We changed'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 151, 'page_label': '138', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='ing and the test set are 0 and 1. But now the dataset looks different. The test points\\nmoved incongruously to the training set, as they were scaled differently. We changed\\nthe arrangement of the data in an arbitrary way. Clearly this is not what we want to\\ndo.\\nAs another way to think about this, imagine your test set is a single point. There is no\\nway to scale a single point correctly, to fulfill the minimum and maximum require‐\\nments of the MinMaxScaler. But the size of your test set should not change your\\nprocessing.\\nShortcuts and \\nEfficient  Alternatives\\nOften, you want to fit a model on some dataset, and then transform it. This is a very\\ncommon task, which can often be computed more efficiently than by simply calling\\nfit and then transform. For this use case, all models that have a transform method\\nalso have a fit_transform method. Here is an example using StandardScaler:\\nIn[9]:\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 151, 'page_label': '138', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='also have a fit_transform method. Here is an example using StandardScaler:\\nIn[9]:\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\n# calling fit and transform in sequence (using method chaining)\\nX_scaled = scaler.fit(X).transform(X)\\n# same result, but more efficient computation\\nX_scaled_d = scaler.fit_transform(X)\\nWhile fit_transform is not necessarily more efficient for all models, it is still good\\npractice to use this method when trying to transform the training set.\\nThe \\nEffect  of Preprocessing on Supervised Learning\\nNow let’s go back to the cancer dataset and see the effect of using the MinMaxScaler\\non learning the SVC (this is a different way of doing the same scaling we did in Chap‐\\nter 2). First, let’s fit the SVC on the original data again for comparison:\\n138 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 152, 'page_label': '139', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[10]:\\nfrom sklearn.svm import SVC\\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\\n                                                    random_state=0)\\nsvm = SVC(C=100)\\nsvm.fit(X_train, y_train)\\nprint(\"Test set accuracy: {:.2f}\".format(svm.score(X_test, y_test)))\\nOut[10]:\\nTest set accuracy: 0.63\\nNow, let’s scale the data using MinMaxScaler before fitting the SVC:\\nIn[11]:\\n# preprocessing using 0-1 scaling\\nscaler = MinMaxScaler()\\nscaler.fit(X_train)\\nX_train_scaled = scaler.transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n# learning an SVM on the scaled training data\\nsvm.fit(X_train_scaled, y_train)\\n# scoring on the scaled test set\\nprint(\"Scaled test set accuracy: {:.2f}\".format(\\n    svm.score(X_test_scaled, y_test)))\\nOut[11]:\\nScaled test set accuracy: 0.97\\nAs we saw before, the effect of scaling the data is quite significant. Even though scal‐\\ning the data doesn’t involve any complicated math, it is good practice to use the scal‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 152, 'page_label': '139', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='As we saw before, the effect of scaling the data is quite significant. Even though scal‐\\ning the data doesn’t involve any complicated math, it is good practice to use the scal‐\\ning mechanisms provided by scikit-learn instead of reimplementing them yourself,\\nas it’s easy to make mistakes even in these simple computations.\\nY ou can also easily replace one preprocessing algorithm with another by changing the\\nclass you use, as all of the preprocessing classes have the same interface, consisting of\\nthe fit and transform methods:\\nIn[12]:\\n# preprocessing using zero mean and unit variance scaling\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\nscaler.fit(X_train)\\nX_train_scaled = scaler.transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\nPreprocessing and Scaling | 139'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 153, 'page_label': '140', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='# learning an SVM on the scaled training data\\nsvm.fit(X_train_scaled, y_train)\\n# scoring on the scaled test set\\nprint(\"SVM test accuracy: {:.2f}\".format(svm.score(X_test_scaled, y_test)))\\nOut[12]:\\nSVM test accuracy: 0.96\\nNow that we’ve seen how simple data transformations for preprocessing work, let’s\\nmove on to more interesting transformations using unsupervised learning.\\nDimensionality Reduction, Feature Extraction, and\\nManifold Learning\\nAs we discussed earlier, transforming data using unsupervised learning can have\\nmany motivations. The most common motivations are visualization, compressing the\\ndata, and finding a representation that is more informative for further processing.\\nOne of the simplest and most widely used algorithms for all of these is principal com‐\\nponent analysis. We’ll also look at two other algorithms: non-negative matrix factori‐\\nzation (NMF), which is commonly used for feature extraction, and t-SNE, which is'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 153, 'page_label': '140', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='ponent analysis. We’ll also look at two other algorithms: non-negative matrix factori‐\\nzation (NMF), which is commonly used for feature extraction, and t-SNE, which is\\ncommonly used for visualization using two-dimensional scatter plots.\\nPrincipal Component Analysis (PCA)\\nPrincipal component analysis is a method that rotates the dataset in a way such that\\nthe rotated features are statistically uncorrelated. This rotation is often followed by\\nselecting only a subset of the new features, according to how important they are for\\nexplaining the data. The following example ( Figure 3-3) illustrates the effect of PCA\\non a synthetic two-dimensional dataset:\\nIn[13]:\\nmglearn.plots.plot_pca_illustration()\\nThe first plot (top left) shows the original data points, colored to distinguish among\\nthem. The algorithm proceeds by first finding the direction of maximum variance,\\nlabeled “Component 1. ” This is the direction (or vector) in the data that contains most'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 153, 'page_label': '140', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='them. The algorithm proceeds by first finding the direction of maximum variance,\\nlabeled “Component 1. ” This is the direction (or vector) in the data that contains most\\nof the information, or in other words, the direction along which the features are most\\ncorrelated with each other. Then, the algorithm finds the direction that contains the\\nmost information while being orthogonal (at a right angle) to the first direction. In\\ntwo dimensions, there is only one possible orientation that is at a right angle, but in\\nhigher-dimensional spaces there would be (infinitely) many orthogonal directions.\\nAlthough the two components are drawn as arrows, it doesn’t really matter where the\\nhead and the tail are; we could have drawn the first component from the center up to\\n140 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 154, 'page_label': '141', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the top left instead of down to the bottom right. The directions found using this pro‐\\ncess are called principal components, as they are the main directions of variance in the\\ndata. In general, there are as many principal components as original features.\\nFigure 3-3. Transformation of data with PCA\\nThe second plot (top right) shows the same data, but now rotated so that the first\\nprincipal component aligns with the x-axis and the second principal component\\naligns with the y-axis. Before the rotation, the mean was subtracted from the data, so\\nthat the transformed data is centered around zero. In the rotated representation\\nfound by PCA, the two axes are uncorrelated, meaning that the correlation matrix of\\nthe data in this representation is zero except for the diagonal.\\nWe can use PCA for dimensionality reduction by retaining only some of the principal\\ncomponents. In this example, we might keep only the first principal component, as'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 154, 'page_label': '141', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='We can use PCA for dimensionality reduction by retaining only some of the principal\\ncomponents. In this example, we might keep only the first principal component, as\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 141'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 155, 'page_label': '142', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='shown in the third panel in Figure 3-3  (bottom left). This reduces the data from a\\ntwo-dimensional dataset to a one-dimensional dataset. Note, however, that instead of\\nkeeping only one of the original features, we found the most interesting direction\\n(top left to bottom right in the first panel) and kept this direction, the first principal\\ncomponent.\\nFinally, we can undo the rotation and add the mean back to the data. This will result\\nin the data shown in the last panel in Figure 3-3. These points are in the original fea‐\\nture space, but we kept only the information contained in the first principal compo‐\\nnent. This transformation is sometimes used to remove noise effects from the data or\\nvisualize what part of the information is retained using the principal components.\\nApplying PCA to the cancer dataset for visualization\\nOne of the most common applications of PCA is visualizing high-dimensional data‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 155, 'page_label': '142', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Applying PCA to the cancer dataset for visualization\\nOne of the most common applications of PCA is visualizing high-dimensional data‐\\nsets. As we saw in Chapter 1, it is hard to create scatter plots of data that has more\\nthan two features. For the Iris dataset, we were able to create a pair plot (Figure 1-3 in\\nChapter 1) that gave us a partial picture of the data by showing us all the possible\\ncombinations of two features. But if we want to look at the Breast Cancer dataset,\\neven using a pair plot is tricky. This dataset has 30 features, which would result in\\n30 * 14 = 420 scatter plots! We’ d never be able to look at all these plots in detail, let\\nalone try to understand them.\\nThere is an even simpler visualization we can use, though—computing histograms of\\neach of the features for the two classes, benign and malignant cancer (Figure 3-4):\\nIn[14]:\\nfig, axes = plt.subplots(15, 2, figsize=(10, 20))\\nmalignant = cancer.data[cancer.target == 0]\\nbenign = cancer.data[cancer.target == 1]'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 155, 'page_label': '142', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[14]:\\nfig, axes = plt.subplots(15, 2, figsize=(10, 20))\\nmalignant = cancer.data[cancer.target == 0]\\nbenign = cancer.data[cancer.target == 1]\\nax = axes.ravel()\\nfor i in range(30):\\n    _, bins = np.histogram(cancer.data[:, i], bins=50)\\n    ax[i].hist(malignant[:, i], bins=bins, color=mglearn.cm3(0), alpha=.5)\\n    ax[i].hist(benign[:, i], bins=bins, color=mglearn.cm3(2), alpha=.5)\\n    ax[i].set_title(cancer.feature_names[i])\\n    ax[i].set_yticks(())\\nax[0].set_xlabel(\"Feature magnitude\")\\nax[0].set_ylabel(\"Frequency\")\\nax[0].legend([\"malignant\", \"benign\"], loc=\"best\")\\nfig.tight_layout()\\n142 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 156, 'page_label': '143', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-4. Per-class feature histograms on the Breast Cancer dataset\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 143'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 157, 'page_label': '144', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Here we create a histogram for each of the features, counting how often a data point\\nappears with a feature in a certain range (called a bin). Each plot overlays two histo‐\\ngrams, one for all of the points in the benign class (blue) and one for all the points in\\nthe malignant class (red). This gives us some idea of how each feature is distributed\\nacross the two classes, and allows us to venture a guess as to which features are better\\nat distinguishing malignant and benign samples. For example, the feature “smooth‐\\nness error” seems quite uninformative, because the two histograms mostly overlap,\\nwhile the feature “worst concave points” seems quite informative, because the histo‐\\ngrams are quite disjoint.\\nHowever, this plot doesn’t show us anything about the interactions between variables\\nand how these relate to the classes. Using PCA, we can capture the main interactions\\nand get a slightly more complete picture. We can find the first two principal compo‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 157, 'page_label': '144', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='and how these relate to the classes. Using PCA, we can capture the main interactions\\nand get a slightly more complete picture. We can find the first two principal compo‐\\nnents, and visualize the data in this new two-dimensional space with a single scatter\\nplot.\\nBefore we apply PCA, we scale our data so that each feature has unit variance using\\nStandardScaler:\\nIn[15]:\\nfrom sklearn.datasets import load_breast_cancer\\ncancer = load_breast_cancer()\\nscaler = StandardScaler()\\nscaler.fit(cancer.data)\\nX_scaled = scaler.transform(cancer.data)\\nLearning the PCA transformation and applying it is as simple as applying a prepro‐\\ncessing transformation. We instantiate the PCA object, find the principal components\\nby calling the fit method, and then apply the rotation and dimensionality reduction\\nby calling transform. By default, PCA only rotates (and shifts) the data, but keeps all\\nprincipal components. To reduce the dimensionality of the data, we need to specify'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 157, 'page_label': '144', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='by calling transform. By default, PCA only rotates (and shifts) the data, but keeps all\\nprincipal components. To reduce the dimensionality of the data, we need to specify\\nhow many components we want to keep when creating the PCA object:\\nIn[16]:\\nfrom sklearn.decomposition import PCA\\n# keep the first two principal components of the data\\npca = PCA(n_components=2)\\n# fit PCA model to breast cancer data\\npca.fit(X_scaled)\\n# transform data onto the first two principal components\\nX_pca = pca.transform(X_scaled)\\nprint(\"Original shape: {}\".format(str(X_scaled.shape)))\\nprint(\"Reduced shape: {}\".format(str(X_pca.shape)))\\n144 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 158, 'page_label': '145', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Out[16]:\\nOriginal shape: (569, 30)\\nReduced shape: (569, 2)\\nWe can now plot the first two principal components (Figure 3-5):\\nIn[17]:\\n# plot first vs. second principal component, colored by class\\nplt.figure(figsize=(8, 8))\\nmglearn.discrete_scatter(X_pca[:, 0], X_pca[:, 1], cancer.target)\\nplt.legend(cancer.target_names, loc=\"best\")\\nplt.gca().set_aspect(\"equal\")\\nplt.xlabel(\"First principal component\")\\nplt.ylabel(\"Second principal component\")\\nFigure 3-5. Two-dimensional scatter plot of the Breast Cancer dataset using the first two\\nprincipal components\\nIt is important to note that PCA is an unsupervised method, and does not use any class\\ninformation when finding the rotation. It simply looks at the correlations in the data.\\nFor the scatter plot shown here, we plotted the first principal component against the\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 145'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 159, 'page_label': '146', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='second principal component, and then used the class information to color the points.\\nY ou can see that the two classes separate quite well in this two-dimensional space.\\nThis leads us to believe that even a linear classifier (that would learn a line in this\\nspace) could do a reasonably good job at distinguishing the two classes. We can also\\nsee that the malignant (red) points are more spread out than the benign (blue) points\\n—something that we could already see a bit from the histograms in Figure 3-4.\\nA downside of PCA is that the two axes in the plot are often not very easy to interpret.\\nThe principal components correspond to directions in the original data, so they are\\ncombinations of the original features. However, these combinations are usually very\\ncomplex, as we’ll see shortly. The principal components themselves are stored in the\\ncomponents_ attribute of the PCA object during fitting:\\nIn[18]:\\nprint(\"PCA component shape: {}\".format(pca.components_.shape))\\nOut[18]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 159, 'page_label': '146', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='components_ attribute of the PCA object during fitting:\\nIn[18]:\\nprint(\"PCA component shape: {}\".format(pca.components_.shape))\\nOut[18]:\\nPCA component shape: (2, 30)\\nEach row in components_ corresponds to one principal component, and they are sor‐\\nted by their importance (the first principal component comes first, etc.). The columns\\ncorrespond to the original features attribute of the PCA in this example, “mean\\nradius, ” “mean texture, ” and so on. Let’s have a look at the content of components_:\\nIn[19]:\\nprint(\"PCA components:\\\\n{}\".format(pca.components_))\\nOut[19]:\\nPCA components:\\n[[ 0.219  0.104  0.228  0.221  0.143  0.239  0.258  0.261  0.138  0.064\\n   0.206  0.017  0.211  0.203  0.015  0.17   0.154  0.183  0.042  0.103\\n   0.228  0.104  0.237  0.225  0.128  0.21   0.229  0.251  0.123  0.132]\\n [-0.234 -0.06  -0.215 -0.231  0.186  0.152  0.06  -0.035  0.19   0.367\\n  -0.106  0.09  -0.089 -0.152  0.204  0.233  0.197  0.13   0.184  0.28'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 159, 'page_label': '146', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='[-0.234 -0.06  -0.215 -0.231  0.186  0.152  0.06  -0.035  0.19   0.367\\n  -0.106  0.09  -0.089 -0.152  0.204  0.233  0.197  0.13   0.184  0.28\\n  -0.22  -0.045 -0.2   -0.219  0.172  0.144  0.098 -0.008  0.142  0.275]]\\nWe can also visualize the coefficients using a heat map ( Figure 3-6), which might be\\neasier to understand:\\nIn[20]:\\nplt.matshow(pca.components_, cmap=\\'viridis\\')\\nplt.yticks([0, 1], [\"First component\", \"Second component\"])\\nplt.colorbar()\\nplt.xticks(range(len(cancer.feature_names)),\\n           cancer.feature_names, rotation=60, ha=\\'left\\')\\nplt.xlabel(\"Feature\")\\nplt.ylabel(\"Principal components\")\\n146 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 160, 'page_label': '147', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-6. Heat map of the first two principal components on the Breast Cancer dataset\\nY ou can see that in the first component, all features have the same sign (it’s negative,\\nbut as we mentioned earlier, it doesn’t matter which direction the arrow points in).\\nThat means that there is a general correlation between all features. As one measure‐\\nment is high, the others are likely to be high as well. The second component has\\nmixed signs, and both of the components involve all of the 30 features. This mixing of\\nall features is what makes explaining the axes in Figure 3-6 so tricky.\\nEigenfaces for feature extraction\\nAnother application of PCA that we mentioned earlier is feature extraction. The idea\\nbehind feature extraction is that it is possible to find a representation of your data\\nthat is better suited to analysis than the raw representation you were given. A great\\nexample of an application where feature extraction is helpful is with images. Images'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 160, 'page_label': '147', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='that is better suited to analysis than the raw representation you were given. A great\\nexample of an application where feature extraction is helpful is with images. Images\\nare made up of pixels, usually stored as red, green, and blue (RGB) intensities.\\nObjects in images are usually made up of thousands of pixels, and only together are\\nthey meaningful.\\nWe will give a very simple application of feature extraction on images using PCA, by\\nworking with face images from the Labeled Faces in the Wild dataset. This dataset\\ncontains face images of celebrities downloaded from the Internet, and it includes\\nfaces of politicians, singers, actors, and athletes from the early 2000s. We use gray‐\\nscale versions of these images, and scale them down for faster processing. Y ou can see\\nsome of the images in Figure 3-7:\\nIn[21]:\\nfrom sklearn.datasets import fetch_lfw_people\\npeople = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\\nimage_shape = people.images[0].shape'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 160, 'page_label': '147', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"some of the images in Figure 3-7:\\nIn[21]:\\nfrom sklearn.datasets import fetch_lfw_people\\npeople = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\\nimage_shape = people.images[0].shape\\nfix, axes = plt.subplots(2, 5, figsize=(15, 8),\\n                         subplot_kw={'xticks': (), 'yticks': ()})\\nfor target, image, ax in zip(people.target, people.images, axes.ravel()):\\n    ax.imshow(image)\\n    ax.set_title(people.target_names[target])\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 147\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 161, 'page_label': '148', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-7. Some images from the Labeled Faces in the Wild dataset\\nThere are 3,023 images, each 87×65 pixels large, belonging to 62 different people:\\nIn[22]:\\nprint(\"people.images.shape: {}\".format(people.images.shape))\\nprint(\"Number of classes: {}\".format(len(people.target_names)))\\nOut[22]:\\npeople.images.shape: (3023, 87, 65)\\nNumber of classes: 62\\nThe dataset is a bit skewed, however, containing a lot of images of George W . Bush\\nand Colin Powell, as you can see here:\\nIn[23]:\\n# count how often each target appears\\ncounts = np.bincount(people.target)\\n# print counts next to target names\\nfor i, (count, name) in enumerate(zip(counts, people.target_names)):\\n    print(\"{0:25} {1:3}\".format(name, count), end=\\'   \\')\\n    if (i + 1) % 3 == 0:\\n        print()\\n148 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 162, 'page_label': '149', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Out[23]:\\nAlejandro Toledo           39   Alvaro Uribe               35\\nAmelie Mauresmo            21   Andre Agassi               36\\nAngelina Jolie             20   Arnold Schwarzenegger      42\\nAtal Bihari Vajpayee       24   Bill Clinton               29\\nCarlos Menem               21   Colin Powell              236\\nDavid Beckham              31   Donald Rumsfeld           121\\nGeorge W Bush             530   George Robertson           22\\nGerhard Schroeder         109   Gloria Macapagal Arroyo    44\\nGray Davis                 26   Guillermo Coria            30\\nHamid Karzai               22   Hans Blix                  39\\nHugo Chavez                71   Igor Ivanov                20\\n[...]                           [...]\\nLaura Bush                 41   Lindsay Davenport          22\\nLleyton Hewitt             41   Luiz Inacio Lula da Silva  48\\nMahmoud Abbas              29   Megawati Sukarnoputri      33\\nMichael Bloomberg          20   Naomi Watts                22'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 162, 'page_label': '149', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Lleyton Hewitt             41   Luiz Inacio Lula da Silva  48\\nMahmoud Abbas              29   Megawati Sukarnoputri      33\\nMichael Bloomberg          20   Naomi Watts                22\\nNestor Kirchner            37   Paul Bremer                20\\nPete Sampras               22   Recep Tayyip Erdogan       30\\nRicardo Lagos              27   Roh Moo-hyun               32\\nRudolph Giuliani           26   Saddam Hussein             23\\nSerena Williams            52   Silvio Berlusconi          33\\nTiger Woods                23   Tom Daschle                25\\nTom Ridge                  33   Tony Blair                144\\nVicente Fox                32   Vladimir Putin             49\\nWinona Ryder               24\\nTo make the data less skewed, we will only take up to 50 images of each person\\n(otherwise, the feature extraction would be overwhelmed by the likelihood of George\\nW . Bush):\\nIn[24]:\\nmask = np.zeros(people.target.shape, dtype=np.bool)\\nfor target in np.unique(people.target):'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 162, 'page_label': '149', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='(otherwise, the feature extraction would be overwhelmed by the likelihood of George\\nW . Bush):\\nIn[24]:\\nmask = np.zeros(people.target.shape, dtype=np.bool)\\nfor target in np.unique(people.target):\\n    mask[np.where(people.target == target)[0][:50]] = 1\\nX_people = people.data[mask]\\ny_people = people.target[mask]\\n# scale the grayscale values to be between 0 and 1\\n# instead of 0 and 255 for better numeric stability\\nX_people = X_people / 255.\\nA common task in face recognition is to ask if a previously unseen face belongs to a\\nknown person from a database. This has applications in photo collection, social\\nmedia, and security applications. One way to solve this problem would be to build a\\nclassifier where each person is a separate class. However, there are usually many dif‐\\nferent people in face databases, and very few images of the same person (i.e., very few\\ntraining examples per class). That makes it hard to train most classifiers. Additionally,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 162, 'page_label': '149', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='ferent people in face databases, and very few images of the same person (i.e., very few\\ntraining examples per class). That makes it hard to train most classifiers. Additionally,\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 149'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 163, 'page_label': '150', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='you often want to be able to add new people easily, without needing to retrain a large\\nmodel.\\nA simple solution is to use a one-nearest-neighbor classifier that looks for the most\\nsimilar face image to the face you are classifying. This classifier could in principle\\nwork with only a single training example per class. Let’s take a look at how well\\nKNeighborsClassifier does here:\\nIn[25]:\\nfrom sklearn.neighbors import KNeighborsClassifier\\n# split the data into training and test sets\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X_people, y_people, stratify=y_people, random_state=0)\\n# build a KNeighborsClassifier using one neighbor\\nknn = KNeighborsClassifier(n_neighbors=1)\\nknn.fit(X_train, y_train)\\nprint(\"Test set score of 1-nn: {:.2f}\".format(knn.score(X_test, y_test)))\\nOut[25]:\\nTest set score of 1-nn: 0.27\\nWe obtain an accuracy of 26.6%, which is not actually that bad for a 62-class classifi‐\\ncation problem (random guessing would give you around 1/62 = 1.5% accuracy), but'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 163, 'page_label': '150', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Test set score of 1-nn: 0.27\\nWe obtain an accuracy of 26.6%, which is not actually that bad for a 62-class classifi‐\\ncation problem (random guessing would give you around 1/62 = 1.5% accuracy), but\\nis also not great. We only correctly identify a person every fourth time.\\nThis is where PCA comes in. Computing distances in the original pixel space is quite\\na bad way to measure similarity between faces. When using a pixel representation to\\ncompare two images, we compare the grayscale value of each individual pixel to the\\nvalue of the pixel in the corresponding position in the other image. This representa‐\\ntion is quite different from how humans would interpret the image of a face, and it is\\nhard to capture the facial features using this raw representation. For example, using\\npixel distances means that shifting a face by one pixel to the right corresponds to a\\ndrastic change, with a completely different representation. We hope that using distan‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 163, 'page_label': '150', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='pixel distances means that shifting a face by one pixel to the right corresponds to a\\ndrastic change, with a completely different representation. We hope that using distan‐\\nces along principal components can improve our accuracy. Here, we enable the\\nwhitening option of PCA, which rescales the principal components to have the same\\nscale. This is the same as using StandardScaler after the transformation. Reusing the\\ndata from Figure 3-3 again, whitening corresponds to not only rotating the data, but\\nalso rescaling it so that the center panel is a circle instead of an ellipse (see\\nFigure 3-8):\\nIn[26]:\\nmglearn.plots.plot_pca_whitening()150 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 164, 'page_label': '151', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-8. Transformation of data with PCA using whitening\\nWe fit the PCA object to the training data and extract the first 100 principal compo‐\\nnents. Then we transform the training and test data:\\nIn[27]:\\npca = PCA(n_components=100, whiten=True, random_state=0).fit(X_train)\\nX_train_pca = pca.transform(X_train)\\nX_test_pca = pca.transform(X_test)\\nprint(\"X_train_pca.shape: {}\".format(X_train_pca.shape))\\nOut[27]:\\nX_train_pca.shape: (1537, 100)\\nThe new data has 100 features, the first 100 principal components. Now, we can use\\nthe new representation to classify our images using a one-nearest-neighbors classifier:\\nIn[28]:\\nknn = KNeighborsClassifier(n_neighbors=1)\\nknn.fit(X_train_pca, y_train)\\nprint(\"Test set accuracy: {:.2f}\".format(knn.score(X_test_pca, y_test)))\\nOut[28]:\\nTest set accuracy: 0.36\\nOur accuracy improved quite significantly, from 26.6% to 35.7%, confirming our\\nintuition that the principal components might provide a better representation of the\\ndata.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 164, 'page_label': '151', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Test set accuracy: 0.36\\nOur accuracy improved quite significantly, from 26.6% to 35.7%, confirming our\\nintuition that the principal components might provide a better representation of the\\ndata.\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 151'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 165, 'page_label': '152', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='For image data, we can also easily visualize the principal components that are found.\\nRemember that components correspond to directions in the input space. The input\\nspace here is 50×37-pixel grayscale images, so directions within this space are also\\n50×37-pixel grayscale images.\\nLet’s look at the first couple of principal components (Figure 3-9):\\nIn[29]:\\nprint(\"pca.components_.shape: {}\".format(pca.components_.shape))\\nOut[29]:\\npca.components_.shape: (100, 5655)\\nIn[30]:\\nfix, axes = plt.subplots(3, 5, figsize=(15, 12),\\n                         subplot_kw={\\'xticks\\': (), \\'yticks\\': ()})\\nfor i, (component, ax) in enumerate(zip(pca.components_, axes.ravel())):\\n    ax.imshow(component.reshape(image_shape),\\n              cmap=\\'viridis\\')\\n    ax.set_title(\"{}. component\".format((i + 1)))\\nWhile we certainly cannot understand all aspects of these components, we can guess\\nwhich aspects of the face images some of the components are capturing. The first'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 165, 'page_label': '152', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='While we certainly cannot understand all aspects of these components, we can guess\\nwhich aspects of the face images some of the components are capturing. The first\\ncomponent seems to mostly encode the contrast between the face and the back‐\\nground, the second component encodes differences in lighting between the right and\\nthe left half of the face, and so on. While this representation is slightly more semantic\\nthan the raw pixel values, it is still quite far from how a human might perceive a face.\\nAs the PCA model is based on pixels, the alignment of the face (the position of eyes,\\nchin, and nose) and the lighting both have a strong influence on how similar two\\nimages are in their pixel representation. But alignment and lighting are probably not\\nwhat a human would perceive first. When asking people to rate similarity of faces,\\nthey are more likely to use attributes like age, gender, facial expression, and hair style,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 165, 'page_label': '152', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='what a human would perceive first. When asking people to rate similarity of faces,\\nthey are more likely to use attributes like age, gender, facial expression, and hair style,\\nwhich are attributes that are hard to infer from the pixel intensities. It’s important to\\nkeep in mind that algorithms often interpret data (particularly visual data, such as\\nimages, which humans are very familiar with) quite differently from how a human\\nwould.\\n152 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 166, 'page_label': '153', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-9. Component vectors of the first 15 principal components of the faces dataset\\nLet’s come back to the specific case of PCA, though. We introduced the PCA transfor‐\\nmation as rotating the data and then dropping the components with low variance.\\nAnother useful interpretation is to try to find some numbers (the new feature values\\nafter the PCA rotation) so that we can express the test points as a weighted sum of the\\nprincipal components (see Figure 3-10).\\nFigure 3-10. Schematic view of PCA as decomposing an image into a weighted sum of\\ncomponents\\nHere, x0, x1, and so on are the coefficients of the principal components for this data\\npoint; in other words, they are the representation of the image in the rotated space.\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 153'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 167, 'page_label': '154', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Another way we can try to understand what a PCA model is doing is by looking at\\nthe reconstructions of the original data using only some components. In Figure 3-3,\\nafter dropping the second component and arriving at the third panel, we undid the\\nrotation and added the mean back to obtain new points in the original space with the\\nsecond component removed, as shown in the last panel. We can do a similar transfor‐\\nmation for the faces by reducing the data to only some principal components and\\nthen rotating back into the original space. This return to the original feature space\\ncan be done using the inverse_transform method. Here, we visualize the recon‐\\nstruction of some faces using 10, 50, 100, 500, or 2,000 components (Figure 3-11):\\nIn[32]:\\nmglearn.plots.plot_pca_faces(X_train, X_test, image_shape)\\nFigure 3-11. Reconstructing three face images using increasing numbers of principal\\ncomponents\\nY ou can see that when we use only the first 10 principal components, only the essence'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 167, 'page_label': '154', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-11. Reconstructing three face images using increasing numbers of principal\\ncomponents\\nY ou can see that when we use only the first 10 principal components, only the essence\\nof the picture, like the face orientation and lighting, is captured. By using more and\\nmore principal components, more and more details in the image are preserved. This\\n154 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 168, 'page_label': '155', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='corresponds to extending the sum in Figure 3-10 to include more and more terms.\\nUsing as many components as there are pixels would mean that we would not discard\\nany information after the rotation, and we would reconstruct the image perfectly.\\nWe can also try to use PCA to visualize all the faces in the dataset in a scatter plot\\nusing the first two principal components ( Figure 3-12), with classes given by who is\\nshown in the image, similarly to what we did for the cancer dataset:\\nIn[33]:\\nmglearn.discrete_scatter(X_train_pca[:, 0], X_train_pca[:, 1], y_train)\\nplt.xlabel(\"First principal component\")\\nplt.ylabel(\"Second principal component\")\\nFigure 3-12. Scatter plot of the faces dataset using the first two principal components (see\\nFigure 3-5 for the corresponding image for the cancer dataset)\\nAs you can see, when we use only the first two principal components the whole data\\nis just a big blob, with no separation of classes visible. This is not very surprising,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 168, 'page_label': '155', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='As you can see, when we use only the first two principal components the whole data\\nis just a big blob, with no separation of classes visible. This is not very surprising,\\ngiven that even with 10 components, as shown earlier in Figure 3-11, PCA only cap‐\\ntures very rough characteristics of the faces.\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 155'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 169, 'page_label': '156', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Non-Negative Matrix Factorization (NMF)\\nNon-negative matrix factorization is another unsupervised learning algorithm that\\naims to extract useful features. It works similarly to PCA and can also be used for\\ndimensionality reduction. As in PCA, we are trying to write each data point as a\\nweighted sum of some components, as illustrated in Figure 3-10. But whereas in PCA\\nwe wanted components that were orthogonal and that explained as much variance of\\nthe data as possible, in NMF , we want the components and the coefficients to be non-\\nnegative; that is, we want both the components and the coefficients to be greater than\\nor equal to zero. Consequently, this method can only be applied to data where each\\nfeature is non-negative, as a non-negative sum of non-negative components cannot\\nbecome negative.\\nThe process of decomposing data into a non-negative weighted sum is particularly\\nhelpful for data that is created as the addition (or overlay) of several independent'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 169, 'page_label': '156', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='become negative.\\nThe process of decomposing data into a non-negative weighted sum is particularly\\nhelpful for data that is created as the addition (or overlay) of several independent\\nsources, such as an audio track of multiple people speaking, or music with many\\ninstruments. In these situations, NMF can identify the original components that\\nmake up the combined data. Overall, NMF leads to more interpretable components\\nthan PCA, as negative components and coefficients can lead to hard-to-interpret can‐\\ncellation effects. The eigenfaces in Figure 3-9, for example, contain both positive and\\nnegative parts, and as we mentioned in the description of PCA, the sign is actually\\narbitrary. Before we apply NMF to the face dataset, let’s briefly revisit the synthetic\\ndata.\\nApplying NMF to synthetic data\\nIn contrast to when using PCA, we need to ensure that our data is positive for NMF\\nto be able to operate on the data. This means where the data lies relative to the origin'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 169, 'page_label': '156', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In contrast to when using PCA, we need to ensure that our data is positive for NMF\\nto be able to operate on the data. This means where the data lies relative to the origin\\n(0, 0) actually matters for NMF . Therefore, you can think of the non-negative compo‐\\nnents that are extracted as directions from (0, 0) toward the data.\\nThe following example ( Figure 3-13 ) shows the results of NMF on the two-\\ndimensional toy data:\\nIn[34]:\\nmglearn.plots.plot_nmf_illustration()\\n156 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 170, 'page_label': '157', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-13. Components found by non-negative matrix factorization with two compo‐\\nnents (left) and one component (right)\\nFor NMF with two components, as shown on the left, it is clear that all points in the\\ndata can be written as a positive combination of the two components. If there are\\nenough components to perfectly reconstruct the data (as many components as there\\nare features), the algorithm will choose directions that point toward the extremes of\\nthe data.\\nIf we only use a single component, NMF creates a component that points toward the\\nmean, as pointing there best explains the data. Y ou can see that in contrast with PCA,\\nreducing the number of components not only removes some directions, but creates\\nan entirely different set of components! Components in NMF are also not ordered in\\nany specific way, so there is no “first non-negative component”: all components play\\nan equal part.\\nNMF uses a random initialization, which might lead to different results depending on'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 170, 'page_label': '157', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='any specific way, so there is no “first non-negative component”: all components play\\nan equal part.\\nNMF uses a random initialization, which might lead to different results depending on\\nthe random seed. In relatively simple cases such as the synthetic data with two com‐\\nponents, where all the data can be explained perfectly, the randomness has little effect\\n(though it might change the order or scale of the components). In more complex sit‐\\nuations, there might be more drastic changes.\\nApplying NMF to face images\\nNow, let’s apply NMF to the Labeled Faces in the Wild dataset we used earlier. The\\nmain parameter of NMF is how many components we want to extract. Usually this is\\nlower than the number of input features (otherwise, the data could be explained by\\nmaking each pixel a separate component).\\nFirst, let’s inspect how the number of components impacts how well the data can be\\nreconstructed using NMF (Figure 3-14):'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 170, 'page_label': '157', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='making each pixel a separate component).\\nFirst, let’s inspect how the number of components impacts how well the data can be\\nreconstructed using NMF (Figure 3-14):\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 157'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 171, 'page_label': '158', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[35]:\\nmglearn.plots.plot_nmf_faces(X_train, X_test, image_shape)\\nFigure 3-14. Reconstructing three face images using increasing numbers of components\\nfound by NMF\\nThe quality of the back-transformed data is similar to when using PCA, but slightly\\nworse. This is expected, as PCA finds the optimum directions in terms of reconstruc‐\\ntion. NMF is usually not used for its ability to reconstruct or encode data, but rather\\nfor finding interesting patterns within the data.\\nAs a first look into the data, let’s try extracting only a few components (say, 15).\\nFigure 3-15 shows the result:\\n158 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 172, 'page_label': '159', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[36]:\\nfrom sklearn.decomposition import NMF\\nnmf = NMF(n_components=15, random_state=0)\\nnmf.fit(X_train)\\nX_train_nmf = nmf.transform(X_train)\\nX_test_nmf = nmf.transform(X_test)\\nfix, axes = plt.subplots(3, 5, figsize=(15, 12),\\n                         subplot_kw={\\'xticks\\': (), \\'yticks\\': ()})\\nfor i, (component, ax) in enumerate(zip(nmf.components_, axes.ravel())):\\n    ax.imshow(component.reshape(image_shape))\\n    ax.set_title(\"{}. component\".format(i))\\nFigure 3-15. The components found by NMF on the faces dataset when using 15 compo‐\\nnents\\nThese components are all positive, and so resemble prototypes of faces much more so\\nthan the components shown for PCA in Figure 3-9. For example, one can clearly see\\nthat component 3 shows a face rotated somewhat to the right, while component 7\\nshows a face somewhat rotated to the left. Let’s look at the images for which these\\ncomponents are particularly strong, shown in Figures 3-16 and 3-17:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 172, 'page_label': '159', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='shows a face somewhat rotated to the left. Let’s look at the images for which these\\ncomponents are particularly strong, shown in Figures 3-16 and 3-17:\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 159'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 173, 'page_label': '160', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"In[37]:\\ncompn = 3\\n# sort by 3rd component, plot first 10 images\\ninds = np.argsort(X_train_nmf[:, compn])[::-1]\\nfig, axes = plt.subplots(2, 5, figsize=(15, 8),\\n                         subplot_kw={'xticks': (), 'yticks': ()})\\nfor i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\\n    ax.imshow(X_train[ind].reshape(image_shape))\\ncompn = 7\\n# sort by 7th component, plot first 10 images\\ninds = np.argsort(X_train_nmf[:, compn])[::-1]\\nfig, axes = plt.subplots(2, 5, figsize=(15, 8),\\n                         subplot_kw={'xticks': (), 'yticks': ()})\\nfor i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\\n    ax.imshow(X_train[ind].reshape(image_shape))\\nFigure 3-16. Faces that have a large coefficient for component 3\\n160 | Chapter 3: Unsupervised Learning and Preprocessing\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 174, 'page_label': '161', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-17. Faces that have a large coefficient for component 7\\nAs expected, faces that have a high coefficient for component 3 are faces looking to\\nthe right (Figure 3-16), while faces with a high coefficient for component 7 are look‐\\ning to the left (Figure 3-17). As mentioned earlier, extracting patterns like these works\\nbest for data with additive structure, including audio, gene expression, and text data.\\nLet’s walk through one example on synthetic data to see what this might look like.\\nLet’s say we are interested in a signal that is a combination of three different sources\\n(Figure 3-18):\\nIn[38]:\\nS = mglearn.datasets.make_signals()\\nplt.figure(figsize=(6, 1))\\nplt.plot(S, \\'-\\')\\nplt.xlabel(\"Time\")\\nplt.ylabel(\"Signal\")\\nFigure 3-18. Original signal sources\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 161'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 175, 'page_label': '162', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Unfortunately we cannot observe the original signals, but only an additive mixture of\\nall three of them. We want to recover the decomposition of the mixed signal into the\\noriginal components. We assume that we have many different ways to observe the\\nmixture (say 100 measurement devices), each of which provides us with a series of\\nmeasurements:\\nIn[39]:\\n# mix data into a 100-dimensional state\\nA = np.random.RandomState(0).uniform(size=(100, 3))\\nX = np.dot(S, A.T)\\nprint(\"Shape of measurements: {}\".format(X.shape))\\nOut[39]:\\nShape of measurements: (2000, 100)\\nWe can use NMF to recover the three signals:\\nIn[40]:\\nnmf = NMF(n_components=3, random_state=42)\\nS_ = nmf.fit_transform(X)\\nprint(\"Recovered signal shape: {}\".format(S_.shape))\\nOut[40]:\\nRecovered signal shape: (2000, 3)\\nFor comparison, we also apply PCA:\\nIn[41]:\\npca = PCA(n_components=3)\\nH = pca.fit_transform(X)\\nFigure 3-19 shows the signal activity that was discovered by NMF and PCA:\\nIn[42]:\\nmodels = [X, S, S_, H]'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 175, 'page_label': '162', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"For comparison, we also apply PCA:\\nIn[41]:\\npca = PCA(n_components=3)\\nH = pca.fit_transform(X)\\nFigure 3-19 shows the signal activity that was discovered by NMF and PCA:\\nIn[42]:\\nmodels = [X, S, S_, H]\\nnames = ['Observations (first three measurements)',\\n         'True sources',\\n         'NMF recovered signals',\\n         'PCA recovered signals']\\nfig, axes = plt.subplots(4, figsize=(8, 4), gridspec_kw={'hspace': .5},\\n                         subplot_kw={'xticks': (), 'yticks': ()})\\nfor model, name, ax in zip(models, names, axes):\\n    ax.set_title(name)\\n    ax.plot(model[:, :3], '-')\\n162 | Chapter 3: Unsupervised Learning and Preprocessing\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 176, 'page_label': '163', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-19. Recovering mixed sources using NMF and PCA\\nThe figure includes 3 of the 100 measurements from X for reference. As you can see,\\nNMF did a reasonable job of discovering the original sources, while PCA failed and\\nused the first component to explain the majority of the variation in the data. Keep in\\nmind that the components produced by NMF have no natural ordering. In this exam‐\\nple, the ordering of the NMF components is the same as in the original signal (see the\\nshading of the three curves), but this is purely accidental.\\nThere are many other algorithms that can be used to decompose each data point into\\na weighted sum of a fixed set of components, as PCA and NMF do. Discussing all of\\nthem is beyond the scope of this book, and describing the constraints made on the\\ncomponents and coefficients often involves probability theory. If you are interested in\\nthis kind of pattern extraction, we recommend that you study the sections of the sci'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 176, 'page_label': '163', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='components and coefficients often involves probability theory. If you are interested in\\nthis kind of pattern extraction, we recommend that you study the sections of the sci\\nkit_learn user guide on independent component analysis (ICA), factor analysis\\n(FA), and sparse coding (dictionary learning), all of which you can find on the page\\nabout decomposition methods.\\nManifold Learning with t-SNE\\nWhile PCA is often a good first approach for transforming your data so that you\\nmight be able to visualize it using a scatter plot, the nature of the method (applying a\\nrotation and then dropping directions) limits its usefulness, as we saw with the scatter\\nplot of the Labeled Faces in the Wild dataset. There is a class of algorithms for visuali‐\\nzation called manifold learning algorithms that allow for much more complex map‐\\npings, and often provide better visualizations. A particularly useful one is the t-SNE\\nalgorithm.\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 163'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 177, 'page_label': '164', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='2 Not to be confused with the much larger MNIST dataset.\\nManifold learning algorithms are mainly aimed at visualization, and so are rarely\\nused to generate more than two new features. Some of them, including t-SNE, com‐\\npute a new representation of the training data, but don’t allow transformations of new\\ndata. This means these algorithms cannot be applied to a test set: rather, they can only\\ntransform the data they were trained for. Manifold learning can be useful for explora‐\\ntory data analysis, but is rarely used if the final goal is supervised learning. The idea\\nbehind t-SNE is to find a two-dimensional representation of the data that preserves\\nthe distances between points as best as possible. t-SNE starts with a random two-\\ndimensional representation for each data point, and then tries to make points that are\\nclose in the original feature space closer, and points that are far apart in the original\\nfeature space farther apart. t-SNE puts more emphasis on points that are close by,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 177, 'page_label': '164', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"close in the original feature space closer, and points that are far apart in the original\\nfeature space farther apart. t-SNE puts more emphasis on points that are close by,\\nrather than preserving distances between far-apart points. In other words, it tries to\\npreserve the information indicating which points are neighbors to each other.\\nWe will apply the t-SNE manifold learning algorithm on a dataset of handwritten dig‐\\nits that is included in scikit-learn.2 Each data point in this dataset is an 8×8 gray‐\\nscale image of a handwritten digit between 0 and 1. Figure 3-20 shows an example\\nimage for each class:\\nIn[43]:\\nfrom sklearn.datasets import load_digits\\ndigits = load_digits()\\nfig, axes = plt.subplots(2, 5, figsize=(10, 5),\\n                         subplot_kw={'xticks':(), 'yticks': ()})\\nfor ax, img in zip(axes.ravel(), digits.images):\\n    ax.imshow(img)\\n164 | Chapter 3: Unsupervised Learning and Preprocessing\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 178, 'page_label': '165', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-20. Example images from the digits dataset\\nLet’s use PCA to visualize the data reduced to two dimensions. We plot the first two\\nprincipal components, and color each dot by its class (see Figure 3-21):\\nIn[44]:\\n# build a PCA model\\npca = PCA(n_components=2)\\npca.fit(digits.data)\\n# transform the digits data onto the first two principal components\\ndigits_pca = pca.transform(digits.data)\\ncolors = [\"#476A2A\", \"#7851B8\", \"#BD3430\", \"#4A2D4E\", \"#875525\",\\n          \"#A83683\", \"#4E655E\", \"#853541\", \"#3A3120\", \"#535D8E\"]\\nplt.figure(figsize=(10, 10))\\nplt.xlim(digits_pca[:, 0].min(), digits_pca[:, 0].max())\\nplt.ylim(digits_pca[:, 1].min(), digits_pca[:, 1].max())\\nfor i in range(len(digits.data)):\\n    # actually plot the digits as text instead of using scatter\\n    plt.text(digits_pca[i, 0], digits_pca[i, 1], str(digits.target[i]),\\n             color = colors[digits.target[i]],\\n             fontdict={\\'weight\\': \\'bold\\', \\'size\\': 9})\\nplt.xlabel(\"First principal component\")'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 178, 'page_label': '165', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='color = colors[digits.target[i]],\\n             fontdict={\\'weight\\': \\'bold\\', \\'size\\': 9})\\nplt.xlabel(\"First principal component\")\\nplt.ylabel(\"Second principal component\")\\nHere, we actually used the true digit classes as glyphs, to show which class is where.\\nThe digits zero, six, and four are relatively well separated using the first two principal\\ncomponents, though they still overlap. Most of the other digits overlap significantly.\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 165'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 179, 'page_label': '166', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-21. Scatter plot of the digits dataset using the first two principal components\\nLet’s apply t-SNE to the same dataset, and compare the results. As t-SNE does not\\nsupport transforming new data, the TSNE class has no transform method. Instead, we\\ncan call the fit_transform method, which will build the model and immediately\\nreturn the transformed data (see Figure 3-22):\\nIn[45]:\\nfrom sklearn.manifold import TSNE\\ntsne = TSNE(random_state=42)\\n# use fit_transform instead of fit, as TSNE has no transform method\\ndigits_tsne = tsne.fit_transform(digits.data)\\n166 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 180, 'page_label': '167', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[46]:\\nplt.figure(figsize=(10, 10))\\nplt.xlim(digits_tsne[:, 0].min(), digits_tsne[:, 0].max() + 1)\\nplt.ylim(digits_tsne[:, 1].min(), digits_tsne[:, 1].max() + 1)\\nfor i in range(len(digits.data)):\\n    # actually plot the digits as text instead of using scatter\\n    plt.text(digits_tsne[i, 0], digits_tsne[i, 1], str(digits.target[i]),\\n             color = colors[digits.target[i]],\\n             fontdict={\\'weight\\': \\'bold\\', \\'size\\': 9})\\nplt.xlabel(\"t-SNE feature 0\")\\nplt.xlabel(\"t-SNE feature 1\")\\nFigure 3-22. Scatter plot of the digits dataset using two components found by t-SNE\\nDimensionality Reduction, Feature Extraction, and Manifold Learning | 167'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 181, 'page_label': '168', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='The result of t-SNE is quite remarkable. All the classes are quite clearly separated.\\nThe ones and nines are somewhat split up, but most of the classes form a single dense\\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\\npletely unsupervised. Still, it can find a representation of the data in two dimensions\\nthat clearly separates the classes, based solely on how close points are in the original\\nspace.\\nThe t-SNE algorithm has some tuning parameters, though it often works well with\\nthe default settings. Y ou can try playing with perplexity and early_exaggeration,\\nbut the effects are usually minor.\\nClustering\\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\\ncalled clusters. The goal is to split up the data in such a way that points within a single\\ncluster are very similar and points in different clusters are different. Similarly to clas‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 181, 'page_label': '168', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='called clusters. The goal is to split up the data in such a way that points within a single\\ncluster are very similar and points in different clusters are different. Similarly to clas‐\\nsification algorithms, clustering algorithms assign (or predict) a number to each data\\npoint, indicating which cluster a particular point belongs to.\\nk-Means Clustering\\nk-means clustering is one of the simplest and most commonly used clustering algo‐\\nrithms. It tries to find cluster centers that are representative of certain regions of the\\ndata. The algorithm alternates between two steps: assigning each data point to the\\nclosest cluster center, and then setting each cluster center as the mean of the data\\npoints that are assigned to it. The algorithm is finished when the assignment of\\ninstances to clusters no longer changes. The following example ( Figure 3-23 ) illus‐\\ntrates the algorithm on a synthetic dataset:\\nIn[47]:\\nmglearn.plots.plot_kmeans_algorithm()'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 181, 'page_label': '168', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='instances to clusters no longer changes. The following example ( Figure 3-23 ) illus‐\\ntrates the algorithm on a synthetic dataset:\\nIn[47]:\\nmglearn.plots.plot_kmeans_algorithm()\\n168 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 182, 'page_label': '169', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-23. Input data and three steps of the k-means algorithm\\nCluster centers are shown as triangles, while data points are shown as circles. Colors\\nindicate cluster membership. We specified that we are looking for three clusters, so\\nthe algorithm was initialized by declaring three data points randomly as cluster cen‐\\nters (see “Initialization”). Then the iterative algorithm starts. First, each data point is\\nassigned to the cluster center it is closest to (see “ Assign Points (1)”). Next, the cluster\\ncenters are updated to be the mean of the assigned points (see “Recompute Centers\\n(1)”). Then the process is repeated two more times. After the third iteration, the\\nassignment of points to cluster centers remained unchanged, so the algorithm stops.\\nGiven new data points, k-means will assign each to the closest cluster center. The next\\nexample (Figure 3-24) shows the boundaries of the cluster centers that were learned\\nin Figure 3-23:\\nIn[48]:\\nmglearn.plots.plot_kmeans_boundaries()'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 182, 'page_label': '169', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='example (Figure 3-24) shows the boundaries of the cluster centers that were learned\\nin Figure 3-23:\\nIn[48]:\\nmglearn.plots.plot_kmeans_boundaries()\\nClustering | 169'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 183, 'page_label': '170', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='3 If you don’t provide n_clusters, it is set to 8 by default. There is no particular reason why you should use this\\nvalue.\\nFigure 3-24. Cluster centers and cluster boundaries found by the k-means algorithm\\nApplying k-means with scikit-learn is quite straightforward. Here, we apply it to\\nthe synthetic data that we used for the preceding plots. We instantiate the KMeans\\nclass, and set the number of clusters we are looking for. 3 Then we call the fit method\\nwith the data:\\nIn[49]:\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.cluster import KMeans\\n# generate synthetic two-dimensional data\\nX, y = make_blobs(random_state=1)\\n# build the clustering model\\nkmeans = KMeans(n_clusters=3)\\nkmeans.fit(X)\\nDuring the algorithm, each training data point in X is assigned a cluster label. Y ou can\\nfind these labels in the kmeans.labels_ attribute:\\n170 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 184, 'page_label': '171', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[50]:\\nprint(\"Cluster memberships:\\\\n{}\".format(kmeans.labels_))\\nOut[50]:\\nCluster memberships:\\n[1 2 2 2 0 0 0 2 1 1 2 2 0 1 0 0 0 1 2 2 0 2 0 1 2 0 0 1 1 0 1 1 0 1 2 0 2\\n 2 2 0 0 2 1 2 2 0 1 1 1 1 2 0 0 0 1 0 2 2 1 1 2 0 0 2 2 0 1 0 1 2 2 2 0 1\\n 1 2 0 0 1 2 1 2 2 0 1 1 1 1 2 1 0 1 1 2 2 0 0 1 0 1]\\nAs we asked for three clusters, the clusters are numbered 0 to 2.\\nY ou can also assign cluster labels to new points, using the predict method. Each new\\npoint is assigned to the closest cluster center when predicting, but the existing model\\nis not changed. Running predict on the training set returns the same result as\\nlabels_:\\nIn[51]:\\nprint(kmeans.predict(X))\\nOut[51]:\\n[1 2 2 2 0 0 0 2 1 1 2 2 0 1 0 0 0 1 2 2 0 2 0 1 2 0 0 1 1 0 1 1 0 1 2 0 2\\n 2 2 0 0 2 1 2 2 0 1 1 1 1 2 0 0 0 1 0 2 2 1 1 2 0 0 2 2 0 1 0 1 2 2 2 0 1\\n 1 2 0 0 1 2 1 2 2 0 1 1 1 1 2 1 0 1 1 2 2 0 0 1 0 1]\\nY ou can see that clustering is somewhat similar to classification, in that each item gets'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 184, 'page_label': '171', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='1 2 0 0 1 2 1 2 2 0 1 1 1 1 2 1 0 1 1 2 2 0 0 1 0 1]\\nY ou can see that clustering is somewhat similar to classification, in that each item gets\\na label. However, there is no ground truth, and consequently the labels themselves\\nhave no a priori meaning. Let’s go back to the example of clustering face images that\\nwe discussed before. It might be that the cluster 3 found by the algorithm contains\\nonly faces of your friend Bela. Y ou can only know that after you look at the pictures,\\nthough, and the number 3 is arbitrary. The only information the algorithm gives you\\nis that all faces labeled as 3 are similar.\\nFor the clustering we just computed on the two-dimensional toy dataset, that means\\nthat we should not assign any significance to the fact that one group was labeled 0\\nand another one was labeled 1. Running the algorithm again might result in a differ‐\\nent numbering of clusters because of the random nature of the initialization.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 184, 'page_label': '171', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"and another one was labeled 1. Running the algorithm again might result in a differ‐\\nent numbering of clusters because of the random nature of the initialization.\\nHere is a plot of this data again ( Figure 3-25). The cluster centers are stored in the\\ncluster_centers_ attribute, and we plot them as triangles:\\nIn[52]:\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], kmeans.labels_, markers='o')\\nmglearn.discrete_scatter(\\n    kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2],\\n    markers='^', markeredgewidth=2)\\nClustering | 171\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 185, 'page_label': '172', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-25. Cluster assignments and cluster centers found by k-means with three\\nclusters\\nWe can also use more or fewer cluster centers (Figure 3-26):\\nIn[53]:\\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\\n# using two cluster centers:\\nkmeans = KMeans(n_clusters=2)\\nkmeans.fit(X)\\nassignments = kmeans.labels_\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[0])\\n# using five cluster centers:\\nkmeans = KMeans(n_clusters=5)\\nkmeans.fit(X)\\nassignments = kmeans.labels_\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[1])\\n172 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 186, 'page_label': '173', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-26. Cluster assignments found by k-means using two clusters (left) and five\\nclusters (right)\\nFailure cases of k-means\\nEven if you know the “right” number of clusters for a given dataset, k-means might\\nnot always be able to recover them. Each cluster is defined solely by its center, which\\nmeans that each cluster is a convex shape. As a result of this, k-means can only cap‐\\nture relatively simple shapes. k-means also assumes that all clusters have the same\\n“diameter” in some sense; it always draws the boundary between clusters to be exactly\\nin the middle between the cluster centers. That can sometimes lead to surprising\\nresults, as shown in Figure 3-27:\\nIn[54]:\\nX_varied, y_varied = make_blobs(n_samples=200,\\n                                cluster_std=[1.0, 2.5, 0.5],\\n                                random_state=170)\\ny_pred = KMeans(n_clusters=3, random_state=0).fit_predict(X_varied)\\nmglearn.discrete_scatter(X_varied[:, 0], X_varied[:, 1], y_pred)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 186, 'page_label': '173', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='random_state=170)\\ny_pred = KMeans(n_clusters=3, random_state=0).fit_predict(X_varied)\\nmglearn.discrete_scatter(X_varied[:, 0], X_varied[:, 1], y_pred)\\nplt.legend([\"cluster 0\", \"cluster 1\", \"cluster 2\"], loc=\\'best\\')\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nClustering | 173'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 187, 'page_label': '174', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-27. Cluster assignments found by k-means when clusters have different\\ndensities\\nOne might have expected the dense region in the lower left to be the first cluster, the\\ndense region in the upper right to be the second, and the less dense region in the cen‐\\nter to be the third. Instead, both cluster 0 and cluster 1 have some points that are far\\naway from all the other points in these clusters that “reach” toward the center.\\nk-means also assumes that all directions are equally important for each cluster. The\\nfollowing plot ( Figure 3-28) shows a two-dimensional dataset where there are three\\nclearly separated parts in the data. However, these groups are stretched toward the\\ndiagonal. As k-means only considers the distance to the nearest cluster center, it can’t\\nhandle this kind of data:\\nIn[55]:\\n# generate some random cluster data\\nX, y = make_blobs(random_state=170, n_samples=600)\\nrng = np.random.RandomState(74)\\n# transform the data to be stretched'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 187, 'page_label': '174', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='handle this kind of data:\\nIn[55]:\\n# generate some random cluster data\\nX, y = make_blobs(random_state=170, n_samples=600)\\nrng = np.random.RandomState(74)\\n# transform the data to be stretched\\ntransformation = rng.normal(size=(2, 2))\\nX = np.dot(X, transformation)\\n174 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 188, 'page_label': '175', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='# cluster the data into three clusters\\nkmeans = KMeans(n_clusters=3)\\nkmeans.fit(X)\\ny_pred = kmeans.predict(X)\\n# plot the cluster assignments and cluster centers\\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm3)\\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\\n            marker=\\'^\\', c=[0, 1, 2], s=100, linewidth=2, cmap=mglearn.cm3)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nFigure 3-28. k-means fails to identify nonspherical clusters\\nk-means also performs poorly if the clusters have more complex shapes, like the\\ntwo_moons data we encountered in Chapter 2 (see Figure 3-29):\\nIn[56]:\\n# generate synthetic two_moons data (with less noise this time)\\nfrom sklearn.datasets import make_moons\\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\\n# cluster the data into two clusters\\nkmeans = KMeans(n_clusters=2)\\nkmeans.fit(X)\\ny_pred = kmeans.predict(X)\\nClustering | 175'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 189, 'page_label': '176', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='# plot the cluster assignments and cluster centers\\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm2, s=60)\\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\\n            marker=\\'^\\', c=[mglearn.cm2(0), mglearn.cm2(1)], s=100, linewidth=2)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nFigure 3-29. k-means fails to identify clusters with complex shapes\\nHere, we would hope that the clustering algorithm can discover the two half-moon\\nshapes. However, this is not possible using the k-means algorithm.\\nVector quantization, or seeing k-means as decomposition\\nEven though k-means is a clustering algorithm, there are interesting parallels between\\nk-means and the decomposition methods like PCA and NMF that we discussed ear‐\\nlier. Y ou might remember that PCA tries to find directions of maximum variance in\\nthe data, while NMF tries to find additive components, which often correspond to'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 189, 'page_label': '176', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='lier. Y ou might remember that PCA tries to find directions of maximum variance in\\nthe data, while NMF tries to find additive components, which often correspond to\\n“extremes” or “parts” of the data (see Figure 3-13). Both methods tried to express the\\ndata points as a sum over some components. k-means, on the other hand, tries to rep‐\\nresent each data point using a cluster center. Y ou can think of that as each point being\\nrepresented using only a single component, which is given by the cluster center. This\\nview of k-means as a decomposition method, where each point is represented using a\\nsingle component, is called vector quantization.\\n176 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 190, 'page_label': '177', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Let’s do a side-by-side comparison of PCA, NMF , and k-means, showing the compo‐\\nnents extracted ( Figure 3-30 ), as well as reconstructions of faces from the test set\\nusing 100 components ( Figure 3-31). For k-means, the reconstruction is the closest\\ncluster center found on the training set:\\nIn[57]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X_people, y_people, stratify=y_people, random_state=0)\\nnmf = NMF(n_components=100, random_state=0)\\nnmf.fit(X_train)\\npca = PCA(n_components=100, random_state=0)\\npca.fit(X_train)\\nkmeans = KMeans(n_clusters=100, random_state=0)\\nkmeans.fit(X_train)\\nX_reconstructed_pca = pca.inverse_transform(pca.transform(X_test))\\nX_reconstructed_kmeans = kmeans.cluster_centers_[kmeans.predict(X_test)]\\nX_reconstructed_nmf = np.dot(nmf.transform(X_test), nmf.components_)\\nIn[58]:\\nfig, axes = plt.subplots(3, 5, figsize=(8, 8),\\n                         subplot_kw={\\'xticks\\': (), \\'yticks\\': ()})\\nfig.suptitle(\"Extracted Components\")'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 190, 'page_label': '177', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[58]:\\nfig, axes = plt.subplots(3, 5, figsize=(8, 8),\\n                         subplot_kw={\\'xticks\\': (), \\'yticks\\': ()})\\nfig.suptitle(\"Extracted Components\")\\nfor ax, comp_kmeans, comp_pca, comp_nmf in zip(\\n        axes.T, kmeans.cluster_centers_, pca.components_, nmf.components_):\\n    ax[0].imshow(comp_kmeans.reshape(image_shape))\\n    ax[1].imshow(comp_pca.reshape(image_shape), cmap=\\'viridis\\')\\n    ax[2].imshow(comp_nmf.reshape(image_shape))\\naxes[0, 0].set_ylabel(\"kmeans\")\\naxes[1, 0].set_ylabel(\"pca\")\\naxes[2, 0].set_ylabel(\"nmf\")\\nfig, axes = plt.subplots(4, 5, subplot_kw={\\'xticks\\': (), \\'yticks\\': ()},\\n                         figsize=(8, 8))\\nfig.suptitle(\"Reconstructions\")\\nfor ax, orig, rec_kmeans, rec_pca, rec_nmf in zip(\\n        axes.T, X_test, X_reconstructed_kmeans, X_reconstructed_pca,\\n        X_reconstructed_nmf):\\n    ax[0].imshow(orig.reshape(image_shape))\\n    ax[1].imshow(rec_kmeans.reshape(image_shape))\\n    ax[2].imshow(rec_pca.reshape(image_shape))'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 190, 'page_label': '177', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='X_reconstructed_nmf):\\n    ax[0].imshow(orig.reshape(image_shape))\\n    ax[1].imshow(rec_kmeans.reshape(image_shape))\\n    ax[2].imshow(rec_pca.reshape(image_shape))\\n    ax[3].imshow(rec_nmf.reshape(image_shape))\\naxes[0, 0].set_ylabel(\"original\")\\naxes[1, 0].set_ylabel(\"kmeans\")\\naxes[2, 0].set_ylabel(\"pca\")\\naxes[3, 0].set_ylabel(\"nmf\")\\nClustering | 177'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 191, 'page_label': '178', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-30. Comparing k-means cluster centers to components found by PCA and NMF\\n178 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 192, 'page_label': '179', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-31. Comparing image reconstructions using k-means, PCA, and NMF with 100\\ncomponents (or cluster centers)—k-means uses only a single cluster center per image\\nAn interesting aspect of vector quantization using k-means is that we can use many\\nmore clusters than input dimensions to encode our data. Let’s go back to the\\ntwo_moons data. Using PCA or NMF , there is nothing much we can do to this data, as\\nit lives in only two dimensions. Reducing it to one dimension with PCA or NMF\\nwould completely destroy the structure of the data. But we can find a more expressive\\nrepresentation with k-means, by using more cluster centers (see Figure 3-32):\\nClustering | 179'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 193, 'page_label': '180', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[59]:\\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\\nkmeans = KMeans(n_clusters=10, random_state=0)\\nkmeans.fit(X)\\ny_pred = kmeans.predict(X)\\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, s=60, cmap=\\'Paired\\')\\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=60,\\n            marker=\\'^\\', c=range(kmeans.n_clusters), linewidth=2, cmap=\\'Paired\\')\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nprint(\"Cluster memberships:\\\\n{}\".format(y_pred))\\nOut[59]:\\nCluster memberships:\\n[9 2 5 4 2 7 9 6 9 6 1 0 2 6 1 9 3 0 3 1 7 6 8 6 8 5 2 7 5 8 9 8 6 5 3 7 0\\n 9 4 5 0 1 3 5 2 8 9 1 5 6 1 0 7 4 6 3 3 6 3 8 0 4 2 9 6 4 8 2 8 4 0 4 0 5\\n 6 4 5 9 3 0 7 8 0 7 5 8 9 8 0 7 3 9 7 1 7 2 2 0 4 5 6 7 8 9 4 5 4 1 2 3 1\\n 8 8 4 9 2 3 7 0 9 9 1 5 8 5 1 9 5 6 7 9 1 4 0 6 2 6 4 7 9 5 5 3 8 1 9 5 6\\n 3 5 0 2 9 3 0 8 6 0 3 3 5 6 3 2 0 2 3 0 2 6 3 4 4 1 5 6 7 1 1 3 2 4 7 2 7\\n 3 8 6 4 1 4 3 9 9 5 1 7 5 8 2]\\nFigure 3-32. Using many k-means clusters to cover the variation in a complex dataset'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 193, 'page_label': '180', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='3 5 0 2 9 3 0 8 6 0 3 3 5 6 3 2 0 2 3 0 2 6 3 4 4 1 5 6 7 1 1 3 2 4 7 2 7\\n 3 8 6 4 1 4 3 9 9 5 1 7 5 8 2]\\nFigure 3-32. Using many k-means clusters to cover the variation in a complex dataset\\n180 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 194, 'page_label': '181', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='4 In this case, “best” means that the sum of variances of the clusters is small.\\nWe used 10 cluster centers, which means each point is now assigned a number\\nbetween 0 and 9. We can see this as the data being represented using 10 components\\n(that is, we have 10 new features), with all features being 0, apart from the one that\\nrepresents the cluster center the point is assigned to. Using this 10-dimensional repre‐\\nsentation, it would now be possible to separate the two half-moon shapes using a lin‐\\near model, which would not have been possible using the original two features. It is\\nalso possible to get an even more expressive representation of the data by using the\\ndistances to each of the cluster centers as features. This can be accomplished using\\nthe transform method of kmeans:\\nIn[60]:\\ndistance_features = kmeans.transform(X)\\nprint(\"Distance feature shape: {}\".format(distance_features.shape))\\nprint(\"Distance features:\\\\n{}\".format(distance_features))\\nOut[60]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 194, 'page_label': '181', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[60]:\\ndistance_features = kmeans.transform(X)\\nprint(\"Distance feature shape: {}\".format(distance_features.shape))\\nprint(\"Distance features:\\\\n{}\".format(distance_features))\\nOut[60]:\\nDistance feature shape: (200, 10)\\nDistance features:\\n[[ 0.922  1.466  1.14  ...,  1.166  1.039  0.233]\\n [ 1.142  2.517  0.12  ...,  0.707  2.204  0.983]\\n [ 0.788  0.774  1.749 ...,  1.971  0.716  0.944]\\n ...,\\n [ 0.446  1.106  1.49  ...,  1.791  1.032  0.812]\\n [ 1.39   0.798  1.981 ...,  1.978  0.239  1.058]\\n [ 1.149  2.454  0.045 ...,  0.572  2.113  0.882]]\\nk-means is a very popular algorithm for clustering, not only because it is relatively\\neasy to understand and implement, but also because it runs relatively quickly. k-\\nmeans scales easily to large datasets, and scikit-learn even includes a more scalable\\nvariant in the MiniBatchKMeans class, which can handle very large datasets.\\nOne of the drawbacks of k-means is that it relies on a random initialization, which'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 194, 'page_label': '181', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='variant in the MiniBatchKMeans class, which can handle very large datasets.\\nOne of the drawbacks of k-means is that it relies on a random initialization, which\\nmeans the outcome of the algorithm depends on a random seed. By default, scikit-\\nlearn runs the algorithm 10 times with 10 different random initializations, and\\nreturns the best result. 4 Further downsides of k-means are the relatively restrictive\\nassumptions made on the shape of clusters, and the requirement to specify the num‐\\nber of clusters you are looking for (which might not be known in a real-world\\napplication).\\nNext, we will look at two more clustering algorithms that improve upon these proper‐\\nties in some ways.\\nClustering | 181'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 195, 'page_label': '182', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Agglomerative Clustering\\nAgglomerative clustering refers to a collection of clustering algorithms that all build\\nupon the same principles: the algorithm starts by declaring each point its own cluster,\\nand then merges the two most similar clusters until some stopping criterion is satis‐\\nfied. The stopping criterion implemented in scikit-learn is the number of clusters,\\nso similar clusters are merged until only the specified number of clusters are left.\\nThere are several linkage criteria that specify how exactly the “most similar cluster” is\\nmeasured. This measure is always defined between two existing clusters.\\nThe following three choices are implemented in scikit-learn:\\nward\\nThe default choice, ward picks the two clusters to merge such that the variance\\nwithin all clusters increases the least. This often leads to clusters that are rela‐\\ntively equally sized.\\naverage\\naverage linkage merges the two clusters that have the smallest average distance\\nbetween all their points.\\ncomplete'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 195, 'page_label': '182', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='tively equally sized.\\naverage\\naverage linkage merges the two clusters that have the smallest average distance\\nbetween all their points.\\ncomplete\\ncomplete linkage (also known as maximum linkage) merges the two clusters that\\nhave the smallest maximum distance between their points.\\nward works on most datasets, and we will use it in our examples. If the clusters have\\nvery dissimilar numbers of members (if one is much bigger than all the others, for\\nexample), average or complete might work better.\\nThe following plot ( Figure 3-33) illustrates the progression of agglomerative cluster‐\\ning on a two-dimensional dataset, looking for three clusters:\\nIn[61]:\\nmglearn.plots.plot_agglomerative_algorithm()\\n182 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 196, 'page_label': '183', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='5 We could also use the labels_ attribute, as we did for k-means.\\nFigure 3-33. Agglomerative clustering iteratively joins the two closest clusters\\nInitially, each point is its own cluster. Then, in each step, the two clusters that are\\nclosest are merged. In the first four steps, two single-point clusters are picked and\\nthese are joined into two-point clusters. In step 5, one of the two-point clusters is\\nextended to a third point, and so on. In step 9, there are only three clusters remain‐\\ning. As we specified that we are looking for three clusters, the algorithm then stops.\\nLet’s have a look at how agglomerative clustering performs on the simple three-\\ncluster data we used here. Because of the way the algorithm works, agglomerative\\nclustering cannot make predictions for new data points. Therefore, Agglomerative\\nClustering has no predict method. To build the model and get the cluster member‐\\nships on the training set, use the fit_predict method instead. 5 The result is shown'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 196, 'page_label': '183', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Clustering has no predict method. To build the model and get the cluster member‐\\nships on the training set, use the fit_predict method instead. 5 The result is shown\\nin Figure 3-34:\\nIn[62]:\\nfrom sklearn.cluster import AgglomerativeClustering\\nX, y = make_blobs(random_state=1)\\nagg = AgglomerativeClustering(n_clusters=3)\\nassignment = agg.fit_predict(X)\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], assignment)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nClustering | 183'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 197, 'page_label': '184', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-34. Cluster assignment using agglomerative clustering with three clusters\\nAs expected, the algorithm recovers the clustering perfectly. While the scikit-learn\\nimplementation of agglomerative clustering requires you to specify the number of\\nclusters you want the algorithm to find, agglomerative clustering methods provide\\nsome help with choosing the right number, which we will discuss next.\\nHierarchical clustering and dendrograms\\nAgglomerative clustering produces what is known as a hierarchical clustering. The\\nclustering proceeds iteratively, and every point makes a journey from being a single\\npoint cluster to belonging to some final cluster. Each intermediate step provides a\\nclustering of the data (with a different number of clusters). It is sometimes helpful to\\nlook at all possible clusterings jointly. The next example (Figure 3-35) shows an over‐\\nlay of all the possible clusterings shown in Figure 3-33, providing some insight into'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 197, 'page_label': '184', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='look at all possible clusterings jointly. The next example (Figure 3-35) shows an over‐\\nlay of all the possible clusterings shown in Figure 3-33, providing some insight into\\nhow each cluster breaks up into smaller clusters:\\nIn[63]:\\nmglearn.plots.plot_agglomerative()\\n184 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 198, 'page_label': '185', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-35. Hierarchical cluster assignment (shown as lines) generated with agglomera‐\\ntive clustering, with numbered data points (cf. Figure 3-36)\\nWhile this visualization provides a very detailed view of the hierarchical clustering, it\\nrelies on the two-dimensional nature of the data and therefore cannot be used on\\ndatasets that have more than two features. There is, however, another tool to visualize\\nhierarchical clustering, called a dendrogram, that can handle multidimensional\\ndatasets.\\nUnfortunately, scikit-learn currently does not have the functionality to draw den‐\\ndrograms. However, you can generate them easily using SciPy. The SciPy clustering\\nalgorithms have a slightly different interface to the scikit-learn clustering algo‐\\nrithms. SciPy provides a function that takes a data array X and computes a linkage\\narray, which encodes hierarchical cluster similarities. We can then feed this linkage\\narray into the scipy dendrogram function to plot the dendrogram (Figure 3-36):'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 198, 'page_label': '185', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='array, which encodes hierarchical cluster similarities. We can then feed this linkage\\narray into the scipy dendrogram function to plot the dendrogram (Figure 3-36):\\nIn[64]:\\n# Import the dendrogram function and the ward clustering function from SciPy\\nfrom scipy.cluster.hierarchy import dendrogram, ward\\nX, y = make_blobs(random_state=0, n_samples=12)\\n# Apply the ward clustering to the data array X\\n# The SciPy ward function returns an array that specifies the distances\\n# bridged when performing agglomerative clustering\\nlinkage_array = ward(X)\\nClustering | 185'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 199, 'page_label': '186', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='# Now we plot the dendrogram for the linkage_array containing the distances\\n# between clusters\\ndendrogram(linkage_array)\\n# Mark the cuts in the tree that signify two or three clusters\\nax = plt.gca()\\nbounds = ax.get_xbound()\\nax.plot(bounds, [7.25, 7.25], \\'--\\', c=\\'k\\')\\nax.plot(bounds, [4, 4], \\'--\\', c=\\'k\\')\\nax.text(bounds[1], 7.25, \\' two clusters\\', va=\\'center\\', fontdict={\\'size\\': 15})\\nax.text(bounds[1], 4, \\' three clusters\\', va=\\'center\\', fontdict={\\'size\\': 15})\\nplt.xlabel(\"Sample index\")\\nplt.ylabel(\"Cluster distance\")\\nFigure 3-36. Dendrogram of the clustering shown in Figure 3-35 with lines indicating\\nsplits into two and three clusters\\nThe dendrogram shows data points as points on the bottom (numbered from 0 to\\n11). Then, a tree is plotted with these points (representing single-point clusters) as the\\nleaves, and a new node parent is added for each two clusters that are joined.\\nReading from bottom to top, the data points 1 and 4 are joined first (as you could see'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 199, 'page_label': '186', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='leaves, and a new node parent is added for each two clusters that are joined.\\nReading from bottom to top, the data points 1 and 4 are joined first (as you could see\\nin Figure 3-33). Next, points 6 and 9 are joined into a cluster, and so on. At the top\\nlevel, there are two branches, one consisting of points 11, 0, 5, 10, 7, 6, and 9, and the\\nother consisting of points 1, 4, 3, 2, and 8. These correspond to the two largest clus‐\\nters in the lefthand side of the plot.\\n186 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 200, 'page_label': '187', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='The y-axis in the dendrogram doesn’t just specify when in the agglomerative algo‐\\nrithm two clusters get merged. The length of each branch also shows how far apart\\nthe merged clusters are. The longest branches in this dendrogram are the three lines\\nthat are marked by the dashed line labeled “three clusters. ” That these are the longest\\nbranches indicates that going from three to two clusters meant merging some very\\nfar-apart points. We see this again at the top of the chart, where merging the two\\nremaining clusters into a single cluster again bridges a relatively large distance.\\nUnfortunately, agglomerative clustering still fails at separating complex shapes like\\nthe two_moons dataset. But the same is not true for the next algorithm we will look at,\\nDBSCAN.\\nDBSCAN\\nAnother very useful clustering algorithm is DBSCAN (which stands for “density-\\nbased spatial clustering of applications with noise”). The main benefits of DBSCAN'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 200, 'page_label': '187', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='DBSCAN.\\nDBSCAN\\nAnother very useful clustering algorithm is DBSCAN (which stands for “density-\\nbased spatial clustering of applications with noise”). The main benefits of DBSCAN\\nare that it does not require the user to set the number of clusters a priori, it can cap‐\\nture clusters of complex shapes, and it can identify points that are not part of any\\ncluster. DBSCAN is somewhat slower than agglomerative clustering and k-means, but\\nstill scales to relatively large datasets.\\nDBSCAN works by identifying points that are in “crowded” regions of the feature\\nspace, where many data points are close together. These regions are referred to as\\ndense regions in feature space. The idea behind DBSCAN is that clusters form dense\\nregions of data, separated by regions that are relatively empty.\\nPoints that are within a dense region are called core samples (or core points), and they\\nare defined as follows. There are two parameters in DBSCAN: min_samples and eps.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 200, 'page_label': '187', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Points that are within a dense region are called core samples (or core points), and they\\nare defined as follows. There are two parameters in DBSCAN: min_samples and eps.\\nIf there are at least min_samples many data points within a distance of eps to a given\\ndata point, that data point is classified as a core sample. Core samples that are closer\\nto each other than the distance eps are put into the same cluster by DBSCAN.\\nThe algorithm works by picking an arbitrary point to start with. It then finds all\\npoints with distance eps or less from that point. If there are less than min_samples\\npoints within distance eps of the starting point, this point is labeled as noise, meaning\\nthat it doesn’t belong to any cluster. If there are more than min_samples points within\\na distance of eps, the point is labeled a core sample and assigned a new cluster label.\\nThen, all neighbors (within eps) of the point are visited. If they have not been'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 200, 'page_label': '187', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='a distance of eps, the point is labeled a core sample and assigned a new cluster label.\\nThen, all neighbors (within eps) of the point are visited. If they have not been\\nassigned a cluster yet, they are assigned the new cluster label that was just created. If\\nthey are core samples, their neighbors are visited in turn, and so on. The cluster\\ngrows until there are no more core samples within distance eps of the cluster. Then\\nanother point that hasn’t yet been visited is picked, and the same procedure is\\nrepeated.\\nClustering | 187'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 201, 'page_label': '188', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In the end, there are three kinds of points: core points, points that are within distance\\neps of core points (called boundary points), and noise. When the DBSCAN algorithm\\nis run on a particular dataset multiple times, the clustering of the core points is always\\nthe same, and the same points will always be labeled as noise. However, a boundary\\npoint might be neighbor to core samples of more than one cluster. Therefore, the\\ncluster membership of boundary points depends on the order in which points are vis‐\\nited. Usually there are only few boundary points, and this slight dependence on the\\norder of points is not important.\\nLet’s apply DBSCAN on the synthetic dataset we used to demonstrate agglomerative\\nclustering. Like agglomerative clustering, DBSCAN does not allow predictions on\\nnew test data, so we will use the fit_predict method to perform clustering and\\nreturn the cluster labels in one step:\\nIn[65]:\\nfrom sklearn.cluster import DBSCAN\\nX, y = make_blobs(random_state=0, n_samples=12)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 201, 'page_label': '188', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='return the cluster labels in one step:\\nIn[65]:\\nfrom sklearn.cluster import DBSCAN\\nX, y = make_blobs(random_state=0, n_samples=12)\\ndbscan = DBSCAN()\\nclusters = dbscan.fit_predict(X)\\nprint(\"Cluster memberships:\\\\n{}\".format(clusters))\\nOut[65]:\\nCluster memberships:\\n[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\nAs you can see, all data points were assigned the label -1, which stands for noise. This\\nis a consequence of the default parameter settings for eps and min_samples, which\\nare not tuned for small toy datasets. The cluster assignments for different values of\\nmin_samples and eps are shown below, and visualized in Figure 3-37:\\nIn[66]:\\nmglearn.plots.plot_dbscan()\\nOut[66]:\\nmin_samples: 2 eps: 1.000000  cluster: [-1  0  0 -1  0 -1  1  1  0  1 -1 -1]\\nmin_samples: 2 eps: 1.500000  cluster: [0 1 1 1 1 0 2 2 1 2 2 0]\\nmin_samples: 2 eps: 2.000000  cluster: [0 1 1 1 1 0 0 0 1 0 0 0]\\nmin_samples: 2 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 201, 'page_label': '188', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='min_samples: 2 eps: 1.500000  cluster: [0 1 1 1 1 0 2 2 1 2 2 0]\\nmin_samples: 2 eps: 2.000000  cluster: [0 1 1 1 1 0 0 0 1 0 0 0]\\nmin_samples: 2 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\\nmin_samples: 3 eps: 1.000000  cluster: [-1  0  0 -1  0 -1  1  1  0  1 -1 -1]\\nmin_samples: 3 eps: 1.500000  cluster: [0 1 1 1 1 0 2 2 1 2 2 0]\\nmin_samples: 3 eps: 2.000000  cluster: [0 1 1 1 1 0 0 0 1 0 0 0]\\nmin_samples: 3 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\\nmin_samples: 5 eps: 1.000000  cluster: [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\nmin_samples: 5 eps: 1.500000  cluster: [-1  0  0  0  0 -1 -1 -1  0 -1 -1 -1]\\nmin_samples: 5 eps: 2.000000  cluster: [-1  0  0  0  0 -1 -1 -1  0 -1 -1 -1]\\nmin_samples: 5 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\\n188 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 202, 'page_label': '189', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-37. Cluster assignments found by DBSCAN with varying settings for the\\nmin_samples and eps parameters\\nIn this plot, points that belong to clusters are solid, while the noise points are shown\\nin white. Core samples are shown as large markers, while boundary points are dis‐\\nplayed as smaller markers. Increasing eps (going from left to right in the figure)\\nmeans that more points will be included in a cluster. This makes clusters grow, but\\nmight also lead to multiple clusters joining into one. Increasing min_samples (going\\nfrom top to bottom in the figure) means that fewer points will be core points, and\\nmore points will be labeled as noise.\\nThe parameter eps is somewhat more important, as it determines what it means for\\npoints to be “close. ” Setting eps to be very small will mean that no points are core\\nsamples, and may lead to all points being labeled as noise. Setting eps to be very large\\nwill result in all points forming a single cluster.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 202, 'page_label': '189', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='samples, and may lead to all points being labeled as noise. Setting eps to be very large\\nwill result in all points forming a single cluster.\\nThe min_samples setting mostly determines whether points in less dense regions will\\nbe labeled as outliers or as their own clusters. If you decrease min_samples, anything\\nthat would have been a cluster with less than min_samples many samples will now be\\nlabeled as noise. min_samples therefore determines the minimum cluster size. Y ou\\ncan see this very clearly in Figure 3-37, when going from min_samples=3 to min_sam\\nples=5 with eps=1.5. With min_samples=3, there are three clusters: one of four\\nClustering | 189'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 203, 'page_label': '190', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='points, one of five points, and one of three points. Using min_samples=5, the two\\nsmaller clusters (with three and four points) are now labeled as noise, and only the\\ncluster with five samples remains.\\nWhile DBSCAN doesn’t require setting the number of clusters explicitly, setting eps\\nimplicitly controls how many clusters will be found. Finding a good setting for eps is\\nsometimes easier after scaling the data using StandardScaler or MinMaxScaler, as\\nusing these scaling techniques will ensure that all features have similar ranges.\\nFigure 3-38  shows the result of running DBSCAN on the two_moons dataset. The\\nalgorithm actually finds the two half-circles and separates them using the default\\nsettings:\\nIn[67]:\\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\\n# rescale the data to zero mean and unit variance\\nscaler = StandardScaler()\\nscaler.fit(X)\\nX_scaled = scaler.transform(X)\\ndbscan = DBSCAN()\\nclusters = dbscan.fit_predict(X_scaled)\\n# plot the cluster assignments'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 203, 'page_label': '190', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='scaler = StandardScaler()\\nscaler.fit(X)\\nX_scaled = scaler.transform(X)\\ndbscan = DBSCAN()\\nclusters = dbscan.fit_predict(X_scaled)\\n# plot the cluster assignments\\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=mglearn.cm2, s=60)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nAs the algorithm produced the desired number of clusters (two), the parameter set‐\\ntings seem to work well. If we decrease eps to 0.2 (from the default of 0.5), we will\\nget eight clusters, which is clearly too many. Increasing eps to 0.7 results in a single\\ncluster.\\nWhen using DBSCAN, you need to be careful about handling the returned cluster\\nassignments. The use of -1 to indicate noise might result in unexpected effects when\\nusing the cluster labels to index another array.\\n190 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 204, 'page_label': '191', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\\nComparing and Evaluating Clustering Algorithms\\nOne of the challenges in applying clustering algorithms is that it is very hard to assess\\nhow well an algorithm worked, and to compare outcomes between different algo‐\\nrithms. After talking about the algorithms behind k-means, agglomerative clustering,\\nand DBSCAN, we will now compare them on some real-world datasets.\\nEvaluating clustering with ground truth\\nThere are metrics that can be used to assess the outcome of a clustering algorithm\\nrelative to a ground truth clustering, the most important ones being the adjusted rand\\nindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐\\ntative measure between 0 and 1.\\nHere, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\\nusing ARI. We also include what it looks like when we randomly assign points to two\\nclusters for comparison (see Figure 3-39):\\nClustering | 191'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 205, 'page_label': '192', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[68]:\\nfrom sklearn.metrics.cluster import adjusted_rand_score\\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\\n# rescale the data to zero mean and unit variance\\nscaler = StandardScaler()\\nscaler.fit(X)\\nX_scaled = scaler.transform(X)\\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\\n                         subplot_kw={\\'xticks\\': (), \\'yticks\\': ()})\\n# make a list of algorithms to use\\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\\n              DBSCAN()]\\n# create a random cluster assignment for reference\\nrandom_state = np.random.RandomState(seed=0)\\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\\n# plot random assignment\\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\\n                cmap=mglearn.cm3, s=60)\\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\\n        adjusted_rand_score(y, random_clusters)))\\nfor ax, algorithm in zip(axes[1:], algorithms):'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 205, 'page_label': '192', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='cmap=mglearn.cm3, s=60)\\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\\n        adjusted_rand_score(y, random_clusters)))\\nfor ax, algorithm in zip(axes[1:], algorithms):\\n    # plot the cluster assignments and cluster centers\\n    clusters = algorithm.fit_predict(X_scaled)\\n    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters,\\n               cmap=mglearn.cm3, s=60)\\n    ax.set_title(\"{} - ARI: {:.2f}\".format(algorithm.__class__.__name__,\\n                                           adjusted_rand_score(y, clusters)))\\nFigure 3-39. Comparing random assignment, k-means, agglomerative clustering, and\\nDBSCAN on the two_moons dataset using the supervised ARI score\\nThe adjusted rand index provides intuitive results, with a random cluster assignment\\nhaving a score of 0 and DBSCAN (which recovers the desired clustering perfectly)\\nhaving a score of 1.\\n192 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 206, 'page_label': '193', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='A common mistake when evaluating clustering in this way is to use accuracy_score\\ninstead of adjusted_rand_score, normalized_mutual_info_score, or some other\\nclustering metric. The problem in using accuracy is that it requires the assigned clus‐\\nter labels to exactly match the ground truth. However, the cluster labels themselves\\nare meaningless—the only thing that matters is which points are in the same cluster:\\nIn[69]:\\nfrom sklearn.metrics import accuracy_score\\n# these two labelings of points correspond to the same clustering\\nclusters1 = [0, 0, 1, 1, 0]\\nclusters2 = [1, 1, 0, 0, 1]\\n# accuracy is zero, as none of the labels are the same\\nprint(\"Accuracy: {:.2f}\".format(accuracy_score(clusters1, clusters2)))\\n# adjusted rand score is 1, as the clustering is exactly the same\\nprint(\"ARI: {:.2f}\".format(adjusted_rand_score(clusters1, clusters2)))\\nOut[69]:\\nAccuracy: 0.00\\nARI: 1.00\\nEvaluating clustering without ground truth'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 206, 'page_label': '193', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='print(\"ARI: {:.2f}\".format(adjusted_rand_score(clusters1, clusters2)))\\nOut[69]:\\nAccuracy: 0.00\\nARI: 1.00\\nEvaluating clustering without ground truth\\nAlthough we have just shown one way to evaluate clustering algorithms, in practice,\\nthere is a big problem with using measures like ARI. When applying clustering algo‐\\nrithms, there is usually no ground truth to which to compare the results. If we knew\\nthe right clustering of the data, we could use this information to build a supervised\\nmodel like a classifier. Therefore, using metrics like ARI and NMI usually only helps\\nin developing algorithms, not in assessing success in an application.\\nThere are scoring metrics for clustering that don’t require ground truth, like the sil‐\\nhouette \\ncoefficient. However, these often don’t work well in practice. The silhouette\\nscore computes the compactness of a cluster, where higher is better, with a perfect\\nscore of 1. While compact clusters are good, compactness doesn’t allow for complex\\nshapes.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 206, 'page_label': '193', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='score computes the compactness of a cluster, where higher is better, with a perfect\\nscore of 1. While compact clusters are good, compactness doesn’t allow for complex\\nshapes.\\nHere is an example comparing the outcome of k-means, agglomerative clustering,\\nand DBSCAN on the two-moons dataset using the silhouette score (Figure 3-40):\\nIn[70]:\\nfrom sklearn.metrics.cluster import silhouette_score\\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\\n# rescale the data to zero mean and unit variance\\nscaler = StandardScaler()\\nscaler.fit(X)\\nX_scaled = scaler.transform(X)\\nClustering | 193'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 207, 'page_label': '194', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='fig, axes = plt.subplots(1, 4, figsize=(15, 3),\\n                         subplot_kw={\\'xticks\\': (), \\'yticks\\': ()})\\n# create a random cluster assignment for reference\\nrandom_state = np.random.RandomState(seed=0)\\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\\n# plot random assignment\\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\\n    cmap=mglearn.cm3, s=60)\\naxes[0].set_title(\"Random assignment: {:.2f}\".format(\\n    silhouette_score(X_scaled, random_clusters)))\\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\\n              DBSCAN()]\\nfor ax, algorithm in zip(axes[1:], algorithms):\\n    clusters = algorithm.fit_predict(X_scaled)\\n    # plot the cluster assignments and cluster centers\\n    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=mglearn.cm3,\\n               s=60)\\n    ax.set_title(\"{} : {:.2f}\".format(algorithm.__class__.__name__,\\n                                      silhouette_score(X_scaled, clusters)))'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 207, 'page_label': '194', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='s=60)\\n    ax.set_title(\"{} : {:.2f}\".format(algorithm.__class__.__name__,\\n                                      silhouette_score(X_scaled, clusters)))\\nFigure 3-40. Comparing random assignment, k-means, agglomerative clustering, and\\nDBSCAN on the two_moons dataset using the unsupervised silhouette score—the more\\nintuitive result of DBSCAN has a lower silhouette score than the assignments found by\\nk-means\\nAs you can see, k-means gets the highest silhouette score, even though we might pre‐\\nfer the result produced by DBSCAN. A slightly better strategy for evaluating clusters\\nis using robustness-based clustering metrics. These run an algorithm after adding\\nsome noise to the data, or using different parameter settings, and compare the out‐\\ncomes. The idea is that if many algorithm parameters and many perturbations of the\\ndata return the same result, it is likely to be trustworthy. Unfortunately, this strategy is\\nnot implemented in scikit-learn at the time of writing.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 207, 'page_label': '194', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='data return the same result, it is likely to be trustworthy. Unfortunately, this strategy is\\nnot implemented in scikit-learn at the time of writing.\\nEven if we get a very robust clustering, or a very high silhouette score, we still don’t\\nknow if there is any semantic meaning in the clustering, or whether the clustering\\n194 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 208, 'page_label': '195', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='reflects an aspect of the data that we are interested in. Let’s go back to the example of\\nface images. We hope to find groups of similar faces—say, men and women, or old\\npeople and young people, or people with beards and without. Let’s say we cluster the\\ndata into two clusters, and all algorithms agree about which points should be clus‐\\ntered together. We still don’t know if the clusters that are found correspond in any\\nway to the concepts we are interested in. It could be that they found side views versus\\nfront views, or pictures taken at night versus pictures taken during the day, or pic‐\\ntures taken with iPhones versus pictures taken with Android phones. The only way to\\nknow whether the clustering corresponds to anything we are interested in is to ana‐\\nlyze the clusters manually.\\nComparing algorithms on the faces dataset\\nLet’s apply the k-means, DBSCAN, and agglomerative clustering algorithms to the'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 208, 'page_label': '195', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='lyze the clusters manually.\\nComparing algorithms on the faces dataset\\nLet’s apply the k-means, DBSCAN, and agglomerative clustering algorithms to the\\nLabeled Faces in the Wild dataset, and see if any of them find interesting structure.\\nWe will use the eigenface representation of the data, as produced by\\nPCA(whiten=True), with 100 components:\\nIn[71]:\\n# extract eigenfaces from lfw data and transform data\\nfrom sklearn.decomposition import PCA\\npca = PCA(n_components=100, whiten=True, random_state=0)\\npca.fit_transform(X_people)\\nX_pca = pca.transform(X_people)\\nWe saw earlier that this is a more semantic representation of the face images than the\\nraw pixels. It will also make computation faster. A good exercise would be for you to\\nrun the following experiments on the original data, without PCA, and see if you find\\nsimilar clusters.\\nAnalyzing the faces dataset with DBSCAN.    We will start by applying DBSCAN, which we\\njust discussed:\\nIn[72]:\\n# apply DBSCAN with default parameters'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 208, 'page_label': '195', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='similar clusters.\\nAnalyzing the faces dataset with DBSCAN.    We will start by applying DBSCAN, which we\\njust discussed:\\nIn[72]:\\n# apply DBSCAN with default parameters\\ndbscan = DBSCAN()\\nlabels = dbscan.fit_predict(X_pca)\\nprint(\"Unique labels: {}\".format(np.unique(labels)))\\nOut[72]:\\nUnique labels: [-1]\\nWe see that all the returned labels are –1, so all of the data was labeled as “noise” by\\nDBSCAN. There are two things we can change to help this: we can make eps higher,\\nto expand the neighborhood of each point, and set min_samples lower, to consider\\nsmaller groups of points as clusters. Let’s try changing min_samples first:\\nClustering | 195'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 209, 'page_label': '196', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[73]:\\ndbscan = DBSCAN(min_samples=3)\\nlabels = dbscan.fit_predict(X_pca)\\nprint(\"Unique labels: {}\".format(np.unique(labels)))\\nOut[73]:\\nUnique labels: [-1]\\nEven when considering groups of three points, everything is labeled as noise. So, we\\nneed to increase eps:\\nIn[74]:\\ndbscan = DBSCAN(min_samples=3, eps=15)\\nlabels = dbscan.fit_predict(X_pca)\\nprint(\"Unique labels: {}\".format(np.unique(labels)))\\nOut[74]:\\nUnique labels: [-1  0]\\nUsing a much larger eps of 15, we get only a single cluster and noise points. We can\\nuse this result to find out what the “noise” looks like compared to the rest of the data.\\nTo understand better what’s happening, let’s look at how many points are noise, and\\nhow many points are inside the cluster:\\nIn[75]:\\n# Count number of points in all clusters and noise.\\n# bincount doesn\\'t allow negative numbers, so we need to add 1.\\n# The first number in the result corresponds to noise points.\\nprint(\"Number of points per cluster: {}\".format(np.bincount(labels + 1)))\\nOut[75]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 209, 'page_label': '196', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='# The first number in the result corresponds to noise points.\\nprint(\"Number of points per cluster: {}\".format(np.bincount(labels + 1)))\\nOut[75]:\\nNumber of points per cluster: [  27 2036]\\nThere are very few noise points—only 27—so we can look at all of them (see\\nFigure 3-41):\\nIn[76]:\\nnoise = X_people[labels==-1]\\nfig, axes = plt.subplots(3, 9, subplot_kw={\\'xticks\\': (), \\'yticks\\': ()},\\n                         figsize=(12, 4))\\nfor image, ax in zip(noise, axes.ravel()):\\n    ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\\n196 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 210, 'page_label': '197', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-41. Samples from the faces dataset labeled as noise by DBSCAN\\nComparing these images to the random sample of face images from Figure 3-7, we\\ncan guess why they were labeled as noise: the fifth image in the first row shows a per‐\\nson drinking from a glass, there are images of people wearing hats, and in the last\\nimage there’s a hand in front of the person’s face. The other images contain odd angles\\nor crops that are too close or too wide.\\nThis kind of analysis—trying to find “the odd one out”—is called outlier detection. If\\nthis was a real application, we might try to do a better job of cropping images, to get\\nmore homogeneous data. There is little we can do about people in photos sometimes\\nwearing hats, drinking, or holding something in front of their faces, but it’s good to\\nknow that these are issues in the data that any algorithm we might apply needs to\\nhandle.\\nIf we want to find more interesting clusters than just one large one, we need to set eps'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 210, 'page_label': '197', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='know that these are issues in the data that any algorithm we might apply needs to\\nhandle.\\nIf we want to find more interesting clusters than just one large one, we need to set eps\\nsmaller, somewhere between 15 and 0.5 (the default). Let’s have a look at what differ‐\\nent values of eps result in:\\nIn[77]:\\nfor eps in [1, 3, 5, 7, 9, 11, 13]:\\n    print(\"\\\\neps={}\".format(eps))\\n    dbscan = DBSCAN(eps=eps, min_samples=3)\\n    labels = dbscan.fit_predict(X_pca)\\n    print(\"Clusters present: {}\".format(np.unique(labels)))\\n    print(\"Cluster sizes: {}\".format(np.bincount(labels + 1)))\\nOut[78]:\\neps=1\\nClusters present: [-1]\\nCluster sizes: [2063]\\neps=3\\nClusters present: [-1]\\nCluster sizes: [2063]\\nClustering | 197'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 211, 'page_label': '198', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='eps=5\\nClusters present: [-1]\\nCluster sizes: [2063]\\neps=7\\nClusters present: [-1  0  1  2  3  4  5  6  7  8  9 10 11 12]\\nCluster sizes: [2006  4  6  6  6  9  3  3  4  3  3  3  3  4]\\neps=9\\nClusters present: [-1  0  1  2]\\nCluster sizes: [1269  788    3    3]\\neps=11\\nClusters present: [-1  0]\\nCluster sizes: [ 430 1633]\\neps=13\\nClusters present: [-1  0]\\nCluster sizes: [ 112 1951]\\nFor low settings of eps, all points are labeled as noise. For eps=7, we get many noise\\npoints and many smaller clusters. For eps=9 we still get many noise points, but we get\\none big cluster and some smaller clusters. Starting from eps=11, we get only one large\\ncluster and noise.\\nWhat is interesting to note is that there is never more than one large cluster. At most,\\nthere is one large cluster containing most of the points, and some smaller clusters.\\nThis indicates that there are not two or three different kinds of face images in the data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 211, 'page_label': '198', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"there is one large cluster containing most of the points, and some smaller clusters.\\nThis indicates that there are not two or three different kinds of face images in the data\\nthat are very distinct, but rather that all images are more or less equally similar to (or\\ndissimilar from) the rest.\\nThe results for eps=7 look most interesting, with many small clusters. We can investi‐\\ngate this clustering in more detail by visualizing all of the points in each of the 13\\nsmall clusters (Figure 3-42):\\nIn[78]:\\ndbscan = DBSCAN(min_samples=3, eps=7)\\nlabels = dbscan.fit_predict(X_pca)\\nfor cluster in range(max(labels) + 1):\\n    mask = labels == cluster\\n    n_images =  np.sum(mask)\\n    fig, axes = plt.subplots(1, n_images, figsize=(n_images * 1.5, 4),\\n                             subplot_kw={'xticks': (), 'yticks': ()})\\n    for image, label, ax in zip(X_people[mask], y_people[mask], axes):\\n        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 211, 'page_label': '198', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='for image, label, ax in zip(X_people[mask], y_people[mask], axes):\\n        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\\n        ax.set_title(people.target_names[label].split()[-1])\\n198 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 212, 'page_label': '199', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-42. Clusters found by DBSCAN with eps=7\\nSome of the clusters correspond to people with very distinct faces (within this data‐\\nset), such as Sharon or Koizumi. Within each cluster, the orientation of the face is also\\nClustering | 199'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 213, 'page_label': '200', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='quite fixed, as well as the facial expression. Some of the clusters contain faces of mul‐\\ntiple people, but they share a similar orientation and expression.\\nThis concludes our analysis of the DBSCAN algorithm applied to the faces dataset. As\\nyou can see, we are doing a manual analysis here, different from the much more auto‐\\nmatic search approach we could use for supervised learning based on R2 score or\\naccuracy.\\nLet’s move on to applying k-means and agglomerative clustering.\\nAnalyzing the faces dataset with k-means.    We saw that it was not possible to create\\nmore than one big cluster using DBSCAN. Agglomerative clustering and k-means are\\nmuch more likely to create clusters of even size, but we do need to set a target num‐\\nber of clusters. We could set the number of clusters to the known number of people in\\nthe dataset, though it is very unlikely that an unsupervised clustering algorithm will\\nrecover them. Instead, we can start with a low number of clusters, like 10, which'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 213, 'page_label': '200', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the dataset, though it is very unlikely that an unsupervised clustering algorithm will\\nrecover them. Instead, we can start with a low number of clusters, like 10, which\\nmight allow us to analyze each of the clusters:\\nIn[79]:\\n# extract clusters with k-means\\nkm = KMeans(n_clusters=10, random_state=0)\\nlabels_km = km.fit_predict(X_pca)\\nprint(\"Cluster sizes k-means: {}\".format(np.bincount(labels_km)))\\nOut[79]:\\nCluster sizes k-means: [269 128 170 186 386 222 237  64 253 148]\\nAs you can see, k-means clustering partitioned the data into relatively similarly sized\\nclusters from 64 to 386. This is quite different from the result of DBSCAN.\\nWe can further analyze the outcome of k-means by visualizing the cluster centers\\n(Figure 3-43 ). As we clustered in the representation produced by PCA, we need to\\nrotate the cluster centers back into the original space to visualize them, using\\npca.inverse_transform:\\nIn[80]:\\nfig, axes = plt.subplots(2, 5, subplot_kw={\\'xticks\\': (), \\'yticks\\': ()},'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 213, 'page_label': '200', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"rotate the cluster centers back into the original space to visualize them, using\\npca.inverse_transform:\\nIn[80]:\\nfig, axes = plt.subplots(2, 5, subplot_kw={'xticks': (), 'yticks': ()},\\n                         figsize=(12, 4))\\nfor center, ax in zip(km.cluster_centers_, axes.ravel()):\\n    ax.imshow(pca.inverse_transform(center).reshape(image_shape),\\n              vmin=0, vmax=1)\\n200 | Chapter 3: Unsupervised Learning and Preprocessing\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 214, 'page_label': '201', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-43. Cluster centers found by k-means when setting the number of clusters to 10\\nThe cluster centers found by k-means are very smooth versions of faces. This is not\\nvery surprising, given that each center is an average of 64 to 386 face images. Working\\nwith a reduced PCA representation adds to the smoothness of the images (compared\\nto the faces reconstructed using 100 PCA dimensions in Figure 3-11). The clustering\\nseems to pick up on different orientations of the face, different expressions (the third\\ncluster center seems to show a smiling face), and the presence of shirt collars (see the\\nsecond-to-last cluster center).\\nFor a more detailed view, in Figure 3-44 we show for each cluster center the five most\\ntypical images in the cluster (the images assigned to the cluster that are closest to the\\ncluster center) and the five most atypical images in the cluster (the images assigned to\\nthe cluster that are furthest from the cluster center):\\nIn[81]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 214, 'page_label': '201', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='cluster center) and the five most atypical images in the cluster (the images assigned to\\nthe cluster that are furthest from the cluster center):\\nIn[81]:\\nmglearn.plots.plot_kmeans_faces(km, pca, X_pca, X_people,\\n                                y_people, people.target_names)\\nClustering | 201'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 215, 'page_label': '202', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-44. Sample images for each cluster found by k-means—the cluster centers are\\non the left, followed by the five closest points to each center and the five points that are\\nassigned to the cluster but are furthest away from the center\\n202 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 216, 'page_label': '203', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-44 confirms our intuition about smiling faces for the third cluster, and also\\nthe importance of orientation for the other clusters. The “atypical” points are not very\\nsimilar to the cluster centers, though, and their assignment seems somewhat arbi‐\\ntrary. This can be attributed to the fact that k-means partitions all the data points and\\ndoesn’t have a concept of “noise” points, as DBSCAN does. Using a larger number of\\nclusters, the algorithm could find finer distinctions. However, adding more clusters\\nmakes manual inspection even harder.\\nAnalyzing the faces dataset with agglomerative clustering.    Now, let’s look at the results of\\nagglomerative clustering:\\nIn[82]:\\n# extract clusters with ward agglomerative clustering\\nagglomerative = AgglomerativeClustering(n_clusters=10)\\nlabels_agg = agglomerative.fit_predict(X_pca)\\nprint(\"Cluster sizes agglomerative clustering: {}\".format(\\n    np.bincount(labels_agg)))\\nOut[82]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 216, 'page_label': '203', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='agglomerative = AgglomerativeClustering(n_clusters=10)\\nlabels_agg = agglomerative.fit_predict(X_pca)\\nprint(\"Cluster sizes agglomerative clustering: {}\".format(\\n    np.bincount(labels_agg)))\\nOut[82]:\\nCluster sizes agglomerative clustering: [255 623  86 102 122 199 265  26 230 155]\\nAgglomerative clustering also produces relatively equally sized clusters, with cluster\\nsizes between 26 and 623. These are more uneven than those produced by k-means,\\nbut much more even than the ones produced by DBSCAN.\\nWe can compute the ARI to measure whether the two partitions of the data given by\\nagglomerative clustering and k-means are similar:\\nIn[83]:\\nprint(\"ARI: {:.2f}\".format(adjusted_rand_score(labels_agg, labels_km)))\\nOut[83]:\\nARI: 0.13\\nAn ARI of only 0.13 means that the two clusterings labels_agg and labels_km have\\nlittle in common. This is not very surprising, given the fact that points further away\\nfrom the cluster centers seem to have little in common for k-means.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 216, 'page_label': '203', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='little in common. This is not very surprising, given the fact that points further away\\nfrom the cluster centers seem to have little in common for k-means.\\nNext, we might want to plot the dendrogram ( Figure 3-45). We’ll limit the depth of\\nthe tree in the plot, as branching down to the individual 2,063 data points would\\nresult in an unreadably dense plot:\\nClustering | 203'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 217, 'page_label': '204', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[84]:\\nlinkage_array = ward(X_pca)\\n# now we plot the dendrogram for the linkage_array\\n# containing the distances between clusters\\nplt.figure(figsize=(20, 5))\\ndendrogram(linkage_array, p=7, truncate_mode=\\'level\\', no_labels=True)\\nplt.xlabel(\"Sample index\")\\nplt.ylabel(\"Cluster distance\")\\nFigure 3-45. Dendrogram of agglomerative clustering on the faces dataset\\nCreating 10 clusters, we cut across the tree at the very top, where there are 10 vertical\\nlines. In the dendrogram for the toy data shown in Figure 3-36, you could see by the\\nlength of the branches that two or three clusters might capture the data appropriately.\\nFor the faces data, there doesn’t seem to be a very natural cutoff point. There are\\nsome branches that represent more distinct groups, but there doesn’t appear to be a\\nparticular number of clusters that is a good fit. This is not surprising, given the results\\nof DBSCAN, which tried to cluster all points together.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 217, 'page_label': '204', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"particular number of clusters that is a good fit. This is not surprising, given the results\\nof DBSCAN, which tried to cluster all points together.\\nLet’s visualize the 10 clusters, as we did for k-means earlier ( Figure 3-46). Note that\\nthere is no notion of cluster center in agglomerative clustering (though we could\\ncompute the mean), and we simply show the first couple of points in each cluster. We\\nshow the number of points in each cluster to the left of the first image:\\nIn[85]:\\nn_clusters = 10\\nfor cluster in range(n_clusters):\\n    mask = labels_agg == cluster\\n    fig, axes = plt.subplots(1, 10, subplot_kw={'xticks': (), 'yticks': ()},\\n                             figsize=(15, 8))\\n    axes[0].set_ylabel(np.sum(mask))\\n    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\\n                                      labels_agg[mask], axes):\\n        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\\n        ax.set_title(people.target_names[label].split()[-1],\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 217, 'page_label': '204', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"labels_agg[mask], axes):\\n        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\\n        ax.set_title(people.target_names[label].split()[-1],\\n                     fontdict={'fontsize': 9})\\n204 | Chapter 3: Unsupervised Learning and Preprocessing\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 218, 'page_label': '205', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-46. Random images from the clusters generated by In[82]—each row corre‐\\nsponds to one cluster; the number to the left lists the number of images in each cluster\\nClustering | 205'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 219, 'page_label': '206', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='While some of the clusters seem to have a semantic theme, many of them are too\\nlarge to be actually homogeneous. To get more homogeneous clusters, we can run the\\nalgorithm again, this time with 40 clusters, and pick out some of the clusters that are\\nparticularly interesting (Figure 3-47):\\nIn[86]:\\n# extract clusters with ward agglomerative clustering\\nagglomerative = AgglomerativeClustering(n_clusters=40)\\nlabels_agg = agglomerative.fit_predict(X_pca)\\nprint(\"cluster sizes agglomerative clustering: {}\".format(np.bincount(labels_agg)))\\nn_clusters = 40\\nfor cluster in [10, 13, 19, 22, 36]: # hand-picked \"interesting\" clusters\\n    mask = labels_agg == cluster\\n    fig, axes = plt.subplots(1, 15, subplot_kw={\\'xticks\\': (), \\'yticks\\': ()},\\n                             figsize=(15, 8))\\n    cluster_size = np.sum(mask)\\n    axes[0].set_ylabel(\"#{}: {}\".format(cluster, cluster_size))\\n    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 219, 'page_label': '206', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='cluster_size = np.sum(mask)\\n    axes[0].set_ylabel(\"#{}: {}\".format(cluster, cluster_size))\\n    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\\n                                      labels_agg[mask], axes):\\n        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\\n        ax.set_title(people.target_names[label].split()[-1],\\n                     fontdict={\\'fontsize\\': 9})\\n    for i in range(cluster_size, 15):\\n        axes[i].set_visible(False)\\nOut[86]:\\ncluster sizes agglomerative clustering:\\n [ 58  80  79  40 222  50  55  78 172  28  26  34  14  11  60  66 152  27\\n  47  31  54   5   8  56   3   5   8  18  22  82  37  89  28  24  41  40\\n  21  10 113  69]\\n206 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 220, 'page_label': '207', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 3-47. Images from selected clusters found by agglomerative clustering when set‐\\nting the number of clusters to 40—the text to the left shows the index of the cluster and\\nthe total number of points in the cluster\\nHere, the clustering seems to have picked up on “dark skinned and smiling, ” “collared\\nshirt, ” “smiling woman, ” “Hussein, ” and “high forehead. ” We could also find these\\nhighly similar clusters using the dendrogram, if we did more a detailed analysis.\\nSummary of Clustering Methods\\nThis section has shown that applying and evaluating clustering is a highly qualitative\\nprocedure, and often most helpful in the exploratory phase of data analysis. We\\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\\ning. All three have a way of controlling the granularity of clustering. k-means and\\nagglomerative clustering allow you to specify the number of desired clusters, while'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 220, 'page_label': '207', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='ing. All three have a way of controlling the granularity of clustering. k-means and\\nagglomerative clustering allow you to specify the number of desired clusters, while\\nDBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\\nences cluster size. All three methods can be used on large, real-world datasets, are rel‐\\natively easy to understand, and allow for clustering into many clusters.\\nEach of the algorithms has somewhat different strengths. k-means allows for a char‐\\nacterization of the clusters using the cluster means. It can also be viewed as a decom‐\\nposition method, where each data point is represented by its cluster center. DBSCAN\\nallows for the detection of “noise points” that are not assigned any cluster, and it can\\nhelp automatically determine the number of clusters. In contrast to the other two\\nmethods, it allow for complex cluster shapes, as we saw in the two_moons example.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 220, 'page_label': '207', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='help automatically determine the number of clusters. In contrast to the other two\\nmethods, it allow for complex cluster shapes, as we saw in the two_moons example.\\nDBSCAN sometimes produces clusters of very differing size, which can be a strength\\nor a weakness. Agglomerative clustering can provide a whole hierarchy of possible\\npartitions of the data, which can be easily inspected via dendrograms.\\nClustering | 207'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 221, 'page_label': '208', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Summary and Outlook\\nThis chapter introduced a range of unsupervised learning algorithms that can be\\napplied for exploratory data analysis and preprocessing. Having the right representa‐\\ntion of the data is often crucial for supervised or unsupervised learning to succeed,\\nand preprocessing and decomposition methods play an important part in data prepa‐\\nration.\\nDecomposition, manifold learning, and clustering are essential tools to further your\\nunderstanding of your data, and can be the only ways to make sense of your data in\\nthe absence of supervision information. Even in a supervised setting, exploratory\\ntools are important for a better understanding of the properties of the data. Often it is\\nhard to quantify the usefulness of an unsupervised algorithm, though this shouldn’t\\ndeter you from using them to gather insights from your data. With these methods\\nunder your belt, you are now equipped with all the essential learning algorithms that\\nmachine learning practitioners use every day.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 221, 'page_label': '208', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='under your belt, you are now equipped with all the essential learning algorithms that\\nmachine learning practitioners use every day.\\nWe encourage you to try clustering and decomposition methods both on two-\\ndimensional toy data and on real-world datasets included in scikit-learn, like the\\ndigits, iris, and cancer datasets.\\n208 | Chapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 222, 'page_label': '209', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Summary of the Estimator Interface\\nLet’s briefly review the API that we introduced in Chapters 2 and 3. All algorithms in\\nscikit-learn, whether preprocessing, supervised learning, or unsupervised learning\\nalgorithms, are implemented as classes. These classes are called estimators in scikit-\\nlearn. To apply an algorithm, you first have to instantiate an object of the particular\\nclass:\\nIn[87]:\\nfrom sklearn.linear_model import LogisticRegression\\nlogreg = LogisticRegression()\\nThe estimator class contains the algorithm, and also stores the model that is learned\\nfrom data using the algorithm.\\nY ou should set any parameters of the model when constructing the model object.\\nThese parameters include regularization, complexity control, number of clusters to\\nfind, etc. All estimators have a fit method, which is used to build the model. The fit\\nmethod always requires as its first argument the data X, represented as a NumPy array'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 222, 'page_label': '209', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='find, etc. All estimators have a fit method, which is used to build the model. The fit\\nmethod always requires as its first argument the data X, represented as a NumPy array\\nor a SciPy sparse matrix, where each row represents a single data point. The data X is\\nalways assumed to be a NumPy array or SciPy sparse matrix that has continuous\\n(floating-point) entries. Supervised algorithms also require a y argument, which is a\\none-dimensional NumPy array containing target values for regression or classifica‐\\ntion (i.e., the known output labels or responses).\\nThere are two main ways to apply a learned model in scikit-learn. To create a pre‐\\ndiction in the form of a new output like y, you use the predict method. To create a\\nnew representation of the input data X, you use the transform method. Table 3-1\\nsummarizes the use cases of the predict and transform methods.\\nTable 3-1. scikit-learn API summary\\nestimator.fit(x_train, [y_train])\\nestimator.predict(X_text) estimator.transform(X_test)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 222, 'page_label': '209', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='summarizes the use cases of the predict and transform methods.\\nTable 3-1. scikit-learn API summary\\nestimator.fit(x_train, [y_train])\\nestimator.predict(X_text) estimator.transform(X_test)\\nClassification Preprocessing\\nRegression Dimensionality reduction\\nClustering Feature extraction\\n Feature selection\\nAdditionally, all supervised models have a score(X_test, y_test)  method that\\nallows an evaluation of the model. In Table 3-1, X_train and y_train refer to the\\ntraining data and training labels, while X_test and y_test refer to the test data and\\ntest labels (if applicable).\\nSummary and Outlook | 209'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 224, 'page_label': '211', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 4\\nRepresenting Data and\\nEngineering Features\\nSo far, we’ve assumed that our data comes in as a two-dimensional array of floating-\\npoint numbers, where each column is a continuous feature that describes the data\\npoints. For many applications, this is not how the data is collected. A particularly\\ncommon type of feature is the categorical features. Also known as discrete features,\\nthese are usually not numeric. The distinction between categorical features and con‐\\ntinuous features is analogous to the distinction between classification and regression,\\nonly on the input side rather than the output side. Examples of continuous features\\nthat we have seen are pixel brightnesses and size measurements of plant flowers.\\nExamples of categorical features are the brand of a product, the color of a product, or\\nthe department (books, clothing, hardware) it is sold in. These are all properties that\\ncan describe a product, but they don’t vary in a continuous way. A product belongs'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 224, 'page_label': '211', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the department (books, clothing, hardware) it is sold in. These are all properties that\\ncan describe a product, but they don’t vary in a continuous way. A product belongs\\neither in the clothing department or in the books department. There is no middle\\nground between books and clothing, and no natural order for the different categories\\n(books is not greater or less than clothing, hardware is not between books and cloth‐\\ning, etc.).\\nRegardless of the types of features your data consists of, how you represent them can\\nhave an enormous effect on the performance of machine learning models. We saw in\\nChapters 2 and 3 that scaling of the data is important. In other words, if you don’t\\nrescale your data (say, to unit variance), then it makes a difference whether you repre‐\\nsent a measurement in centimeters or inches. We also saw in Chapter 2 that it can be\\nhelpful to augment your data with additional features, like adding interactions (prod‐\\nucts) of features or more general polynomials.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 224, 'page_label': '211', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='helpful to augment your data with additional features, like adding interactions (prod‐\\nucts) of features or more general polynomials.\\nThe question of how to represent your data best for a particular application is known\\nas feature engineering, and it is one of the main tasks of data scientists and machine\\n211'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 225, 'page_label': '212', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='learning practitioners trying to solve real-world problems. Representing your data in\\nthe right way can have a bigger influence on the performance of a supervised model\\nthan the exact parameters you choose.\\nIn this chapter, we will first go over the important and very common case of categori‐\\ncal features, and then give some examples of helpful transformations for specific\\ncombinations of features and models.\\nCategorical Variables\\nAs an example, we will use the dataset of adult incomes in the United States, derived\\nfrom the 1994 census database. The task of the adult dataset is to predict whether a\\nworker has an income of over $50,000 or under $50,000. The features in this dataset\\ninclude the workers’ ages, how they are employed (self employed, private industry\\nemployee, government employee, etc.), their education, their gender, their working\\nhours per week, occupation, and more. Table 4-1 shows the first few entries in the\\ndataset.\\nTable 4-1. The first few entries in the adult dataset'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 225, 'page_label': '212', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='hours per week, occupation, and more. Table 4-1 shows the first few entries in the\\ndataset.\\nTable 4-1. The first few entries in the adult dataset\\nage workclass education gender hours-per-week occupation income\\n0 39 State-gov Bachelors Male 40 Adm-clerical <=50K\\n1 50 Self-emp-not-inc Bachelors Male 13 Exec-managerial <=50K\\n2 38 Private HS-grad Male 40 Handlers-cleaners <=50K\\n3 53 Private 11th Male 40 Handlers-cleaners <=50K\\n4 28 Private Bachelors Female 40 Prof-specialty <=50K\\n5 37 Private Masters Female 40 Exec-managerial <=50K\\n6 49 Private 9th Female 16 Other-service <=50K\\n7 52 Self-emp-not-inc HS-grad Male 45 Exec-managerial >50K\\n8 31 Private Masters Female 50 Prof-specialty >50K\\n9 42 Private Bachelors Male 40 Exec-managerial >50K\\n10 37 Private Some-college Male 80 Exec-managerial >50K\\nThe task is phrased as a classification task with the two classes being income <=50k\\nand >50k. It would also be possible to predict the exact income, and make this a'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 225, 'page_label': '212', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='The task is phrased as a classification task with the two classes being income <=50k\\nand >50k. It would also be possible to predict the exact income, and make this a\\nregression task. However, that would be much more difficult, and the 50K division is\\ninteresting to understand on its own.\\nIn this dataset, age and hours-per-week are continuous features, which we know\\nhow to treat. The workclass, education, sex, and occupation features are categori‐\\ncal, however. All of them come from a fixed list of possible values, as opposed to a\\nrange, and denote a qualitative property, as opposed to a quantity.\\n212 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 226, 'page_label': '213', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='As a starting point, let’s say we want to learn a logistic regression classifier on this\\ndata. We know from Chapter 2 that a logistic regression makes predictions, ŷ, using\\nthe following formula:\\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0\\nwhere w[i] and b are coefficients learned from the training set and x[i] are the input\\nfeatures. This formula makes sense when x[i] are numbers, but not when x[2] is\\n\"Masters\" or \"Bachelors\". Clearly we need to represent our data in some different\\nway when applying logistic regression. The next section will explain how we can\\novercome this problem.\\nOne-Hot-Encoding (Dummy Variables)\\nBy far the most common way to represent categorical variables is using the one-hot-\\nencoding or one-out-of-N encoding, also known as dummy variables. The idea behind\\ndummy variables is to replace a categorical variable with one or more new features\\nthat can have the values 0 and 1. The values 0 and 1 make sense in the formula for'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 226, 'page_label': '213', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='dummy variables is to replace a categorical variable with one or more new features\\nthat can have the values 0 and 1. The values 0 and 1 make sense in the formula for\\nlinear binary classification (and for all other models in scikit-learn), and we can\\nrepresent any number of categories by introducing one new feature per category, as\\ndescribed here.\\nLet’s say for the workclass feature we have possible values of \"Government\\nEmployee\", \"Private Employee\", \"Self Employed\", and \"Self Employed Incorpo\\nrated\". To encode these four possible values, we create four new features, called \"Gov\\nernment Employee\", \"Private Employee\", \"Self Employed\", and \"Self Employed\\nIncorporated\". A feature is 1 if workclass for this person has the corresponding\\nvalue and 0 otherwise, so exactly one of the four new features will be 1 for each data\\npoint. This is why this is called one-hot or one-out-of-N encoding.\\nThe principle is illustrated in Table 4-2. A single feature is encoded using four new'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 226, 'page_label': '213', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='point. This is why this is called one-hot or one-out-of-N encoding.\\nThe principle is illustrated in Table 4-2. A single feature is encoded using four new\\nfeatures. When using this data in a machine learning algorithm, we would drop the\\noriginal workclass feature and only keep the 0–1 features.\\nTable 4-2. Encoding the workclass feature using one-hot encoding\\nworkclass Government Employee Private Employee Self Employed Self Employed Incorporated\\nGovernment Employee 1 0 0 0\\nPrivate Employee 0 1 0 0\\nSelf Employed 0 0 1 0\\nSelf Employed Incorporated 0 0 0 1\\nCategorical Variables | 213'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 227, 'page_label': '214', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='The one-hot encoding we use is quite similar, but not identical, to\\nthe dummy encoding used in statistics. For simplicity, we encode\\neach category with a different binary feature. In statistics, it is com‐\\nmon to encode a categorical feature with k different possible values\\ninto k–1 features (the last one is represented as all zeros). This is\\ndone to simplify the analysis (more technically, this will avoid mak‐\\ning the data matrix rank-deficient).\\nThere are two ways to convert your data to a one-hot encoding of categorical vari‐\\nables, using either pandas or scikit-learn. At the time of writing, using pandas is\\nslightly easier, so let’s go this route. First we load the data using pandas from a\\ncomma-separated values (CSV) file:\\nIn[2]:\\nimport pandas as pd\\n# The file has no headers naming the columns, so we pass header=None\\n# and provide the column names explicitly in \"names\"\\ndata = pd.read_csv(\\n    \"/home/andy/datasets/adult.data\", header=None, index_col=False,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 227, 'page_label': '214', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='# and provide the column names explicitly in \"names\"\\ndata = pd.read_csv(\\n    \"/home/andy/datasets/adult.data\", header=None, index_col=False,\\n    names=[\\'age\\', \\'workclass\\', \\'fnlwgt\\', \\'education\\',  \\'education-num\\',\\n           \\'marital-status\\', \\'occupation\\', \\'relationship\\', \\'race\\', \\'gender\\',\\n           \\'capital-gain\\', \\'capital-loss\\', \\'hours-per-week\\', \\'native-country\\',\\n           \\'income\\'])\\n# For illustration purposes, we only select some of the columns\\ndata = data[[\\'age\\', \\'workclass\\', \\'education\\', \\'gender\\', \\'hours-per-week\\',\\n             \\'occupation\\', \\'income\\']]\\n# IPython.display allows nice output formatting within the Jupyter notebook\\ndisplay(data.head())\\nTable 4-3 shows the result.\\nTable 4-3. The first five rows of the adult dataset\\nage workclass education gender hours-per-week occupation income\\n0 39 State-gov Bachelors Male 40 Adm-clerical <=50K\\n1 50 Self-emp-not-inc Bachelors Male 13 Exec-managerial <=50K\\n2 38 Private HS-grad Male 40 Handlers-cleaners <=50K'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 227, 'page_label': '214', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='0 39 State-gov Bachelors Male 40 Adm-clerical <=50K\\n1 50 Self-emp-not-inc Bachelors Male 13 Exec-managerial <=50K\\n2 38 Private HS-grad Male 40 Handlers-cleaners <=50K\\n3 53 Private 11th Male 40 Handlers-cleaners <=50K\\n4 28 Private Bachelors Female 40 Prof-specialty <=50K\\nChecking string-encoded categorical data\\nAfter reading a dataset like this, it is often good to first check if a column actually\\ncontains meaningful categorical data. When working with data that was input by\\nhumans (say, users on a website), there might not be a fixed set of categories, and dif‐\\nferences in spelling and capitalization might require preprocessing. For example, it\\nmight be that some people specified gender as “male” and some as “man, ” and we\\n214 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 228, 'page_label': '215', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='might want to represent these two inputs using the same category. A good way to\\ncheck the contents of a column is using the value_counts function of a pandas\\nSeries (the type of a single column in a DataFrame), to show us what the unique val‐\\nues are and how often they appear:\\nIn[3]:\\nprint(data.gender.value_counts())\\nOut[3]:\\n Male      21790\\n Female    10771\\nName: gender, dtype: int64\\nWe can see that there are exactly two values for gender in this dataset, Male and\\nFemale, meaning the data is already in a good format to be represented using one-\\nhot-encoding. In a real application, you should look at all columns and check their\\nvalues. We will skip this here for brevity’s sake.\\nThere is a very simple way to encode the data in pandas, using the get_dummies func‐\\ntion. The get_dummies function automatically transforms all columns that have\\nobject type (like strings) or are categorical (which is a special pandas concept that we\\nhaven’t talked about yet):\\nIn[4]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 228, 'page_label': '215', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='object type (like strings) or are categorical (which is a special pandas concept that we\\nhaven’t talked about yet):\\nIn[4]:\\nprint(\"Original features:\\\\n\", list(data.columns), \"\\\\n\")\\ndata_dummies = pd.get_dummies(data)\\nprint(\"Features after get_dummies:\\\\n\", list(data_dummies.columns))\\nOut[4]:\\nOriginal features:\\n [\\'age\\', \\'workclass\\', \\'education\\', \\'gender\\', \\'hours-per-week\\', \\'occupation\\',\\n  \\'income\\']\\nFeatures after get_dummies:\\n [\\'age\\', \\'hours-per-week\\', \\'workclass_ ?\\', \\'workclass_ Federal-gov\\',\\n  \\'workclass_ Local-gov\\', \\'workclass_ Never-worked\\', \\'workclass_ Private\\',\\n  \\'workclass_ Self-emp-inc\\', \\'workclass_ Self-emp-not-inc\\',\\n  \\'workclass_ State-gov\\', \\'workclass_ Without-pay\\', \\'education_ 10th\\',\\n  \\'education_ 11th\\', \\'education_ 12th\\', \\'education_ 1st-4th\\',\\n   ...\\n  \\'education_ Preschool\\', \\'education_ Prof-school\\', \\'education_ Some-college\\',\\n  \\'gender_ Female\\', \\'gender_ Male\\', \\'occupation_ ?\\',\\n  \\'occupation_ Adm-clerical\\', \\'occupation_ Armed-Forces\\','),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 228, 'page_label': '215', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"...\\n  'education_ Preschool', 'education_ Prof-school', 'education_ Some-college',\\n  'gender_ Female', 'gender_ Male', 'occupation_ ?',\\n  'occupation_ Adm-clerical', 'occupation_ Armed-Forces',\\n  'occupation_ Craft-repair', 'occupation_ Exec-managerial',\\n  'occupation_ Farming-fishing', 'occupation_ Handlers-cleaners',\\n  ...\\n  'occupation_ Tech-support', 'occupation_ Transport-moving',\\n  'income_ <=50K', 'income_ >50K']\\nCategorical Variables | 215\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 229, 'page_label': '216', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Y ou can see that the continuous features age and hours-per-week were not touched,\\nwhile the categorical features were expanded into one new feature for each possible\\nvalue:\\nIn[5]:\\ndata_dummies.head()\\nOut[5]:\\nage hours-\\nper-\\nweek\\nworkclass_ ? workclass_\\nFederal-\\ngov\\nworkclass_\\nLocal-gov\\n… occupation_\\nTech-\\nsupport\\noccupation_\\nTransport-\\nmoving\\nincome_\\n<=50K\\nincome_\\n>50K\\n0 39 40 0.0 0.0 0.0 … 0.0 0.0 1.0 0.0\\n1 50 13 0.0 0.0 0.0 … 0.0 0.0 1.0 0.0\\n2 38 40 0.0 0.0 0.0 … 0.0 0.0 1.0 0.0\\n3 53 40 0.0 0.0 0.0 … 0.0 0.0 1.0 0.0\\n4 28 40 0.0 0.0 0.0 … 0.0 0.0 1.0 0.0\\n5 rows × 46 columns\\nWe can now use the values attribute to convert the data_dummies DataFrame into a\\nNumPy array, and then train a machine learning model on it. Be careful to separate\\nthe target variable (which is now encoded in two income columns) from the data\\nbefore training a model. Including the output variable, or some derived property of\\nthe output variable, into the feature representation is a very common mistake in'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 229, 'page_label': '216', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='before training a model. Including the output variable, or some derived property of\\nthe output variable, into the feature representation is a very common mistake in\\nbuilding supervised machine learning models.\\nBe careful: column indexing in pandas includes the end of the\\nrange, so \\'age\\':\\'occupation_ Transport-moving\\' is inclusive of\\noccupation_ Transport-moving . This is different from slicing a\\nNumPy array, where the end of a range is not included: for exam‐\\nple, np.arange(11)[0:10] doesn’t include the entry with index 10.\\nIn this case, we extract only the columns containing features—that is, all columns\\nfrom age to occupation_ Transport-moving. This range contains all the features but\\nnot the target:\\nIn[6]:\\nfeatures = data_dummies.ix[:, \\'age\\':\\'occupation_ Transport-moving\\']\\n# Extract NumPy arrays\\nX = features.values\\ny = data_dummies[\\'income_ >50K\\'].values\\nprint(\"X.shape: {}  y.shape: {}\".format(X.shape, y.shape))\\n216 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 230, 'page_label': '217', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Out[6]:\\nX.shape: (32561, 44)  y.shape: (32561,)\\nNow the data is represented in a way that scikit-learn can work with, and we can\\nproceed as usual:\\nIn[7]:\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\nlogreg = LogisticRegression()\\nlogreg.fit(X_train, y_train)\\nprint(\"Test score: {:.2f}\".format(logreg.score(X_test, y_test)))\\nOut[7]:\\nTest score: 0.81\\nIn this example, we called get_dummies on a DataFrame containing\\nboth the training and the test data. This is important to ensure cat‐\\negorical values are represented in the same way in the training set\\nand the test set.\\nImagine we have the training and test sets in two different Data\\nFrames. If the \"Private Employee\" value for the workclass feature\\ndoes not appear in the test set, pandas will assume there are only\\nthree possible values for this feature and will create only three new'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 230, 'page_label': '217', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='does not appear in the test set, pandas will assume there are only\\nthree possible values for this feature and will create only three new\\ndummy features. Now our training and test sets have different\\nnumbers of features, and we can’t apply the model we learned on\\nthe training set to the test set anymore. Even worse, imagine the\\nworkclass feature has the values \"Government Employee\"  and\\n\"Private Employee\" in the training set, and \"Self Employed\" and\\n\"Self Employed Incorporated\"  in the test set. In both cases,\\npandas will create two new dummy features, so the encoded Data\\nFrames will have the same number of features. However, the two\\ndummy features have entirely different meanings in the training\\nand test sets. The column that means \"Government Employee\" for\\nthe training set would encode \"Self Employed\" for the test set.\\nIf we built a machine learning model on this data it would work\\nvery badly, because it would assume the columns mean the same'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 230, 'page_label': '217', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the training set would encode \"Self Employed\" for the test set.\\nIf we built a machine learning model on this data it would work\\nvery badly, because it would assume the columns mean the same\\nthings (because they are in the same position) when in fact they\\nmean very different things. To fix this, either call get_dummies on a\\nDataFrame that contains both the training and the test data points,\\nor make sure that the column names are the same for the training\\nand test sets after calling get_dummies, to ensure they have the\\nsame semantics.\\nCategorical Variables | 217'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 231, 'page_label': '218', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Numbers Can Encode Categoricals\\nIn the example of the adult dataset, the categorical variables were encoded as strings.\\nOn the one hand, that opens up the possibility of spelling errors, but on the other\\nhand, it clearly marks a variable as categorical. Often, whether for ease of storage or\\nbecause of the way the data is collected, categorical variables are encoded as integers.\\nFor example, imagine the census data in the adult dataset was collected using a ques‐\\ntionnaire, and the answers for workclass were recorded as 0 (first box ticked), 1 (sec‐\\nond box ticked), 2 (third box ticked), and so on. Now the column will contain\\nnumbers from 0 to 8, instead of strings like \"Private\", and it won’t be immediately\\nobvious to someone looking at the table representing the dataset whether they should\\ntreat this variable as continuous or categorical. Knowing that the numbers indicate\\nemployment status, however, it is clear that these are very distinct states and should'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 231, 'page_label': '218', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='treat this variable as continuous or categorical. Knowing that the numbers indicate\\nemployment status, however, it is clear that these are very distinct states and should\\nnot be modeled by a single continuous variable.\\nCategorical features are often encoded using integers. That they are\\nnumbers doesn’t mean that they should necessarily be treated as\\ncontinuous features. It is not always clear whether an integer fea‐\\nture should be treated as continuous or discrete (and one-hot-\\nencoded). If there is no ordering between the semantics that are\\nencoded (like in the workclass example), the feature must be\\ntreated as discrete. For other cases, like five-star ratings, the better\\nencoding depends on the particular task and data and which\\nmachine learning algorithm is used.\\nThe get_dummies function in pandas treats all numbers as continuous and will not\\ncreate dummy variables for them. To get around this, you can either use scikit-'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 231, 'page_label': '218', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"machine learning algorithm is used.\\nThe get_dummies function in pandas treats all numbers as continuous and will not\\ncreate dummy variables for them. To get around this, you can either use scikit-\\nlearn’s OneHotEncoder, for which you can specify which variables are continuous\\nand which are discrete, or convert numeric columns in the DataFrame to strings. To\\nillustrate, let’s create a DataFrame object with two columns, one containing strings\\nand one containing integers:\\nIn[8]:\\n# create a DataFrame with an integer feature and a categorical string feature\\ndemo_df = pd.DataFrame({'Integer Feature': [0, 1, 2, 1],\\n                        'Categorical Feature': ['socks', 'fox', 'socks', 'box']})\\ndisplay(demo_df)\\nTable 4-4 shows the result.\\n218 | Chapter 4: Representing Data and Engineering Features\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 232, 'page_label': '219', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"Table 4-4. DataFrame containing categorical string features and integer features\\nCategorical Feature Integer Feature\\n0 socks 0\\n1 fox 1\\n2 socks 2\\n3 box 1\\nUsing get_dummies will only encode the string feature and will not change the integer\\nfeature, as you can see in Table 4-5:\\nIn[9]:\\npd.get_dummies(demo_df)\\nTable 4-5. One-hot-encoded version of the data from Table 4-4, leaving the integer feature\\nunchanged\\nInteger Feature Categorical Feature_box Categorical Feature_fox Categorical Feature_socks\\n0 0 0.0 0.0 1.0\\n1 1 0.0 1.0 0.0\\n2 2 0.0 0.0 1.0\\n3 1 1.0 0.0 0.0\\nIf you want dummy variables to be created for the “Integer Feature” column, you can\\nexplicitly list the columns you want to encode using the columns parameter. Then,\\nboth features will be treated as categorical (see Table 4-6):\\nIn[10]:\\ndemo_df['Integer Feature'] = demo_df['Integer Feature'].astype(str)\\npd.get_dummies(demo_df, columns=['Integer Feature', 'Categorical Feature'])\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 232, 'page_label': '219', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"In[10]:\\ndemo_df['Integer Feature'] = demo_df['Integer Feature'].astype(str)\\npd.get_dummies(demo_df, columns=['Integer Feature', 'Categorical Feature'])\\nTable 4-6. One-hot encoding of the data shown in Table 4-4, encoding the integer and string\\nfeatures\\nInteger\\nFeature_0\\nInteger\\nFeature_1\\nInteger\\nFeature_2\\nCategorical\\nFeature_box\\nCategorical\\nFeature_fox\\nCategorical\\nFeature_socks\\n0 1.0 0.0 0.0 0.0 0.0 1.0\\n1 0.0 1.0 0.0 0.0 1.0 0.0\\n2 0.0 0.0 1.0 0.0 0.0 1.0\\n3 0.0 1.0 0.0 1.0 0.0 0.0\\nCategorical Variables | 219\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 233, 'page_label': '220', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Binning, Discretization, Linear Models, and Trees\\nThe best way to represent data depends not only on the semantics of the data, but also\\non the kind of model you are using. Linear models and tree-based models (such as\\ndecision trees, gradient boosted trees, and random forests), two large and very com‐\\nmonly used families, have very different properties when it comes to how they work\\nwith different feature representations. Let’s go back to the wave regression dataset that\\nwe used in Chapter 2. It has only a single input feature. Here is a comparison of a\\nlinear regression model and a decision tree regressor on this dataset (see Figure 4-1):\\nIn[11]:\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.tree import DecisionTreeRegressor\\nX, y = mglearn.datasets.make_wave(n_samples=100)\\nline = np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)\\nreg = DecisionTreeRegressor(min_samples_split=3).fit(X, y)\\nplt.plot(line, reg.predict(line), label=\"decision tree\")'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 233, 'page_label': '220', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='line = np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)\\nreg = DecisionTreeRegressor(min_samples_split=3).fit(X, y)\\nplt.plot(line, reg.predict(line), label=\"decision tree\")\\nreg = LinearRegression().fit(X, y)\\nplt.plot(line, reg.predict(line), label=\"linear regression\")\\nplt.plot(X[:, 0], y, \\'o\\', c=\\'k\\')\\nplt.ylabel(\"Regression output\")\\nplt.xlabel(\"Input feature\")\\nplt.legend(loc=\"best\")\\nAs you know, linear models can only model linear relationships, which are lines in\\nthe case of a single feature. The decision tree can build a much more complex model\\nof the data. However, this is strongly dependent on the representation of the data.\\nOne way to make linear models more powerful on continuous data is to use binning\\n(also known as discretization) of the feature to split it up into multiple features, as\\ndescribed here.\\n220 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 234, 'page_label': '221', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 4-1. Comparing linear regression and a decision tree on the wave dataset\\nWe imagine a partition of the input range for the feature (in this case, the numbers\\nfrom –3 to 3) into a fixed number of bins—say, 10. A data point will then be repre‐\\nsented by which bin it falls into. To determine this, we first have to define the bins. In\\nthis case, we’ll define 10 bins equally spaced between –3 and 3. We use the\\nnp.linspace function for this, creating 11 entries, which will create 10 bins—they are\\nthe spaces in between two consecutive boundaries:\\nIn[12]:\\nbins = np.linspace(-3, 3, 11)\\nprint(\"bins: {}\".format(bins))\\nOut[12]:\\nbins: [-3.  -2.4 -1.8 -1.2 -0.6  0.   0.6  1.2  1.8  2.4  3. ]\\nHere, the first bin contains all data points with feature values –3 to –2.68, the second\\nbin contains all points with feature values from –2.68 to –2.37, and so on.\\nNext, we record for each data point which bin it falls into. This can be easily compu‐\\nted using the np.digitize function:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 234, 'page_label': '221', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='bin contains all points with feature values from –2.68 to –2.37, and so on.\\nNext, we record for each data point which bin it falls into. This can be easily compu‐\\nted using the np.digitize function:\\nBinning, Discretization, Linear Models, and Trees | 221'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 235, 'page_label': '222', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[13]:\\nwhich_bin = np.digitize(X, bins=bins)\\nprint(\"\\\\nData points:\\\\n\", X[:5])\\nprint(\"\\\\nBin membership for data points:\\\\n\", which_bin[:5])\\nOut[13]:\\nData points:\\n [[-0.753]\\n  [ 2.704]\\n  [ 1.392]\\n  [ 0.592]\\n [-2.064]]\\nBin membership for data points:\\n [[ 4]\\n  [10]\\n  [ 8]\\n  [ 6]\\n [ 2]]\\nWhat we did here is transform the single continuous input feature in the wave dataset\\ninto a categorical feature that encodes which bin a data point is in. To use a scikit-\\nlearn model on this data, we transform this discrete feature to a one-hot encoding\\nusing the OneHotEncoder from the preprocessing module. The OneHotEncoder does\\nthe same encoding as pandas.get_dummies, though it currently only works on cate‐\\ngorical variables that are integers:\\nIn[14]:\\nfrom sklearn.preprocessing import OneHotEncoder\\n# transform using the OneHotEncoder\\nencoder = OneHotEncoder(sparse=False)\\n# encoder.fit finds the unique values that appear in which_bin\\nencoder.fit(which_bin)\\n# transform creates the one-hot encoding'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 235, 'page_label': '222', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='encoder = OneHotEncoder(sparse=False)\\n# encoder.fit finds the unique values that appear in which_bin\\nencoder.fit(which_bin)\\n# transform creates the one-hot encoding\\nX_binned = encoder.transform(which_bin)\\nprint(X_binned[:5])\\nOut[14]:\\n[[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\\n [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\\n [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\\n [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]\\nBecause we specified 10 bins, the transformed dataset X_binned now is made up of 10\\nfeatures:\\n222 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 236, 'page_label': '223', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[15]:\\nprint(\"X_binned.shape: {}\".format(X_binned.shape))\\nOut[15]:\\nX_binned.shape: (100, 10)\\nNow we build a new linear regression model and a new decision tree model on the\\none-hot-encoded data. The result is visualized in Figure 4-2 , together with the bin\\nboundaries, shown as dotted black lines:\\nIn[16]:\\nline_binned = encoder.transform(np.digitize(line, bins=bins))\\nreg = LinearRegression().fit(X_binned, y)\\nplt.plot(line, reg.predict(line_binned), label=\\'linear regression binned\\')\\nreg = DecisionTreeRegressor(min_samples_split=3).fit(X_binned, y)\\nplt.plot(line, reg.predict(line_binned), label=\\'decision tree binned\\')\\nplt.plot(X[:, 0], y, \\'o\\', c=\\'k\\')\\nplt.vlines(bins, -3, 3, linewidth=1, alpha=.2)\\nplt.legend(loc=\"best\")\\nplt.ylabel(\"Regression output\")\\nplt.xlabel(\"Input feature\")\\nFigure 4-2. Comparing linear regression and decision tree regression on binned features\\nBinning, Discretization, Linear Models, and Trees | 223'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 237, 'page_label': '224', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='The dashed line and solid line are exactly on top of each other, meaning the linear\\nregression model and the decision tree make exactly the same predictions. For each\\nbin, they predict a constant value. As features are constant within each bin, any\\nmodel must predict the same value for all points within a bin. Comparing what the\\nmodels learned before binning the features and after, we see that the linear model\\nbecame much more flexible, because it now has a different value for each bin, while\\nthe decision tree model got much less flexible. Binning features generally has no ben‐\\neficial effect for tree-based models, as these models can learn to split up the data any‐\\nwhere. In a sense, that means decision trees can learn whatever binning is most useful\\nfor predicting on this data. Additionally, decision trees look at multiple features at\\nonce, while binning is usually done on a per-feature basis. However, the linear model'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 237, 'page_label': '224', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='for predicting on this data. Additionally, decision trees look at multiple features at\\nonce, while binning is usually done on a per-feature basis. However, the linear model\\nbenefited greatly in expressiveness from the transformation of the data.\\nIf there are good reasons to use a linear model for a particular dataset—say, because it\\nis very large and high-dimensional, but some features have nonlinear relations with\\nthe output—binning can be a great way to increase modeling power.\\nInteractions and Polynomials\\nAnother way to enrich a feature representation, particularly for linear models, is\\nadding interaction features and polynomial features of the original data. This kind of\\nfeature engineering is often used in statistical modeling, but it’s also common in many\\npractical machine learning applications.\\nAs a first example, look again at Figure 4-2. The linear model learned a constant value\\nfor each bin in the wave dataset. We know, however, that linear models can learn not'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 237, 'page_label': '224', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"As a first example, look again at Figure 4-2. The linear model learned a constant value\\nfor each bin in the wave dataset. We know, however, that linear models can learn not\\nonly offsets, but also slopes. One way to add a slope to the linear model on the binned\\ndata is to add the original feature (the x-axis in the plot) back in. This leads to an 11-\\ndimensional dataset, as seen in Figure 4-3:\\nIn[17]:\\nX_combined = np.hstack([X, X_binned])\\nprint(X_combined.shape)\\nOut[17]:\\n(100, 11)\\nIn[18]:\\nreg = LinearRegression().fit(X_combined, y)\\nline_combined = np.hstack([line, line_binned])\\nplt.plot(line, reg.predict(line_combined), label='linear regression combined')\\nfor bin in bins:\\n    plt.plot([bin, bin], [-3, 3], ':', c='k')\\n224 | Chapter 4: Representing Data and Engineering Features\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 238, 'page_label': '225', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='plt.legend(loc=\"best\")\\nplt.ylabel(\"Regression output\")\\nplt.xlabel(\"Input feature\")\\nplt.plot(X[:, 0], y, \\'o\\', c=\\'k\\')\\nFigure 4-3. Linear regression using binned features and a single global slope\\nIn this example, the model learned an offset for each bin, together with a slope. The\\nlearned slope is downward, and shared across all the bins—there is a single x-axis fea‐\\nture, which has a single slope. Because the slope is shared across all bins, it doesn’t\\nseem to be very helpful. We would rather have a separate slope for each bin! We can\\nachieve this by adding an interaction or product feature that indicates which bin a\\ndata point is in and where it lies on the x-axis. This feature is a product of the bin\\nindicator and the original feature. Let’s create this dataset:\\nIn[19]:\\nX_product = np.hstack([X_binned, X * X_binned])\\nprint(X_product.shape)\\nOut[19]:\\n(100, 20)\\nThe dataset now has 20 features: the indicators for which bin a data point is in, and a'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 238, 'page_label': '225', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[19]:\\nX_product = np.hstack([X_binned, X * X_binned])\\nprint(X_product.shape)\\nOut[19]:\\n(100, 20)\\nThe dataset now has 20 features: the indicators for which bin a data point is in, and a\\nproduct of the original feature and the bin indicator. Y ou can think of the product\\nInteractions and Polynomials | 225'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 239, 'page_label': '226', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='feature as a separate copy of the x-axis feature for each bin. It is the original feature\\nwithin the bin, and zero everywhere else. Figure 4-4  shows the result of the linear\\nmodel on this new representation:\\nIn[20]:\\nreg = LinearRegression().fit(X_product, y)\\nline_product = np.hstack([line_binned, line * line_binned])\\nplt.plot(line, reg.predict(line_product), label=\\'linear regression product\\')\\nfor bin in bins:\\n    plt.plot([bin, bin], [-3, 3], \\':\\', c=\\'k\\')\\nplt.plot(X[:, 0], y, \\'o\\', c=\\'k\\')\\nplt.ylabel(\"Regression output\")\\nplt.xlabel(\"Input feature\")\\nplt.legend(loc=\"best\")\\nFigure 4-4. Linear regression with a separate slope per bin\\nAs you can see, now each bin has its own offset and slope in this model.\\n226 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 240, 'page_label': '227', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Using binning is one way to expand a continuous feature. Another one is to use poly‐\\nnomials of the original features. For a given feature x, we might want to consider\\nx ** 2, x ** 3, x ** 4, and so on. This is implemented in PolynomialFeatures in\\nthe preprocessing module:\\nIn[21]:\\nfrom sklearn.preprocessing import PolynomialFeatures\\n# include polynomials up to x ** 10:\\n# the default \"include_bias=True\" adds a feature that\\'s constantly 1\\npoly = PolynomialFeatures(degree=10, include_bias=False)\\npoly.fit(X)\\nX_poly = poly.transform(X)\\nUsing a degree of 10 yields 10 features:\\nIn[22]:\\nprint(\"X_poly.shape: {}\".format(X_poly.shape))\\nOut[22]:\\nX_poly.shape: (100, 10)\\nLet’s compare the entries of X_poly to those of X:\\nIn[23]:\\nprint(\"Entries of X:\\\\n{}\".format(X[:5]))\\nprint(\"Entries of X_poly:\\\\n{}\".format(X_poly[:5]))\\nOut[23]:\\nEntries of X:\\n[[-0.753]\\n [ 2.704]\\n [ 1.392]\\n [ 0.592]\\n [-2.064]]\\nEntries of X_poly:\\n[[    -0.753      0.567     -0.427      0.321     -0.242      0.182'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 240, 'page_label': '227', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Out[23]:\\nEntries of X:\\n[[-0.753]\\n [ 2.704]\\n [ 1.392]\\n [ 0.592]\\n [-2.064]]\\nEntries of X_poly:\\n[[    -0.753      0.567     -0.427      0.321     -0.242      0.182\\n      -0.137      0.103     -0.078      0.058]\\n [     2.704      7.313     19.777     53.482    144.632    391.125\\n    1057.714   2860.360   7735.232  20918.278]\\n [     1.392      1.938      2.697      3.754      5.226      7.274\\n      10.125     14.094     19.618     27.307]\\n [     0.592      0.350      0.207      0.123      0.073      0.043\\n       0.025      0.015      0.009      0.005]\\n [    -2.064      4.260     -8.791     18.144    -37.448     77.289\\n    -159.516    329.222   -679.478   1402.367]]\\nY ou can obtain the semantics of the features by calling the get_feature_names\\nmethod, which provides the exponent for each feature:\\nInteractions and Polynomials | 227'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 241, 'page_label': '228', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[24]:\\nprint(\"Polynomial feature names:\\\\n{}\".format(poly.get_feature_names()))\\nOut[24]:\\nPolynomial feature names:\\n[\\'x0\\', \\'x0^2\\', \\'x0^3\\', \\'x0^4\\', \\'x0^5\\', \\'x0^6\\', \\'x0^7\\', \\'x0^8\\', \\'x0^9\\', \\'x0^10\\']\\nY ou can see that the first column of X_poly corresponds exactly to X, while the other\\ncolumns are the powers of the first entry. It’s interesting to see how large some of the\\nvalues can get. The second column has entries above 20,000, orders of magnitude dif‐\\nferent from the rest.\\nUsing polynomial features together with a linear regression model yields the classical\\nmodel of polynomial regression (see Figure 4-5):\\nIn[26]:\\nreg = LinearRegression().fit(X_poly, y)\\nline_poly = poly.transform(line)\\nplt.plot(line, reg.predict(line_poly), label=\\'polynomial linear regression\\')\\nplt.plot(X[:, 0], y, \\'o\\', c=\\'k\\')\\nplt.ylabel(\"Regression output\")\\nplt.xlabel(\"Input feature\")\\nplt.legend(loc=\"best\")\\nFigure 4-5. Linear regression with tenth-degree polynomial features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 241, 'page_label': '228', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='plt.plot(X[:, 0], y, \\'o\\', c=\\'k\\')\\nplt.ylabel(\"Regression output\")\\nplt.xlabel(\"Input feature\")\\nplt.legend(loc=\"best\")\\nFigure 4-5. Linear regression with tenth-degree polynomial features\\n228 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 242, 'page_label': '229', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='As you can see, polynomial features yield a very smooth fit on this one-dimensional\\ndata. However, polynomials of high degree tend to behave in extreme ways on the\\nboundaries or in regions with little data.\\nAs a comparison, here is a kernel SVM model learned on the original data, without\\nany transformation (see Figure 4-6):\\nIn[26]:\\nfrom sklearn.svm import SVR\\nfor gamma in [1, 10]:\\n    svr = SVR(gamma=gamma).fit(X, y)\\n    plt.plot(line, svr.predict(line), label=\\'SVR gamma={}\\'.format(gamma))\\nplt.plot(X[:, 0], y, \\'o\\', c=\\'k\\')\\nplt.ylabel(\"Regression output\")\\nplt.xlabel(\"Input feature\")\\nplt.legend(loc=\"best\")\\nFigure 4-6. Comparison of different gamma parameters for an SVM with RBF kernel\\nUsing a more complex model, a kernel SVM, we are able to learn a similarly complex\\nprediction to the polynomial regression without an explicit transformation of the\\nfeatures.\\nInteractions and Polynomials | 229'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 243, 'page_label': '230', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='As a more realistic application of interactions and polynomials, let’s look again at the\\nBoston Housing dataset. We already used polynomial features on this dataset in\\nChapter 2. Now let’s have a look at how these features were constructed, and at how\\nmuch the polynomial features help. First we load the data, and rescale it to be\\nbetween 0 and 1 using MinMaxScaler:\\nIn[27]:\\nfrom sklearn.datasets import load_boston\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import MinMaxScaler\\nboston = load_boston()\\nX_train, X_test, y_train, y_test = train_test_split\\n    (boston.data, boston.target, random_state=0)\\n# rescale data\\nscaler = MinMaxScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\nNow, we extract polynomial features and interactions up to a degree of 2:\\nIn[28]:\\npoly = PolynomialFeatures(degree=2).fit(X_train_scaled)\\nX_train_poly = poly.transform(X_train_scaled)\\nX_test_poly = poly.transform(X_test_scaled)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 243, 'page_label': '230', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[28]:\\npoly = PolynomialFeatures(degree=2).fit(X_train_scaled)\\nX_train_poly = poly.transform(X_train_scaled)\\nX_test_poly = poly.transform(X_test_scaled)\\nprint(\"X_train.shape: {}\".format(X_train.shape))\\nprint(\"X_train_poly.shape: {}\".format(X_train_poly.shape))\\nOut[28]:\\nX_train.shape: (379, 13)\\nX_train_poly.shape: (379, 105)\\nThe data originally had 13 features, which were expanded into 105 interaction fea‐\\ntures. These new features represent all possible interactions between two different\\noriginal features, as well as the square of each original feature. degree=2 here means\\nthat we look at all features that are the product of up to two original features. The\\nexact correspondence between input and output features can be found using the\\nget_feature_names method:\\nIn[29]:\\nprint(\"Polynomial feature names:\\\\n{}\".format(poly.get_feature_names()))\\nOut[29]:\\nPolynomial feature names:\\n[\\'1\\', \\'x0\\', \\'x1\\', \\'x2\\', \\'x3\\', \\'x4\\', \\'x5\\', \\'x6\\', \\'x7\\', \\'x8\\', \\'x9\\', \\'x10\\','),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 243, 'page_label': '230', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[29]:\\nprint(\"Polynomial feature names:\\\\n{}\".format(poly.get_feature_names()))\\nOut[29]:\\nPolynomial feature names:\\n[\\'1\\', \\'x0\\', \\'x1\\', \\'x2\\', \\'x3\\', \\'x4\\', \\'x5\\', \\'x6\\', \\'x7\\', \\'x8\\', \\'x9\\', \\'x10\\',\\n\\'x11\\', \\'x12\\', \\'x0^2\\', \\'x0 x1\\', \\'x0 x2\\', \\'x0 x3\\', \\'x0 x4\\', \\'x0 x5\\', \\'x0 x6\\',\\n\\'x0 x7\\', \\'x0 x8\\', \\'x0 x9\\', \\'x0 x10\\', \\'x0 x11\\', \\'x0 x12\\', \\'x1^2\\', \\'x1 x2\\',\\n230 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 244, 'page_label': '231', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='\\'x1 x3\\', \\'x1 x4\\', \\'x1 x5\\', \\'x1 x6\\', \\'x1 x7\\', \\'x1 x8\\', \\'x1 x9\\', \\'x1 x10\\',\\n\\'x1 x11\\', \\'x1 x12\\', \\'x2^2\\', \\'x2 x3\\', \\'x2 x4\\', \\'x2 x5\\', \\'x2 x6\\', \\'x2 x7\\',\\n\\'x2 x8\\', \\'x2 x9\\', \\'x2 x10\\', \\'x2 x11\\', \\'x2 x12\\', \\'x3^2\\', \\'x3 x4\\', \\'x3 x5\\',\\n\\'x3 x6\\', \\'x3 x7\\', \\'x3 x8\\', \\'x3 x9\\', \\'x3 x10\\', \\'x3 x11\\', \\'x3 x12\\', \\'x4^2\\',\\n\\'x4 x5\\', \\'x4 x6\\', \\'x4 x7\\', \\'x4 x8\\', \\'x4 x9\\', \\'x4 x10\\', \\'x4 x11\\', \\'x4 x12\\',\\n\\'x5^2\\', \\'x5 x6\\', \\'x5 x7\\', \\'x5 x8\\', \\'x5 x9\\', \\'x5 x10\\', \\'x5 x11\\', \\'x5 x12\\',\\n\\'x6^2\\', \\'x6 x7\\', \\'x6 x8\\', \\'x6 x9\\', \\'x6 x10\\', \\'x6 x11\\', \\'x6 x12\\', \\'x7^2\\',\\n\\'x7 x8\\', \\'x7 x9\\', \\'x7 x10\\', \\'x7 x11\\', \\'x7 x12\\', \\'x8^2\\', \\'x8 x9\\', \\'x8 x10\\',\\n\\'x8 x11\\', \\'x8 x12\\', \\'x9^2\\', \\'x9 x10\\', \\'x9 x11\\', \\'x9 x12\\', \\'x10^2\\', \\'x10 x11\\',\\n\\'x10 x12\\', \\'x11^2\\', \\'x11 x12\\', \\'x12^2\\']\\nThe first new feature is a constant feature, called \"1\" here. The next 13 features are\\nthe original features (called \"x0\" to \"x12\"). Then follows the first feature squared\\n(\"x0^2\") and combinations of the first and the other features.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 244, 'page_label': '231', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the original features (called \"x0\" to \"x12\"). Then follows the first feature squared\\n(\"x0^2\") and combinations of the first and the other features.\\nLet’s compare the performance using Ridge on the data with and without interac‐\\ntions:\\nIn[30]:\\nfrom sklearn.linear_model import Ridge\\nridge = Ridge().fit(X_train_scaled, y_train)\\nprint(\"Score without interactions: {:.3f}\".format(\\n    ridge.score(X_test_scaled, y_test)))\\nridge = Ridge().fit(X_train_poly, y_train)\\nprint(\"Score with interactions: {:.3f}\".format(\\n    ridge.score(X_test_poly, y_test)))\\nOut[30]:\\nScore without interactions: 0.621\\nScore with interactions: 0.753\\nClearly, the interactions and polynomial features gave us a good boost in perfor‐\\nmance when using Ridge. When using a more complex model like a random forest,\\nthe story is a bit different, though:\\nIn[31]:\\nfrom sklearn.ensemble import RandomForestRegressor\\nrf = RandomForestRegressor(n_estimators=100).fit(X_train_scaled, y_train)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 244, 'page_label': '231', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the story is a bit different, though:\\nIn[31]:\\nfrom sklearn.ensemble import RandomForestRegressor\\nrf = RandomForestRegressor(n_estimators=100).fit(X_train_scaled, y_train)\\nprint(\"Score without interactions: {:.3f}\".format(\\n    rf.score(X_test_scaled, y_test)))\\nrf = RandomForestRegressor(n_estimators=100).fit(X_train_poly, y_train)\\nprint(\"Score with interactions: {:.3f}\".format(rf.score(X_test_poly, y_test)))\\nOut[31]:\\nScore without interactions: 0.799\\nScore with interactions: 0.763\\nInteractions and Polynomials | 231'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 245, 'page_label': '232', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Y ou can see that even without additional features, the random forest beats the\\nperformance of Ridge. Adding interactions and polynomials actually decreases per‐\\nformance slightly.\\nUnivariate Nonlinear Transformations\\nWe just saw that adding squared or cubed features can help linear models for regres‐\\nsion. There are other transformations that often prove useful for transforming certain\\nfeatures: in particular, applying mathematical functions like log, exp, or sin. While\\ntree-based models only care about the ordering of the features, linear models and\\nneural networks are very tied to the scale and distribution of each feature, and if there\\nis a nonlinear relation between the feature and the target, that becomes hard to model\\n—particularly in regression. The functions log and exp can help by adjusting the rel‐\\native scales in the data so that they can be captured better by a linear model or neural\\nnetwork. We saw an application of that in Chapter 2 with the memory price data. The'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 245, 'page_label': '232', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='ative scales in the data so that they can be captured better by a linear model or neural\\nnetwork. We saw an application of that in Chapter 2 with the memory price data. The\\nsin and cos functions can come in handy when dealing with data that encodes peri‐\\nodic patterns.\\nMost models work best when each feature (and in regression also the target) is loosely\\nGaussian distributed—that is, a histogram of each feature should have something\\nresembling the familiar “bell curve” shape. Using transformations like log and exp is\\na hacky but simple and efficient way to achieve this. A particularly common case\\nwhen such a transformation can be helpful is when dealing with integer count data.\\nBy count data, we mean features like “how often did user A log in?” Counts are never\\nnegative, and often follow particular statistical patterns. We are using a synthetic\\ndataset of counts here that has properties similar to those you can find in the wild.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 245, 'page_label': '232', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='negative, and often follow particular statistical patterns. We are using a synthetic\\ndataset of counts here that has properties similar to those you can find in the wild.\\nThe features are all integer-valued, while the response is continuous:\\nIn[32]:\\nrnd = np.random.RandomState(0)\\nX_org = rnd.normal(size=(1000, 3))\\nw = rnd.normal(size=3)\\nX = rnd.poisson(10 * np.exp(X_org))\\ny = np.dot(X_org, w)\\nLet’s look at the first 10 entries of the first feature. All are integer values and positive,\\nbut apart from that it’s hard to make out a particular pattern.\\nIf we count the appearance of each value, the distribution of values becomes clearer:\\n232 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 246, 'page_label': '233', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[33]:\\nprint(\"Number of feature appearances:\\\\n{}\".format(np.bincount(X[:, 0])))\\nOut[33]:\\nNumber of feature appearances:\\n[28 38 68 48 61 59 45 56 37 40 35 34 36 26 23 26 27 21 23 23 18 21 10  9 17\\n  9  7 14 12  7  3  8  4  5  5  3  4  2  4  1  1  3  2  5  3  8  2  5  2  1\\n  2  3  3  2  2  3  3  0  1  2  1  0  0  3  1  0  0  0  1  3  0  1  0  2  0\\n  1  1  0  0  0  0  1  0  0  2  2  0  1  1  0  0  0  0  1  1  0  0  0  0  0\\n  0  0  1  0  0  0  0  0  1  1  0  0  1  0  0  0  0  0  0  0  1  0  0  0  0\\n  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]\\nThe value 2 seems to be the most common, with 62 appearances ( bincount always\\nstarts at 0), and the counts for higher values fall quickly. However, there are some\\nvery high values, like 134 appearing twice. We visualize the counts in Figure 4-7:\\nIn[34]:\\nbins = np.bincount(X[:, 0])\\nplt.bar(range(len(bins)), bins, color=\\'w\\')\\nplt.ylabel(\"Number of appearances\")\\nplt.xlabel(\"Value\")\\nFigure 4-7. Histogram of feature values for X[0]'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 246, 'page_label': '233', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[34]:\\nbins = np.bincount(X[:, 0])\\nplt.bar(range(len(bins)), bins, color=\\'w\\')\\nplt.ylabel(\"Number of appearances\")\\nplt.xlabel(\"Value\")\\nFigure 4-7. Histogram of feature values for X[0]\\nUnivariate Nonlinear Transformations | 233'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 247, 'page_label': '234', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='1 This is a Poisson distribution, which is quite fundamental to count data.\\nFeatures X[:, 1] and X[:, 2] have similar properties. This kind of distribution of\\nvalues (many small ones and a few very large ones) is very common in practice. 1\\nHowever, it is something most linear models can’t handle very well. Let’s try to fit a\\nridge regression to this model:\\nIn[35]:\\nfrom sklearn.linear_model import Ridge\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\nscore = Ridge().fit(X_train, y_train).score(X_test, y_test)\\nprint(\"Test score: {:.3f}\".format(score))\\nOut[35]:\\nTest score: 0.622\\nAs you can see from the relatively low R2 score, Ridge was not able to really capture\\nthe relationship between X and y. Applying a logarithmic transformation can help,\\nthough. Because the value 0 appears in the data (and the logarithm is not defined at\\n0), we can’t actually just apply log, but we have to compute log(X + 1):\\nIn[36]:\\nX_train_log = np.log(X_train + 1)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 247, 'page_label': '234', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='though. Because the value 0 appears in the data (and the logarithm is not defined at\\n0), we can’t actually just apply log, but we have to compute log(X + 1):\\nIn[36]:\\nX_train_log = np.log(X_train + 1)\\nX_test_log = np.log(X_test + 1)\\nAfter the transformation, the distribution of the data is less asymmetrical and doesn’t\\nhave very large outliers anymore (see Figure 4-8):\\nIn[37]:\\nplt.hist(np.log(X_train_log[:, 0] + 1), bins=25, color=\\'gray\\')\\nplt.ylabel(\"Number of appearances\")\\nplt.xlabel(\"Value\")\\n234 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 248, 'page_label': '235', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='2 This is a very crude approximation of using Poisson regression, which would be the proper solution from a\\nprobabilistic standpoint.\\nFigure 4-8. Histogram of feature values for X[0] after logarithmic transformation\\nBuilding a ridge model on the new data provides a much better fit:\\nIn[38]:\\nscore = Ridge().fit(X_train_log, y_train).score(X_test_log, y_test)\\nprint(\"Test score: {:.3f}\".format(score))\\nOut[38]:\\nTest score: 0.875\\nFinding the transformation that works best for each combination of dataset and\\nmodel is somewhat of an art. In this example, all the features had the same properties.\\nThis is rarely the case in practice, and usually only a subset of the features should be\\ntransformed, or sometimes each feature needs to be transformed in a different way.\\nAs we mentioned earlier, these kinds of transformations are irrelevant for tree-based\\nmodels but might be essential for linear models. Sometimes it is also a good idea to'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 248, 'page_label': '235', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='As we mentioned earlier, these kinds of transformations are irrelevant for tree-based\\nmodels but might be essential for linear models. Sometimes it is also a good idea to\\ntransform the target variable y in regression. Trying to predict counts (say, number of\\norders) is a fairly common task, and using the log(y + 1)  transformation often\\nhelps.2\\nUnivariate Nonlinear Transformations | 235'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 249, 'page_label': '236', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='As you saw in the previous examples, binning, polynomials, and interactions can\\nhave a huge influence on how models perform on a given dataset. This is particularly\\ntrue for less complex models like linear models and naive Bayes models. Tree-based\\nmodels, on the other hand, are often able to discover important interactions them‐\\nselves, and don’t require transforming the data explicitly most of the time. Other\\nmodels, like SVMs, nearest neighbors, and neural networks, might sometimes benefit\\nfrom using binning, interactions, or polynomials, but the implications there are usu‐\\nally much less clear than in the case of linear models.\\nAutomatic Feature Selection\\nWith so many ways to create new features, you might get tempted to increase the\\ndimensionality of the data way beyond the number of original features. However,\\nadding more features makes all models more complex, and so increases the chance of\\noverfitting. When adding new features, or with high-dimensional datasets in general,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 249, 'page_label': '236', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='adding more features makes all models more complex, and so increases the chance of\\noverfitting. When adding new features, or with high-dimensional datasets in general,\\nit can be a good idea to reduce the number of features to only the most useful ones,\\nand discard the rest. This can lead to simpler models that generalize better. But how\\ncan you know how good each feature is? There are three basic strategies: univariate\\nstatistics, model-based selection , and iterative selection. We will discuss all three of\\nthem in detail. All of these methods are supervised methods, meaning they need the\\ntarget for fitting the model. This means we need to split the data into training and test\\nsets, and fit the feature selection only on the training part of the data.\\nUnivariate Statistics\\nIn univariate statistics, we compute whether there is a statistically significant relation‐\\nship between each feature and the target. Then the features that are related with the'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 249, 'page_label': '236', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Univariate Statistics\\nIn univariate statistics, we compute whether there is a statistically significant relation‐\\nship between each feature and the target. Then the features that are related with the\\nhighest confidence are selected. In the case of classification, this is also known as\\nanalysis of variance (ANOV A). A key property of these tests is that they are univari‐\\nate, meaning that they only consider each feature individually. Consequently, a fea‐\\nture will be discarded if it is only informative when combined with another feature.\\nUnivariate tests are often very fast to compute, and don’t require building a model.\\nOn the other hand, they are completely independent of the model that you might\\nwant to apply after the feature selection.\\nTo use univariate feature selection in scikit-learn, you need to choose a test, usu‐\\nally either f_classif (the default) for classification or f_regression for regression,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 249, 'page_label': '236', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='To use univariate feature selection in scikit-learn, you need to choose a test, usu‐\\nally either f_classif (the default) for classification or f_regression for regression,\\nand a method to discard features based on the p-values determined in the test. All\\nmethods for discarding parameters use a threshold to discard all features with too\\nhigh a p-value (which means they are unlikely to be related to the target). The meth‐\\nods differ in how they compute this threshold, with the simplest ones being SelectKB\\nest, which selects a fixed number k of features, and SelectPercentile, which selects\\na fixed percentage of features. Let’s apply the feature selection for classification on the\\n236 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 250, 'page_label': '237', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='cancer dataset. To make the task a bit harder, we’ll add some noninformative noise\\nfeatures to the data. We expect the feature selection to be able to identify the features\\nthat are noninformative and remove them:\\nIn[39]:\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.feature_selection import SelectPercentile\\nfrom sklearn.model_selection import train_test_split\\ncancer = load_breast_cancer()\\n# get deterministic random numbers\\nrng = np.random.RandomState(42)\\nnoise = rng.normal(size=(len(cancer.data), 50))\\n# add noise features to the data\\n# the first 30 features are from the dataset, the next 50 are noise\\nX_w_noise = np.hstack([cancer.data, noise])\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X_w_noise, cancer.target, random_state=0, test_size=.5)\\n# use f_classif (the default) and SelectPercentile to select 50% of features\\nselect = SelectPercentile(percentile=50)\\nselect.fit(X_train, y_train)\\n# transform training set\\nX_train_selected = select.transform(X_train)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 250, 'page_label': '237', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='select = SelectPercentile(percentile=50)\\nselect.fit(X_train, y_train)\\n# transform training set\\nX_train_selected = select.transform(X_train)\\nprint(\"X_train.shape: {}\".format(X_train.shape))\\nprint(\"X_train_selected.shape: {}\".format(X_train_selected.shape))\\nOut[39]:\\nX_train.shape: (284, 80)\\nX_train_selected.shape: (284, 40)\\nAs you can see, the number of features was reduced from 80 to 40 (50 percent of the\\noriginal number of features). We can find out which features have been selected using\\nthe get_support method, which returns a Boolean mask of the selected features\\n(visualized in Figure 4-9):\\nIn[40]:\\nmask = select.get_support()\\nprint(mask)\\n# visualize the mask -- black is True, white is False\\nplt.matshow(mask.reshape(1, -1), cmap=\\'gray_r\\')\\nplt.xlabel(\"Sample index\")\\nOut[40]:\\n[ True  True  True  True  True  True  True  True  True False  True False\\n  True  True  True  True  True  True False False  True  True  True  True'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 250, 'page_label': '237', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='plt.xlabel(\"Sample index\")\\nOut[40]:\\n[ True  True  True  True  True  True  True  True  True False  True False\\n  True  True  True  True  True  True False False  True  True  True  True\\n  True  True  True  True  True  True False False False  True False  True\\nAutomatic Feature Selection | 237'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 251, 'page_label': '238', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='False False  True False False False False  True False False  True False\\n False  True False  True False False False False False False  True False\\n  True False False False False  True False  True False False False False\\n  True  True False  True False False False False]\\nFigure 4-9. Features selected by SelectPercentile\\nAs you can see from the visualization of the mask, most of the selected features are\\nthe original features, and most of the noise features were removed. However, the\\nrecovery of the original features is not perfect. Let’s compare the performance of\\nlogistic regression on all features against the performance using only the selected\\nfeatures:\\nIn[41]:\\nfrom sklearn.linear_model import LogisticRegression\\n# transform test data\\nX_test_selected = select.transform(X_test)\\nlr = LogisticRegression()\\nlr.fit(X_train, y_train)\\nprint(\"Score with all features: {:.3f}\".format(lr.score(X_test, y_test)))\\nlr.fit(X_train_selected, y_train)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 251, 'page_label': '238', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='lr = LogisticRegression()\\nlr.fit(X_train, y_train)\\nprint(\"Score with all features: {:.3f}\".format(lr.score(X_test, y_test)))\\nlr.fit(X_train_selected, y_train)\\nprint(\"Score with only selected features: {:.3f}\".format(\\n    lr.score(X_test_selected, y_test)))\\nOut[41]:\\nScore with all features: 0.930\\nScore with only selected features: 0.940\\nIn this case, removing the noise features improved performance, even though some\\nof the original features were lost. This was a very simple synthetic example, and out‐\\ncomes on real data are usually mixed. Univariate feature selection can still be very\\nhelpful, though, if there is such a large number of features that building a model on\\nthem is infeasible, or if you suspect that many features are completely uninformative.\\nModel-Based Feature Selection\\nModel-based feature selection uses a supervised machine learning model to judge the\\nimportance of each feature, and keeps only the most important ones. The supervised'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 251, 'page_label': '238', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Model-Based Feature Selection\\nModel-based feature selection uses a supervised machine learning model to judge the\\nimportance of each feature, and keeps only the most important ones. The supervised\\nmodel that is used for feature selection doesn’t need to be the same model that is used\\nfor the final supervised modeling. The feature selection model needs to provide some\\nmeasure of importance for each feature, so that they can be ranked by this measure.\\nDecision trees and decision tree–based models provide a feature_importances_\\n238 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 252, 'page_label': '239', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='attribute, which directly encodes the importance of each feature. Linear models have\\ncoefficients, which can also be used to capture feature importances by considering the\\nabsolute values. As we saw in Chapter 2, linear models with L1 penalty learn sparse\\ncoefficients, which only use a small subset of features. This can be viewed as a form of\\nfeature selection for the model itself, but can also be used as a preprocessing step to\\nselect features for another model. In contrast to univariate selection, model-based\\nselection considers all features at once, and so can capture interactions (if the model\\ncan capture them). To use model-based feature selection, we need to use the\\nSelectFromModel transformer:\\nIn[42]:\\nfrom sklearn.feature_selection import SelectFromModel\\nfrom sklearn.ensemble import RandomForestClassifier\\nselect = SelectFromModel(\\n    RandomForestClassifier(n_estimators=100, random_state=42),\\n    threshold=\"median\")'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 252, 'page_label': '239', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='from sklearn.ensemble import RandomForestClassifier\\nselect = SelectFromModel(\\n    RandomForestClassifier(n_estimators=100, random_state=42),\\n    threshold=\"median\")\\nThe SelectFromModel class selects all features that have an importance measure of\\nthe feature (as provided by the supervised model) greater than the provided thresh‐\\nold. To get a comparable result to what we got with univariate feature selection, we\\nused the median as a threshold, so that half of the features will be selected. We use a\\nrandom forest classifier with 100 trees to compute the feature importances. This is a\\nquite complex model and much more powerful than using univariate tests. Now let’s\\nactually fit the model:\\nIn[43]:\\nselect.fit(X_train, y_train)\\nX_train_l1 = select.transform(X_train)\\nprint(\"X_train.shape: {}\".format(X_train.shape))\\nprint(\"X_train_l1.shape: {}\".format(X_train_l1.shape))\\nOut[43]:\\nX_train.shape: (284, 80)\\nX_train_l1.shape: (284, 40)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 252, 'page_label': '239', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='print(\"X_train.shape: {}\".format(X_train.shape))\\nprint(\"X_train_l1.shape: {}\".format(X_train_l1.shape))\\nOut[43]:\\nX_train.shape: (284, 80)\\nX_train_l1.shape: (284, 40)\\nAgain, we can have a look at the features that were selected (Figure 4-10):\\nIn[44]:\\nmask = select.get_support()\\n# visualize the mask -- black is True, white is False\\nplt.matshow(mask.reshape(1, -1), cmap=\\'gray_r\\')\\nplt.xlabel(\"Sample index\")\\nFigure 4-10. Features selected by SelectFromModel using the RandomForestClassifier\\nAutomatic Feature Selection | 239'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 253, 'page_label': '240', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='This time, all but two of the original features were selected. Because we specified to\\nselect 40 features, some of the noise features are also selected. Let’s take a look at the\\nperformance:\\nIn[45]:\\nX_test_l1 = select.transform(X_test)\\nscore = LogisticRegression().fit(X_train_l1, y_train).score(X_test_l1, y_test)\\nprint(\"Test score: {:.3f}\".format(score))\\nOut[45]:\\nTest score: 0.951\\nWith the better feature selection, we also gained some improvements here.\\nIterative Feature Selection\\nIn univariate testing we used no model, while in model-based selection we used a sin‐\\ngle model to select features. In iterative feature selection, a series of models are built,\\nwith varying numbers of features. There are two basic methods: starting with no fea‐\\ntures and adding features one by one until some stopping criterion is reached, or\\nstarting with all features and removing features one by one until some stopping crite‐\\nrion is reached. Because a series of models are built, these methods are much more'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 253, 'page_label': '240', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='starting with all features and removing features one by one until some stopping crite‐\\nrion is reached. Because a series of models are built, these methods are much more\\ncomputationally expensive than the methods we discussed previously. One particular\\nmethod of this kind is recursive feature elimination (RFE), which starts with all fea‐\\ntures, builds a model, and discards the least important feature according to the\\nmodel. Then a new model is built using all but the discarded feature, and so on until\\nonly a prespecified number of features are left. For this to work, the model used for\\nselection needs to provide some way to determine feature importance, as was the case\\nfor the model-based selection. Here, we use the same random forest model that we\\nused earlier, and get the results shown in Figure 4-11:\\nIn[46]:\\nfrom sklearn.feature_selection import RFE\\nselect = RFE(RandomForestClassifier(n_estimators=100, random_state=42),\\n             n_features_to_select=40)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 253, 'page_label': '240', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[46]:\\nfrom sklearn.feature_selection import RFE\\nselect = RFE(RandomForestClassifier(n_estimators=100, random_state=42),\\n             n_features_to_select=40)\\nselect.fit(X_train, y_train)\\n# visualize the selected features:\\nmask = select.get_support()\\nplt.matshow(mask.reshape(1, -1), cmap=\\'gray_r\\')\\nplt.xlabel(\"Sample index\")\\n240 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 254, 'page_label': '241', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 4-11. Features selected by recursive feature elimination with the random forest\\nclassifier model\\nThe feature selection got better compared to the univariate and model-based selec‐\\ntion, but one feature was still missed. Running this code also takes significantly longer\\nthan that for the model-based selection, because a random forest model is trained 40\\ntimes, once for each feature that is dropped. Let’s test the accuracy of the logistic\\nregression model when using RFE for feature selection:\\nIn[47]:\\nX_train_rfe= select.transform(X_train)\\nX_test_rfe= select.transform(X_test)\\nscore = LogisticRegression().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)\\nprint(\"Test score: {:.3f}\".format(score))\\nOut[47]:\\nTest score: 0.951\\nWe can also use the model used inside the RFE to make predictions. This uses only\\nthe feature set that was selected:\\nIn[48]:\\nprint(\"Test score: {:.3f}\".format(select.score(X_test, y_test)))\\nOut[48]:\\nTest score: 0.951'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 254, 'page_label': '241', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the feature set that was selected:\\nIn[48]:\\nprint(\"Test score: {:.3f}\".format(select.score(X_test, y_test)))\\nOut[48]:\\nTest score: 0.951\\nHere, the performance of the random forest used inside the RFE is the same as that\\nachieved by training a logistic regression model on top of the selected features. In\\nother words, once we’ve selected the right features, the linear model performs as well\\nas the random forest.\\nIf you are unsure when selecting what to use as input to your machine learning algo‐\\nrithms, automatic feature selection can be quite helpful. It is also great for reducing\\nthe amount of features needed—for example, to speed up prediction or to allow for\\nmore interpretable models. In most real-world cases, applying feature selection is\\nunlikely to provide large gains in performance. However, it is still a valuable tool in\\nthe toolbox of the feature engineer.\\nAutomatic Feature Selection | 241'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 255, 'page_label': '242', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Utilizing Expert Knowledge\\nFeature engineering is often an important place to use expert knowledge for a particu‐\\nlar application. While the purpose of machine learning in many cases is to avoid hav‐\\ning to create a set of expert-designed rules, that doesn’t mean that prior knowledge of\\nthe application or domain should be discarded. Often, domain experts can help in\\nidentifying useful features that are much more informative than the initial represen‐\\ntation of the data. Imagine you work for a travel agency and want to predict flight\\nprices. Let’s say you have a record of prices together with dates, airlines, start loca‐\\ntions, and destinations. A machine learning model might be able to build a decent\\nmodel from that. Some important factors in flight prices, however, cannot be learned.\\nFor example, flights are usually more expensive during peak vacation months and\\naround holidays. While the dates of some holidays (like Christmas) are fixed, and'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 255, 'page_label': '242', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='For example, flights are usually more expensive during peak vacation months and\\naround holidays. While the dates of some holidays (like Christmas) are fixed, and\\ntheir effect can therefore be learned from the date, others might depend on the phases\\nof the moon (like Hanukkah and Easter) or be set by authorities (like school holi‐\\ndays). These events cannot be learned from the data if each flight is only recorded\\nusing the (Gregorian) date. However, it is easy to add a feature that encodes whether a\\nflight was on, preceding, or following a public or school holiday. In this way, prior\\nknowledge about the nature of the task can be encoded in the features to aid a\\nmachine learning algorithm. Adding a feature does not force a machine learning\\nalgorithm to use it, and even if the holiday information turns out to be noninforma‐\\ntive for flight prices, augmenting the data with this information doesn’t hurt.\\nWe’ll now look at one particular case of using expert knowledge—though in this case'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 255, 'page_label': '242', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='tive for flight prices, augmenting the data with this information doesn’t hurt.\\nWe’ll now look at one particular case of using expert knowledge—though in this case\\nit might be more rightfully called “common sense. ” The task is predicting bicycle rent‐\\nals in front of Andreas’s house.\\nIn New Y ork, Citi Bike operates a network of bicycle rental stations with a subscrip‐\\ntion system. The stations are all over the city and provide a convenient way to get\\naround. Bike rental data is made public in an anonymized form  and has been ana‐\\nlyzed in various ways. The task we want to solve is to predict for a given time and day\\nhow many people will rent a bike in front of Andreas’s house—so he knows if any\\nbikes will be left for him.\\nWe first load the data for August 2015 for this particular station as a pandas Data\\nFrame. We resample the data into three-hour intervals to obtain the main trends for\\neach day:\\nIn[49]:\\ncitibike = mglearn.datasets.load_citibike()'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 255, 'page_label': '242', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Frame. We resample the data into three-hour intervals to obtain the main trends for\\neach day:\\nIn[49]:\\ncitibike = mglearn.datasets.load_citibike()\\n242 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 256, 'page_label': '243', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[50]:\\nprint(\"Citi Bike data:\\\\n{}\".format(citibike.head()))\\nOut[50]:\\nCiti Bike data:\\nstarttime\\n2015-08-01 00:00:00     3.0\\n2015-08-01 03:00:00     0.0\\n2015-08-01 06:00:00     9.0\\n2015-08-01 09:00:00    41.0\\n2015-08-01 12:00:00    39.0\\nFreq: 3H, Name: one, dtype: float64\\nThe following example shows a visualization of the rental frequencies for the whole\\nmonth (Figure 4-12):\\nIn[51]:\\nplt.figure(figsize=(10, 3))\\nxticks = pd.date_range(start=citibike.index.min(), end=citibike.index.max(),\\n                       freq=\\'D\\')\\nplt.xticks(xticks, xticks.strftime(\"%a %m-%d\"), rotation=90, ha=\"left\")\\nplt.plot(citibike, linewidth=1)\\nplt.xlabel(\"Date\")\\nplt.ylabel(\"Rentals\")\\nFigure 4-12. Number of bike rentals over time for a selected Citi Bike station\\nLooking at the data, we can clearly distinguish day and night for each 24-hour inter‐\\nval. The patterns for weekdays and weekends also seem to be quite different. When'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 256, 'page_label': '243', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Looking at the data, we can clearly distinguish day and night for each 24-hour inter‐\\nval. The patterns for weekdays and weekends also seem to be quite different. When\\nevaluating a prediction task on a time series like this, we usually want to learn from\\nthe past and predict for the future. This means when doing a split into a training and a\\ntest set, we want to use all the data up to a certain date as the training set and all the\\ndata past that date as the test set. This is how we would usually use time series predic‐\\ntion: given everything that we know about rentals in the past, what do we think will\\nUtilizing Expert Knowledge | 243'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 257, 'page_label': '244', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='happen tomorrow? We will use the first 184 data points, corresponding to the first 23\\ndays, as our training set, and the remaining 64 data points, corresponding to the\\nremaining 8 days, as our test set.\\nThe only feature that we are using in our prediction task is the date and time when a\\nparticular number of rentals occurred. So, the input feature is the date and time—say,\\n2015-08-01 00:00:00 —and the output is the number of rentals in the following\\nthree hours (three in this case, according to our DataFrame).\\nA (surprisingly) common way that dates are stored on computers is using POSIX\\ntime, which is the number of seconds since January 1970 00:00:00 (aka the beginning\\nof Unix time). As a first try, we can use this single integer feature as our data repre‐\\nsentation:\\nIn[52]:\\n# extract the target values (number of rentals)\\ny = citibike.values\\n# convert the time to POSIX time using \"%s\"\\nX = citibike.index.strftime(\"%s\").astype(\"int\").reshape(-1, 1)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 257, 'page_label': '244', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='sentation:\\nIn[52]:\\n# extract the target values (number of rentals)\\ny = citibike.values\\n# convert the time to POSIX time using \"%s\"\\nX = citibike.index.strftime(\"%s\").astype(\"int\").reshape(-1, 1)\\nWe first define a function to split the data into training and test sets, build the model,\\nand visualize the result:\\nIn[54]:\\n# use the first 184 data points for training, and the rest for testing\\nn_train = 184\\n# function to evaluate and plot a regressor on a given feature set\\ndef eval_on_features(features, target, regressor):\\n    # split the given features into a training and a test set\\n    X_train, X_test = features[:n_train], features[n_train:]\\n    # also split the target array\\n    y_train, y_test = target[:n_train], target[n_train:]\\n    regressor.fit(X_train, y_train)\\n    print(\"Test-set R^2: {:.2f}\".format(regressor.score(X_test, y_test)))\\n    y_pred = regressor.predict(X_test)\\n    y_pred_train = regressor.predict(X_train)\\n    plt.figure(figsize=(10, 3))'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 257, 'page_label': '244', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='print(\"Test-set R^2: {:.2f}\".format(regressor.score(X_test, y_test)))\\n    y_pred = regressor.predict(X_test)\\n    y_pred_train = regressor.predict(X_train)\\n    plt.figure(figsize=(10, 3))\\n    plt.xticks(range(0, len(X), 8), xticks.strftime(\"%a %m-%d\"), rotation=90,\\n               ha=\"left\")\\n    plt.plot(range(n_train), y_train, label=\"train\")\\n    plt.plot(range(n_train, len(y_test) + n_train), y_test, \\'-\\', label=\"test\")\\n    plt.plot(range(n_train), y_pred_train, \\'--\\', label=\"prediction train\")\\n    plt.plot(range(n_train, len(y_test) + n_train), y_pred, \\'--\\',\\n             label=\"prediction test\")\\n    plt.legend(loc=(1.01, 0))\\n    plt.xlabel(\"Date\")\\n    plt.ylabel(\"Rentals\")\\n244 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 258, 'page_label': '245', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='We saw earlier that random forests require very little preprocessing of the data, which\\nmakes this seem like a good model to start with. We use the POSIX time feature X and\\npass a random forest regressor to our eval_on_features function. Figure 4-13 shows\\nthe result:\\nIn[55]:\\nfrom sklearn.ensemble import RandomForestRegressor\\nregressor = RandomForestRegressor(n_estimators=100, random_state=0)\\nplt.figure()\\neval_on_features(X, y, regressor)\\nOut[55]:\\nTest-set R^2: -0.04\\nFigure 4-13. Predictions made by a random forest using only the POSIX time\\nThe predictions on the training set are quite good, as is usual for random forests.\\nHowever, for the test set, a constant line is predicted. The R2 is –0.03, which means\\nthat we learned nothing. What happened?\\nThe problem lies in the combination of our feature and the random forest. The value\\nof the POSIX time feature for the test set is outside of the range of the feature values'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 258, 'page_label': '245', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='The problem lies in the combination of our feature and the random forest. The value\\nof the POSIX time feature for the test set is outside of the range of the feature values\\nin the training set: the points in the test set have timestamps that are later than all the\\npoints in the training set. Trees, and therefore random forests, cannot extrapolate to\\nfeature ranges outside the training set. The result is that the model simply predicts the\\ntarget value of the closest point in the training set—which is the last time it observed\\nany data.\\nClearly we can do better than this. This is where our “expert knowledge” comes in.\\nFrom looking at the rental figures in the training data, two factors seem to be very\\nimportant: the time of day and the day of the week. So, let’s add these two features.\\nWe can’t really learn anything from the POSIX time, so we drop that feature. First,\\nlet’s use only the hour of the day. As Figure 4-14 shows, now the predictions have the'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 258, 'page_label': '245', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='We can’t really learn anything from the POSIX time, so we drop that feature. First,\\nlet’s use only the hour of the day. As Figure 4-14 shows, now the predictions have the\\nsame pattern for each day of the week:\\nUtilizing Expert Knowledge | 245'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 259, 'page_label': '246', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[56]:\\nX_hour = citibike.index.hour.reshape(-1, 1)\\neval_on_features(X_hour, y, regressor)\\nOut[56]:\\nTest-set R^2: 0.60\\nFigure 4-14. Predictions made by a random forest using only the hour of the day\\nThe R2 is already much better, but the predictions clearly miss the weekly pattern.\\nNow let’s also add the day of the week (see Figure 4-15):\\nIn[57]:\\nX_hour_week = np.hstack([citibike.index.dayofweek.reshape(-1, 1),\\n                         citibike.index.hour.reshape(-1, 1)])\\neval_on_features(X_hour_week, y, regressor)\\nOut[57]:\\nTest-set R^2: 0.84\\nFigure 4-15. Predictions with a random forest using day of week and hour of day\\nfeatures\\n246 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 260, 'page_label': '247', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Now we have a model that captures the periodic behavior by considering the day of\\nweek and time of day. It has an R2 of 0.84, and shows pretty good predictive perfor‐\\nmance. What this model likely is learning is the mean number of rentals for each\\ncombination of weekday and time of day from the first 23 days of August. This\\nactually does not require a complex model like a random forest, so let’s try with a\\nsimpler model, LinearRegression (see Figure 4-16):\\nIn[58]:\\nfrom sklearn.linear_model import LinearRegression\\neval_on_features(X_hour_week, y, LinearRegression())\\nOut[58]:\\nTest-set R^2: 0.13\\nFigure 4-16. Predictions made by linear regression using day of week and hour of day as\\nfeatures\\nLinearRegression works much worse, and the periodic pattern looks odd. The rea‐\\nson for this is that we encoded day of week and time of day using integers, which are\\ninterpreted as categorical variables. Therefore, the linear model can only learn a lin‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 260, 'page_label': '247', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='son for this is that we encoded day of week and time of day using integers, which are\\ninterpreted as categorical variables. Therefore, the linear model can only learn a lin‐\\near function of the time of day—and it learned that later in the day, there are more\\nrentals. However, the patterns are much more complex than that. We can capture this\\nby interpreting the integers as categorical variables, by transforming them using One\\nHotEncoder (see Figure 4-17):\\nIn[59]:\\nenc = OneHotEncoder()\\nX_hour_week_onehot = enc.fit_transform(X_hour_week).toarray()\\nUtilizing Expert Knowledge | 247'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 261, 'page_label': '248', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[60]:\\neval_on_features(X_hour_week_onehot, y, Ridge())\\nOut[60]:\\nTest-set R^2: 0.62\\nFigure 4-17. Predictions made by linear regression using a one-hot encoding of hour of\\nday and day of week\\nThis gives us a much better match than the continuous feature encoding. Now the\\nlinear model learns one coefficient for each day of the week, and one coefficient for\\neach time of the day. That means that the “time of day” pattern is shared over all days\\nof the week, though.\\nUsing interaction features, we can allow the model to learn one coefficient for each\\ncombination of day and time of day (see Figure 4-18):\\nIn[61]:\\npoly_transformer = PolynomialFeatures(degree=2, interaction_only=True,\\n                                      include_bias=False)\\nX_hour_week_onehot_poly = poly_transformer.fit_transform(X_hour_week_onehot)\\nlr = Ridge()\\neval_on_features(X_hour_week_onehot_poly, y, lr)\\nOut[61]:\\nTest-set R^2: 0.85\\n248 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 262, 'page_label': '249', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 4-18. Predictions made by linear regression using a product of the day of week\\nand hour of day features\\nThis transformation finally yields a model that performs similarly well to the random\\nforest. A big benefit of this model is that it is very clear what is learned: one coeffi‐\\ncient for each day and time. We can simply plot the coefficients learned by the model,\\nsomething that would not be possible for the random forest.\\nFirst, we create feature names for the hour and day features:\\nIn[62]:\\nhour = [\"%02d:00\" % i for i in range(0, 24, 3)]\\nday = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\\nfeatures =  day + hour\\nThen we name all the interaction features extracted by PolynomialFeatures, using\\nthe get_feature_names method, and keep only the features with nonzero coeffi‐\\ncients:\\nIn[63]:\\nfeatures_poly = poly_transformer.get_feature_names(features)\\nfeatures_nonzero = np.array(features_poly)[lr.coef_ != 0]\\ncoef_nonzero = lr.coef_[lr.coef_ != 0]'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 262, 'page_label': '249', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='cients:\\nIn[63]:\\nfeatures_poly = poly_transformer.get_feature_names(features)\\nfeatures_nonzero = np.array(features_poly)[lr.coef_ != 0]\\ncoef_nonzero = lr.coef_[lr.coef_ != 0]\\nNow we can visualize the coefficients learned by the linear model, as seen in\\nFigure 4-19:\\nIn[64]:\\nplt.figure(figsize=(15, 2))\\nplt.plot(coef_nonzero, \\'o\\')\\nplt.xticks(np.arange(len(coef_nonzero)), features_nonzero, rotation=90)\\nplt.xlabel(\"Feature magnitude\")\\nplt.ylabel(\"Feature\")\\nUtilizing Expert Knowledge | 249'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 263, 'page_label': '250', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 4-19. Coefficients of the linear regression model using a product of hour and day\\nSummary and Outlook\\nIn this chapter, we discussed how to deal with different data types (in particular, with\\ncategorical variables). We emphasized the importance of representing data in a way\\nthat is suitable for the machine learning algorithm—for example, by one-hot-\\nencoding categorical variables. We also discussed the importance of engineering new\\nfeatures, and the possibility of utilizing expert knowledge in creating derived features\\nfrom your data. In particular, linear models might benefit greatly from generating\\nnew features via binning and adding polynomials and interactions, while more com‐\\nplex, nonlinear models like random forests and SVMs might be able to learn more\\ncomplex tasks without explicitly expanding the feature space. In practice, the features\\nthat are used (and the match between features and method) is often the most impor‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 263, 'page_label': '250', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='complex tasks without explicitly expanding the feature space. In practice, the features\\nthat are used (and the match between features and method) is often the most impor‐\\ntant piece in making a machine learning approach work well.\\nNow that you have a good idea of how to represent your data in an appropriate way\\nand which algorithm to use for which task, the next chapter will focus on evaluating\\nthe performance of machine learning models and selecting the right parameter\\nsettings.\\n250 | Chapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 264, 'page_label': '251', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 5\\nModel Evaluation and Improvement\\nHaving discussed the fundamentals of supervised and unsupervised learning, and\\nhaving explored a variety of machine learning algorithms, we will now dive more\\ndeeply into evaluating models and selecting parameters.\\nWe will focus on the supervised methods, regression and classification, as evaluating\\nand selecting models in unsupervised learning is often a very qualitative process (as\\nwe saw in Chapter 3).\\nTo evaluate our supervised models, so far we have split our dataset into a training set\\nand a test set using the train_test_split function, built a model on the training set\\nby calling the fit method, and evaluated it on the test set using the score method,\\nwhich for classification computes the fraction of correctly classified samples. Here’s\\nan example of that process:\\nIn[2]:\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 264, 'page_label': '251', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='an example of that process:\\nIn[2]:\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\n# create a synthetic dataset\\nX, y = make_blobs(random_state=0)\\n# split data and labels into a training and a test set\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n# instantiate a model and fit it to the training set\\nlogreg = LogisticRegression().fit(X_train, y_train)\\n# evaluate the model on the test set\\nprint(\"Test set score: {:.2f}\".format(logreg.score(X_test, y_test)))\\nOut[2]:\\nTest set score: 0.88\\n251'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 265, 'page_label': '252', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Remember, the reason we split our data into training and test sets is that we are inter‐\\nested in measuring how well our model generalizes to new, previously unseen data.\\nWe are not interested in how well our model fit the training set, but rather in how\\nwell it can make predictions for data that was not observed during training.\\nIn this chapter, we will expand on two aspects of this evaluation. We will first intro‐\\nduce cross-validation, a more robust way to assess generalization performance, and\\ndiscuss methods to evaluate classification and regression performance that go beyond\\nthe default measures of accuracy and R2 provided by the score method.\\nWe will also discuss grid search, an effective method for adjusting the parameters in\\nsupervised models for the best generalization performance.\\nCross-Validation\\nCross-validation is a statistical method of evaluating generalization performance that\\nis more stable and thorough than using a split into a training and a test set. In cross-'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 265, 'page_label': '252', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Cross-Validation\\nCross-validation is a statistical method of evaluating generalization performance that\\nis more stable and thorough than using a split into a training and a test set. In cross-\\nvalidation, the data is instead split repeatedly and multiple models are trained. The\\nmost commonly used version of cross-validation is k-fold cross-validation, where k is\\na user-specified number, usually 5 or 10. When performing five-fold cross-validation,\\nthe data is first partitioned into five parts of (approximately) equal size, called folds.\\nNext, a sequence of models is trained. The first model is trained using the first fold as\\nthe test set, and the remaining folds (2–5) are used as the training set. The model is\\nbuilt using the data in folds 2–5, and then the accuracy is evaluated on fold 1. Then\\nanother model is built, this time using fold 2 as the test set and the data in folds 1, 3,\\n4, and 5 as the training set. This process is repeated using folds 3, 4, and 5 as test sets.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 265, 'page_label': '252', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='another model is built, this time using fold 2 as the test set and the data in folds 1, 3,\\n4, and 5 as the training set. This process is repeated using folds 3, 4, and 5 as test sets.\\nFor each of these five splits of the data into training and test sets, we compute the\\naccuracy. In the end, we have collected five accuracy values. The process is illustrated\\nin Figure 5-1:\\nIn[3]:\\nmglearn.plots.plot_cross_validation()\\nFigure 5-1. Data splitting in five-fold cross-validation\\nUsually, the first fifth of the data is the first fold, the second fifth of the data is the\\nsecond fold, and so on.\\n252 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 266, 'page_label': '253', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Cross-Validation in scikit-learn\\nCross-validation is implemented in scikit-learn using the cross_val_score func‐\\ntion from the model_selection module. The parameters of the cross_val_score\\nfunction are the model we want to evaluate, the training data, and the ground-truth\\nlabels. Let’s evaluate LogisticRegression on the iris dataset:\\nIn[4]:\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.linear_model import LogisticRegression\\niris = load_iris()\\nlogreg = LogisticRegression()\\nscores = cross_val_score(logreg, iris.data, iris.target)\\nprint(\"Cross-validation scores: {}\".format(scores))\\nOut[4]:\\nCross-validation scores: [ 0.961  0.922  0.958]\\nBy default, cross_val_score performs three-fold cross-validation, returning three\\naccuracy values. We can change the number of folds used by changing the cv parame‐\\nter:\\nIn[5]:\\nscores = cross_val_score(logreg, iris.data, iris.target, cv=5)\\nprint(\"Cross-validation scores: {}\".format(scores))\\nOut[5]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 266, 'page_label': '253', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='ter:\\nIn[5]:\\nscores = cross_val_score(logreg, iris.data, iris.target, cv=5)\\nprint(\"Cross-validation scores: {}\".format(scores))\\nOut[5]:\\nCross-validation scores: [ 1.     0.967  0.933  0.9    1.   ]\\nA common way to summarize the cross-validation accuracy is to compute the mean:\\nIn[6]:\\nprint(\"Average cross-validation score: {:.2f}\".format(scores.mean()))\\nOut[6]:\\nAverage cross-validation score: 0.96\\nUsing the mean cross-validation we can conclude that we expect the model to be\\naround 96% accurate on average. Looking at all five scores produced by the five-fold\\ncross-validation, we can also conclude that there is a relatively high variance in the\\naccuracy between folds, ranging from 100% accuracy to 90% accuracy. This could\\nimply that the model is very dependent on the particular folds used for training, but it\\ncould also just be a consequence of the small size of the dataset.\\nCross-Validation | 253'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 267, 'page_label': '254', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Benefits  of Cross-Validation\\nThere are several benefits to using cross-validation instead of a single split into a\\ntraining and a test set. First, remember that train_test_split performs a random\\nsplit of the data. Imagine that we are “lucky” when randomly splitting the data, and\\nall examples that are hard to classify end up in the training set. In that case, the test\\nset will only contain “easy” examples, and our test set accuracy will be unrealistically\\nhigh. Conversely, if we are “unlucky, ” we might have randomly put all the hard-to-\\nclassify examples in the test set and consequently obtain an unrealistically low score.\\nHowever, when using cross-validation, each example will be in the training set exactly\\nonce: each example is in one of the folds, and each fold is the test set once. Therefore,\\nthe model needs to generalize well to all of the samples in the dataset for all of the\\ncross-validation scores (and their mean) to be high.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 267, 'page_label': '254', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the model needs to generalize well to all of the samples in the dataset for all of the\\ncross-validation scores (and their mean) to be high.\\nHaving multiple splits of the data also provides some information about how sensi‐\\ntive our model is to the selection of the training dataset. For the iris dataset, we saw\\naccuracies between 90% and 100%. This is quite a range, and it provides us with an\\nidea about how the model might perform in the worst case and best case scenarios\\nwhen applied to new data.\\nAnother benefit of cross-validation as compared to using a single split of the data is\\nthat we use our data more effectively. When using train_test_split, we usually use\\n75% of the data for training and 25% of the data for evaluation. When using five-fold\\ncross-validation, in each iteration we can use four-fifths of the data (80%) to fit the\\nmodel. When using 10-fold cross-validation, we can use nine-tenths of the data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 267, 'page_label': '254', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='cross-validation, in each iteration we can use four-fifths of the data (80%) to fit the\\nmodel. When using 10-fold cross-validation, we can use nine-tenths of the data\\n(90%) to fit the model. More data will usually result in more accurate models.\\nThe main disadvantage of cross-validation is increased computational cost. As we are\\nnow training k models instead of a single model, cross-validation will be roughly k\\ntimes slower than doing a single split of the data.\\nIt is important to keep in mind that cross-validation is not a way to\\nbuild a model that can be applied to new data. Cross-validation\\ndoes not return a model. When calling cross_val_score, multiple\\nmodels are built internally, but the purpose of cross-validation is\\nonly to evaluate how well a given algorithm will generalize when\\ntrained on a specific dataset.\\nStratified  k-Fold Cross-Validation and Other Strategies\\nSplitting the dataset into k folds by starting with the first one- k-th part of the data, as'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 267, 'page_label': '254', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='trained on a specific dataset.\\nStratified  k-Fold Cross-Validation and Other Strategies\\nSplitting the dataset into k folds by starting with the first one- k-th part of the data, as\\ndescribed in the previous section, might not always be a good idea. For example, let’s\\nhave a look at the iris dataset:\\n254 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 268, 'page_label': '255', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[7]:\\nfrom sklearn.datasets import load_iris\\niris = load_iris()\\nprint(\"Iris labels:\\\\n{}\".format(iris.target))\\nOut[7]:\\nIris labels:\\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\\n 2 2]\\nAs you can see, the first third of the data is the class 0, the second third is the class 1,\\nand the last third is the class 2. Imagine doing three-fold cross-validation on this\\ndataset. The first fold would be only class 0, so in the first split of the data, the test set\\nwould be only class 0, and the training set would be only classes 1 and 2. As the\\nclasses in training and test sets would be different for all three splits, the three-fold\\ncross-validation accuracy would be zero on this dataset. That is not very helpful, as'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 268, 'page_label': '255', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='classes in training and test sets would be different for all three splits, the three-fold\\ncross-validation accuracy would be zero on this dataset. That is not very helpful, as\\nwe can do much better than 0% accuracy on iris.\\nAs the simple k-fold strategy fails here, scikit-learn does not use it for classifica‐\\ntion, but rather uses \\nstratified k-fold cross-validation. In stratified cross-validation, we\\nsplit the data such that the proportions between classes are the same in each fold as\\nthey are in the whole dataset, as illustrated in Figure 5-2:\\nIn[8]:\\nmglearn.plots.plot_stratified_cross_validation()\\nFigure 5-2. Comparison of standard cross-validation and stratified cross-validation\\nwhen the data is ordered by class label\\nCross-Validation | 255'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 269, 'page_label': '256', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='For example, if 90% of your samples belong to class A and 10% of your samples\\nbelong to class B, then stratified cross-validation ensures that in each fold, 90% of\\nsamples belong to class A and 10% of samples belong to class B.\\nIt is usually a good idea to use stratified k-fold cross-validation instead of k-fold\\ncross-validation to evaluate a classifier, because it results in more reliable estimates of\\ngeneralization performance. In the case of only 10% of samples belonging to class B,\\nusing standard k-fold cross-validation it might easily happen that one fold only con‐\\ntains samples of class A. Using this fold as a test set would not be very informative\\nabout the overall performance of the classifier.\\nFor regression, scikit-learn uses the standard k-fold cross-validation by default. It\\nwould be possible to also try to make each fold representative of the different values\\nthe regression target has, but this is not a commonly used strategy and would be sur‐\\nprising to most users.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 269, 'page_label': '256', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='would be possible to also try to make each fold representative of the different values\\nthe regression target has, but this is not a commonly used strategy and would be sur‐\\nprising to most users.\\nMore control over cross-validation\\nWe saw earlier that we can adjust the number of folds that are used in\\ncross_val_score using the cv parameter. However, scikit-learn allows for much\\nfiner control over what happens during the splitting of the data by providing a cross-\\nvalidation splitter as the cv parameter. For most use cases, the defaults of k-fold cross-\\nvalidation for regression and stratified k-fold for classification work well, but there\\nare some cases where you might want to use a different strategy. Say, for example, we\\nwant to use the standard k-fold cross-validation on a classification dataset to repro‐\\nduce someone else’s results. To do this, we first have to import the KFold splitter class\\nfrom the model_selection module and instantiate it with the number of folds we'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 269, 'page_label': '256', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='duce someone else’s results. To do this, we first have to import the KFold splitter class\\nfrom the model_selection module and instantiate it with the number of folds we\\nwant to use:\\nIn[9]:\\nfrom sklearn.model_selection import KFold\\nkfold = KFold(n_splits=5)\\nThen, we can pass the kfold splitter object as the cv parameter to cross_val_score:\\nIn[10]:\\nprint(\"Cross-validation scores:\\\\n{}\".format(\\n      cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\\nOut[10]:\\nCross-validation scores:\\n[ 1.     0.933  0.433  0.967  0.433]\\nThis way, we can verify that it is indeed a really bad idea to use three-fold (nonstrati‐\\nfied) cross-validation on the iris dataset:\\n256 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 270, 'page_label': '257', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[11]:\\nkfold = KFold(n_splits=3)\\nprint(\"Cross-validation scores:\\\\n{}\".format(\\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\\nOut[11]:\\nCross-validation scores:\\n[ 0.  0.  0.]\\nRemember: each fold corresponds to one of the classes in the iris dataset, and so\\nnothing can be learned. Another way to resolve this problem is to shuffle the data\\ninstead of stratifying the folds, to remove the ordering of the samples by label. We can\\ndo that by setting the shuffle parameter of KFold to True. If we shuffle the data, we\\nalso need to fix the random_state to get a reproducible shuffling. Otherwise, each run\\nof cross_val_score would yield a different result, as each time a different split would\\nbe used (this might not be a problem, but can be surprising). Shuffling the data before\\nsplitting it yields a much better result:\\nIn[12]:\\nkfold = KFold(n_splits=3, shuffle=True, random_state=0)\\nprint(\"Cross-validation scores:\\\\n{}\".format('),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 270, 'page_label': '257', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='splitting it yields a much better result:\\nIn[12]:\\nkfold = KFold(n_splits=3, shuffle=True, random_state=0)\\nprint(\"Cross-validation scores:\\\\n{}\".format(\\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\\nOut[12]:\\nCross-validation scores:\\n[ 0.9   0.96  0.96]\\nLeave-one-out cross-validation\\nAnother frequently used cross-validation method is leave-one-out. Y ou can think of\\nleave-one-out cross-validation as k-fold cross-validation where each fold is a single\\nsample. For each split, you pick a single data point to be the test set. This can be very\\ntime consuming, particularly for large datasets, but sometimes provides better esti‐\\nmates on small datasets:\\nIn[13]:\\nfrom sklearn.model_selection import LeaveOneOut\\nloo = LeaveOneOut()\\nscores = cross_val_score(logreg, iris.data, iris.target, cv=loo)\\nprint(\"Number of cv iterations: \", len(scores))\\nprint(\"Mean accuracy: {:.2f}\".format(scores.mean()))\\nOut[13]:\\nNumber of cv iterations:  150\\nMean accuracy: 0.95\\nCross-Validation | 257'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 271, 'page_label': '258', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Shuffle-split  cross-validation\\nAnother, very flexible strategy for cross-validation is shuffle-split cross-validation. In\\nshuffle-split cross-validation, each split samples train_size many points for the\\ntraining set and test_size many (disjoint) point for the test set. This splitting is\\nrepeated n_iter times. Figure 5-3  illustrates running four iterations of splitting a\\ndataset consisting of 10 points, with a training set of 5 points and test sets of 2 points\\neach (you can use integers for train_size and test_size to use absolute sizes for\\nthese sets, or floating-point numbers to use fractions of the whole dataset):\\nIn[14]:\\nmglearn.plots.plot_shuffle_split()\\nFigure 5-3. ShuffleSplit with 10 points, train_size=5, test_size=2, and n_iter=4\\nThe following code splits the dataset into 50% training set and 50% test set for 10\\niterations:\\nIn[15]:\\nfrom sklearn.model_selection import ShuffleSplit\\nshuffle_split = ShuffleSplit(test_size=.5, train_size=.5, n_splits=10)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 271, 'page_label': '258', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='iterations:\\nIn[15]:\\nfrom sklearn.model_selection import ShuffleSplit\\nshuffle_split = ShuffleSplit(test_size=.5, train_size=.5, n_splits=10)\\nscores = cross_val_score(logreg, iris.data, iris.target, cv=shuffle_split)\\nprint(\"Cross-validation scores:\\\\n{}\".format(scores))\\nOut[15]:\\nCross-validation scores:\\n[ 0.96   0.907  0.947  0.96   0.96   0.907  0.893  0.907  0.92   0.973]\\nShuffle-split cross-validation allows for control over the number of iterations inde‐\\npendently of the training and test sizes, which can sometimes be helpful. It also allows\\nfor using only part of the data in each iteration, by providing train_size and\\ntest_size settings that don’t add up to one. Subsampling the data in this way can be\\nuseful for experimenting with large datasets.\\nThere is also a stratified variant of ShuffleSplit, aptly named StratifiedShuffleS\\nplit, which can provide more reliable results for classification tasks.\\n258 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 272, 'page_label': '259', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Cross-validation with groups\\nAnother very common setting for cross-validation is when there are groups in the\\ndata that are highly related. Say you want to build a system to recognize emotions\\nfrom pictures of faces, and you collect a dataset of pictures of 100 people where each\\nperson is captured multiple times, showing various emotions. The goal is to build a\\nclassifier that can correctly identify emotions of people not in the dataset. Y ou could\\nuse the default stratified cross-validation to measure the performance of a classifier\\nhere. However, it is likely that pictures of the same person will be in both the training\\nand the test set. It will be much easier for a classifier to detect emotions in a face that\\nis part of the training set, compared to a completely new face. To accurately evaluate\\nthe generalization to new faces, we must therefore ensure that the training and test\\nsets contain images of different people.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 272, 'page_label': '259', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the generalization to new faces, we must therefore ensure that the training and test\\nsets contain images of different people.\\nTo achieve this, we can use GroupKFold, which takes an array of groups as argument\\nthat we can use to indicate which person is in the image. The groups array here indi‐\\ncates groups in the data that should not be split when creating the training and test\\nsets, and should not be confused with the class label.\\nThis example of groups in the data is common in medical applications, where you\\nmight have multiple samples from the same patient, but are interested in generalizing\\nto new patients. Similarly, in speech recognition, you might have multiple recordings\\nof the same speaker in your dataset, but are interested in recognizing speech of new\\nspeakers.\\nThe following is an example of using a synthetic dataset with a grouping given by the\\ngroups array. The dataset consists of 12 data points, and for each of the data points,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 272, 'page_label': '259', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='speakers.\\nThe following is an example of using a synthetic dataset with a grouping given by the\\ngroups array. The dataset consists of 12 data points, and for each of the data points,\\ngroups specifies which group (think patient) the point belongs to. The groups specify\\nthat there are four groups, and the first three samples belong to the first group, the\\nnext four samples belong to the second group, and so on:\\nIn[17]:\\nfrom sklearn.model_selection import GroupKFold\\n# create synthetic dataset\\nX, y = make_blobs(n_samples=12, random_state=0)\\n# assume the first three samples belong to the same group,\\n# then the next four, etc.\\ngroups = [0, 0, 0, 1, 1, 1, 1, 2, 2, 3, 3, 3]\\nscores = cross_val_score(logreg, X, y, groups, cv=GroupKFold(n_splits=3))\\nprint(\"Cross-validation scores:\\\\n{}\".format(scores))\\nOut[17]:\\nCross-validation scores:\\n[ 0.75   0.8    0.667]\\nThe samples don’t need to be ordered by group; we just did this for illustration pur‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 272, 'page_label': '259', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='print(\"Cross-validation scores:\\\\n{}\".format(scores))\\nOut[17]:\\nCross-validation scores:\\n[ 0.75   0.8    0.667]\\nThe samples don’t need to be ordered by group; we just did this for illustration pur‐\\nposes. The splits that are calculated based on these labels are visualized in Figure 5-4.\\nCross-Validation | 259'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 273, 'page_label': '260', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='As you can see, for each split, each group is either entirely in the training set or\\nentirely in the test set:\\nIn[16]:\\nmglearn.plots.plot_label_kfold()\\nFigure 5-4. Label-dependent splitting with GroupKFold\\nThere are more splitting strategies for cross-validation in scikit-learn, which allow\\nfor an even greater variety of use cases (you can find these in the scikit-learn user\\nguide). However, the standard KFold, StratifiedKFold, and GroupKFold are by far\\nthe most commonly used ones.\\nGrid Search\\nNow that we know how to evaluate how well a model generalizes, we can take the\\nnext step and improve the model’s generalization performance by tuning its parame‐\\nters. We discussed the parameter settings of many of the algorithms in scikit-learn\\nin Chapters 2 and 3, and it is important to understand what the parameters mean\\nbefore trying to adjust them. Finding the values of the important parameters of a\\nmodel (the ones that provide the best generalization performance) is a tricky task, but'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 273, 'page_label': '260', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='before trying to adjust them. Finding the values of the important parameters of a\\nmodel (the ones that provide the best generalization performance) is a tricky task, but\\nnecessary for almost all models and datasets. Because it is such a common task, there\\nare standard methods in scikit-learn to help you with it. The most commonly used\\nmethod is grid search, which basically means trying all possible combinations of the\\nparameters of interest.\\nConsider the case of a kernel SVM with an RBF (radial basis function) kernel, as\\nimplemented in the SVC class. As we discussed in Chapter 2, there are two important\\nparameters: the kernel bandwidth, gamma, and the regularization parameter, C. Say we\\nwant to try the values 0.001, 0.01, 0.1, 1, 10, and 100 for the parameter C, and the\\nsame for gamma. Because we have six different settings for C and gamma that we want to\\ntry, we have 36 combinations of parameters in total. Looking at all possible combina‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 273, 'page_label': '260', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='same for gamma. Because we have six different settings for C and gamma that we want to\\ntry, we have 36 combinations of parameters in total. Looking at all possible combina‐\\ntions creates a table (or grid) of parameter settings for the SVM, as shown here:\\n260 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 274, 'page_label': '261', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='C = 0.001 C = 0.01 … C = 10\\ngamma=0.001 SVC(C=0.001, gamma=0.001) SVC(C=0.01, gamma=0.001) … SVC(C=10, gamma=0.001)\\ngamma=0.01 SVC(C=0.001, gamma=0.01) SVC(C=0.01, gamma=0.01) … SVC(C=10, gamma=0.01)\\n… … … … …\\ngamma=100 SVC(C=0.001, gamma=100) SVC(C=0.01, gamma=100) … SVC(C=10, gamma=100)\\nSimple Grid Search\\nWe can implement a simple grid search just as for loops over the two parameters,\\ntraining and evaluating a classifier for each combination:\\nIn[18]:\\n# naive grid search implementation\\nfrom sklearn.svm import SVC\\nX_train, X_test, y_train, y_test = train_test_split(\\n    iris.data, iris.target, random_state=0)\\nprint(\"Size of training set: {}   size of test set: {}\".format(\\n      X_train.shape[0], X_test.shape[0]))\\nbest_score = 0\\nfor gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\\n    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\\n        # for each combination of parameters, train an SVC\\n        svm = SVC(gamma=gamma, C=C)\\n        svm.fit(X_train, y_train)\\n        # evaluate the SVC on the test set'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 274, 'page_label': '261', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='# for each combination of parameters, train an SVC\\n        svm = SVC(gamma=gamma, C=C)\\n        svm.fit(X_train, y_train)\\n        # evaluate the SVC on the test set\\n        score = svm.score(X_test, y_test)\\n        # if we got a better score, store the score and parameters\\n        if score > best_score:\\n            best_score = score\\n            best_parameters = {\\'C\\': C, \\'gamma\\': gamma}\\nprint(\"Best score: {:.2f}\".format(best_score))\\nprint(\"Best parameters: {}\".format(best_parameters))\\nOut[18]:\\nSize of training set: 112   size of test set: 38\\nBest score: 0.97\\nBest parameters: {\\'C\\': 100, \\'gamma\\': 0.001}\\nThe Danger of \\nOverfitting  the Parameters and the Validation Set\\nGiven this result, we might be tempted to report that we found a model that performs\\nwith 97% accuracy on our dataset. However, this claim could be overly optimistic (or\\njust wrong), for the following reason: we tried many different parameters and\\nGrid Search | 261'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 275, 'page_label': '262', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='selected the one with best accuracy on the test set, but this accuracy won’t necessarily\\ncarry over to new data. Because we used the test data to adjust the parameters, we can\\nno longer use it to assess how good the model is. This is the same reason we needed\\nto split the data into training and test sets in the first place; we need an independent\\ndataset to evaluate, one that was not used to create the model.\\nOne way to resolve this problem is to split the data again, so we have three sets: the\\ntraining set to build the model, the validation (or development) set to select the\\nparameters of the model, and the test set to evaluate the performance of the selected\\nparameters. Figure 5-5 shows what this looks like:\\nIn[19]:\\nmglearn.plots.plot_threefold_split()\\nFigure 5-5. A threefold split of data into training set, validation set, and test set\\nAfter selecting the best parameters using the validation set, we can rebuild a model'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 275, 'page_label': '262', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 5-5. A threefold split of data into training set, validation set, and test set\\nAfter selecting the best parameters using the validation set, we can rebuild a model\\nusing the parameter settings we found, but now training on both the training data\\nand the validation data. This way, we can use as much data as possible to build our\\nmodel. This leads to the following implementation:\\nIn[20]:\\nfrom sklearn.svm import SVC\\n# split data into train+validation set and test set\\nX_trainval, X_test, y_trainval, y_test = train_test_split(\\n    iris.data, iris.target, random_state=0)\\n# split train+validation set into training and validation sets\\nX_train, X_valid, y_train, y_valid = train_test_split(\\n    X_trainval, y_trainval, random_state=1)\\nprint(\"Size of training set: {}   size of validation set: {}   size of test set:\"\\n      \" {}\\\\n\".format(X_train.shape[0], X_valid.shape[0], X_test.shape[0]))\\nbest_score = 0\\nfor gamma in [0.001, 0.01, 0.1, 1, 10, 100]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 275, 'page_label': '262', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='\" {}\\\\n\".format(X_train.shape[0], X_valid.shape[0], X_test.shape[0]))\\nbest_score = 0\\nfor gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\\n    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\\n        # for each combination of parameters, train an SVC\\n        svm = SVC(gamma=gamma, C=C)\\n        svm.fit(X_train, y_train)\\n        # evaluate the SVC on the test set\\n        score = svm.score(X_valid, y_valid)\\n        # if we got a better score, store the score and parameters\\n        if score > best_score:\\n            best_score = score\\n            best_parameters = {\\'C\\': C, \\'gamma\\': gamma}\\n262 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 276, 'page_label': '263', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='# rebuild a model on the combined training and validation set,\\n# and evaluate it on the test set\\nsvm = SVC(**best_parameters)\\nsvm.fit(X_trainval, y_trainval)\\ntest_score = svm.score(X_test, y_test)\\nprint(\"Best score on validation set: {:.2f}\".format(best_score))\\nprint(\"Best parameters: \", best_parameters)\\nprint(\"Test set score with best parameters: {:.2f}\".format(test_score))\\nOut[20]:\\nSize of training set: 84   size of validation set: 28   size of test set: 38\\nBest score on validation set: 0.96\\nBest parameters:  {\\'C\\': 10, \\'gamma\\': 0.001}\\nTest set score with best parameters: 0.92\\nThe best score on the validation set is 96%: slightly lower than before, probably\\nbecause we used less data to train the model (X_train is smaller now because we split\\nour dataset twice). However, the score on the test set—the score that actually tells us\\nhow well we generalize—is even lower, at 92%. So we can only claim to classify new\\ndata 92% correctly, not 97% correctly as we thought before!'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 276, 'page_label': '263', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='how well we generalize—is even lower, at 92%. So we can only claim to classify new\\ndata 92% correctly, not 97% correctly as we thought before!\\nThe distinction between the training set, validation set, and test set is fundamentally\\nimportant to applying machine learning methods in practice. Any choices made\\nbased on the test set accuracy “leak” information from the test set into the model.\\nTherefore, it is important to keep a separate test set, which is only used for the final\\nevaluation. It is good practice to do all exploratory analysis and model selection using\\nthe combination of a training and a validation set, and reserve the test set for a final\\nevaluation—this is even true for exploratory visualization. Strictly speaking, evaluat‐\\ning more than one model on the test set and choosing the better of the two will result\\nin an overly optimistic estimate of how accurate the model is.\\nGrid Search with Cross-Validation'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 276, 'page_label': '263', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"ing more than one model on the test set and choosing the better of the two will result\\nin an overly optimistic estimate of how accurate the model is.\\nGrid Search with Cross-Validation\\nWhile the method of splitting the data into a training, a validation, and a test set that\\nwe just saw is workable, and relatively commonly used, it is quite sensitive to how\\nexactly the data is split. From the output of the previous code snippet we can see that\\nGridSearchCV selects 'C': 10, 'gamma': 0.001 as the best parameters, while the\\noutput of the code in the previous section selects 'C': 100, 'gamma': 0.001 as the\\nbest parameters. For a better estimate of the generalization performance, instead of\\nusing a single split into a training and a validation set, we can use cross-validation to\\nevaluate the performance of each parameter combination. This method can be coded\\nup as follows:\\nGrid Search | 263\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 277, 'page_label': '264', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"In[21]:\\nfor gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\\n    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\\n        # for each combination of parameters,\\n        # train an SVC\\n        svm = SVC(gamma=gamma, C=C)\\n        # perform cross-validation\\n        scores = cross_val_score(svm, X_trainval, y_trainval, cv=5)\\n        # compute mean cross-validation accuracy\\n        score = np.mean(scores)\\n        # if we got a better score, store the score and parameters\\n        if score > best_score:\\n            best_score = score\\n            best_parameters = {'C': C, 'gamma': gamma}\\n# rebuild a model on the combined training and validation set\\nsvm = SVC(**best_parameters)\\nsvm.fit(X_trainval, y_trainval)\\nTo evaluate the accuracy of the SVM using a particular setting of C and gamma using\\nfive-fold cross-validation, we need to train 36 * 5 = 180 models. As you can imagine,\\nthe main downside of the use of cross-validation is the time it takes to train all these\\nmodels.\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 277, 'page_label': '264', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='five-fold cross-validation, we need to train 36 * 5 = 180 models. As you can imagine,\\nthe main downside of the use of cross-validation is the time it takes to train all these\\nmodels.\\nThe following visualization ( Figure 5-6) illustrates how the best parameter setting is\\nselected in the preceding code:\\nIn[22]:\\nmglearn.plots.plot_cross_val_selection()\\nFigure 5-6. Results of grid search with cross-validation\\nFor each parameter setting (only a subset is shown), five accuracy values are compu‐\\nted, one for each split in the cross-validation. Then the mean validation accuracy is\\ncomputed for each parameter setting. The parameters with the highest mean valida‐\\ntion accuracy are chosen, marked by the circle.\\n264 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 278, 'page_label': '265', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='As we said earlier, cross-validation is a way to evaluate a given algo‐\\nrithm on a specific dataset. However, it is often used in conjunction\\nwith parameter search methods like grid search. For this reason,\\nmany people use the term cross-validation colloquially to refer to\\ngrid search with cross-validation.\\nThe overall process of splitting the data, running the grid search, and evaluating the\\nfinal parameters is illustrated in Figure 5-7:\\nIn[23]:\\nmglearn.plots.plot_grid_search_overview()\\nFigure 5-7. Overview of the process of parameter selection and model evaluation with\\nGridSearchCV\\nBecause grid search with cross-validation is such a commonly used method to adjust\\nparameters, scikit-learn provides the GridSearchCV class, which implements it in\\nthe form of an estimator. To use the GridSearchCV class, you first need to specify the\\nparameters you want to search over using a dictionary. GridSearchCV will then per‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 278, 'page_label': '265', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the form of an estimator. To use the GridSearchCV class, you first need to specify the\\nparameters you want to search over using a dictionary. GridSearchCV will then per‐\\nform all the necessary model fits. The keys of the dictionary are the names of parame‐\\nters we want to adjust (as given when constructing the model—in this case, C and\\ngamma), and the values are the parameter settings we want to try out. Trying the val‐\\nues 0.001, 0.01, 0.1, 1, 10, and 100 for C and gamma translates to the following\\ndictionary:\\nIn[24]:\\nparam_grid = {\\'C\\': [0.001, 0.01, 0.1, 1, 10, 100],\\n              \\'gamma\\': [0.001, 0.01, 0.1, 1, 10, 100]}\\nprint(\"Parameter grid:\\\\n{}\".format(param_grid))\\nOut[24]:\\nParameter grid:\\n{\\'C\\': [0.001, 0.01, 0.1, 1, 10, 100], \\'gamma\\': [0.001, 0.01, 0.1, 1, 10, 100]}\\nGrid Search | 265'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 279, 'page_label': '266', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='1 A scikit-learn estimator that is created using another estimator is called a meta-estimator. GridSearchCV is\\nthe most commonly used meta-estimator, but we will see more later.\\nWe can now instantiate the GridSearchCV class with the model ( SVC), the parameter\\ngrid to search ( param_grid), and the cross-validation strategy we want to use (say,\\nfive-fold stratified cross-validation):\\nIn[25]:\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.svm import SVC\\ngrid_search = GridSearchCV(SVC(), param_grid, cv=5)\\nGridSearchCV will use cross-validation in place of the split into a training and valida‐\\ntion set that we used before. However, we still need to split the data into a training\\nand a test set, to avoid overfitting the parameters:\\nIn[26]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    iris.data, iris.target, random_state=0)\\nThe grid_search object that we created behaves just like a classifier; we can call the'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 279, 'page_label': '266', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[26]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    iris.data, iris.target, random_state=0)\\nThe grid_search object that we created behaves just like a classifier; we can call the\\nstandard methods fit, predict, and score on it. 1 However, when we call fit, it will\\nrun cross-validation for each combination of parameters we specified in param_grid:\\nIn[27]:\\ngrid_search.fit(X_train, y_train)\\nFitting the GridSearchCV object not only searches for the best parameters, but also\\nautomatically fits a new model on the whole training dataset with the parameters that\\nyielded the best cross-validation performance. What happens in fit is therefore\\nequivalent to the result of the In[21] code we saw at the beginning of this section. The\\nGridSearchCV class provides a very convenient interface to access the retrained\\nmodel using the predict and score methods. To evaluate how well the best found\\nparameters generalize, we can call score on the test set:\\nIn[28]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 279, 'page_label': '266', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='model using the predict and score methods. To evaluate how well the best found\\nparameters generalize, we can call score on the test set:\\nIn[28]:\\nprint(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))\\nOut[28]:\\nTest set score: 0.97\\nChoosing the parameters using cross-validation, we actually found a model that ach‐\\nieves 97% accuracy on the test set. The important thing here is that we did not use the\\ntest set to choose the parameters. The parameters that were found are scored in the\\n266 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 280, 'page_label': '267', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='best_params_ attribute, and the best cross-validation accuracy (the mean accuracy\\nover the different splits for this parameter setting) is stored in best_score_:\\nIn[29]:\\nprint(\"Best parameters: {}\".format(grid_search.best_params_))\\nprint(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\\nOut[29]:\\nBest parameters: {\\'C\\': 100, \\'gamma\\': 0.01}\\nBest cross-validation score: 0.97\\nAgain, be careful not to confuse best_score_ with the generaliza‐\\ntion performance of the model as computed by the score method\\non the test set. Using the score method (or evaluating the output of\\nthe predict method) employs a model trained on the whole train‐\\ning set. The best_score_ attribute stores the mean cross-validation\\naccuracy, with cross-validation performed on the training set.\\nSometimes it is helpful to have access to the actual model that was found—for exam‐\\nple, to look at coefficients or feature importances. Y ou can access the model with the'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 280, 'page_label': '267', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Sometimes it is helpful to have access to the actual model that was found—for exam‐\\nple, to look at coefficients or feature importances. Y ou can access the model with the\\nbest parameters trained on the whole training set using the best_estimator_\\nattribute:\\nIn[30]:\\nprint(\"Best estimator:\\\\n{}\".format(grid_search.best_estimator_))\\nOut[30]:\\nBest estimator:\\nSVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\\n   decision_function_shape=None, degree=3, gamma=0.01, kernel=\\'rbf\\',\\n   max_iter=-1, probability=False, random_state=None, shrinking=True,\\n   tol=0.001, verbose=False)\\nBecause grid_search itself has predict and score methods, using best_estimator_\\nis not needed to make predictions or evaluate the model.\\nAnalyzing the result of cross-validation\\nIt is often helpful to visualize the results of cross-validation, to understand how the\\nmodel generalization depends on the parameters we are searching. As grid searches'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 280, 'page_label': '267', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='It is often helpful to visualize the results of cross-validation, to understand how the\\nmodel generalization depends on the parameters we are searching. As grid searches\\nare quite computationally expensive to run, often it is a good idea to start with a rela‐\\ntively coarse and small grid. We can then inspect the results of the cross-validated\\ngrid search, and possibly expand our search. The results of a grid search can be found\\nin the cv_results_ attribute, which is a dictionary storing all aspects of the search. It\\nGrid Search | 267'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 281, 'page_label': '268', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"contains a lot of details, as you can see in the following output, and is best looked at\\nafter converting it to a pandas DataFrame:\\nIn[31]:\\nimport pandas as pd\\n# convert to DataFrame\\nresults = pd.DataFrame(grid_search.cv_results_)\\n# show the first 5 rows\\ndisplay(results.head())\\nOut[31]:\\n    param_C   param_gamma   params                        mean_test_score\\n0   0.001     0.001         {'C': 0.001, 'gamma': 0.001}       0.366\\n1   0.001      0.01         {'C': 0.001, 'gamma': 0.01}        0.366\\n2   0.001       0.1         {'C': 0.001, 'gamma': 0.1}         0.366\\n3   0.001         1         {'C': 0.001, 'gamma': 1}           0.366\\n4   0.001        10         {'C': 0.001, 'gamma': 10}          0.366\\n       rank_test_score  split0_test_score  split1_test_score  split2_test_score\\n0               22              0.375           0.347           0.363\\n1               22              0.375           0.347           0.363\\n2               22              0.375           0.347           0.363\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 281, 'page_label': '268', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='1               22              0.375           0.347           0.363\\n2               22              0.375           0.347           0.363\\n3               22              0.375           0.347           0.363\\n4               22              0.375           0.347           0.363\\n       split3_test_score  split4_test_score  std_test_score\\n0           0.363              0.380           0.011\\n1           0.363              0.380           0.011\\n2           0.363              0.380           0.011\\n3           0.363              0.380           0.011\\n4           0.363              0.380           0.011\\nEach row in results corresponds to one particular parameter setting. For each set‐\\nting, the results of all cross-validation splits are recorded, as well as the mean and\\nstandard deviation over all splits. As we were searching a two-dimensional grid of\\nparameters (C and gamma), this is best visualized as a heat map ( Figure 5-8). First we'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 281, 'page_label': '268', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='standard deviation over all splits. As we were searching a two-dimensional grid of\\nparameters (C and gamma), this is best visualized as a heat map ( Figure 5-8). First we\\nextract the mean validation scores, then we reshape the scores so that the axes corre‐\\nspond to C and gamma:\\nIn[32]:\\nscores = np.array(results.mean_test_score).reshape(6, 6)\\n# plot the mean cross-validation scores\\nmglearn.tools.heatmap(scores, xlabel=\\'gamma\\', xticklabels=param_grid[\\'gamma\\'],\\n                      ylabel=\\'C\\', yticklabels=param_grid[\\'C\\'], cmap=\"viridis\")\\n268 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 282, 'page_label': '269', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 5-8. Heat map of mean cross-validation score as a function of C and gamma\\nEach point in the heat map corresponds to one run of cross-validation, with a partic‐\\nular parameter setting. The color encodes the cross-validation accuracy, with light\\ncolors meaning high accuracy and dark colors meaning low accuracy. Y ou can see\\nthat SVC is very sensitive to the setting of the parameters. For many of the parameter\\nsettings, the accuracy is around 40%, which is quite bad; for other settings the accu‐\\nracy is around 96%. We can take away from this plot several things. First, the parame‐\\nters we adjusted are very important for obtaining good performance. Both parameters\\n(C and gamma) matter a lot, as adjusting them can change the accuracy from 40% to\\n96%. Additionally, the ranges we picked for the parameters are ranges in which we\\nsee significant changes in the outcome. It’s also important to note that the ranges for'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 282, 'page_label': '269', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='96%. Additionally, the ranges we picked for the parameters are ranges in which we\\nsee significant changes in the outcome. It’s also important to note that the ranges for\\nthe parameters are large enough: the optimum values for each parameter are not on\\nthe edges of the plot.\\nNow let’s look at some plots (shown in Figure 5-9 ) where the result is less ideal,\\nbecause the search ranges were not chosen properly:\\nFigure 5-9. Heat map visualizations of misspecified search grids\\nGrid Search | 269'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 283, 'page_label': '270', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[33]:\\nfig, axes = plt.subplots(1, 3, figsize=(13, 5))\\nparam_grid_linear = {\\'C\\': np.linspace(1, 2, 6),\\n                     \\'gamma\\':  np.linspace(1, 2, 6)}\\nparam_grid_one_log = {\\'C\\': np.linspace(1, 2, 6),\\n                      \\'gamma\\':  np.logspace(-3, 2, 6)}\\nparam_grid_range = {\\'C\\': np.logspace(-3, 2, 6),\\n                    \\'gamma\\':  np.logspace(-7, -2, 6)}\\nfor param_grid, ax in zip([param_grid_linear, param_grid_one_log,\\n                           param_grid_range], axes):\\n    grid_search = GridSearchCV(SVC(), param_grid, cv=5)\\n    grid_search.fit(X_train, y_train)\\n    scores = grid_search.cv_results_[\\'mean_test_score\\'].reshape(6, 6)\\n    # plot the mean cross-validation scores\\n    scores_image = mglearn.tools.heatmap(\\n        scores, xlabel=\\'gamma\\', ylabel=\\'C\\', xticklabels=param_grid[\\'gamma\\'],\\n        yticklabels=param_grid[\\'C\\'], cmap=\"viridis\", ax=ax)\\nplt.colorbar(scores_image, ax=axes.tolist())\\nThe first panel shows no changes at all, with a constant color over the whole parame‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 283, 'page_label': '270', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='yticklabels=param_grid[\\'C\\'], cmap=\"viridis\", ax=ax)\\nplt.colorbar(scores_image, ax=axes.tolist())\\nThe first panel shows no changes at all, with a constant color over the whole parame‐\\nter grid. In this case, this is caused by improper scaling and range of the parameters C\\nand gamma. However, if no change in accuracy is visible over the different parameter\\nsettings, it could also be that a parameter is just not important at all. It is usually good\\nto try very extreme values first, to see if there are any changes in the accuracy as a\\nresult of changing a parameter.\\nThe second panel shows a vertical stripe pattern. This indicates that only the setting\\nof the gamma parameter makes any difference. This could mean that the gamma param‐\\neter is searching over interesting values but the C parameter is not—or it could mean\\nthe C parameter is not important.\\nThe third panel shows changes in both C and gamma. However, we can see that in the'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 283, 'page_label': '270', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the C parameter is not important.\\nThe third panel shows changes in both C and gamma. However, we can see that in the\\nentire bottom left of the plot, nothing interesting is happening. We can probably\\nexclude the very small values from future grid searches. The optimum parameter set‐\\nting is at the top right. As the optimum is in the border of the plot, we can expect that\\nthere might be even better values beyond this border, and we might want to change\\nour search range to include more parameters in this region.\\nTuning the parameter grid based on the cross-validation scores is perfectly fine, and a\\ngood way to explore the importance of different parameters. However, you should\\nnot test different parameter ranges on the final test set—as we discussed earlier, eval‐\\n270 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 284, 'page_label': '271', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"uation of the test set should happen only once we know exactly what model we want\\nto use.\\nSearch over spaces that are not grids\\nIn some cases, trying all possible combinations of all parameters as GridSearchCV\\nusually does, is not a good idea. For example, SVC has a kernel parameter, and\\ndepending on which kernel is chosen, other parameters will be relevant. If ker\\nnel='linear', the model is linear, and only the C parameter is used. If kernel='rbf',\\nboth the C and gamma parameters are used (but not other parameters like degree). In\\nthis case, searching over all possible combinations of C, gamma, and kernel wouldn’t\\nmake sense: if kernel='linear', gamma is not used, and trying different values for\\ngamma would be a waste of time. To deal with these kinds of “conditional” parameters,\\nGridSearchCV allows the param_grid to be a list of dictionaries. Each dictionary in the\\nlist is expanded into an independent grid. A possible grid search involving kernel and\\nparameters could look like this:\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 284, 'page_label': '271', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='list is expanded into an independent grid. A possible grid search involving kernel and\\nparameters could look like this:\\nIn[34]:\\nparam_grid = [{\\'kernel\\': [\\'rbf\\'],\\n               \\'C\\': [0.001, 0.01, 0.1, 1, 10, 100],\\n               \\'gamma\\': [0.001, 0.01, 0.1, 1, 10, 100]},\\n              {\\'kernel\\': [\\'linear\\'],\\n               \\'C\\': [0.001, 0.01, 0.1, 1, 10, 100]}]\\nprint(\"List of grids:\\\\n{}\".format(param_grid))\\nOut[34]:\\nList of grids:\\n[{\\'kernel\\': [\\'rbf\\'], \\'C\\': [0.001, 0.01, 0.1, 1, 10, 100],\\n  \\'gamma\\': [0.001, 0.01, 0.1, 1, 10, 100]},\\n {\\'kernel\\': [\\'linear\\'], \\'C\\': [0.001, 0.01, 0.1, 1, 10, 100]}]\\nIn the first grid, the kernel parameter is always set to \\'rbf\\' (not that the entry for\\nkernel is a list of length one), and both the C and gamma parameters are varied. In the\\nsecond grid, the kernel parameter is always set to linear, and only C is varied. Now\\nlet’s apply this more complex parameter search:\\nIn[35]:\\ngrid_search = GridSearchCV(SVC(), param_grid, cv=5)\\ngrid_search.fit(X_train, y_train)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 284, 'page_label': '271', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='let’s apply this more complex parameter search:\\nIn[35]:\\ngrid_search = GridSearchCV(SVC(), param_grid, cv=5)\\ngrid_search.fit(X_train, y_train)\\nprint(\"Best parameters: {}\".format(grid_search.best_params_))\\nprint(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\\nOut[35]:\\nBest parameters: {\\'C\\': 100, \\'kernel\\': \\'rbf\\', \\'gamma\\': 0.01}\\nBest cross-validation score: 0.97\\nGrid Search | 271'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 285, 'page_label': '272', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"Let’s look at the cv_results_ again. As expected, if kernel is 'linear', then only C is\\nvaried:\\nIn[36]:\\nresults = pd.DataFrame(grid_search.cv_results_)\\n# we display the transposed table so that it better fits on the page:\\ndisplay(results.T)\\nOut[36]:\\n0 1 2 3 … 38 39 40 41\\nparam_C 0.001 0.001 0.001 0.001 … 0.1 1 10 100\\nparam_gamma 0.001 0.01 0.1 1 … NaN NaN NaN NaN\\nparam_kernel rbf rbf rbf rbf … linear linear linear linear\\nparams {C: 0.001,\\nkernel: rbf,\\ngamma:\\n0.001}\\n{C: 0.001,\\nkernel: rbf,\\ngamma:\\n0.01}\\n{C: 0.001,\\nkernel: rbf,\\ngamma:\\n0.1}\\n{C: 0.001,\\nkernel: rbf,\\ngamma: 1}\\n… {C: 0.1,\\nkernel:\\nlinear}\\n{C: 1,\\nkernel:\\nlinear}\\n{C: 10,\\nkernel:\\nlinear}\\n{C: 100,\\nkernel:\\nlinear}\\nmean_test_score 0.37 0.37 0.37 0.37 … 0.95 0.97 0.96 0.96\\nrank_test_score 27 27 27 27 … 11 1 3 3\\nsplit0_test_score 0.38 0.38 0.38 0.38 … 0.96 1 0.96 0.96\\nsplit1_test_score 0.35 0.35 0.35 0.35 … 0.91 0.96 1 1\\nsplit2_test_score 0.36 0.36 0.36 0.36 … 1 1 1 1\\nsplit3_test_score 0.36 0.36 0.36 0.36 … 0.91 0.95 0.91 0.91\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 285, 'page_label': '272', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='split1_test_score 0.35 0.35 0.35 0.35 … 0.91 0.96 1 1\\nsplit2_test_score 0.36 0.36 0.36 0.36 … 1 1 1 1\\nsplit3_test_score 0.36 0.36 0.36 0.36 … 0.91 0.95 0.91 0.91\\nsplit4_test_score 0.38 0.38 0.38 0.38 … 0.95 0.95 0.95 0.95\\nstd_test_score 0.011 0.011 0.011 0.011 … 0.033 0.022 0.034 0.034\\n12 rows × 42 columns\\nUsing different  cross-validation strategies with grid search\\nSimilarly to cross_val_score, GridSearchCV uses stratified k-fold cross-validation\\nby default for classification, and k-fold cross-validation for regression. However, you\\ncan also pass any cross-validation splitter, as described in “More control over cross-\\nvalidation” on page 256, as the cv parameter in GridSearchCV. In particular, to get\\nonly a single split into a training and a validation set, you can use ShuffleSplit or\\nStratifiedShuffleSplit with n_iter=1. This might be helpful for very large data‐\\nsets, or very slow models.\\nNested cross-validation'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 285, 'page_label': '272', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='StratifiedShuffleSplit with n_iter=1. This might be helpful for very large data‐\\nsets, or very slow models.\\nNested cross-validation\\nIn the preceding examples, we went from using a single split of the data into training,\\nvalidation, and test sets to splitting the data into training and test sets and then per‐\\nforming cross-validation on the training set. But when using GridSearchCV as\\n272 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 286, 'page_label': '273', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='described earlier, we still have a single split of the data into training and test sets,\\nwhich might make our results unstable and make us depend too much on this single\\nsplit of the data. We can go a step further, and instead of splitting the original data\\ninto training and test sets once, use multiple splits of cross-validation. This will result\\nin what is called nested cross-validation. In nested cross-validation, there is an outer\\nloop over splits of the data into training and test sets. For each of them, a grid search\\nis run (which might result in different best parameters for each split in the outer\\nloop). Then, for each outer split, the test set score using the best settings is reported.\\nThe result of this procedure is a list of scores—not a model, and not a parameter set‐\\nting. The scores tell us how well a model generalizes, given the best parameters found\\nby the grid. As it doesn’t provide a model that can be used on new data, nested cross-'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 286, 'page_label': '273', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='ting. The scores tell us how well a model generalizes, given the best parameters found\\nby the grid. As it doesn’t provide a model that can be used on new data, nested cross-\\nvalidation is rarely used when looking for a predictive model to apply to future data.\\nHowever, it can be useful for evaluating how well a given model works on a particular\\ndataset.\\nImplementing nested cross-validation in scikit-learn is straightforward. We call\\ncross_val_score with an instance of GridSearchCV as the model:\\nIn[34]:\\nscores = cross_val_score(GridSearchCV(SVC(), param_grid, cv=5),\\n                         iris.data, iris.target, cv=5)\\nprint(\"Cross-validation scores: \", scores)\\nprint(\"Mean cross-validation score: \", scores.mean())\\nOut[34]:\\nCross-validation scores:  [ 0.967  1.     0.967  0.967  1.   ]\\nMean cross-validation score:  0.98\\nThe result of our nested cross-validation can be summarized as “SVC can achieve 98%\\nmean cross-validation accuracy on the iris dataset”—nothing more and nothing\\nless.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 286, 'page_label': '273', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='The result of our nested cross-validation can be summarized as “SVC can achieve 98%\\nmean cross-validation accuracy on the iris dataset”—nothing more and nothing\\nless.\\nHere, we used stratified five-fold cross-validation in both the inner and the outer\\nloop. As our param_grid contains 36 combinations of parameters, this results in a\\nwhopping 36 * 5 * 5 = 900 models being built, making nested cross-validation a very\\nexpensive procedure. Here, we used the same cross-validation splitter in the inner\\nand the outer loop; however, this is not necessary and you can use any combination\\nof cross-validation strategies in the inner and outer loops. It can be a bit tricky to\\nunderstand what is happening in the single line given above, and it can be helpful to\\nvisualize it as for loops, as done in the following simplified implementation:\\nGrid Search | 273'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 287, 'page_label': '274', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[35]:\\ndef nested_cv(X, y, inner_cv, outer_cv, Classifier, parameter_grid):\\n    outer_scores = []\\n    # for each split of the data in the outer cross-validation\\n    # (split method returns indices)\\n    for training_samples, test_samples in outer_cv.split(X, y):\\n        # find best parameter using inner cross-validation\\n        best_parms = {}\\n        best_score = -np.inf\\n        # iterate over parameters\\n        for parameters in parameter_grid:\\n            # accumulate score over inner splits\\n            cv_scores = []\\n            # iterate over inner cross-validation\\n            for inner_train, inner_test in inner_cv.split(\\n                    X[training_samples], y[training_samples]):\\n                # build classifier given parameters and training data\\n                clf = Classifier(**parameters)\\n                clf.fit(X[inner_train], y[inner_train])\\n                # evaluate on inner test set\\n                score = clf.score(X[inner_test], y[inner_test])'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 287, 'page_label': '274', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='clf.fit(X[inner_train], y[inner_train])\\n                # evaluate on inner test set\\n                score = clf.score(X[inner_test], y[inner_test])\\n                cv_scores.append(score)\\n            # compute mean score over inner folds\\n            mean_score = np.mean(cv_scores)\\n            if mean_score > best_score:\\n                # if better than so far, remember parameters\\n                best_score = mean_score\\n                best_params = parameters\\n        # build classifier on best parameters using outer training set\\n        clf = Classifier(**best_params)\\n        clf.fit(X[training_samples], y[training_samples])\\n        # evaluate\\n        outer_scores.append(clf.score(X[test_samples], y[test_samples]))\\n    return np.array(outer_scores)\\nNow, let’s run this function on the iris dataset:\\nIn[36]:\\nfrom sklearn.model_selection import ParameterGrid, StratifiedKFold\\nscores = nested_cv(iris.data, iris.target, StratifiedKFold(5),'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 287, 'page_label': '274', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Now, let’s run this function on the iris dataset:\\nIn[36]:\\nfrom sklearn.model_selection import ParameterGrid, StratifiedKFold\\nscores = nested_cv(iris.data, iris.target, StratifiedKFold(5),\\n          StratifiedKFold(5), SVC, ParameterGrid(param_grid))\\nprint(\"Cross-validation scores: {}\".format(scores))\\nOut[36]:\\nCross-validation scores: [ 0.967  1.     0.967  0.967  1.   ]\\nParallelizing cross-validation and grid search\\nWhile running a grid search over many parameters and on large datasets can be com‐\\nputationally challenging, it is also embarrassingly parallel. This means that building a\\n274 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 288, 'page_label': '275', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='model using a particular parameter setting on a particular cross-validation split can\\nbe done completely independently from the other parameter settings and models.\\nThis makes grid search and cross-validation ideal candidates for parallelization over\\nmultiple CPU cores or over a cluster. Y ou can make use of multiple cores in Grid\\nSearchCV and cross_val_score by setting the n_jobs parameter to the number of\\nCPU cores you want to use. Y ou can set n_jobs=-1 to use all available cores.\\nY ou should be aware that scikit-learn does not allow nesting of parallel operations .\\nSo, if you are using the n_jobs option on your model (for example, a random forest),\\nyou cannot use it in GridSearchCV to search over this model. If your dataset and\\nmodel are very large, it might be that using many cores uses up too much memory,\\nand you should monitor your memory usage when building large models in parallel.\\nIt is also possible to parallelize grid search and cross-validation over multiple'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 288, 'page_label': '275', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='and you should monitor your memory usage when building large models in parallel.\\nIt is also possible to parallelize grid search and cross-validation over multiple\\nmachines in a cluster, although at the time of writing this is not supported within\\nscikit-learn. It is, however, possible to use the IPython parallel framework for par‐\\nallel grid searches, if you don’t mind writing the for loop over parameters as we did\\nin “Simple Grid Search” on page 261.\\nFor Spark users, there is also the recently developed spark-sklearn package, which\\nallows running a grid search over an already established Spark cluster.\\nEvaluation Metrics and Scoring\\nSo far, we have evaluated classification performance using accuracy (the fraction of\\ncorrectly classified samples) and regression performance using R2. However, these are\\nonly two of the many possible ways to summarize how well a supervised model per‐\\nforms on a given dataset. In practice, these evaluation metrics might not be appropri‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 288, 'page_label': '275', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='only two of the many possible ways to summarize how well a supervised model per‐\\nforms on a given dataset. In practice, these evaluation metrics might not be appropri‐\\nate for your application, and it is important to choose the right metric when selecting\\nbetween models and adjusting parameters.\\nKeep the End Goal in Mind\\nWhen selecting a metric, you should always have the end goal of the machine learn‐\\ning application in mind. In practice, we are usually interested not just in making\\naccurate predictions, but in using these predictions as part of a larger decision-\\nmaking process. Before picking a machine learning metric, you should think about\\nthe high-level goal of the application, often called the business metric. The conse‐\\nquences of choosing a particular algorithm for a machine learning application are\\nEvaluation Metrics and Scoring | 275'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 289, 'page_label': '276', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='2 We ask scientifically minded readers to excuse the commercial language in this section. Not losing track of the\\nend goal is equally important in science, though the authors are not aware of a similar phrase to “business\\nimpact” being used in that realm.\\ncalled the business impact.2 Maybe the high-level goal is avoiding traffic accidents, or\\ndecreasing the number of hospital admissions. It could also be getting more users for\\nyour website, or having users spend more money in your shop. When choosing a\\nmodel or adjusting parameters, you should pick the model or parameter values that\\nhave the most positive influence on the business metric. Often this is hard, as assess‐\\ning the business impact of a particular model might require putting it in production\\nin a real-life system.\\nIn the early stages of development, and for adjusting parameters, it is often infeasible\\nto put models into production just for testing purposes, because of the high business'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 289, 'page_label': '276', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='in a real-life system.\\nIn the early stages of development, and for adjusting parameters, it is often infeasible\\nto put models into production just for testing purposes, because of the high business\\nor personal risks that can be involved. Imagine evaluating the pedestrian avoidance\\ncapabilities of a self-driving car by just letting it drive around, without verifying it\\nfirst; if your model is bad, pedestrians will be in trouble! Therefore we often need to\\nfind some surrogate evaluation procedure, using an evaluation metric that is easier to\\ncompute. For example, we could test classifying images of pedestrians against non-\\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\\npays off to find the closest metric to the original business goal that is feasible to evalu‐\\nate. This closest metric should be used whenever possible for model evaluation and\\nselection. The result of this evaluation might not be a single number—the conse‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 289, 'page_label': '276', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='ate. This closest metric should be used whenever possible for model evaluation and\\nselection. The result of this evaluation might not be a single number—the conse‐\\nquence of your algorithm could be that you have 10% more customers, but each cus‐\\ntomer will spend 15% less—but it should capture the expected business impact of\\nchoosing one model over another.\\nIn this section, we will first discuss metrics for the important special case of binary\\nclassification, then turn to multiclass classification and finally regression.\\nMetrics for Binary \\nClassification\\nBinary classification is arguably the most common and conceptually simple applica‐\\ntion of machine learning in practice. However, there are still a number of caveats in\\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\\nlook at the ways in which measuring accuracy might be misleading. Remember that\\nfor binary classification, we often speak of a positive class and a negative class, with'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 289, 'page_label': '276', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='look at the ways in which measuring accuracy might be misleading. Remember that\\nfor binary classification, we often speak of a positive class and a negative class, with\\nthe understanding that the positive class is the one we are looking for.\\nKinds of errors\\nOften, accuracy is not a good measure of predictive performance, as the number of\\nmistakes we make does not contain all the information we are interested in. Imagine\\nan application to screen for the early detection of cancer using an automated test. If\\n276 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 290, 'page_label': '277', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the test is negative, the patient will be assumed healthy, while if the test is positive, the\\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\\ncation of cancer) the positive class, and a negative test the negative class. We can’t\\nassume that our model will always work perfectly, and it will make mistakes. For any\\napplication, we need to ask ourselves what the consequences of these mistakes might\\nbe in the real world.\\nOne possible mistake is that a healthy patient will be classified as positive, leading to\\nadditional testing. This leads to some costs and an inconvenience for the patient (and\\npossibly some mental distress). An incorrect positive prediction is called a false posi‐\\ntive. The other possible mistake is that a sick patient will be classified as negative, and\\nwill not receive further tests and treatment. The undiagnosed cancer might lead to\\nserious health issues, and could even be fatal. A mistake of this kind—an incorrect'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 290, 'page_label': '277', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='will not receive further tests and treatment. The undiagnosed cancer might lead to\\nserious health issues, and could even be fatal. A mistake of this kind—an incorrect\\nnegative prediction—is called a false negative . In statistics, a false positive is also\\nknown as type I error, and a false negative as type II error. We will stick to “false nega‐\\ntive” and “false positive, ” as they are more explicit and easier to remember. In the can‐\\ncer diagnosis example, it is clear that we want to avoid false negatives as much as\\npossible, while false positives can be viewed as more of a minor nuisance.\\nWhile this is a particularly drastic example, the consequence of false positives and\\nfalse negatives are rarely the same. In commercial applications, it might be possible to\\nassign dollar values to both kinds of mistakes, which would allow measuring the error\\nof a particular prediction in dollars, instead of accuracy. This might be much more'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 290, 'page_label': '277', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='assign dollar values to both kinds of mistakes, which would allow measuring the error\\nof a particular prediction in dollars, instead of accuracy. This might be much more\\nmeaningful for making business decisions on which model to use.\\nImbalanced datasets\\nTypes of errors play an important role when one of two classes is much more frequent\\nthan the other one. This is very common in practice; a good example is click-through\\nprediction, where each data point represents an “impression, ” an item that was shown\\nto a user. This item might be an ad, or a related story, or a related person to follow on\\na social media site. The goal is to predict whether, if shown a particular item, a user\\nwill click on it (indicating they are interested). Most things users are shown on the\\nInternet (in particular, ads) will not result in a click. Y ou might need to show a user\\n100 ads or articles before they find something interesting enough to click on. This'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 290, 'page_label': '277', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Internet (in particular, ads) will not result in a click. Y ou might need to show a user\\n100 ads or articles before they find something interesting enough to click on. This\\nresults in a dataset where for each 99 “no click” data points, there is 1 “clicked” data\\npoint; in other words, 99% of the samples belong to the “no click” class. Datasets in\\nwhich one class is much more frequent than the other are often called imbalanced\\ndatasets, or datasets with imbalanced classes. In reality, imbalanced data is the norm,\\nand it is rare that the events of interest have equal or even similar frequency in the\\ndata.\\nNow let’s say you build a classifier that is 99% accurate on the click prediction task.\\nWhat does that tell you? 99% accuracy sounds impressive, but this doesn’t take the\\nEvaluation Metrics and Scoring | 277'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 291, 'page_label': '278', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"class imbalance into account. Y ou can achieve 99% accuracy without building a\\nmachine learning model, by always predicting “no click. ” On the other hand, even\\nwith imbalanced data, a 99% accurate model could in fact be quite good. However,\\naccuracy doesn’t allow us to distinguish the constant “no click” model from a poten‐\\ntially good model.\\nTo illustrate, we’ll create a 9:1 imbalanced dataset from the digits dataset, by classify‐\\ning the digit 9 against the nine other classes:\\nIn[37]:\\nfrom sklearn.datasets import load_digits\\ndigits = load_digits()\\ny = digits.target == 9\\nX_train, X_test, y_train, y_test = train_test_split(\\n    digits.data, y, random_state=0)\\nWe can use the DummyClassifier to always predict the majority class (here\\n“not nine”) to see how uninformative accuracy can be:\\nIn[38]:\\nfrom sklearn.dummy import DummyClassifier\\ndummy_majority = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\\npred_most_frequent = dummy_majority.predict(X_test)\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 291, 'page_label': '278', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[38]:\\nfrom sklearn.dummy import DummyClassifier\\ndummy_majority = DummyClassifier(strategy=\\'most_frequent\\').fit(X_train, y_train)\\npred_most_frequent = dummy_majority.predict(X_test)\\nprint(\"Unique predicted labels: {}\".format(np.unique(pred_most_frequent)))\\nprint(\"Test score: {:.2f}\".format(dummy_majority.score(X_test, y_test)))\\nOut[38]:\\nUnique predicted labels: [False]\\nTest score: 0.90\\nWe obtained close to 90% accuracy without learning anything. This might seem strik‐\\ning, but think about it for a minute. Imagine someone telling you their model is 90%\\naccurate. Y ou might think they did a very good job. But depending on the problem,\\nthat might be possible by just predicting one class! Let’s compare this against using an\\nactual classifier:\\nIn[39]:\\nfrom sklearn.tree import DecisionTreeClassifier\\ntree = DecisionTreeClassifier(max_depth=2).fit(X_train, y_train)\\npred_tree = tree.predict(X_test)\\nprint(\"Test score: {:.2f}\".format(tree.score(X_test, y_test)))\\nOut[39]:\\nTest score: 0.92'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 291, 'page_label': '278', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='tree = DecisionTreeClassifier(max_depth=2).fit(X_train, y_train)\\npred_tree = tree.predict(X_test)\\nprint(\"Test score: {:.2f}\".format(tree.score(X_test, y_test)))\\nOut[39]:\\nTest score: 0.92\\n278 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 292, 'page_label': '279', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='According to accuracy, the DecisionTreeClassifier is only slightly better than the\\nconstant predictor. This could indicate either that something is wrong with how we\\nused DecisionTreeClassifier, or that accuracy is in fact not a good measure here.\\nFor comparison purposes, let’s evaluate two more classifiers, LogisticRegression\\nand the default DummyClassifier, which makes random predictions but produces\\nclasses with the same proportions as in the training set:\\nIn[40]:\\nfrom sklearn.linear_model import LogisticRegression\\ndummy = DummyClassifier().fit(X_train, y_train)\\npred_dummy = dummy.predict(X_test)\\nprint(\"dummy score: {:.2f}\".format(dummy.score(X_test, y_test)))\\nlogreg = LogisticRegression(C=0.1).fit(X_train, y_train)\\npred_logreg = logreg.predict(X_test)\\nprint(\"logreg score: {:.2f}\".format(logreg.score(X_test, y_test)))\\nOut[40]:\\ndummy score: 0.80\\nlogreg score: 0.98\\nThe dummy classifier that produces random output is clearly the worst of the lot'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 292, 'page_label': '279', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='print(\"logreg score: {:.2f}\".format(logreg.score(X_test, y_test)))\\nOut[40]:\\ndummy score: 0.80\\nlogreg score: 0.98\\nThe dummy classifier that produces random output is clearly the worst of the lot\\n(according to accuracy), while LogisticRegression produces very good results.\\nHowever, even the random classifier yields over 80% accuracy. This makes it very\\nhard to judge which of these results is actually helpful. The problem here is that accu‐\\nracy is an inadequate measure for quantifying predictive performance in this imbal‐\\nanced setting. For the rest of this chapter, we will explore alternative metrics that\\nprovide better guidance in selecting models. In particular, we would like to have met‐\\nrics that tell us how much better a model is than making “most frequent” predictions\\nor random predictions, as they are computed in pred_most_frequent and\\npred_dummy. If we use a metric to assess our models, it should definitely be able to\\nweed out these nonsense predictions.\\nConfusion matrices'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 292, 'page_label': '279', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='pred_dummy. If we use a metric to assess our models, it should definitely be able to\\nweed out these nonsense predictions.\\nConfusion matrices\\nOne of the most comprehensive ways to represent the result of evaluating binary clas‐\\nsification is using confusion matrices. Let’s inspect the predictions of LogisticRegres\\nsion from the previous section using the confusion_matrix function. We already\\nstored the predictions on the test set in pred_logreg:\\nEvaluation Metrics and Scoring | 279'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 293, 'page_label': '280', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[41]:\\nfrom sklearn.metrics import confusion_matrix\\nconfusion = confusion_matrix(y_test, pred_logreg)\\nprint(\"Confusion matrix:\\\\n{}\".format(confusion))\\nOut[41]:\\nConfusion matrix:\\n[[401   2]\\n [  8  39]]\\nThe output of confusion_matrix is a two-by-two array, where the rows correspond\\nto the true classes and the columns correspond to the predicted classes. Each entry\\ncounts how often a sample that belongs to the class corresponding to the row (here,\\n“not nine” and “nine”) was classified as the class corresponding to the column. The\\nfollowing plot (Figure 5-10) illustrates this meaning:\\nIn[42]:\\nmglearn.plots.plot_confusion_matrix_illustration()\\nFigure 5-10. Confusion matrix of the “nine vs. rest” classification task\\n280 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 294, 'page_label': '281', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='3 The main diagonal of a two-dimensional array or matrix A is A[i, i].\\nEntries on the main diagonal 3 of the confusion matrix correspond to correct classifi‐\\ncations, while other entries tell us how many samples of one class got mistakenly clas‐\\nsified as another class.\\nIf we declare “a nine” the positive class, we can relate the entries of the confusion\\nmatrix with the terms false positive and false negative that we introduced earlier. To\\ncomplete the picture, we call correctly classified samples belonging to the positive\\nclass true positives and correctly classified samples belonging to the negative class true\\nnegatives. These terms are usually abbreviated FP , FN, TP , and TN and lead to the fol‐\\nlowing interpretation for the confusion matrix (Figure 5-11):\\nIn[43]:\\nmglearn.plots.plot_binary_confusion_matrix()\\nFigure 5-11. Confusion matrix for binary classification\\nNow let’s use the confusion matrix to compare the models we fitted earlier (the two'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 294, 'page_label': '281', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[43]:\\nmglearn.plots.plot_binary_confusion_matrix()\\nFigure 5-11. Confusion matrix for binary classification\\nNow let’s use the confusion matrix to compare the models we fitted earlier (the two\\ndummy models, the decision tree, and the logistic regression):\\nIn[44]:\\nprint(\"Most frequent class:\")\\nprint(confusion_matrix(y_test, pred_most_frequent))\\nprint(\"\\\\nDummy model:\")\\nprint(confusion_matrix(y_test, pred_dummy))\\nprint(\"\\\\nDecision tree:\")\\nprint(confusion_matrix(y_test, pred_tree))\\nprint(\"\\\\nLogistic Regression\")\\nprint(confusion_matrix(y_test, pred_logreg))\\nEvaluation Metrics and Scoring | 281'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 295, 'page_label': '282', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Out[44]:\\nMost frequent class:\\n[[403   0]\\n [ 47   0]]\\nDummy model:\\n[[361  42]\\n [ 43   4]]\\nDecision tree:\\n[[390  13]\\n [ 24  23]]\\nLogistic Regression\\n[[401   2]\\n [  8  39]]\\nLooking at the confusion matrix, it is quite clear that something is wrong with\\npred_most_frequent, because it always predicts the same class. pred_dummy, on the\\nother hand, has a very small number of true positives (4), particularly compared to\\nthe number of false negatives and false positives—there are many more false positives\\nthan true positives! The predictions made by the decision tree make much more\\nsense than the dummy predictions, even though the accuracy was nearly the same.\\nFinally, we can see that logistic regression does better than pred_tree in all aspects: it\\nhas more true positives and true negatives while having fewer false positives and false\\nnegatives. From this comparison, it is clear that only the decision tree and the logistic'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 295, 'page_label': '282', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='has more true positives and true negatives while having fewer false positives and false\\nnegatives. From this comparison, it is clear that only the decision tree and the logistic\\nregression give reasonable results, and that the logistic regression works better than\\nthe tree on all accounts. However, inspecting the full confusion matrix is a bit cum‐\\nbersome, and while we gained a lot of insight from looking at all aspects of the\\nmatrix, the process was very manual and qualitative. There are several ways to sum‐\\nmarize the information in the confusion matrix, which we will discuss next.\\nRelation to accuracy.    We already saw one way to summarize the result in the confu‐\\nsion matrix—by computing accuracy, which can be expressed as:\\nAccuracy = TP+TN\\nTP+TN + FP + FN\\nIn other words, accuracy is the number of correct predictions (TP and TN) divided\\nby the number of all samples (all entries of the confusion matrix summed up).'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 295, 'page_label': '282', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Accuracy = TP+TN\\nTP+TN + FP + FN\\nIn other words, accuracy is the number of correct predictions (TP and TN) divided\\nby the number of all samples (all entries of the confusion matrix summed up).\\nPrecision, recall, and f-score.    There are several other ways to summarize the confusion\\nmatrix, with the most common ones being precision and recall. Precision measures\\nhow many of the samples predicted as positive are actually positive:\\n282 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 296, 'page_label': '283', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Precision = TP\\nTP+FP\\nPrecision is used as a performance metric when the goal is to limit the number of\\nfalse positives. As an example, imagine a model for predicting whether a new drug\\nwill be effective in treating a disease in clinical trials. Clinical trials are notoriously\\nexpensive, and a pharmaceutical company will only want to run an experiment if it is\\nvery sure that the drug will actually work. Therefore, it is important that the model\\ndoes not produce many false positives—in other words, that it has a high precision.\\nPrecision is also known as positive predictive value (PPV).\\nRecall, on the other hand, measures how many of the positive samples are captured\\nby the positive predictions:\\nRecall = TP\\nTP+FN\\nRecall is used as performance metric when we need to identify all positive samples;\\nthat is, when it is important to avoid false negatives. The cancer diagnosis example\\nfrom earlier in this chapter is a good example for this: it is important to find all peo‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 296, 'page_label': '283', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='that is, when it is important to avoid false negatives. The cancer diagnosis example\\nfrom earlier in this chapter is a good example for this: it is important to find all peo‐\\nple that are sick, possibly including healthy patients in the prediction. Other names\\nfor recall are sensitivity, hit rate, or true positive rate (TPR).\\nThere is a trade-off between optimizing recall and optimizing precision. Y ou can triv‐\\nially obtain a perfect recall if you predict all samples to belong to the positive class—\\nthere will be no false negatives, and no true negatives either. However, predicting all\\nsamples as positive will result in many false positives, and therefore the precision will\\nbe very low. On the other hand, if you find a model that predicts only the single data\\npoint it is most sure about as positive and the rest as negative, then precision will be\\nperfect (assuming this data point is in fact positive), but recall will be very bad.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 296, 'page_label': '283', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='point it is most sure about as positive and the rest as negative, then precision will be\\nperfect (assuming this data point is in fact positive), but recall will be very bad.\\nPrecision and recall are only two of many classification measures\\nderived from TP , FP , TN, and FN. Y ou can find a great summary of\\nall the measures on Wikipedia. In the machine learning commu‐\\nnity, precision and recall are arguably the most commonly used\\nmeasures for binary classification, but other communities might\\nuse other related metrics.\\nSo, while precision and recall are very important measures, looking at only one of\\nthem will not provide you with the full picture. One way to summarize them is the\\nf-score or f-measure, which is with the harmonic mean of precision and recall:\\nF = 2 · precision·recall\\nprecision+recall\\nEvaluation Metrics and Scoring | 283'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 297, 'page_label': '284', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='This particular variant is also known as the f1-score. As it takes precision and recall\\ninto account, it can be a better measure than accuracy on imbalanced binary classifi‐\\ncation datasets. Let’s run it on the predictions for the “nine vs. rest” dataset that we\\ncomputed earlier. Here, we will assume that the “nine” class is the positive class (it is\\nlabeled as True while the rest is labeled as False), so the positive class is the minority\\nclass:\\nIn[45]:\\nfrom sklearn.metrics import f1_score\\nprint(\"f1 score most frequent: {:.2f}\".format(\\n        f1_score(y_test, pred_most_frequent)))\\nprint(\"f1 score dummy: {:.2f}\".format(f1_score(y_test, pred_dummy)))\\nprint(\"f1 score tree: {:.2f}\".format(f1_score(y_test, pred_tree)))\\nprint(\"f1 score logistic regression: {:.2f}\".format(\\n        f1_score(y_test, pred_logreg)))\\nOut[45]:\\nf1 score most frequent: 0.00\\nf1 score dummy: 0.10\\nf1 score tree: 0.55\\nf1 score logistic regression: 0.89'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 297, 'page_label': '284', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='f1_score(y_test, pred_logreg)))\\nOut[45]:\\nf1 score most frequent: 0.00\\nf1 score dummy: 0.10\\nf1 score tree: 0.55\\nf1 score logistic regression: 0.89\\nWe can note two things here. First, we get an error message for the most_frequent\\nprediction, as there were no predictions of the positive class (which makes the\\ndenominator in the f-score zero). Also, we can see a pretty strong distinction between\\nthe dummy predictions and the tree predictions, which wasn’t clear when looking at\\naccuracy alone. Using the f-score for evaluation, we summarized the predictive per‐\\nformance again in one number. However, the f-score seems to capture our intuition\\nof what makes a good model much better than accuracy did. A disadvantage of the\\nf-score, however, is that it is harder to interpret and explain than accuracy.\\nIf we want a more comprehensive summary of precision, recall, and f1-score, we can\\nuse the classification_report convenience function to compute all three at once,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 297, 'page_label': '284', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='If we want a more comprehensive summary of precision, recall, and f1-score, we can\\nuse the classification_report convenience function to compute all three at once,\\nand print them in a nice format:\\nIn[46]:\\nfrom sklearn.metrics import classification_report\\nprint(classification_report(y_test, pred_most_frequent,\\n                            target_names=[\"not nine\", \"nine\"]))\\n284 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 298, 'page_label': '285', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Out[46]:\\n             precision    recall  f1-score   support\\n   not nine       0.90      1.00      0.94       403\\n       nine       0.00      0.00      0.00        47\\navg / total       0.80      0.90      0.85       450\\nThe classification_report function produces one line per class (here, True and\\nFalse) and reports precision, recall, and f-score with this class as the positive class.\\nBefore, we assumed the minority “nine” class was the positive class. If we change the\\npositive class to “not nine, ” we can see from the output of classification_report\\nthat we obtain an f-score of 0.94 with the most_frequent model. Furthermore, for the\\n“not nine” class we have a recall of 1, as we classified all samples as “not nine. ” The\\nlast column next to the f-score provides the support of each class, which simply means\\nthe number of samples in this class according to the ground truth.\\nThe last row in the classification report shows a weighted (by the number of samples'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 298, 'page_label': '285', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the number of samples in this class according to the ground truth.\\nThe last row in the classification report shows a weighted (by the number of samples\\nin the class) average of the numbers for each class. Here are two more reports, one for\\nthe dummy classifier and one for the logistic regression:\\nIn[47]:\\nprint(classification_report(y_test, pred_dummy,\\n                            target_names=[\"not nine\", \"nine\"]))\\nOut[47]:\\n             precision    recall  f1-score   support\\n   not nine       0.90      0.92      0.91       403\\n       nine       0.11      0.09      0.10        47\\navg / total       0.81      0.83      0.82       450\\nIn[48]:\\nprint(classification_report(y_test, pred_logreg,\\n                            target_names=[\"not nine\", \"nine\"]))\\nOut[48]:\\n             precision    recall  f1-score   support\\n   not nine       0.98      1.00      0.99       403\\n       nine       0.95      0.83      0.89        47\\navg / total       0.98      0.98      0.98       450'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 298, 'page_label': '285', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='not nine       0.98      1.00      0.99       403\\n       nine       0.95      0.83      0.89        47\\navg / total       0.98      0.98      0.98       450\\nEvaluation Metrics and Scoring | 285'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 299, 'page_label': '286', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='As you may notice when looking at the reports, the differences between the dummy\\nmodels and a very good model are not as clear any more. Picking which class is\\ndeclared the positive class has a big impact on the metrics. While the f-score for the\\ndummy classification is 0.13 (vs. 0.89 for the logistic regression) on the “nine” class,\\nfor the “not nine” class it is 0.90 vs. 0.99, which both seem like reasonable results.\\nLooking at all the numbers together paints a pretty accurate picture, though, and we\\ncan clearly see the superiority of the logistic regression model.\\nTaking uncertainty into account\\nThe confusion matrix and the classification report provide a very detailed analysis of\\na particular set of predictions. However, the predictions themselves already threw\\naway a lot of information that is contained in the model. As we discussed in Chap‐\\nter 2, most classifiers provide a decision_function or a predict_proba method to'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 299, 'page_label': '286', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='away a lot of information that is contained in the model. As we discussed in Chap‐\\nter 2, most classifiers provide a decision_function or a predict_proba method to\\nassess degrees of certainty about predictions. Making predictions can be seen as\\nthresholding the output of decision_function or predict_proba at a certain fixed\\npoint—in binary classification we use 0 for the decision function and 0.5 for\\npredict_proba.\\nThe following is an example of an imbalanced binary classification task, with 400\\npoints in the negative class classified against 50 points in the positive class. The train‐\\ning data is shown on the left in Figure 5-12. We train a kernel SVM model on this\\ndata, and the plots to the right of the training data illustrate the values of the decision\\nfunction as a heat map. Y ou can see a black circle in the plot in the top center, which\\ndenotes the threshold of the decision_function being exactly zero. Points inside this'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 299, 'page_label': '286', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='function as a heat map. Y ou can see a black circle in the plot in the top center, which\\ndenotes the threshold of the decision_function being exactly zero. Points inside this\\ncircle will be classified as the positive class, and points outside as the negative class:\\nIn[49]:\\nfrom mglearn.datasets import make_blobs\\nX, y = make_blobs(n_samples=(400, 50), centers=2, cluster_std=[7.0, 2],\\n                  random_state=22)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\nsvc = SVC(gamma=.05).fit(X_train, y_train)\\nIn[50]:\\nmglearn.plots.plot_decision_threshold()\\n286 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 300, 'page_label': '287', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 5-12. Heatmap of the decision function and the impact of changing the decision\\nthreshold\\nWe can use the classification_report function to evaluate precision and recall for\\nboth classes:\\nIn[51]:\\nprint(classification_report(y_test, svc.predict(X_test)))\\nOut[51]:\\n             precision    recall  f1-score   support\\n          0       0.97      0.89      0.93       104\\n          1       0.35      0.67      0.46         9\\navg / total       0.92      0.88      0.89       113\\nFor class 1, we get a fairly small recall, and precision is mixed. Because class 0 is so\\nmuch larger, the classifier focuses on getting class 0 right, and not the smaller class 1.\\nLet’s assume in our application it is more important to have a high recall for class 1, as\\nin the cancer screening example earlier. This means we are willing to risk more false\\npositives (false class 1) in exchange for more true positives (which will increase the'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 300, 'page_label': '287', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='in the cancer screening example earlier. This means we are willing to risk more false\\npositives (false class 1) in exchange for more true positives (which will increase the\\nrecall). The predictions generated by svc.predict really do not fulfill this require‐\\nment, but we can adjust the predictions to focus on a higher recall of class 1 by\\nchanging the decision threshold away from 0. By default, points with a deci\\nsion_function value greater than 0 will be classified as class 1. We want more points\\nto be classified as class 1, so we need to decrease the threshold:\\nEvaluation Metrics and Scoring | 287'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 301, 'page_label': '288', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[52]:\\ny_pred_lower_threshold = svc.decision_function(X_test) > -.8\\nLet’s look at the classification report for this prediction:\\nIn[53]:\\nprint(classification_report(y_test, y_pred_lower_threshold))\\nOut[53]:\\n             precision    recall  f1-score   support\\n          0       1.00      0.82      0.90       104\\n          1       0.32      1.00      0.49         9\\navg / total       0.95      0.83      0.87       113\\nAs expected, the recall of class 1 went up, and the precision went down. We are now\\nclassifying a larger region of space as class 1, as illustrated in the top-right panel of\\nFigure 5-12. If you value precision over recall or the other way around, or your data is\\nheavily imbalanced, changing the decision threshold is the easiest way to obtain bet‐\\nter results. As the decision_function can have arbitrary ranges, it is hard to provide\\na rule of thumb regarding how to pick a threshold.\\nIf you do set a threshold, you need to be careful not to do so using'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 301, 'page_label': '288', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='a rule of thumb regarding how to pick a threshold.\\nIf you do set a threshold, you need to be careful not to do so using\\nthe test set. As with any other parameter, setting a decision thresh‐\\nold on the test set is likely to yield overly optimistic results. Use a\\nvalidation set or cross-validation instead.\\nPicking a threshold for models that implement the predict_proba method can be\\neasier, as the output of predict_proba is on a fixed 0 to 1 scale, and models probabil‐\\nities. By default, the threshold of 0.5 means that if the model is more than 50% “sure”\\nthat a point is of the positive class, it will be classified as such. Increasing the thresh‐\\nold means that the model needs to be more confident to make a positive decision\\n(and less confident to make a negative decision). While working with probabilities\\nmay be more intuitive than working with arbitrary thresholds, not all models provide\\nrealistic models of uncertainty (a DecisionTree that is grown to its full depth is'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 301, 'page_label': '288', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='may be more intuitive than working with arbitrary thresholds, not all models provide\\nrealistic models of uncertainty (a DecisionTree that is grown to its full depth is\\nalways 100% sure of its decisions, even though it might often be wrong). This relates\\nto the concept of calibration: a calibrated model is a model that provides an accurate\\nmeasure of its uncertainty. Discussing calibration in detail is beyond the scope of this\\nbook, but you can find more details in the paper “Predicting Good Probabilities with\\nSupervised Learning” by Alexandru Niculescu-Mizil and Rich Caruana.\\n288 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 302, 'page_label': '289', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Precision-recall curves and ROC curves\\nAs we just discussed, changing the threshold that is used to make a classification deci‐\\nsion in a model is a way to adjust the trade-off of precision and recall for a given clas‐\\nsifier. Maybe you want to miss less than 10% of positive samples, meaning a desired\\nrecall of 90%. This decision depends on the application, and it should be driven by\\nbusiness goals. Once a particular goal is set—say, a particular recall or precision value\\nfor a class—a threshold can be set appropriately. It is always possible to set a thresh‐\\nold to fulfill a particular target, like 90% recall. The hard part is to develop a model\\nthat still has reasonable precision with this threshold—if you classify everything as\\npositive, you will have 100% recall, but your model will be useless.\\nSetting a requirement on a classifier like 90% recall is often called setting the operat‐\\ning point. Fixing an operating point is often helpful in business settings to make per‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 302, 'page_label': '289', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Setting a requirement on a classifier like 90% recall is often called setting the operat‐\\ning point. Fixing an operating point is often helpful in business settings to make per‐\\nformance guarantees to customers or other groups inside your organization.\\nOften, when developing a new model, it is not entirely clear what the operating point\\nwill be. For this reason, and to understand a modeling problem better, it is instructive\\nto look at all possible thresholds, or all possible trade-offs of precision and recalls at\\nonce. This is possible using a tool called the precision-recall curve. Y ou can find the\\nfunction to compute the precision-recall curve in the sklearn.metrics module. It\\nneeds the ground truth labeling and predicted uncertainties, created via either\\ndecision_function or predict_proba:\\nIn[54]:\\nfrom sklearn.metrics import precision_recall_curve\\nprecision, recall, thresholds = precision_recall_curve(\\n    y_test, svc.decision_function(X_test))'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 302, 'page_label': '289', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"decision_function or predict_proba:\\nIn[54]:\\nfrom sklearn.metrics import precision_recall_curve\\nprecision, recall, thresholds = precision_recall_curve(\\n    y_test, svc.decision_function(X_test))\\nThe precision_recall_curve function returns a list of precision and recall values\\nfor all possible thresholds (all values that appear in the decision function) in sorted\\norder, so we can plot a curve, as seen in Figure 5-13:\\nIn[55]:\\n# Use more data points for a smoother curve\\nX, y = make_blobs(n_samples=(4000, 500), centers=2, cluster_std=[7.0, 2],\\n                  random_state=22)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\nsvc = SVC(gamma=.05).fit(X_train, y_train)\\nprecision, recall, thresholds = precision_recall_curve(\\n    y_test, svc.decision_function(X_test))\\n# find threshold closest to zero\\nclose_zero = np.argmin(np.abs(thresholds))\\nplt.plot(precision[close_zero], recall[close_zero], 'o', markersize=10,\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 302, 'page_label': '289', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='y_test, svc.decision_function(X_test))\\n# find threshold closest to zero\\nclose_zero = np.argmin(np.abs(thresholds))\\nplt.plot(precision[close_zero], recall[close_zero], \\'o\\', markersize=10,\\n         label=\"threshold zero\", fillstyle=\"none\", c=\\'k\\', mew=2)\\nplt.plot(precision, recall, label=\"precision recall curve\")\\nplt.xlabel(\"Precision\")\\nplt.ylabel(\"Recall\")\\nEvaluation Metrics and Scoring | 289'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 303, 'page_label': '290', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 5-13. Precision recall curve for SVC(gamma=0.05)\\nEach point along the curve in Figure 5-13 corresponds to a possible threshold of the\\ndecision_function. We can see, for example, that we can achieve a recall of 0.4 at a\\nprecision of about 0.75. The black circle marks the point that corresponds to a thresh‐\\nold of 0, the default threshold for decision_function. This point is the trade-off that\\nis chosen when calling the predict method.\\nThe closer a curve stays to the upper-right corner, the better the classifier. A point at\\nthe upper right means high precision and high recall for the same threshold. The\\ncurve starts at the top-left corner, corresponding to a very low threshold, classifying\\neverything as the positive class. Raising the threshold moves the curve toward higher\\nprecision, but also lower recall. Raising the threshold more and more, we get to a sit‐\\nuation where most of the points classified as being positive are true positives, leading'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 303, 'page_label': '290', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='precision, but also lower recall. Raising the threshold more and more, we get to a sit‐\\nuation where most of the points classified as being positive are true positives, leading\\nto a very high precision but lower recall. The more the model keeps recall high as\\nprecision goes up, the better.\\nLooking at this particular curve a bit more, we can see that with this model it is possi‐\\nble to get a precision of up to around 0.5 with very high recall. If we want a much\\nhigher precision, we have to sacrifice a lot of recall. In other words, on the left the\\ncurve is relatively flat, meaning that recall does not go down a lot when we require\\nincreased precision. For precision greater than 0.5, each gain in precision costs us a\\nlot of recall.\\nDifferent classifiers can work well in different parts of the curve—that is, at different\\noperating points. Let’s compare the SVM we trained to a random forest trained on the\\nsame dataset. The RandomForestClassifier doesn’t have a decision_function, only'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 303, 'page_label': '290', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='operating points. Let’s compare the SVM we trained to a random forest trained on the\\nsame dataset. The RandomForestClassifier doesn’t have a decision_function, only\\npredict_proba. The precision_recall_curve function expects as its second argu‐\\nment a certainty measure for the positive class (class 1), so we pass the probability of\\na sample being class 1—that is, rf.predict_proba(X_test)[:, 1] . The default\\nthreshold for predict_proba in binary classification is 0.5, so this is the point we\\nmarked on the curve (see Figure 5-14):\\n290 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 304, 'page_label': '291', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[56]:\\nfrom sklearn.ensemble import RandomForestClassifier\\nrf = RandomForestClassifier(n_estimators=100, random_state=0, max_features=2)\\nrf.fit(X_train, y_train)\\n# RandomForestClassifier has predict_proba, but not decision_function\\nprecision_rf, recall_rf, thresholds_rf = precision_recall_curve(\\n    y_test, rf.predict_proba(X_test)[:, 1])\\nplt.plot(precision, recall, label=\"svc\")\\nplt.plot(precision[close_zero], recall[close_zero], \\'o\\', markersize=10,\\n         label=\"threshold zero svc\", fillstyle=\"none\", c=\\'k\\', mew=2)\\nplt.plot(precision_rf, recall_rf, label=\"rf\")\\nclose_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))\\nplt.plot(precision_rf[close_default_rf], recall_rf[close_default_rf], \\'^\\', c=\\'k\\',\\n         markersize=10, label=\"threshold 0.5 rf\", fillstyle=\"none\", mew=2)\\nplt.xlabel(\"Precision\")\\nplt.ylabel(\"Recall\")\\nplt.legend(loc=\"best\")\\nFigure 5-14. Comparing precision recall curves of SVM and random forest'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 304, 'page_label': '291', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='plt.xlabel(\"Precision\")\\nplt.ylabel(\"Recall\")\\nplt.legend(loc=\"best\")\\nFigure 5-14. Comparing precision recall curves of SVM and random forest\\nFrom the comparison plot we can see that the random forest performs better at the\\nextremes, for very high recall or very high precision requirements. Around the mid‐\\ndle (approximately precision=0.7), the SVM performs better. If we only looked at the\\nf1-score to compare overall performance, we would have missed these subtleties. The\\nf1-score only captures one point on the precision-recall curve, the one given by the\\ndefault threshold:\\nEvaluation Metrics and Scoring | 291'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 305, 'page_label': '292', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='4 There are some minor technical differences between the area under the precision-recall curve and average\\nprecision. However, this explanation conveys the general idea.\\nIn[57]:\\nprint(\"f1_score of random forest: {:.3f}\".format(\\n    f1_score(y_test, rf.predict(X_test))))\\nprint(\"f1_score of svc: {:.3f}\".format(f1_score(y_test, svc.predict(X_test))))\\nOut[57]:\\nf1_score of random forest: 0.610\\nf1_score of svc: 0.656\\nComparing two precision-recall curves provides a lot of detailed insight, but is a fairly\\nmanual process. For automatic model comparison, we might want to summarize the\\ninformation contained in the curve, without limiting ourselves to a particular thresh‐\\nold or operating point. One particular way to summarize the precision-recall curve is\\nby computing the integral or area under the curve of the precision-recall curve, also\\nknown as the average precision.4 Y ou can use the average_precision_score function'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 305, 'page_label': '292', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='by computing the integral or area under the curve of the precision-recall curve, also\\nknown as the average precision.4 Y ou can use the average_precision_score function\\nto compute the average precision. Because we need to compute the ROC curve and\\nconsider multiple thresholds, the result of decision_function or predict_proba\\nneeds to be passed to average_precision_score, not the result of predict:\\nIn[58]:\\nfrom sklearn.metrics import average_precision_score\\nap_rf = average_precision_score(y_test, rf.predict_proba(X_test)[:, 1])\\nap_svc = average_precision_score(y_test, svc.decision_function(X_test))\\nprint(\"Average precision of random forest: {:.3f}\".format(ap_rf))\\nprint(\"Average precision of svc: {:.3f}\".format(ap_svc))\\nOut[58]:\\nAverage precision of random forest: 0.666\\nAverage precision of svc: 0.663\\nWhen averaging over all possible thresholds, we see that the random forest and SVC\\nperform similarly well, with the random forest even slightly ahead. This is quite dif‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 305, 'page_label': '292', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='When averaging over all possible thresholds, we see that the random forest and SVC\\nperform similarly well, with the random forest even slightly ahead. This is quite dif‐\\nferent from the result we got from f1_score earlier. Because average precision is the\\narea under a curve that goes from 0 to 1, average precision always returns a value\\nbetween 0 (worst) and 1 (best). The average precision of a classifier that assigns\\ndecision_function at random is the fraction of positive samples in the dataset.\\nReceiver operating characteristics (ROC) and AUC\\nThere is another tool that is commonly used to analyze the behavior of classifiers at\\ndifferent thresholds: the receiver operating characteristics curve , or ROC curve  for\\nshort. Similar to the precision-recall curve, the ROC curve considers all possible\\n292 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 306, 'page_label': '293', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='thresholds for a given classifier, but instead of reporting precision and recall, it shows\\nthe false positive rate (FPR) against the true positive rate (TPR). Recall that the true\\npositive rate is simply another name for recall, while the false positive rate is the frac‐\\ntion of false positives out of all negative samples:\\nFPR = FP\\nFP+TN\\nThe ROC curve can be computed using the roc_curve function (see Figure 5-15):\\nIn[59]:\\nfrom sklearn.metrics import roc_curve\\nfpr, tpr, thresholds = roc_curve(y_test, svc.decision_function(X_test))\\nplt.plot(fpr, tpr, label=\"ROC Curve\")\\nplt.xlabel(\"FPR\")\\nplt.ylabel(\"TPR (recall)\")\\n# find threshold closest to zero\\nclose_zero = np.argmin(np.abs(thresholds))\\nplt.plot(fpr[close_zero], tpr[close_zero], \\'o\\', markersize=10,\\n         label=\"threshold zero\", fillstyle=\"none\", c=\\'k\\', mew=2)\\nplt.legend(loc=4)\\nFigure 5-15. ROC curve for SVM\\nFor the ROC curve, the ideal curve is close to the top left: you want a classifier that'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 306, 'page_label': '293', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='plt.legend(loc=4)\\nFigure 5-15. ROC curve for SVM\\nFor the ROC curve, the ideal curve is close to the top left: you want a classifier that\\nproduces a high recall while keeping a low false positive rate. Compared to the default\\nthreshold of 0, the curve shows that we can achieve a significantly higher recall\\n(around 0.9) while only increasing the FPR slightly. The point closest to the top left\\nmight be a better operating point than the one chosen by default. Again, be aware that\\nchoosing a threshold should not be done on the test set, but on a separate validation\\nset.\\nEvaluation Metrics and Scoring | 293'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 307, 'page_label': '294', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Y ou can find a comparison of the random forest and the SVM using ROC curves in\\nFigure 5-16:\\nIn[60]:\\nfrom sklearn.metrics import roc_curve\\nfpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, rf.predict_proba(X_test)[:, 1])\\nplt.plot(fpr, tpr, label=\"ROC Curve SVC\")\\nplt.plot(fpr_rf, tpr_rf, label=\"ROC Curve RF\")\\nplt.xlabel(\"FPR\")\\nplt.ylabel(\"TPR (recall)\")\\nplt.plot(fpr[close_zero], tpr[close_zero], \\'o\\', markersize=10,\\n         label=\"threshold zero SVC\", fillstyle=\"none\", c=\\'k\\', mew=2)\\nclose_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))\\nplt.plot(fpr_rf[close_default_rf], tpr[close_default_rf], \\'^\\', markersize=10,\\n         label=\"threshold 0.5 RF\", fillstyle=\"none\", c=\\'k\\', mew=2)\\nplt.legend(loc=4)\\nFigure 5-16. Comparing ROC curves for SVM and random forest\\nAs for the precision-recall curve, we often want to summarize the ROC curve using a\\nsingle number, the area under the curve (this is commonly just referred to as the'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 307, 'page_label': '294', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='As for the precision-recall curve, we often want to summarize the ROC curve using a\\nsingle number, the area under the curve (this is commonly just referred to as the\\nAUC, and it is understood that the curve in question is the ROC curve). We can com‐\\npute the area under the ROC curve using the roc_auc_score function:\\n294 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 308, 'page_label': '295', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[61]:\\nfrom sklearn.metrics import roc_auc_score\\nrf_auc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])\\nsvc_auc = roc_auc_score(y_test, svc.decision_function(X_test))\\nprint(\"AUC for Random Forest: {:.3f}\".format(rf_auc))\\nprint(\"AUC for SVC: {:.3f}\".format(svc_auc))\\nOut[61]:\\nAUC for Random Forest: 0.937\\nAUC for SVC: 0.916\\nComparing the random forest and SVM using the AUC score, we find that the ran‐\\ndom forest performs quite a bit better than the SVM. Recall that because average pre‐\\ncision is the area under a curve that goes from 0 to 1, average precision always returns\\na value between 0 (worst) and 1 (best). Predicting randomly always produces an AUC\\nof 0.5, no matter how imbalanced the classes in a dataset are. This makes AUC a\\nmuch better metric for imbalanced classification problems than accuracy. The AUC\\ncan be interpreted as evaluating the ranking of positive samples. It’s equivalent to the'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 308, 'page_label': '295', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='much better metric for imbalanced classification problems than accuracy. The AUC\\ncan be interpreted as evaluating the ranking of positive samples. It’s equivalent to the\\nprobability that a randomly picked point of the positive class will have a higher score\\naccording to the classifier than a randomly picked point from the negative class. So, a\\nperfect AUC of 1 means that all positive points have a higher score than all negative\\npoints. For classification problems with imbalanced classes, using AUC for model\\nselection is often much more meaningful than using accuracy.\\nLet’s go back to the problem we studied earlier of classifying all nines in the digits\\ndataset versus all other digits. We will classify the dataset with an SVM with three dif‐\\nferent settings of the kernel bandwidth, gamma (see Figure 5-17):\\nIn[62]:\\ny = digits.target == 9\\nX_train, X_test, y_train, y_test = train_test_split(\\n    digits.data, y, random_state=0)\\nplt.figure()\\nfor gamma in [1, 0.05, 0.01]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 308, 'page_label': '295', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[62]:\\ny = digits.target == 9\\nX_train, X_test, y_train, y_test = train_test_split(\\n    digits.data, y, random_state=0)\\nplt.figure()\\nfor gamma in [1, 0.05, 0.01]:\\n    svc = SVC(gamma=gamma).fit(X_train, y_train)\\n    accuracy = svc.score(X_test, y_test)\\n    auc = roc_auc_score(y_test, svc.decision_function(X_test))\\n    fpr, tpr, _ = roc_curve(y_test , svc.decision_function(X_test))\\n    print(\"gamma = {:.2f}  accuracy = {:.2f}  AUC = {:.2f}\".format(\\n    gamma, accuracy, auc))\\n    plt.plot(fpr, tpr, label=\"gamma={:.3f}\".format(gamma))\\nplt.xlabel(\"FPR\")\\nplt.ylabel(\"TPR\")\\nplt.xlim(-0.01, 1)\\nplt.ylim(0, 1.02)\\nplt.legend(loc=\"best\")\\nEvaluation Metrics and Scoring | 295'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 309, 'page_label': '296', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='5 Looking at the curve for gamma=0.01 in detail, you can see a small kink close to the top left. That means that at\\nleast one point was not ranked correctly. The AUC of 1.0 is a consequence of rounding to the second decimal\\npoint.\\nOut[62]:\\ngamma = 1.00  accuracy = 0.90  AUC = 0.50\\ngamma = 0.05  accuracy = 0.90  AUC = 0.90\\ngamma = 0.01  accuracy = 0.90  AUC = 1.00\\nFigure 5-17. Comparing ROC curves of SVMs with different settings of gamma\\nThe accuracy of all three settings of gamma is the same, 90%. This might be the same\\nas chance performance, or it might not. Looking at the AUC and the corresponding\\ncurve, however, we see a clear distinction between the three models. With gamma=1.0,\\nthe AUC is actually at chance level, meaning that the output of the decision_func\\ntion is as good as random. With gamma=0.05, performance drastically improves to an\\nAUC of 0.5. Finally, with gamma=0.01, we get a perfect AUC of 1.0. That means that'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 309, 'page_label': '296', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='tion is as good as random. With gamma=0.05, performance drastically improves to an\\nAUC of 0.5. Finally, with gamma=0.01, we get a perfect AUC of 1.0. That means that\\nall positive points are ranked higher than all negative points according to the decision\\nfunction. In other words, with the right threshold, this model can classify the data\\nperfectly!5 Knowing this, we can adjust the threshold on this model and obtain great\\npredictions. If we had only used accuracy, we would never have discovered this.\\nFor this reason, we highly recommend using AUC when evaluating models on imbal‐\\nanced data. Keep in mind that AUC does not make use of the default threshold,\\nthough, so adjusting the decision threshold might be necessary to obtain useful classi‐\\nfication results from a model with a high AUC.\\nMetrics for Multiclass Classification\\nNow that we have discussed evaluation of binary classification tasks in depth, let’s'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 309, 'page_label': '296', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='fication results from a model with a high AUC.\\nMetrics for Multiclass Classification\\nNow that we have discussed evaluation of binary classification tasks in depth, let’s\\nmove on to metrics to evaluate multiclass classification. Basically, all metrics for\\nmulticlass classification are derived from binary classification metrics, but averaged\\n296 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 310, 'page_label': '297', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='over all classes. Accuracy for multiclass classification is again defined as the fraction\\nof correctly classified examples. And again, when classes are imbalanced, accuracy is\\nnot a great evaluation measure. Imagine a three-class classification problem with 85%\\nof points belonging to class A, 10% belonging to class B, and 5% belonging to class C.\\nWhat does being 85% accurate mean on this dataset? In general, multiclass classifica‐\\ntion results are harder to understand than binary classification results. Apart from\\naccuracy, common tools are the confusion matrix and the classification report we saw\\nin the binary case in the previous section. Let’s apply these two detailed evaluation\\nmethods on the task of classifying the 10 different handwritten digits in the digits\\ndataset:\\nIn[63]:\\nfrom sklearn.metrics import accuracy_score\\nX_train, X_test, y_train, y_test = train_test_split(\\n    digits.data, digits.target, random_state=0)\\nlr = LogisticRegression().fit(X_train, y_train)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 310, 'page_label': '297', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[63]:\\nfrom sklearn.metrics import accuracy_score\\nX_train, X_test, y_train, y_test = train_test_split(\\n    digits.data, digits.target, random_state=0)\\nlr = LogisticRegression().fit(X_train, y_train)\\npred = lr.predict(X_test)\\nprint(\"Accuracy: {:.3f}\".format(accuracy_score(y_test, pred)))\\nprint(\"Confusion matrix:\\\\n{}\".format(confusion_matrix(y_test, pred)))\\nOut[63]:\\nAccuracy: 0.953\\nConfusion matrix:\\n[[37  0  0  0  0  0  0  0  0  0]\\n [ 0 39  0  0  0  0  2  0  2  0]\\n [ 0  0 41  3  0  0  0  0  0  0]\\n [ 0  0  1 43  0  0  0  0  0  1]\\n [ 0  0  0  0 38  0  0  0  0  0]\\n [ 0  1  0  0  0 47  0  0  0  0]\\n [ 0  0  0  0  0  0 52  0  0  0]\\n [ 0  1  0  1  1  0  0 45  0  0]\\n [ 0  3  1  0  0  0  0  0 43  1]\\n [ 0  0  0  1  0  1  0  0  1 44]]\\nThe model has an accuracy of 95.3%, which already tells us that we are doing pretty\\nwell. The confusion matrix provides us with some more detail. As for the binary case,\\neach row corresponds to a true label, and each column corresponds to a predicted'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 310, 'page_label': '297', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='well. The confusion matrix provides us with some more detail. As for the binary case,\\neach row corresponds to a true label, and each column corresponds to a predicted\\nlabel. Y ou can find a visually more appealing plot in Figure 5-18:\\nIn[64]:\\nscores_image = mglearn.tools.heatmap(\\n    confusion_matrix(y_test, pred), xlabel=\\'Predicted label\\',\\n    ylabel=\\'True label\\', xticklabels=digits.target_names,\\n    yticklabels=digits.target_names, cmap=plt.cm.gray_r, fmt=\"%d\")\\nplt.title(\"Confusion matrix\")\\nplt.gca().invert_yaxis()\\nEvaluation Metrics and Scoring | 297'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 311, 'page_label': '298', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 5-18. Confusion matrix for the 10-digit classification task\\nFor the first class, the digit 0, there are 37 samples in the class, and all of these sam‐\\nples were classified as class 0 (there are no false negatives for class 0). We can see that\\nbecause all other entries in the first row of the confusion matrix are 0. We can also see\\nthat no other digits were mistakenly classified as 0, because all other entries in the\\nfirst column of the confusion matrix are 0 (there are no false positives for class 0).\\nSome digits were confused with others, though—for example, the digit 2 (third row),\\nthree of which were classified as the digit 3 (fourth column). There was also one digit\\n3 that was classified as 2 (third column, fourth row) and one digit 8 that was classified\\nas 2 (thrid column, fourth row).\\nWith the classification_report function, we can compute the precision, recall,\\nand f-score for each class:\\nIn[65]:\\nprint(classification_report(y_test, pred))\\nOut[65]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 311, 'page_label': '298', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='as 2 (thrid column, fourth row).\\nWith the classification_report function, we can compute the precision, recall,\\nand f-score for each class:\\nIn[65]:\\nprint(classification_report(y_test, pred))\\nOut[65]:\\n             precision    recall  f1-score   support\\n          0       1.00      1.00      1.00        37\\n          1       0.89      0.91      0.90        43\\n          2       0.95      0.93      0.94        44\\n          3       0.90      0.96      0.92        45\\n          4       0.97      1.00      0.99        38\\n          5       0.98      0.98      0.98        48\\n          6       0.96      1.00      0.98        52\\n          7       1.00      0.94      0.97        48\\n          8       0.93      0.90      0.91        48\\n          9       0.96      0.94      0.95        47\\navg / total       0.95      0.95      0.95       450\\n298 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 312, 'page_label': '299', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Unsurprisingly, precision and recall are a perfect 1 for class 0, as there are no confu‐\\nsions with this class. For class 7, on the other hand, precision is 1 because no other\\nclass was mistakenly classified as 7, while for class 6, there are no false negatives, so\\nthe recall is 1. We can also see that the model has particular difficulties with classes 8\\nand 3.\\nThe most commonly used metric for imbalanced datasets in the multiclass setting is\\nthe multiclass version of the f-score. The idea behind the multiclass f-score is to com‐\\npute one binary f-score per class, with that class being the positive class and the other\\nclasses making up the negative classes. Then, these per-class f-scores are averaged\\nusing one of the following strategies:\\n• \"macro\" averaging computes the unweighted per-class f-scores. This gives equal\\nweight to all classes, no matter what their size is.\\n• \"weighted\" averaging computes the mean of the per-class f-scores, weighted by'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 312, 'page_label': '299', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='weight to all classes, no matter what their size is.\\n• \"weighted\" averaging computes the mean of the per-class f-scores, weighted by\\ntheir support. This is what is reported in the classification report.\\n• \"micro\" averaging computes the total number of false positives, false negatives,\\nand true positives over all classes, and then computes precision, recall, and f-\\nscore using these counts.\\nIf you care about each sample equally much, it is recommended to use the \"micro\"\\naverage f1-score; if you care about each class equally much, it is recommended to use\\nthe \"macro\" average f1-score:\\nIn[66]:\\nprint(\"Micro average f1 score: {:.3f}\".format\\n       (f1_score(y_test, pred, average=\"micro\")))\\nprint(\"Macro average f1 score: {:.3f}\".format\\n       (f1_score(y_test, pred, average=\"macro\")))\\nOut[66]:\\nMicro average f1 score: 0.953\\nMacro average f1 score: 0.954\\nRegression Metrics\\nEvaluation for regression can be done in similar detail as we did for classification—'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 312, 'page_label': '299', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Out[66]:\\nMicro average f1 score: 0.953\\nMacro average f1 score: 0.954\\nRegression Metrics\\nEvaluation for regression can be done in similar detail as we did for classification—\\nfor example, by analyzing overpredicting the target versus underpredicting the target.\\nHowever, in most applications we’ve seen, using the default R2 used in the score\\nmethod of all regressors is enough. Sometimes business decisions are made on the\\nbasis of mean squared error or mean absolute error, which might give incentive to\\ntune models using these metrics. In general, though, we have found R2 to be a more\\nintuitive metric to evaluate regression models.\\nEvaluation Metrics and Scoring | 299'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 313, 'page_label': '300', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Using Evaluation Metrics in Model Selection\\nWe have discussed many evaluation methods in detail, and how to apply them given\\nthe ground truth and a model. However, we often want to use metrics like AUC in\\nmodel selection using GridSearchCV or cross_val_score. Luckily scikit-learn\\nprovides a very simple way to achieve this, via the scoring argument that can be used\\nin both GridSearchCV and cross_val_score. Y ou can simply provide a string\\ndescribing the evaluation metric you want to use. Say, for example, we want to evalu‐\\nate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUC\\nscore. Changing the score from the default (accuracy) to AUC can be done by provid‐\\ning \"roc_auc\" as the scoring parameter:\\nIn[67]:\\n# default scoring for classification is accuracy\\nprint(\"Default scoring: {}\".format(\\n    cross_val_score(SVC(), digits.data, digits.target == 9)))\\n# providing scoring=\"accuracy\" doesn\\'t change the results'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 313, 'page_label': '300', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='print(\"Default scoring: {}\".format(\\n    cross_val_score(SVC(), digits.data, digits.target == 9)))\\n# providing scoring=\"accuracy\" doesn\\'t change the results\\nexplicit_accuracy =  cross_val_score(SVC(), digits.data, digits.target == 9,\\n                                     scoring=\"accuracy\")\\nprint(\"Explicit accuracy scoring: {}\".format(explicit_accuracy))\\nroc_auc =  cross_val_score(SVC(), digits.data, digits.target == 9,\\n                           scoring=\"roc_auc\")\\nprint(\"AUC scoring: {}\".format(roc_auc))\\nOut[67]:\\nDefault scoring: [ 0.9  0.9  0.9]\\nExplicit accuracy scoring: [ 0.9  0.9  0.9]\\nAUC scoring: [ 0.994  0.99   0.996]\\nSimilarly, we can change the metric used to pick the best parameters in Grid\\nSearchCV:\\nIn[68]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    digits.data, digits.target == 9, random_state=0)\\n# we provide a somewhat bad grid to illustrate the point:\\nparam_grid = {\\'gamma\\': [0.0001, 0.01, 0.1, 1, 10]}\\n# using the default scoring of accuracy:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 313, 'page_label': '300', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='# we provide a somewhat bad grid to illustrate the point:\\nparam_grid = {\\'gamma\\': [0.0001, 0.01, 0.1, 1, 10]}\\n# using the default scoring of accuracy:\\ngrid = GridSearchCV(SVC(), param_grid=param_grid)\\ngrid.fit(X_train, y_train)\\nprint(\"Grid-Search with accuracy\")\\nprint(\"Best parameters:\", grid.best_params_)\\nprint(\"Best cross-validation score (accuracy)): {:.3f}\".format(grid.best_score_))\\nprint(\"Test set AUC: {:.3f}\".format(\\n    roc_auc_score(y_test, grid.decision_function(X_test))))\\nprint(\"Test set accuracy: {:.3f}\".format(grid.score(X_test, y_test)))\\n300 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 314, 'page_label': '301', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='6 Finding a higher-accuracy solution using AUC is likely a consequence of accuracy being a bad measure of\\nmodel performance on imbalanced data.\\nOut[68]:\\nGrid-Search with accuracy\\nBest parameters: {\\'gamma\\': 0.0001}\\nBest cross-validation score (accuracy)): 0.970\\nTest set AUC: 0.992\\nTest set accuracy: 0.973\\nIn[69]:\\n# using AUC scoring instead:\\ngrid = GridSearchCV(SVC(), param_grid=param_grid, scoring=\"roc_auc\")\\ngrid.fit(X_train, y_train)\\nprint(\"\\\\nGrid-Search with AUC\")\\nprint(\"Best parameters:\", grid.best_params_)\\nprint(\"Best cross-validation score (AUC): {:.3f}\".format(grid.best_score_))\\nprint(\"Test set AUC: {:.3f}\".format(\\n    roc_auc_score(y_test, grid.decision_function(X_test))))\\nprint(\"Test set accuracy: {:.3f}\".format(grid.score(X_test, y_test)))\\nOut[69]:\\nGrid-Search with AUC\\nBest parameters: {\\'gamma\\': 0.01}\\nBest cross-validation score (AUC): 0.997\\nTest set AUC: 1.000\\nTest set accuracy: 1.000\\nWhen using accuracy, the parameter gamma=0.0001 is selected, while gamma=0.01 is'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 314, 'page_label': '301', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"Best parameters: {'gamma': 0.01}\\nBest cross-validation score (AUC): 0.997\\nTest set AUC: 1.000\\nTest set accuracy: 1.000\\nWhen using accuracy, the parameter gamma=0.0001 is selected, while gamma=0.01 is\\nselected when using AUC. The cross-validation accuracy is consistent with the test set\\naccuracy in both cases. However, using AUC found a better parameter setting in\\nterms of AUC and even in terms of accuracy.6\\nThe most important values for the scoring parameter for classification are accuracy\\n(the default); roc_auc for the area under the ROC curve; average_precision for the\\narea under the precision-recall curve; f1, f1_macro, f1_micro, and f1_weighted for\\nthe binary f1-score and the different weighted variants. For regression, the most com‐\\nmonly used values are r2 for the R2 score, mean_squared_error for mean squared\\nerror, and mean_absolute_error for mean absolute error. Y ou can find a full list of\\nsupported arguments in the documentation or by looking at the SCORER dictionary\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 314, 'page_label': '301', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='error, and mean_absolute_error for mean absolute error. Y ou can find a full list of\\nsupported arguments in the documentation or by looking at the SCORER dictionary\\ndefined in the metrics.scorer module:\\nEvaluation Metrics and Scoring | 301'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 315, 'page_label': '302', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='7 We highly recommend Foster Provost and Tom Fawcett’s book Data Science for Business (O’Reilly) for more\\ninformation on this topic.\\nIn[70]:\\nfrom sklearn.metrics.scorer import SCORERS\\nprint(\"Available scorers:\\\\n{}\".format(sorted(SCORERS.keys())))\\nOut[70]:\\nAvailable scorers:\\n[\\'accuracy\\', \\'adjusted_rand_score\\', \\'average_precision\\', \\'f1\\', \\'f1_macro\\',\\n \\'f1_micro\\', \\'f1_samples\\', \\'f1_weighted\\', \\'log_loss\\', \\'mean_absolute_error\\',\\n \\'mean_squared_error\\', \\'median_absolute_error\\', \\'precision\\', \\'precision_macro\\',\\n \\'precision_micro\\', \\'precision_samples\\', \\'precision_weighted\\', \\'r2\\', \\'recall\\',\\n \\'recall_macro\\', \\'recall_micro\\', \\'recall_samples\\', \\'recall_weighted\\', \\'roc_auc\\']\\nSummary and Outlook\\nIn this chapter we discussed cross-validation, grid search, and evaluation metrics, the\\ncornerstones of evaluating and improving machine learning algorithms. The tools\\ndescribed in this chapter, together with the algorithms described in Chapters 2 and 3,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 315, 'page_label': '302', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='cornerstones of evaluating and improving machine learning algorithms. The tools\\ndescribed in this chapter, together with the algorithms described in Chapters 2 and 3,\\nare the bread and butter of every machine learning practitioner.\\nThere are two particular points that we made in this chapter that warrant repeating,\\nbecause they are often overlooked by new practitioners. The first has to do with\\ncross-validation. Cross-validation or the use of a test set allow us to evaluate a\\nmachine learning model as it will perform in the future. However, if we use the test\\nset or cross-validation to select a model or select model parameters, we “use up” the\\ntest data, and using the same data to evaluate how well our model will do in the future\\nwill lead to overly optimistic estimates. We therefore need to resort to a split into\\ntraining data for model building, validation data for model and parameter selection,\\nand test data for model evaluation. Instead of a simple split, we can replace each of'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 315, 'page_label': '302', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='training data for model building, validation data for model and parameter selection,\\nand test data for model evaluation. Instead of a simple split, we can replace each of\\nthese splits with cross-validation. The most commonly used form (as described ear‐\\nlier) is a training/test split for evaluation, and using cross-validation on the training\\nset for model and parameter selection.\\nThe second point has to do with the importance of the evaluation metric or scoring\\nfunction used for model selection and model evaluation. The theory of how to make\\nbusiness decisions from the predictions of a machine learning model is somewhat\\nbeyond the scope of this book. 7 However, it is rarely the case that the end goal of a\\nmachine learning task is building a model with a high accuracy. Make sure that the\\nmetric you choose to evaluate and select a model for is a good stand-in for what the\\nmodel will actually be used for. In reality, classification problems rarely have balanced'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 315, 'page_label': '302', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='metric you choose to evaluate and select a model for is a good stand-in for what the\\nmodel will actually be used for. In reality, classification problems rarely have balanced\\nclasses, and often false positives and false negatives have very different consequences.\\n302 | Chapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 316, 'page_label': '303', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Make sure you understand what these consequences are, and pick an evaluation met‐\\nric accordingly.\\nThe model evaluation and selection techniques we have described so far are the most\\nimportant tools in a data scientist’s toolbox. Grid search and cross-validation as we’ve\\ndescribed them in this chapter can only be applied to a single supervised model. We\\nhave seen before, however, that many models require preprocessing, and that in some\\napplications, like the face recognition example in Chapter 3 , extracting a different\\nrepresentation of the data can be useful. In the next chapter, we will introduce the\\nPipeline class, which allows us to use grid search and cross-validation on these com‐\\nplex chains of algorithms.\\nSummary and Outlook | 303'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 318, 'page_label': '305', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 6\\nAlgorithm Chains and Pipelines\\nFor many machine learning algorithms, the particular representation of the data that\\nyou provide is very important, as we discussed in Chapter 4. This starts with scaling\\nthe data and combining features by hand and goes all the way to learning features\\nusing unsupervised machine learning, as we saw in Chapter 3. Consequently, most\\nmachine learning applications require not only the application of a single algorithm,\\nbut the chaining together of many different processing steps and machine learning\\nmodels. In this chapter, we will cover how to use the Pipeline class to simplify the\\nprocess of building chains of transformations and models. In particular, we will see\\nhow we can combine Pipeline and GridSearchCV to search over parameters for all\\nprocessing steps at once.\\nAs an example of the importance of chaining models, we noticed that we can greatly\\nimprove the performance of a kernel SVM on the cancer dataset by using the Min'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 318, 'page_label': '305', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='processing steps at once.\\nAs an example of the importance of chaining models, we noticed that we can greatly\\nimprove the performance of a kernel SVM on the cancer dataset by using the Min\\nMaxScaler for preprocessing. Here’s code for splitting the data, computing the mini‐\\nmum and maximum, scaling the data, and training the SVM:\\nIn[1]:\\nfrom sklearn.svm import SVC\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import MinMaxScaler\\n# load and split the data\\ncancer = load_breast_cancer()\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, random_state=0)\\n# compute minimum and maximum on the training data\\nscaler = MinMaxScaler().fit(X_train)\\n305'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 319, 'page_label': '306', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[2]:\\n# rescale the training data\\nX_train_scaled = scaler.transform(X_train)\\nsvm = SVC()\\n# learn an SVM on the scaled training data\\nsvm.fit(X_train_scaled, y_train)\\n# scale the test data and score the scaled data\\nX_test_scaled = scaler.transform(X_test)\\nprint(\"Test score: {:.2f}\".format(svm.score(X_test_scaled, y_test)))\\nOut[2]:\\nTest score: 0.95\\nParameter Selection with Preprocessing\\nNow let’s say we want to find better parameters for SVC using GridSearchCV, as dis‐\\ncussed in Chapter 5. How should we go about doing this? A naive approach might\\nlook like this:\\nIn[3]:\\nfrom sklearn.model_selection import GridSearchCV\\n# for illustration purposes only, don\\'t use this code!\\nparam_grid = {\\'C\\': [0.001, 0.01, 0.1, 1, 10, 100],\\n              \\'gamma\\': [0.001, 0.01, 0.1, 1, 10, 100]}\\ngrid = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\\ngrid.fit(X_train_scaled, y_train)\\nprint(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 319, 'page_label': '306', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='grid = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\\ngrid.fit(X_train_scaled, y_train)\\nprint(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\\nprint(\"Best set score: {:.2f}\".format(grid.score(X_test_scaled, y_test)))\\nprint(\"Best parameters: \", grid.best_params_)\\nOut[3]:\\nBest cross-validation accuracy: 0.98\\nBest set score: 0.97\\nBest parameters:  {\\'gamma\\': 1, \\'C\\': 1}\\nHere, we ran the grid search over the parameters of SVC using the scaled data. How‐\\never, there is a subtle catch in what we just did. When scaling the data, we used all the\\ndata in the training set to find out how to train it. We then use the scaled training data\\nto run our grid search using cross-validation. For each split in the cross-validation,\\nsome part of the original training set will be declared the training part of the split,\\nand some the test part of the split. The test part is used to measure what new data will'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 319, 'page_label': '306', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='some part of the original training set will be declared the training part of the split,\\nand some the test part of the split. The test part is used to measure what new data will\\nlook like to a model trained on the training part. However, we already used the infor‐\\nmation contained in the test part of the split, when scaling the data. Remember that\\nthe test part in each split in the cross-validation is part of the training set, and we\\nused the information from the entire training set to find the right scaling of the data.\\n306 | Chapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 320, 'page_label': '307', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='This is fundamentally different from how new data looks to the model.  If we observe\\nnew data (say, in form of our test set), this data will not have been used to scale the\\ntraining data, and it might have a different minimum and maximum than the train‐\\ning data. The following example ( Figure 6-1) shows how the data processing during\\ncross-validation and the final evaluation differ:\\nIn[4]:\\nmglearn.plots.plot_improper_processing()\\nFigure 6-1. Data usage when preprocessing outside the cross-validation loop\\nSo, the splits in the cross-validation no longer correctly mirror how new data will\\nlook to the modeling process. We already leaked information from these parts of the\\ndata into our modeling process. This will lead to overly optimistic results during\\ncross-validation, and possibly the selection of suboptimal parameters.\\nTo get around this problem, the splitting of the dataset during cross-validation should'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 320, 'page_label': '307', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='cross-validation, and possibly the selection of suboptimal parameters.\\nTo get around this problem, the splitting of the dataset during cross-validation should\\nbe done before doing any preprocessing. Any process that extracts knowledge from the\\ndataset should only ever be applied to the training portion of the dataset, so any\\ncross-validation should be the “outermost loop” in your processing.\\nTo achieve this in scikit-learn with the cross_val_score function and the Grid\\nSearchCV function, we can use the Pipeline class. The Pipeline class is a class that\\nallows “gluing” together multiple processing steps into a single scikit-learn estima‐\\nParameter Selection with Preprocessing | 307'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 321, 'page_label': '308', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='1 With one exception: the name can’t contain a double underscore, __.\\ntor. The Pipeline class itself has fit, predict, and score methods and behaves just\\nlike any other model in scikit-learn. The most common use case of the Pipeline\\nclass is in chaining preprocessing steps (like scaling of the data) together with a\\nsupervised model like a classifier.\\nBuilding Pipelines\\nLet’s look at how we can use the Pipeline class to express the workflow for training\\nan SVM after scaling the data with MinMaxScaler (for now without the grid search).\\nFirst, we build a pipeline object by providing it with a list of steps. Each step is a tuple\\ncontaining a name (any string of your choosing1) and an instance of an estimator:\\nIn[5]:\\nfrom sklearn.pipeline import Pipeline\\npipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])\\nHere, we created two steps: the first, called \"scaler\", is an instance of MinMaxScaler,\\nand the second, called \"svm\", is an instance of SVC. Now, we can fit the pipeline, like'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 321, 'page_label': '308', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Here, we created two steps: the first, called \"scaler\", is an instance of MinMaxScaler,\\nand the second, called \"svm\", is an instance of SVC. Now, we can fit the pipeline, like\\nany other scikit-learn estimator:\\nIn[6]:\\npipe.fit(X_train, y_train)\\nHere, pipe.fit first calls fit on the first step (the scaler), then transforms the train‐\\ning data using the scaler, and finally fits the SVM with the scaled data. To evaluate on\\nthe test data, we simply call pipe.score:\\nIn[7]:\\nprint(\"Test score: {:.2f}\".format(pipe.score(X_test, y_test)))\\nOut[7]:\\nTest score: 0.95\\nCalling the score method on the pipeline first transforms the test data using the\\nscaler, and then calls the score method on the SVM using the scaled test data. As you\\ncan see, the result is identical to the one we got from the code at the beginning of the\\nchapter, when doing the transformations by hand. Using the pipeline, we reduced the\\ncode needed for our “preprocessing + classification” process. The main benefit of'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 321, 'page_label': '308', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='chapter, when doing the transformations by hand. Using the pipeline, we reduced the\\ncode needed for our “preprocessing + classification” process. The main benefit of\\nusing the pipeline, however, is that we can now use this single estimator in\\ncross_val_score or GridSearchCV.\\n308 | Chapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 322, 'page_label': '309', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Using Pipelines in Grid Searches\\nUsing a pipeline in a grid search works the same way as using any other estimator. We\\ndefine a parameter grid to search over, and construct a GridSearchCV from the pipe‐\\nline and the parameter grid. When specifying the parameter grid, there is a slight\\nchange, though. We need to specify for each parameter which step of the pipeline it\\nbelongs to. Both parameters that we want to adjust, C and gamma, are parameters of\\nSVC, the second step. We gave this step the name \"svm\". The syntax to define a param‐\\neter grid for a pipeline is to specify for each parameter the step name, followed by __\\n(a double underscore), followed by the parameter name. To search over the C param‐\\neter of SVC we therefore have to use \"svm__C\" as the key in the parameter grid dictio‐\\nnary, and similarly for gamma:\\nIn[8]:\\nparam_grid = {\\'svm__C\\': [0.001, 0.01, 0.1, 1, 10, 100],\\n              \\'svm__gamma\\': [0.001, 0.01, 0.1, 1, 10, 100]}'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 322, 'page_label': '309', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='nary, and similarly for gamma:\\nIn[8]:\\nparam_grid = {\\'svm__C\\': [0.001, 0.01, 0.1, 1, 10, 100],\\n              \\'svm__gamma\\': [0.001, 0.01, 0.1, 1, 10, 100]}\\nWith this parameter grid we can use GridSearchCV as usual:\\nIn[9]:\\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\\ngrid.fit(X_train, y_train)\\nprint(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\\nprint(\"Test set score: {:.2f}\".format(grid.score(X_test, y_test)))\\nprint(\"Best parameters: {}\".format(grid.best_params_))\\nOut[9]:\\nBest cross-validation accuracy: 0.98\\nTest set score: 0.97\\nBest parameters: {\\'svm__C\\': 1, \\'svm__gamma\\': 1}\\nIn contrast to the grid search we did before, now for each split in the cross-validation,\\nthe MinMaxScaler is refit with only the training splits and no information is leaked\\nfrom the test split into the parameter search. Compare this ( Figure 6-2 ) with\\nFigure 6-1 earlier in this chapter:\\nIn[10]:\\nmglearn.plots.plot_proper_processing()\\nUsing Pipelines in Grid Searches | 309'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 323, 'page_label': '310', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 6-2. Data usage when preprocessing inside the cross-validation loop with a\\npipeline\\nThe impact of leaking information in the cross-validation varies depending on the\\nnature of the preprocessing step. Estimating the scale of the data using the test fold\\nusually doesn’t have a terrible impact, while using the test fold in feature extraction\\nand feature selection can lead to substantial differences in outcomes.\\nIllustrating Information Leakage\\nA great example of leaking information in cross-validation is given in Hastie, Tibshir‐\\nani, and Friedman’s book The Elements of Statistical Learning , and we reproduce an\\nadapted version here. Let’s consider a synthetic regression task with 100 samples and\\n1,000 features that are sampled independently from a Gaussian distribution. We also\\nsample the response from a Gaussian distribution:\\nIn[11]:\\nrnd = np.random.RandomState(seed=0)\\nX = rnd.normal(size=(100, 10000))\\ny = rnd.normal(size=(100,))'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 323, 'page_label': '310', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='sample the response from a Gaussian distribution:\\nIn[11]:\\nrnd = np.random.RandomState(seed=0)\\nX = rnd.normal(size=(100, 10000))\\ny = rnd.normal(size=(100,))\\nGiven the way we created the dataset, there is no relation between the data, X, and the\\ntarget, y (they are independent), so it should not be possible to learn anything from\\nthis dataset. We will now do the following. First, select the most informative of the 10\\nfeatures using SelectPercentile feature selection, and then we evaluate a Ridge\\nregressor using cross-validation:\\n310 | Chapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 324, 'page_label': '311', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[12]:\\nfrom sklearn.feature_selection import SelectPercentile, f_regression\\nselect = SelectPercentile(score_func=f_regression, percentile=5).fit(X, y)\\nX_selected = select.transform(X)\\nprint(\"X_selected.shape: {}\".format(X_selected.shape))\\nOut[12]:\\nX_selected.shape: (100, 500)\\nIn[13]:\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.linear_model import Ridge\\nprint(\"Cross-validation accuracy (cv only on ridge): {:.2f}\".format(\\n      np.mean(cross_val_score(Ridge(), X_selected, y, cv=5))))\\nOut[13]:\\nCross-validation accuracy (cv only on ridge): 0.91\\nThe mean R2 computed by cross-validation is 0.91, indicating a very good model.\\nThis clearly cannot be right, as our data is entirely random. What happened here is\\nthat our feature selection picked out some features among the 10,000 random features\\nthat are (by chance) very well correlated with the target. Because we fit the feature\\nselection outside of the cross-validation, it could find features that are correlated both'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 324, 'page_label': '311', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='that are (by chance) very well correlated with the target. Because we fit the feature\\nselection outside of the cross-validation, it could find features that are correlated both\\non the training and the test folds. The information we leaked from the test folds was\\nvery informative, leading to highly unrealistic results. Let’s compare this to a proper\\ncross-validation using a pipeline:\\nIn[14]:\\npipe = Pipeline([(\"select\", SelectPercentile(score_func=f_regression,\\n                                             percentile=5)),\\n                 (\"ridge\", Ridge())])\\nprint(\"Cross-validation accuracy (pipeline): {:.2f}\".format(\\n      np.mean(cross_val_score(pipe, X, y, cv=5))))\\nOut[14]:\\nCross-validation accuracy (pipeline): -0.25\\nThis time, we get a negative R2 score, indicating a very poor model. Using the pipe‐\\nline, the feature selection is now inside the cross-validation loop. This means features\\ncan only be selected using the training folds of the data, not the test fold. The feature'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 324, 'page_label': '311', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='line, the feature selection is now inside the cross-validation loop. This means features\\ncan only be selected using the training folds of the data, not the test fold. The feature\\nselection finds features that are correlated with the target on the training set, but\\nbecause the data is entirely random, these features are not correlated with the target\\non the test set. In this example, rectifying the data leakage issue in the feature selec‐\\ntion makes the difference between concluding that a model works very well and con‐\\ncluding that a model works not at all.\\nUsing Pipelines in Grid Searches | 311'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 325, 'page_label': '312', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='2 Or just fit_transform.\\nThe General Pipeline Interface\\nThe Pipeline class is not restricted to preprocessing and classification, but can in\\nfact join any number of estimators together. For example, you could build a pipeline\\ncontaining feature extraction, feature selection, scaling, and classification, for a total\\nof four steps. Similarly, the last step could be regression or clustering instead of classi‐\\nfication.\\nThe only requirement for estimators in a pipeline is that all but the last step need to\\nhave a transform method, so they can produce a new representation of the data that\\ncan be used in the next step.\\nInternally, during the call to Pipeline.fit, the pipeline calls fit and then transform\\non each step in turn, 2 with the input given by the output of the transform method of\\nthe previous step. For the last step in the pipeline, just fit is called.\\nBrushing over some finer details, this is implemented as follows. Remember that pipe'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 325, 'page_label': '312', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the previous step. For the last step in the pipeline, just fit is called.\\nBrushing over some finer details, this is implemented as follows. Remember that pipe\\nline.steps is a list of tuples, so pipeline.steps[0][1] is the first estimator, pipe\\nline.steps[1][1] is the second estimator, and so on:\\nIn[15]:\\ndef fit(self, X, y):\\n    X_transformed = X\\n    for name, estimator in self.steps[:-1]:\\n        # iterate over all but the final step\\n        # fit and transform the data\\n        X_transformed = estimator.fit_transform(X_transformed, y)\\n    # fit the last step\\n    self.steps[-1][1].fit(X_transformed, y)\\n    return self\\nWhen predicting using Pipeline, we similarly transform the data using all but the\\nlast step, and then call predict on the last step:\\nIn[16]:\\ndef predict(self, X):\\n    X_transformed = X\\n    for step in self.steps[:-1]:\\n        # iterate over all but the final step\\n        # transform the data\\n        X_transformed = step[1].transform(X_transformed)\\n    # fit the last step'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 325, 'page_label': '312', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='for step in self.steps[:-1]:\\n        # iterate over all but the final step\\n        # transform the data\\n        X_transformed = step[1].transform(X_transformed)\\n    # fit the last step\\n    return self.steps[-1][1].predict(X_transformed)\\n312 | Chapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 326, 'page_label': '313', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='The process is illustrated in Figure 6-3  for two transformers, T1 and T2, and a\\nclassifier (called Classifier).\\nFigure 6-3. Overview of the pipeline training and prediction process\\nThe pipeline is actually even more general than this. There is no requirement for the\\nlast step in a pipeline to have a predict function, and we could create a pipeline just\\ncontaining, for example, a scaler and PCA. Then, because the last step ( PCA) has a\\ntransform method, we could call transform on the pipeline to get the output of\\nPCA.transform applied to the data that was processed by the previous step. The last\\nstep of a pipeline is only required to have a fit method.\\nConvenient Pipeline Creation with make_pipeline\\nCreating a pipeline using the syntax described earlier is sometimes a bit cumbersome,\\nand we often don’t need user-specified names for each step. There is a convenience\\nfunction, make_pipeline, that will create a pipeline for us and automatically name'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 326, 'page_label': '313', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='and we often don’t need user-specified names for each step. There is a convenience\\nfunction, make_pipeline, that will create a pipeline for us and automatically name\\neach step based on its class. The syntax for make_pipeline is as follows:\\nIn[17]:\\nfrom sklearn.pipeline import make_pipeline\\n# standard syntax\\npipe_long = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\\n# abbreviated syntax\\npipe_short = make_pipeline(MinMaxScaler(), SVC(C=100))\\nThe General Pipeline Interface | 313'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 327, 'page_label': '314', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='The pipeline objects pipe_long and pipe_short do exactly the same thing, but\\npipe_short has steps that were automatically named. We can see the names of the\\nsteps by looking at the steps attribute:\\nIn[18]:\\nprint(\"Pipeline steps:\\\\n{}\".format(pipe_short.steps))\\nOut[18]:\\nPipeline steps:\\n[(\\'minmaxscaler\\', MinMaxScaler(copy=True, feature_range=(0, 1))),\\n (\\'svc\\', SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\\n      decision_function_shape=None, degree=3, gamma=\\'auto\\',\\n             kernel=\\'rbf\\', max_iter=-1, probability=False,\\n             random_state=None, shrinking=True, tol=0.001,\\n             verbose=False))]\\nThe steps are named minmaxscaler and svc. In general, the step names are just low‐\\nercase versions of the class names. If multiple steps have the same class, a number is\\nappended:\\nIn[19]:\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.decomposition import PCA\\npipe = make_pipeline(StandardScaler(), PCA(n_components=2), StandardScaler())'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 327, 'page_label': '314', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='appended:\\nIn[19]:\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.decomposition import PCA\\npipe = make_pipeline(StandardScaler(), PCA(n_components=2), StandardScaler())\\nprint(\"Pipeline steps:\\\\n{}\".format(pipe.steps))\\nOut[19]:\\nPipeline steps:\\n[(\\'standardscaler-1\\', StandardScaler(copy=True, with_mean=True, with_std=True)),\\n (\\'pca\\', PCA(copy=True, iterated_power=4, n_components=2, random_state=None,\\n             svd_solver=\\'auto\\', tol=0.0, whiten=False)),\\n (\\'standardscaler-2\\', StandardScaler(copy=True, with_mean=True, with_std=True))]\\nAs you can see, the first StandardScaler step was named standardscaler-1 and the\\nsecond standardscaler-2. However, in such settings it might be better to use the\\nPipeline construction with explicit names, to give more semantic names to each\\nstep.\\nAccessing Step Attributes\\nOften you will want to inspect attributes of one of the steps of the pipeline—say, the\\ncoefficients of a linear model or the components extracted by PCA. The easiest way to'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 327, 'page_label': '314', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Accessing Step Attributes\\nOften you will want to inspect attributes of one of the steps of the pipeline—say, the\\ncoefficients of a linear model or the components extracted by PCA. The easiest way to\\naccess the steps in a pipeline is via the named_steps attribute, which is a dictionary\\nfrom the step names to the estimators:\\n314 | Chapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 328, 'page_label': '315', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[20]:\\n# fit the pipeline defined before to the cancer dataset\\npipe.fit(cancer.data)\\n# extract the first two principal components from the \"pca\" step\\ncomponents = pipe.named_steps[\"pca\"].components_\\nprint(\"components.shape: {}\".format(components.shape))\\nOut[20]:\\ncomponents.shape: (2, 30)\\nAccessing Attributes in a Grid-Searched Pipeline\\nAs we discussed earlier in this chapter, one of the main reasons to use pipelines is for\\ndoing grid searches. A common task is to access some of the steps of a pipeline inside\\na grid search. Let’s grid search a LogisticRegression classifier on the cancer dataset,\\nusing Pipeline and StandardScaler to scale the data before passing it to the Logisti\\ncRegression classifier. First we create a pipeline using the make_pipeline function:\\nIn[21]:\\nfrom sklearn.linear_model import LogisticRegression\\npipe = make_pipeline(StandardScaler(), LogisticRegression())\\nNext, we create a parameter grid. As explained in Chapter 2 , the regularization'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 328, 'page_label': '315', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"from sklearn.linear_model import LogisticRegression\\npipe = make_pipeline(StandardScaler(), LogisticRegression())\\nNext, we create a parameter grid. As explained in Chapter 2 , the regularization\\nparameter to tune for LogisticRegression is the parameter C. We use a logarithmic\\ngrid for this parameter, searching between 0.01 and 100. Because we used the\\nmake_pipeline function, the name of the LogisticRegression step in the pipeline is\\nthe lowercased class name, logisticregression. To tune the parameter C, we there‐\\nfore have to specify a parameter grid for logisticregression__C:\\nIn[22]:\\nparam_grid = {'logisticregression__C': [0.01, 0.1, 1, 10, 100]}\\nAs usual, we split the cancer dataset into training and test sets, and fit a grid search:\\nIn[23]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, random_state=4)\\ngrid = GridSearchCV(pipe, param_grid, cv=5)\\ngrid.fit(X_train, y_train)\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 328, 'page_label': '315', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[23]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, random_state=4)\\ngrid = GridSearchCV(pipe, param_grid, cv=5)\\ngrid.fit(X_train, y_train)\\nSo how do we access the coefficients of the best LogisticRegression model that was\\nfound by GridSearchCV? From Chapter 5  we know that the best model found by\\nGridSearchCV, trained on all the training data, is stored in grid.best_estimator_:\\nThe General Pipeline Interface | 315'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 329, 'page_label': '316', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[24]:\\nprint(\"Best estimator:\\\\n{}\".format(grid.best_estimator_))\\nOut[24]:\\nBest estimator:\\nPipeline(steps=[\\n    (\\'standardscaler\\', StandardScaler(copy=True, with_mean=True, with_std=True)),\\n    (\\'logisticregression\\', LogisticRegression(C=0.1, class_weight=None,\\n    dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100,\\n    multi_class=\\'ovr\\', n_jobs=1, penalty=\\'l2\\', random_state=None,\\n    solver=\\'liblinear\\', tol=0.0001, verbose=0, warm_start=False))])\\nThis best_estimator_ in our case is a pipeline with two steps, standardscaler and\\nlogisticregression. To access the logisticregression step, we can use the\\nnamed_steps attribute of the pipeline, as explained earlier:\\nIn[25]:\\nprint(\"Logistic regression step:\\\\n{}\".format(\\n      grid.best_estimator_.named_steps[\"logisticregression\"]))\\nOut[25]:\\nLogistic regression step:\\nLogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                  intercept_scaling=1, max_iter=100, multi_class=\\'ovr\\', n_jobs=1,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 329, 'page_label': '316', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Out[25]:\\nLogistic regression step:\\nLogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                  intercept_scaling=1, max_iter=100, multi_class=\\'ovr\\', n_jobs=1,\\n                  penalty=\\'l2\\', random_state=None, solver=\\'liblinear\\', tol=0.0001,\\n                  verbose=0, warm_start=False)\\nNow that we have the trained LogisticRegression instance, we can access the coeffi‐\\ncients (weights) associated with each input feature:\\nIn[26]:\\nprint(\"Logistic regression coefficients:\\\\n{}\".format(\\n      grid.best_estimator_.named_steps[\"logisticregression\"].coef_))\\nOut[26]:\\nLogistic regression coefficients:\\n[[-0.389 -0.375 -0.376 -0.396 -0.115  0.017 -0.355 -0.39  -0.058  0.209\\n  -0.495 -0.004 -0.371 -0.383 -0.045  0.198  0.004 -0.049  0.21   0.224\\n  -0.547 -0.525 -0.499 -0.515 -0.393 -0.123 -0.388 -0.417 -0.325 -0.139]]\\nThis might be a somewhat lengthy expression, but often it comes in handy in under‐\\nstanding your models.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 329, 'page_label': '316', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='-0.547 -0.525 -0.499 -0.515 -0.393 -0.123 -0.388 -0.417 -0.325 -0.139]]\\nThis might be a somewhat lengthy expression, but often it comes in handy in under‐\\nstanding your models.\\n316 | Chapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 330, 'page_label': '317', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Grid-Searching Preprocessing Steps and Model\\nParameters\\nUsing pipelines, we can encapsulate all the processing steps in our machine learning\\nworkflow in a single scikit-learn estimator. Another benefit of doing this is that we\\ncan now adjust the parameters of the preprocessing using the outcome of a supervised\\ntask like regression or classification. In previous chapters, we used polynomial fea‐\\ntures on the boston dataset before applying the ridge regressor. Let’s model that using\\na pipeline instead. The pipeline contains three steps—scaling the data, computing\\npolynomial features, and ridge regression:\\nIn[27]:\\nfrom sklearn.datasets import load_boston\\nboston = load_boston()\\nX_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,\\n                                                    random_state=0)\\nfrom sklearn.preprocessing import PolynomialFeatures\\npipe = make_pipeline(\\n    StandardScaler(),\\n    PolynomialFeatures(),\\n    Ridge())'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 330, 'page_label': '317', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"from sklearn.preprocessing import PolynomialFeatures\\npipe = make_pipeline(\\n    StandardScaler(),\\n    PolynomialFeatures(),\\n    Ridge())\\nHow do we know which degrees of polynomials to choose, or whether to choose any\\npolynomials or interactions at all? Ideally we want to select the degree parameter\\nbased on the outcome of the classification. Using our pipeline, we can search over the\\ndegree parameter together with the parameter alpha of Ridge. To do this, we define a\\nparam_grid that contains both, appropriately prefixed by the step names:\\nIn[28]:\\nparam_grid = {'polynomialfeatures__degree': [1, 2, 3],\\n              'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\\nNow we can run our grid search again:\\nIn[29]:\\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1)\\ngrid.fit(X_train, y_train)\\nWe can visualize the outcome of the cross-validation using a heat map (Figure 6-4), as\\nwe did in Chapter 5:\\nIn[30]:\\nplt.matshow(grid.cv_results_['mean_test_score'].reshape(3, -1),\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 330, 'page_label': '317', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='We can visualize the outcome of the cross-validation using a heat map (Figure 6-4), as\\nwe did in Chapter 5:\\nIn[30]:\\nplt.matshow(grid.cv_results_[\\'mean_test_score\\'].reshape(3, -1),\\n            vmin=0, cmap=\"viridis\")\\nplt.xlabel(\"ridge__alpha\")\\nplt.ylabel(\"polynomialfeatures__degree\")\\nGrid-Searching Preprocessing Steps and Model Parameters | 317'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 331, 'page_label': '318', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='plt.xticks(range(len(param_grid[\\'ridge__alpha\\'])), param_grid[\\'ridge__alpha\\'])\\nplt.yticks(range(len(param_grid[\\'polynomialfeatures__degree\\'])),\\n           param_grid[\\'polynomialfeatures__degree\\'])\\nplt.colorbar()\\nFigure 6-4. Heat map of mean cross-validation score as a function of the degree of the\\npolynomial features and alpha parameter of Ridge\\nLooking at the results produced by the cross-validation, we can see that using polyno‐\\nmials of degree two helps, but that degree-three polynomials are much worse than\\neither degree one or two. This is reflected in the best parameters that were found:\\nIn[31]:\\nprint(\"Best parameters: {}\".format(grid.best_params_))\\nOut[31]:\\nBest parameters: {\\'polynomialfeatures__degree\\': 2, \\'ridge__alpha\\': 10}\\nWhich lead to the following score:\\nIn[32]:\\nprint(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test)))\\nOut[32]:\\nTest-set score: 0.77\\nLet’s run a grid search without polynomial features for comparison:\\nIn[33]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 331, 'page_label': '318', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[32]:\\nprint(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test)))\\nOut[32]:\\nTest-set score: 0.77\\nLet’s run a grid search without polynomial features for comparison:\\nIn[33]:\\nparam_grid = {\\'ridge__alpha\\': [0.001, 0.01, 0.1, 1, 10, 100]}\\npipe = make_pipeline(StandardScaler(), Ridge())\\ngrid = GridSearchCV(pipe, param_grid, cv=5)\\ngrid.fit(X_train, y_train)\\nprint(\"Score without poly features: {:.2f}\".format(grid.score(X_test, y_test)))\\n318 | Chapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 332, 'page_label': '319', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Out[33]:\\nScore without poly features: 0.63\\nAs we would expect looking at the grid search results visualized in Figure 6-4, using\\nno polynomial features leads to decidedly worse results.\\nSearching over preprocessing parameters together with model parameters is a very\\npowerful strategy. However, keep in mind that GridSearchCV tries all possible combi‐\\nnations of the specified parameters. Therefore, adding more parameters to your grid\\nexponentially increases the number of models that need to be built.\\nGrid-Searching Which Model To Use\\nY ou can even go further in combining GridSearchCV and Pipeline: it is also possible\\nto search over the actual steps being performed in the pipeline (say whether to use\\nStandardScaler or MinMaxScaler). This leads to an even bigger search space and\\nshould be considered carefully. Trying all possible solutions is usually not a viable\\nmachine learning strategy. However, here is an example comparing a RandomForest'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 332, 'page_label': '319', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"should be considered carefully. Trying all possible solutions is usually not a viable\\nmachine learning strategy. However, here is an example comparing a RandomForest\\nClassifier and an SVC on the iris dataset. We know that the SVC might need the\\ndata to be scaled, so we also search over whether to use StandardScaler or no pre‐\\nprocessing. For the RandomForestClassifier, we know that no preprocessing is nec‐\\nessary. We start by defining the pipeline. Here, we explicitly name the steps. We want\\ntwo steps, one for the preprocessing and then a classifier. We can instantiate this\\nusing SVC and StandardScaler:\\nIn[34]:\\npipe = Pipeline([('preprocessing', StandardScaler()), ('classifier', SVC())])\\nNow we can define the parameter_grid to search over. We want the classifier to\\nbe either RandomForestClassifier or SVC. Because they have different parameters to\\ntune, and need different preprocessing, we can make use of the list of search grids we\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 332, 'page_label': '319', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"be either RandomForestClassifier or SVC. Because they have different parameters to\\ntune, and need different preprocessing, we can make use of the list of search grids we\\ndiscussed in “Search over spaces that are not grids” on page 271. To assign an estima‐\\ntor to a step, we use the name of the step as the parameter name. When we wanted to\\nskip a step in the pipeline (for example, because we don’t need preprocessing for the\\nRandomForest), we can set that step to None:\\nIn[35]:\\nfrom sklearn.ensemble import RandomForestClassifier\\nparam_grid = [\\n    {'classifier': [SVC()], 'preprocessing': [StandardScaler(), None],\\n     'classifier__gamma': [0.001, 0.01, 0.1, 1, 10, 100],\\n     'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]},\\n    {'classifier': [RandomForestClassifier(n_estimators=100)],\\n     'preprocessing': [None], 'classifier__max_features': [1, 2, 3]}]\\nGrid-Searching Which Model To Use | 319\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 333, 'page_label': '320', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Now we can instantiate and run the grid search as usual, here on the cancer dataset:\\nIn[36]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, random_state=0)\\ngrid = GridSearchCV(pipe, param_grid, cv=5)\\ngrid.fit(X_train, y_train)\\nprint(\"Best params:\\\\n{}\\\\n\".format(grid.best_params_))\\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\\nprint(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test)))\\nOut[36]:\\nBest params:\\n{\\'classifier\\':\\n SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\\n     decision_function_shape=None, degree=3, gamma=0.01, kernel=\\'rbf\\',\\n     max_iter=-1, probability=False, random_state=None, shrinking=True,\\n     tol=0.001, verbose=False),\\n \\'preprocessing\\':\\n StandardScaler(copy=True, with_mean=True, with_std=True),\\n \\'classifier__C\\': 10, \\'classifier__gamma\\': 0.01}\\nBest cross-validation score: 0.99\\nTest-set score: 0.98\\nThe outcome of the grid search is that SVC with StandardScaler preprocessing, C=10,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 333, 'page_label': '320', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"'classifier__C': 10, 'classifier__gamma': 0.01}\\nBest cross-validation score: 0.99\\nTest-set score: 0.98\\nThe outcome of the grid search is that SVC with StandardScaler preprocessing, C=10,\\nand gamma=0.01 gave the best result.\\nSummary and Outlook\\nIn this chapter we introduced the Pipeline class, a general-purpose tool to chain\\ntogether multiple processing steps in a machine learning workflow. Real-world appli‐\\ncations of machine learning rarely involve an isolated use of a model, and instead are\\na sequence of processing steps. Using pipelines allows us to encapsulate multiple steps\\ninto a single Python object that adheres to the familiar scikit-learn interface of fit,\\npredict, and transform. In particular when doing model evaluation using cross-\\nvalidation and parameter selection using grid search, using the Pipeline class to cap‐\\nture all the processing steps is essential for proper evaluation. The Pipeline class also\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 333, 'page_label': '320', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='validation and parameter selection using grid search, using the Pipeline class to cap‐\\nture all the processing steps is essential for proper evaluation. The Pipeline class also\\nallows writing more succinct code, and reduces the likelihood of mistakes that can\\nhappen when building processing chains without the pipeline class (like forgetting\\nto apply all transformers on the test set, or not applying them in the right order).\\nChoosing the right combination of feature extraction, preprocessing, and models is\\nsomewhat of an art, and often requires some trial and error. However, using pipe‐\\nlines, this “trying out” of many different processing steps is quite simple. When\\n320 | Chapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 334, 'page_label': '321', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='experimenting, be careful not to overcomplicate your processes, and make sure to\\nevaluate whether every component you are including in your model is necessary.\\nWith this chapter, we have completed our survey of general-purpose tools and algo‐\\nrithms provided by scikit-learn. Y ou now possess all the required skills and know\\nthe necessary mechanisms to apply machine learning in practice. In the next chapter,\\nwe will dive in more detail into one particular type of data that is commonly seen in\\npractice, and that requires some special expertise to handle correctly: text data.\\nSummary and Outlook | 321'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 336, 'page_label': '323', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 7\\nWorking with Text Data\\nIn Chapter 4, we talked about two kinds of features that can represent properties of\\nthe data: continuous features that describe a quantity, and categorical features that are\\nitems from a fixed list. There is a third kind of feature that can be found in many\\napplications, which is text. For example, if we want to classify an email message as\\neither a legitimate email or spam, the content of the email will certainly contain\\nimportant information for this classification task. Or maybe we want to learn about\\nthe opinion of a politician on the topic of immigration. Here, that individual’s\\nspeeches or tweets might provide useful information. In customer service, we often\\nwant to find out if a message is a complaint or an inquiry. We can use the subject line\\nand content of a message to automatically determine the customer’s intent, which\\nallows us to send the message to the appropriate department, or even send a fully\\nautomatic reply.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 336, 'page_label': '323', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='and content of a message to automatically determine the customer’s intent, which\\nallows us to send the message to the appropriate department, or even send a fully\\nautomatic reply.\\nText data is usually represented as strings, made up of characters. In any of the exam‐\\nples just given, the length of the text data will vary. This feature is clearly very differ‐\\nent from the numeric features that we’ve discussed so far, and we will need to process\\nthe data before we can apply our machine learning algorithms to it.\\nTypes of Data Represented as Strings\\nBefore we dive into the processing steps that go into representing text data for\\nmachine learning, we want to briefly discuss different kinds of text data that you\\nmight encounter. Text is usually just a string in your dataset, but not all string features\\nshould be treated as text. A string feature can sometimes represent categorical vari‐\\nables, as we discussed in Chapter 5. There is no way to know how to treat a string'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 336, 'page_label': '323', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='should be treated as text. A string feature can sometimes represent categorical vari‐\\nables, as we discussed in Chapter 5. There is no way to know how to treat a string\\nfeature before looking at the data.\\n323'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 337, 'page_label': '324', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='There are four kinds of string data you might see:\\n• Categorical data\\n• Free strings that can be semantically mapped to categories\\n• Structured string data\\n• Text data\\nCategorical data is data that comes from a fixed list. Say you collect data via a survey\\nwhere you ask people their favorite color, with a drop-down menu that allows them\\nto select from “red, ” “green, ” “blue, ” “yellow, ” “black, ” “white, ” “purple, ” and “pink. ”\\nThis will result in a dataset with exactly eight different possible values, which clearly\\nencode a categorical variable. Y ou can check whether this is the case for your data by\\neyeballing it (if you see very many different strings it is unlikely that this is a categori‐\\ncal variable) and confirm it by computing the unique values over the dataset, and\\npossibly a histogram over how often each appears. Y ou also might want to check\\nwhether each variable actually corresponds to a category that makes sense for your'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 337, 'page_label': '324', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='possibly a histogram over how often each appears. Y ou also might want to check\\nwhether each variable actually corresponds to a category that makes sense for your\\napplication. Maybe halfway through the existence of your survey, someone found that\\n“black” was misspelled as “blak” and subsequently fixed the survey. As a result, your\\ndataset contains both “blak” and “black, ” which correspond to the same semantic\\nmeaning and should be consolidated.\\nNow imagine instead of providing a drop-down menu, you provide a text field for the\\nusers to provide their own favorite colors. Many people might respond with a color\\nname like “black” or “blue. ” Others might make typographical errors, use different\\nspellings like “gray” and “grey, ” or use more evocative and specific names like “mid‐\\nnight blue. ” Y ou will also have some very strange entries. Some good examples come\\nfrom the xkcd Color Survey , where people had to name colors and came up with'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 337, 'page_label': '324', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='night blue. ” Y ou will also have some very strange entries. Some good examples come\\nfrom the xkcd Color Survey , where people had to name colors and came up with\\nnames like “velociraptor cloaka” and “my dentist’s office orange. I still remember his\\ndandruff slowly wafting into my gaping yaw, ” which are hard to map to colors auto‐\\nmatically (or at all). The responses you can obtain from a text field belong to the sec‐\\nond category in the list, free strings that can be semantically mapped to categories . It\\nwill probably be best to encode this data as a categorical variable, where you can\\nselect the categories either by using the most common entries, or by defining cate‐\\ngories that will capture responses in a way that makes sense for your application. Y ou\\nmight then have some categories for standard colors, maybe a category “multicol‐\\nored” for people that gave answers like “green and red stripes, ” and an “other” cate‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 337, 'page_label': '324', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='might then have some categories for standard colors, maybe a category “multicol‐\\nored” for people that gave answers like “green and red stripes, ” and an “other” cate‐\\ngory for things that cannot be encoded otherwise. This kind of preprocessing of\\nstrings can take a lot of manual effort and is not easily automated. If you are in a posi‐\\ntion where you can influence data collection, we highly recommend avoiding man‐\\nually entered values for concepts that are better captured using categorical variables.\\nOften, manually entered values do not correspond to fixed categories, but still have\\nsome underlying structure, like addresses, names of places or people, dates, telephone\\n324 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 338, 'page_label': '325', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='1 Arguably, the content of websites linked to in tweets contains more information than the text of the tweets\\nthemselves.\\n2 Most of what we will talk about in the rest of the chapter also applies to other languages that use the Roman\\nalphabet, and partially to other languages with word boundary delimiters. Chinese, for example, does not\\ndelimit word boundaries, and has other challenges that make applying the techniques in this chapter difficult.\\n3 The dataset is available at http://ai.stanford.edu/~amaas/data/sentiment/.\\nnumbers, or other identifiers. These kinds of strings are often very hard to parse, and\\ntheir treatment is highly dependent on context and domain. A systematic treatment\\nof these cases is beyond the scope of this book.\\nThe final category of string data is freeform text data that consists of phrases or sen‐\\ntences. Examples include tweets, chat logs, and hotel reviews, as well as the collected'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 338, 'page_label': '325', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='The final category of string data is freeform text data that consists of phrases or sen‐\\ntences. Examples include tweets, chat logs, and hotel reviews, as well as the collected\\nworks of Shakespeare, the content of Wikipedia, or the Project Gutenberg collection\\nof 50,000 ebooks. All of these collections contain information mostly as sentences\\ncomposed of words. 1 For simplicity’s sake, let’s assume all our documents are in one\\nlanguage, English. 2 In the context of text analysis, the dataset is often called the cor‐\\npus, and each data point, represented as a single text, is called a document. These\\nterms come from the information retrieval (IR) and natural language processing (NLP)\\ncommunity, which both deal mostly in text data.\\nExample Application: Sentiment Analysis of Movie\\nReviews\\nAs a running example in this chapter, we will use a dataset of movie reviews from the\\nIMDb (Internet Movie Database) website collected by Stanford researcher Andrew'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 338, 'page_label': '325', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Reviews\\nAs a running example in this chapter, we will use a dataset of movie reviews from the\\nIMDb (Internet Movie Database) website collected by Stanford researcher Andrew\\nMaas.3 This dataset contains the text of the reviews, together with a label that indi‐\\ncates whether a review is “positive” or “negative. ” The IMDb website itself contains\\nratings from 1 to 10. To simplify the modeling, this annotation is summarized as a\\ntwo-class classification dataset where reviews with a score of 6 or higher are labeled as\\npositive, and the rest as negative. We will leave the question of whether this is a good\\nrepresentation of the data open, and simply use the data as provided by Andrew\\nMaas.\\nAfter unpacking the data, the dataset is provided as text files in two separate folders,\\none for the training data and one for the test data. Each of these in turn has two sub‐\\nfolders, one called pos and one called neg:\\nExample Application: Sentiment Analysis of Movie Reviews | 325'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 339, 'page_label': '326', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[2]:\\n!tree -L 2 data/aclImdb\\nOut[2]:\\ndata/aclImdb\\n├── test\\n│   ├── neg\\n│   └── pos\\n└── train\\n    ├── neg\\n    └── pos\\n6 directories, 0 files\\nThe pos folder contains all the positive reviews, each as a separate text file, and simi‐\\nlarly for the neg folder. There is a helper function in scikit-learn to load files stored\\nin such a folder structure, where each subfolder corresponds to a label, called\\nload_files. We apply the load_files function first to the training data:\\nIn[3]:\\nfrom sklearn.datasets import load_files\\nreviews_train = load_files(\"data/aclImdb/train/\")\\n# load_files returns a bunch, containing training texts and training labels\\ntext_train, y_train = reviews_train.data, reviews_train.target\\nprint(\"type of text_train: {}\".format(type(text_train)))\\nprint(\"length of text_train: {}\".format(len(text_train)))\\nprint(\"text_train[1]:\\\\n{}\".format(text_train[1]))\\nOut[3]:\\ntype of text_train:  <class \\'list\\'>\\nlength of text_train:  25000\\ntext_train[1]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 339, 'page_label': '326', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='print(\"length of text_train: {}\".format(len(text_train)))\\nprint(\"text_train[1]:\\\\n{}\".format(text_train[1]))\\nOut[3]:\\ntype of text_train:  <class \\'list\\'>\\nlength of text_train:  25000\\ntext_train[1]:\\nb\\'Words can\\\\\\'t describe how bad this movie is. I can\\\\\\'t explain it by writing\\n  only. You have too see it for yourself to get at grip of how horrible a movie\\n  really can be. Not that I recommend you to do that. There are so many\\n  clich\\\\xc3\\\\xa9s, mistakes (and all other negative things you can imagine) here\\n  that will just make you cry. To start with the technical first, there are a\\n  LOT of mistakes regarding the airplane. I won\\\\\\'t list them here, but just\\n  mention the coloring of the plane. They didn\\\\\\'t even manage to show an\\n  airliner in the colors of a fictional airline, but instead used a 747\\n  painted in the original Boeing livery. Very bad. The plot is stupid and has\\n  been done many times before, only much, much better. There are so many'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 339, 'page_label': '326', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='painted in the original Boeing livery. Very bad. The plot is stupid and has\\n  been done many times before, only much, much better. There are so many\\n  ridiculous moments here that i lost count of it really early. Also, I was on\\n  the bad guys\\\\\\' side all the time in the movie, because the good guys were so\\n  stupid. \"Executive Decision\" should without a doubt be you\\\\\\'re choice over\\n  this one, even the \"Turbulence\"-movies are better. In fact, every other\\n  movie in the world is better than this one.\\'\\nY ou can see that text_train is a list of length 25,000, where each entry is a string\\ncontaining a review. We printed the review with index 1. Y ou can also see that the\\nreview contains some HTML line breaks (<br />). While these are unlikely to have a\\n326 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 340, 'page_label': '327', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='large impact on our machine learning models, it is better to clean the data and\\nremove this formatting before we proceed:\\nIn[4]:\\ntext_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]\\nThe type of the entries of text_train will depend on your Python version. In Python\\n3, they will be of type bytes which represents a binary encoding of the string data. In\\nPython 2, text_train contains strings. We won’t go into the details of the different\\nstring types in Python here, but we recommend that you read the Python 2  and/or\\nPython 3 documentation regarding strings and Unicode.\\nThe dataset was collected such that the positive class and the negative class balanced,\\nso that there are as many positive as negative strings:\\nIn[5]:\\nprint(\"Samples per class (training): {}\".format(np.bincount(y_train)))\\nOut[5]:\\nSamples per class (training): [12500 12500]\\nWe load the test dataset in the same manner:\\nIn[6]:\\nreviews_test = load_files(\"data/aclImdb/test/\")'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 340, 'page_label': '327', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Out[5]:\\nSamples per class (training): [12500 12500]\\nWe load the test dataset in the same manner:\\nIn[6]:\\nreviews_test = load_files(\"data/aclImdb/test/\")\\ntext_test, y_test = reviews_test.data, reviews_test.target\\nprint(\"Number of documents in test data: {}\".format(len(text_test)))\\nprint(\"Samples per class (test): {}\".format(np.bincount(y_test)))\\ntext_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\\nOut[6]:\\nNumber of documents in test data: 25000\\nSamples per class (test): [12500 12500]\\nThe task we want to solve is as follows: given a review, we want to assign the label\\n“positive” or “negative” based on the text content of the review. This is a standard\\nbinary classification task. However, the text data is not in a format that a machine\\nlearning model can handle. We need to convert the string representation of the text\\ninto a numeric representation that we can apply our machine learning algorithms to.\\nRepresenting Text Data as a Bag of Words'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 340, 'page_label': '327', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='into a numeric representation that we can apply our machine learning algorithms to.\\nRepresenting Text Data as a Bag of Words\\nOne of the most simple but effective and commonly used ways to represent text for\\nmachine learning is using the bag-of-words representation. When using this represen‐\\ntation, we discard most of the structure of the input text, like chapters, paragraphs,\\nsentences, and formatting, and only count how often each word appears in each text in\\nRepresenting Text Data as a Bag of Words | 327'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 341, 'page_label': '328', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the corpus. Discarding the structure and counting only word occurrences leads to the\\nmental image of representing text as a “bag. ”\\nComputing the bag-of-words representation for a corpus of documents consists of\\nthe following three steps:\\n1. Tokenization. Split each document into the words that appear in it (called tokens),\\nfor example by splitting them on whitespace and punctuation.\\n2. Vocabulary building. Collect a vocabulary of all words that appear in any of the\\ndocuments, and number them (say, in alphabetical order).\\n3. Encoding. For each document, count how often each of the words in the vocabu‐\\nlary appear in this document.\\nThere are some subtleties involved in step 1 and step 2, which we will discuss in more\\ndetail later in this chapter. For now, let’s look at how we can apply the bag-of-words\\nprocessing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\\nis how you get ants.\" . The output is one vector of word counts for each docu‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 341, 'page_label': '328', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='processing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\\nis how you get ants.\" . The output is one vector of word counts for each docu‐\\nment. For each word in the vocabulary, we have a count of how often it appears in\\neach document. That means our numeric representation has one feature for each\\nunique word in the whole dataset. Note how the order of the words in the original\\nstring is completely irrelevant to the bag-of-words feature representation.\\nFigure 7-1. Bag-of-words processing\\n328 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 342, 'page_label': '329', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Applying Bag-of-Words to a Toy Dataset\\nThe bag-of-words representation is implemented in CountVectorizer, which is a\\ntransformer. Let’s first apply it to a toy dataset, consisting of two samples, to see it\\nworking:\\nIn[7]:\\nbards_words =[\"The fool doth think he is wise,\",\\n              \"but the wise man knows himself to be a fool\"]\\nWe import and instantiate the CountVectorizer and fit it to our toy data as follows:\\nIn[8]:\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nvect = CountVectorizer()\\nvect.fit(bards_words)\\nFitting the CountVectorizer consists of the tokenization of the training data and\\nbuilding of the vocabulary, which we can access as the vocabulary_ attribute:\\nIn[9]:\\nprint(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\\nprint(\"Vocabulary content:\\\\n {}\".format(vect.vocabulary_))\\nOut[9]:\\nVocabulary size: 13\\nVocabulary content:\\n {\\'the\\': 9, \\'himself\\': 5, \\'wise\\': 12, \\'he\\': 4, \\'doth\\': 2, \\'to\\': 11, \\'knows\\': 7,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 342, 'page_label': '329', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='print(\"Vocabulary content:\\\\n {}\".format(vect.vocabulary_))\\nOut[9]:\\nVocabulary size: 13\\nVocabulary content:\\n {\\'the\\': 9, \\'himself\\': 5, \\'wise\\': 12, \\'he\\': 4, \\'doth\\': 2, \\'to\\': 11, \\'knows\\': 7,\\n  \\'man\\': 8, \\'fool\\': 3, \\'is\\': 6, \\'be\\': 0, \\'think\\': 10, \\'but\\': 1}\\nThe vocabulary consists of 13 words, from \"be\" to \"wise\".\\nTo create the bag-of-words representation for the training data, we call the transform\\nmethod:\\nIn[10]:\\nbag_of_words = vect.transform(bards_words)\\nprint(\"bag_of_words: {}\".format(repr(bag_of_words)))\\nOut[10]:\\nbag_of_words: <2x13 sparse matrix of type \\'<class \\'numpy.int64\\'>\\'\\n    with 16 stored elements in Compressed Sparse Row format>\\nThe bag-of-words representation is stored in a SciPy sparse matrix that only stores\\nthe entries that are nonzero (see Chapter 1). The matrix is of shape 2×13, with one\\nrow for each of the two data points and one feature for each of the words in the\\nvocabulary. A sparse matrix is used as most documents only contain a small subset of'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 342, 'page_label': '329', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='row for each of the two data points and one feature for each of the words in the\\nvocabulary. A sparse matrix is used as most documents only contain a small subset of\\nthe words in the vocabulary, meaning most entries in the feature array are 0. Think\\nRepresenting Text Data as a Bag of Words | 329'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 343, 'page_label': '330', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='4 This is possible because we are using a small toy dataset that contains only 13 words. For any real dataset, this\\nwould result in a MemoryError.\\nabout how many different words might appear in a movie review compared to all the\\nwords in the English language (which is what the vocabulary models). Storing all\\nthose zeros would be prohibitive, and a waste of memory. To look at the actual con‐\\ntent of the sparse matrix, we can convert it to a “dense” NumPy array (that also stores\\nall the 0 entries) using the toarray method:4\\nIn[11]:\\nprint(\"Dense representation of bag_of_words:\\\\n{}\".format(\\n    bag_of_words.toarray()))\\nOut[11]:\\nDense representation of bag_of_words:\\n[[0 0 1 1 1 0 1 0 0 1 1 0 1]\\n [1 1 0 1 0 1 0 1 1 1 0 1 1]]\\nWe can see that the word counts for each word are either 0 or 1; neither of the two\\nstrings in bards_words contains a word twice. Let’s take a look at how to read these\\nfeature vectors. The first string ( \"The fool doth think he is wise,\" ) is repre‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 343, 'page_label': '330', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='strings in bards_words contains a word twice. Let’s take a look at how to read these\\nfeature vectors. The first string ( \"The fool doth think he is wise,\" ) is repre‐\\nsented as the first row in, and it contains the first word in the vocabulary, \"be\", zero\\ntimes. It also contains the second word in the vocabulary, \"but\", zero times. It con‐\\ntains the third word, \"doth\", once, and so on. Looking at both rows, we can see that\\nthe fourth word, \"fool\", the tenth word, \"the\", and the thirteenth word, \"wise\",\\nappear in both strings.\\nBag-of-Words for Movie Reviews\\nNow that we’ve gone through the bag-of-words process in detail, let’s apply it to our\\ntask of sentiment analysis for movie reviews. Earlier, we loaded our training and test\\ndata from the IMDb reviews into lists of strings ( text_train and text_test), which\\nwe will now process:\\nIn[12]:\\nvect = CountVectorizer().fit(text_train)\\nX_train = vect.transform(text_train)\\nprint(\"X_train:\\\\n{}\".format(repr(X_train)))\\nOut[12]:\\nX_train:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 343, 'page_label': '330', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='we will now process:\\nIn[12]:\\nvect = CountVectorizer().fit(text_train)\\nX_train = vect.transform(text_train)\\nprint(\"X_train:\\\\n{}\".format(repr(X_train)))\\nOut[12]:\\nX_train:\\n<25000x74849 sparse matrix of type \\'<class \\'numpy.int64\\'>\\'\\n    with 3431196 stored elements in Compressed Sparse Row format>\\n330 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 344, 'page_label': '331', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='5 A quick analysis of the data confirms that this is indeed the case. Try confirming it yourself.\\nThe shape of X_train, the bag-of-words representation of the training data, is\\n25,000×74,849, indicating that the vocabulary contains 74,849 entries. Again, the data\\nis stored as a SciPy sparse matrix. Let’s look at the vocabulary in a bit more detail.\\nAnother way to access the vocabulary is using the get_feature_name method of the\\nvectorizer, which returns a convenient list where each entry corresponds to one fea‐\\nture:\\nIn[13]:\\nfeature_names = vect.get_feature_names()\\nprint(\"Number of features: {}\".format(len(feature_names)))\\nprint(\"First 20 features:\\\\n{}\".format(feature_names[:20]))\\nprint(\"Features 20010 to 20030:\\\\n{}\".format(feature_names[20010:20030]))\\nprint(\"Every 2000th feature:\\\\n{}\".format(feature_names[::2000]))\\nOut[13]:\\nNumber of features: 74849\\nFirst 20 features:\\n[\\'00\\', \\'000\\', \\'0000000000001\\', \\'00001\\', \\'00015\\', \\'000s\\', \\'001\\', \\'003830\\','),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 344, 'page_label': '331', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='print(\"Every 2000th feature:\\\\n{}\".format(feature_names[::2000]))\\nOut[13]:\\nNumber of features: 74849\\nFirst 20 features:\\n[\\'00\\', \\'000\\', \\'0000000000001\\', \\'00001\\', \\'00015\\', \\'000s\\', \\'001\\', \\'003830\\',\\n \\'006\\', \\'007\\', \\'0079\\', \\'0080\\', \\'0083\\', \\'0093638\\', \\'00am\\', \\'00pm\\', \\'00s\\',\\n \\'01\\', \\'01pm\\', \\'02\\']\\nFeatures 20010 to 20030:\\n[\\'dratted\\', \\'draub\\', \\'draught\\', \\'draughts\\', \\'draughtswoman\\', \\'draw\\', \\'drawback\\',\\n \\'drawbacks\\', \\'drawer\\', \\'drawers\\', \\'drawing\\', \\'drawings\\', \\'drawl\\',\\n \\'drawled\\', \\'drawling\\', \\'drawn\\', \\'draws\\', \\'draza\\', \\'dre\\', \\'drea\\']\\nEvery 2000th feature:\\n[\\'00\\', \\'aesir\\', \\'aquarian\\', \\'barking\\', \\'blustering\\', \\'bête\\', \\'chicanery\\',\\n \\'condensing\\', \\'cunning\\', \\'detox\\', \\'draper\\', \\'enshrined\\', \\'favorit\\', \\'freezer\\',\\n \\'goldman\\', \\'hasan\\', \\'huitieme\\', \\'intelligible\\', \\'kantrowitz\\', \\'lawful\\',\\n \\'maars\\', \\'megalunged\\', \\'mostey\\', \\'norrland\\', \\'padilla\\', \\'pincher\\',\\n \\'promisingly\\', \\'receptionist\\', \\'rivals\\', \\'schnaas\\', \\'shunning\\', \\'sparse\\',\\n \\'subset\\', \\'temptations\\', \\'treatises\\', \\'unproven\\', \\'walkman\\', \\'xylophonist\\']'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 344, 'page_label': '331', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='\\'promisingly\\', \\'receptionist\\', \\'rivals\\', \\'schnaas\\', \\'shunning\\', \\'sparse\\',\\n \\'subset\\', \\'temptations\\', \\'treatises\\', \\'unproven\\', \\'walkman\\', \\'xylophonist\\']\\nAs you can see, possibly a bit surprisingly, the first 10 entries in the vocabulary are all\\nnumbers. All these numbers appear somewhere in the reviews, and are therefore\\nextracted as words. Most of these numbers don’t have any immediate semantic mean‐\\ning—apart from \"007\", which in the particular context of movies is likely to refer to\\nthe James Bond character. 5 Weeding out the meaningful from the nonmeaningful\\n“words” is sometimes tricky. Looking further along in the vocabulary, we find a col‐\\nlection of English words starting with “dra” . Y ou might notice that for \"draught\",\\n\"drawback\", and \"drawer\" both the singular and plural forms are contained in the\\nvocabulary as distinct words. These words have very closely related semantic mean‐\\nings, and counting them as different words, corresponding to different features,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 344, 'page_label': '331', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='vocabulary as distinct words. These words have very closely related semantic mean‐\\nings, and counting them as different words, corresponding to different features,\\nmight not be ideal.\\nRepresenting Text Data as a Bag of Words | 331'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 345, 'page_label': '332', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='6 The attentive reader might notice that we violate our lesson from Chapter 6 on cross-validation with prepro‐\\ncessing here. Using the default settings of CountVectorizer, it actually does not collect any statistics, so our\\nresults are valid. Using Pipeline from the start would be a better choice for applications, but we defer it for\\nease of exposure.\\nBefore we try to improve our feature extraction, let’s obtain a quantitative measure of\\nperformance by actually building a classifier. We have the training labels stored in\\ny_train and the bag-of-words representation of the training data in X_train, so we\\ncan train a classifier on this data. For high-dimensional, sparse data like this, linear\\nmodels like LogisticRegression often work best.\\nLet’s start by evaluating LogisticRegresssion using cross-validation:6\\nIn[14]:\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.linear_model import LogisticRegression\\nscores = cross_val_score(LogisticRegression(), X_train, y_train, cv=5)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 345, 'page_label': '332', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[14]:\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.linear_model import LogisticRegression\\nscores = cross_val_score(LogisticRegression(), X_train, y_train, cv=5)\\nprint(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))\\nOut[14]:\\nMean cross-validation accuracy: 0.88\\nWe obtain a mean cross-validation score of 88%, which indicates reasonable perfor‐\\nmance for a balanced binary classification task. We know that LogisticRegression\\nhas a regularization parameter, C, which we can tune via cross-validation:\\nIn[15]:\\nfrom sklearn.model_selection import GridSearchCV\\nparam_grid = {\\'C\\': [0.001, 0.01, 0.1, 1, 10]}\\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\\ngrid.fit(X_train, y_train)\\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\\nprint(\"Best parameters: \", grid.best_params_)\\nOut[15]:\\nBest cross-validation score: 0.89\\nBest parameters:  {\\'C\\': 0.1}'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 345, 'page_label': '332', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\\nprint(\"Best parameters: \", grid.best_params_)\\nOut[15]:\\nBest cross-validation score: 0.89\\nBest parameters:  {\\'C\\': 0.1}\\nWe obtain a cross-validation score of 89% using C=0.1. We can now assess the gener‐\\nalization performance of this parameter setting on the test set:\\nIn[16]:\\nX_test = vect.transform(text_test)\\nprint(\"{:.2f}\".format(grid.score(X_test, y_test)))\\nOut[16]:\\n0.88\\n332 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 346, 'page_label': '333', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Now, let’s see if we can improve the extraction of words. The CountVectorizer\\nextracts tokens using a regular expression. By default, the regular expression that is\\nused is \"\\\\b\\\\w\\\\w+\\\\b\". If you are not familiar with regular expressions, this means it\\nfinds all sequences of characters that consist of at least two letters or numbers ( \\\\w)\\nand that are separated by word boundaries ( \\\\b). It does not find single-letter words,\\nand it splits up contractions like “doesn’t” or “bit.ly” , but it matches “h8ter” as a single\\nword. The CountVectorizer then converts all words to lowercase characters, so that\\n“soon” , “Soon” , and “sOon” all correspond to the same token (and therefore feature).\\nThis simple mechanism works quite well in practice, but as we saw earlier, we get\\nmany uninformative features (like the numbers). One way to cut back on these is to\\nonly use tokens that appear in at least two documents (or at least five documents, and'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 346, 'page_label': '333', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='many uninformative features (like the numbers). One way to cut back on these is to\\nonly use tokens that appear in at least two documents (or at least five documents, and\\nso on). A token that appears only in a single document is unlikely to appear in the test\\nset and is therefore not helpful. We can set the minimum number of documents a\\ntoken needs to appear in with the min_df parameter:\\nIn[17]:\\nvect = CountVectorizer(min_df=5).fit(text_train)\\nX_train = vect.transform(text_train)\\nprint(\"X_train with min_df: {}\".format(repr(X_train)))\\nOut[17]:\\nX_train with min_df: <25000x27271 sparse matrix of type \\'<class \\'numpy.int64\\'>\\'\\n    with 3354014 stored elements in Compressed Sparse Row format>\\nBy requiring at least five appearances of each token, we can bring down the number\\nof features to 27,271, as seen in the preceding output—only about a third of the origi‐\\nnal features. Let’s look at some tokens again:\\nIn[18]:\\nfeature_names = vect.get_feature_names()'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 346, 'page_label': '333', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='of features to 27,271, as seen in the preceding output—only about a third of the origi‐\\nnal features. Let’s look at some tokens again:\\nIn[18]:\\nfeature_names = vect.get_feature_names()\\nprint(\"First 50 features:\\\\n{}\".format(feature_names[:50]))\\nprint(\"Features 20010 to 20030:\\\\n{}\".format(feature_names[20010:20030]))\\nprint(\"Every 700th feature:\\\\n{}\".format(feature_names[::700]))\\nOut[18]:\\nFirst 50 features:\\n[\\'00\\', \\'000\\', \\'007\\', \\'00s\\', \\'01\\', \\'02\\', \\'03\\', \\'04\\', \\'05\\', \\'06\\', \\'07\\', \\'08\\',\\n \\'09\\', \\'10\\', \\'100\\', \\'1000\\', \\'100th\\', \\'101\\', \\'102\\', \\'103\\', \\'104\\', \\'105\\', \\'107\\',\\n \\'108\\', \\'10s\\', \\'10th\\', \\'11\\', \\'110\\', \\'112\\', \\'116\\', \\'117\\', \\'11th\\', \\'12\\', \\'120\\',\\n \\'12th\\', \\'13\\', \\'135\\', \\'13th\\', \\'14\\', \\'140\\', \\'14th\\', \\'15\\', \\'150\\', \\'15th\\', \\'16\\',\\n \\'160\\', \\'1600\\', \\'16mm\\', \\'16s\\', \\'16th\\']\\nFeatures 20010 to 20030:\\n[\\'repentance\\', \\'repercussions\\', \\'repertoire\\', \\'repetition\\', \\'repetitions\\',\\n \\'repetitious\\', \\'repetitive\\', \\'rephrase\\', \\'replace\\', \\'replaced\\', \\'replacement\\','),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 346, 'page_label': '333', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"Features 20010 to 20030:\\n['repentance', 'repercussions', 'repertoire', 'repetition', 'repetitions',\\n 'repetitious', 'repetitive', 'rephrase', 'replace', 'replaced', 'replacement',\\n 'replaces', 'replacing', 'replay', 'replayable', 'replayed', 'replaying',\\n 'replays', 'replete', 'replica']\\nRepresenting Text Data as a Bag of Words | 333\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 347, 'page_label': '334', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Every 700th feature:\\n[\\'00\\', \\'affections\\', \\'appropriately\\', \\'barbra\\', \\'blurbs\\', \\'butchered\\',\\n \\'cheese\\', \\'commitment\\', \\'courts\\', \\'deconstructed\\', \\'disgraceful\\', \\'dvds\\',\\n \\'eschews\\', \\'fell\\', \\'freezer\\', \\'goriest\\', \\'hauser\\', \\'hungary\\', \\'insinuate\\',\\n \\'juggle\\', \\'leering\\', \\'maelstrom\\', \\'messiah\\', \\'music\\', \\'occasional\\', \\'parking\\',\\n \\'pleasantville\\', \\'pronunciation\\', \\'recipient\\', \\'reviews\\', \\'sas\\', \\'shea\\',\\n \\'sneers\\', \\'steiger\\', \\'swastika\\', \\'thrusting\\', \\'tvs\\', \\'vampyre\\', \\'westerns\\']\\nThere are clearly many fewer numbers, and some of the more obscure words or mis‐\\nspellings seem to have vanished. Let’s see how well our model performs by doing a\\ngrid search again:\\nIn[19]:\\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\\ngrid.fit(X_train, y_train)\\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\\nOut[19]:\\nBest cross-validation score: 0.89\\nThe best validation accuracy of the grid search is still 89%, unchanged from before.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 347, 'page_label': '334', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\\nOut[19]:\\nBest cross-validation score: 0.89\\nThe best validation accuracy of the grid search is still 89%, unchanged from before.\\nWe didn’t improve our model, but having fewer features to deal with speeds up pro‐\\ncessing and throwing away useless features might make the model more interpretable.\\nIf the transform method of CountVectorizer is called on a docu‐\\nment that contains words that were not contained in the training\\ndata, these words will be ignored as they are not part of the dictio‐\\nnary. This is not really an issue for classification, as it’s not possible\\nto learn anything about words that are not in the training data. For\\nsome applications, like spam detection, it might be helpful to add a\\nfeature that encodes how many so-called “out of vocabulary” words\\nthere are in a particular document, though. For this to work, you\\nneed to set min_df; otherwise, this feature will never be active dur‐\\ning training.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 347, 'page_label': '334', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='there are in a particular document, though. For this to work, you\\nneed to set min_df; otherwise, this feature will never be active dur‐\\ning training.\\nStopwords\\nAnother way that we can get rid of uninformative words is by discarding words that\\nare too frequent to be informative. There are two main approaches: using a language-\\nspecific list of stopwords, or discarding words that appear too frequently. scikit-\\nlearn has a built-in list of English stopwords in the feature_extraction.text\\nmodule:\\n334 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 348, 'page_label': '335', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[20]:\\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\\nprint(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS)))\\nprint(\"Every 10th stopword:\\\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))\\nOut[20]:\\nNumber of stop words: 318\\nEvery 10th stopword:\\n[\\'above\\', \\'elsewhere\\', \\'into\\', \\'well\\', \\'rather\\', \\'fifteen\\', \\'had\\', \\'enough\\',\\n \\'herein\\', \\'should\\', \\'third\\', \\'although\\', \\'more\\', \\'this\\', \\'none\\', \\'seemed\\',\\n \\'nobody\\', \\'seems\\', \\'he\\', \\'also\\', \\'fill\\', \\'anyone\\', \\'anything\\', \\'me\\', \\'the\\',\\n \\'yet\\', \\'go\\', \\'seeming\\', \\'front\\', \\'beforehand\\', \\'forty\\', \\'i\\']\\nClearly, removing the stopwords in the list can only decrease the number of features\\nby the length of the list—here, 318—but it might lead to an improvement in perfor‐\\nmance. Let’s give it a try:\\nIn[21]:\\n# Specifying stop_words=\"english\" uses the built-in list.\\n# We could also augment it and pass our own.\\nvect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\\nX_train = vect.transform(text_train)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 348, 'page_label': '335', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='# We could also augment it and pass our own.\\nvect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\\nX_train = vect.transform(text_train)\\nprint(\"X_train with stop words:\\\\n{}\".format(repr(X_train)))\\nOut[21]:\\nX_train with stop words:\\n<25000x26966 sparse matrix of type \\'<class \\'numpy.int64\\'>\\'\\n    with 2149958 stored elements in Compressed Sparse Row format>\\nThere are now 305 (27,271–26,966) fewer features in the dataset, which means that\\nmost, but not all, of the stopwords appeared. Let’s run the grid search again:\\nIn[22]:\\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\\ngrid.fit(X_train, y_train)\\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\\nOut[22]:\\nBest cross-validation score: 0.88\\nThe grid search performance decreased slightly using the stopwords—not enough to\\nworry about, but given that excluding 305 features out of over 27,000 is unlikely to\\nchange performance or interpretability a lot, it doesn’t seem worth using this list.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 348, 'page_label': '335', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='worry about, but given that excluding 305 features out of over 27,000 is unlikely to\\nchange performance or interpretability a lot, it doesn’t seem worth using this list.\\nFixed lists are mostly helpful for small datasets, which might not contain enough\\ninformation for the model to determine which words are stopwords from the data\\nitself. As an exercise, you can try out the other approach, discarding frequently\\nStopwords | 335'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 349, 'page_label': '336', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='7 We provide this formula here mostly for completeness; you don’t need to remember it to use the tf–idf\\nencoding.\\nappearing words, by setting the max_df option of CountVectorizer and see how it\\ninfluences the number of features and the performance.\\nRescaling the Data with tf–idf\\nInstead of dropping features that are deemed unimportant, another approach is to\\nrescale features by how informative we expect them to be. One of the most common\\nways to do this is using the term frequency–inverse document frequency  (tf–idf)\\nmethod. The intuition of this method is to give high weight to any term that appears\\noften in a particular document, but not in many documents in the corpus. If a word\\nappears often in a particular document, but not in very many documents, it is likely\\nto be very descriptive of the content of that document. scikit-learn implements the\\ntf–idf method in two classes: TfidfTransformer, which takes in the sparse matrix'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 349, 'page_label': '336', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='to be very descriptive of the content of that document. scikit-learn implements the\\ntf–idf method in two classes: TfidfTransformer, which takes in the sparse matrix\\noutput produced by CountVectorizer and transforms it, and TfidfVectorizer,\\nwhich takes in the text data and does both the bag-of-words feature extraction and\\nthe tf–idf transformation. There are several variants of the tf–idf rescaling scheme,\\nwhich you can read about on Wikipedia . The tf–idf score for word w in document d\\nas implemented in both the TfidfTransformer and TfidfVectorizer classes is given\\nby:7\\ntfidf w, d = tf log N + 1\\nNw + 1 + 1\\nwhere N is the number of documents in the training set, Nw is the number of docu‐\\nments in the training set that the word w appears in, and tf (the term frequency) is the\\nnumber of times that the word w appears in the query document d (the document\\nyou want to transform or encode). Both classes also apply L2 normalization after'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 349, 'page_label': '336', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='number of times that the word w appears in the query document d (the document\\nyou want to transform or encode). Both classes also apply L2 normalization after\\ncomputing the tf–idf representation; in other words, they rescale the representation\\nof each document to have Euclidean norm 1. Rescaling in this way means that the\\nlength of a document (the number of words) does not change the vectorized repre‐\\nsentation.\\nBecause tf–idf actually makes use of the statistical properties of the training data, we\\nwill use a pipeline, as described in Chapter 6, to ensure the results of our grid search\\nare valid. This leads to the following code:\\n336 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 350, 'page_label': '337', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[23]:\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.pipeline import make_pipeline\\npipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None),\\n                     LogisticRegression())\\nparam_grid = {\\'logisticregression__C\\': [0.001, 0.01, 0.1, 1, 10]}\\ngrid = GridSearchCV(pipe, param_grid, cv=5)\\ngrid.fit(text_train, y_train)\\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\\nOut[23]:\\nBest cross-validation score: 0.89\\nAs you can see, there is some improvement when using tf–idf instead of just word\\ncounts. We can also inspect which words tf–idf found most important. Keep in mind\\nthat the tf–idf scaling is meant to find words that distinguish documents, but it is a\\npurely unsupervised technique. So, “important” here does not necessarily relate to the\\n“positive review” and “negative review” labels we are interested in. First, we extract\\nthe TfidfVectorizer from the pipeline:\\nIn[24]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 350, 'page_label': '337', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='“positive review” and “negative review” labels we are interested in. First, we extract\\nthe TfidfVectorizer from the pipeline:\\nIn[24]:\\nvectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\\n# transform the training dataset\\nX_train = vectorizer.transform(text_train)\\n# find maximum value for each of the features over the dataset\\nmax_value = X_train.max(axis=0).toarray().ravel()\\nsorted_by_tfidf = max_value.argsort()\\n# get feature names\\nfeature_names = np.array(vectorizer.get_feature_names())\\nprint(\"Features with lowest tfidf:\\\\n{}\".format(\\n    feature_names[sorted_by_tfidf[:20]]))\\nprint(\"Features with highest tfidf: \\\\n{}\".format(\\n    feature_names[sorted_by_tfidf[-20:]]))\\nOut[24]:\\nFeatures with lowest tfidf:\\n[\\'poignant\\' \\'disagree\\' \\'instantly\\' \\'importantly\\' \\'lacked\\' \\'occurred\\'\\n \\'currently\\' \\'altogether\\' \\'nearby\\' \\'undoubtedly\\' \\'directs\\' \\'fond\\' \\'stinker\\'\\n \\'avoided\\' \\'emphasis\\' \\'commented\\' \\'disappoint\\' \\'realizing\\' \\'downhill\\'\\n \\'inane\\']\\nFeatures with highest tfidf:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 350, 'page_label': '337', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"'currently' 'altogether' 'nearby' 'undoubtedly' 'directs' 'fond' 'stinker'\\n 'avoided' 'emphasis' 'commented' 'disappoint' 'realizing' 'downhill'\\n 'inane']\\nFeatures with highest tfidf:\\n['coop' 'homer' 'dillinger' 'hackenstein' 'gadget' 'taker' 'macarthur'\\n 'vargas' 'jesse' 'basket' 'dominick' 'the' 'victor' 'bridget' 'victoria'\\n 'khouri' 'zizek' 'rob' 'timon' 'titanic']\\nRescaling the Data with tf–idf | 337\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 351, 'page_label': '338', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Features with low tf–idf are those that either are very commonly used across docu‐\\nments or are only used sparingly, and only in very long documents. Interestingly,\\nmany of the high-tf–idf features actually identify certain shows or movies. These\\nterms only appear in reviews for this particular show or franchise, but tend to appear\\nvery often in these particular reviews. This is very clear, for example, for \"pokemon\",\\n\"smallville\", and \"doodlebops\", but \"scanners\" here actually also refers to a\\nmovie title. These words are unlikely to help us in our sentiment classification task\\n(unless maybe some franchises are universally reviewed positively or negatively) but\\ncertainly contain a lot of specific information about the reviews.\\nWe can also find the words that have low inverse document frequency—that is, those\\nthat appear frequently and are therefore deemed less important. The inverse docu‐\\nment frequency values found on the training set are stored in the idf_ attribute:\\nIn[25]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 351, 'page_label': '338', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='that appear frequently and are therefore deemed less important. The inverse docu‐\\nment frequency values found on the training set are stored in the idf_ attribute:\\nIn[25]:\\nsorted_by_idf = np.argsort(vectorizer.idf_)\\nprint(\"Features with lowest idf:\\\\n{}\".format(\\n    feature_names[sorted_by_idf[:100]]))\\nOut[25]:\\nFeatures with lowest idf:\\n[\\'the\\' \\'and\\' \\'of\\' \\'to\\' \\'this\\' \\'is\\' \\'it\\' \\'in\\' \\'that\\' \\'but\\' \\'for\\' \\'with\\'\\n \\'was\\' \\'as\\' \\'on\\' \\'movie\\' \\'not\\' \\'have\\' \\'one\\' \\'be\\' \\'film\\' \\'are\\' \\'you\\' \\'all\\'\\n \\'at\\' \\'an\\' \\'by\\' \\'so\\' \\'from\\' \\'like\\' \\'who\\' \\'they\\' \\'there\\' \\'if\\' \\'his\\' \\'out\\'\\n \\'just\\' \\'about\\' \\'he\\' \\'or\\' \\'has\\' \\'what\\' \\'some\\' \\'good\\' \\'can\\' \\'more\\' \\'when\\'\\n \\'time\\' \\'up\\' \\'very\\' \\'even\\' \\'only\\' \\'no\\' \\'would\\' \\'my\\' \\'see\\' \\'really\\' \\'story\\'\\n \\'which\\' \\'well\\' \\'had\\' \\'me\\' \\'than\\' \\'much\\' \\'their\\' \\'get\\' \\'were\\' \\'other\\'\\n \\'been\\' \\'do\\' \\'most\\' \\'don\\' \\'her\\' \\'also\\' \\'into\\' \\'first\\' \\'made\\' \\'how\\' \\'great\\'\\n \\'because\\' \\'will\\' \\'people\\' \\'make\\' \\'way\\' \\'could\\' \\'we\\' \\'bad\\' \\'after\\' \\'any\\'\\n \\'too\\' \\'then\\' \\'them\\' \\'she\\' \\'watch\\' \\'think\\' \\'acting\\' \\'movies\\' \\'seen\\' \\'its\\''),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 351, 'page_label': '338', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='\\'because\\' \\'will\\' \\'people\\' \\'make\\' \\'way\\' \\'could\\' \\'we\\' \\'bad\\' \\'after\\' \\'any\\'\\n \\'too\\' \\'then\\' \\'them\\' \\'she\\' \\'watch\\' \\'think\\' \\'acting\\' \\'movies\\' \\'seen\\' \\'its\\'\\n \\'him\\']\\nAs expected, these are mostly English stopwords like \"the\" and \"no\". But some are\\nclearly domain-specific to the movie reviews, like \"movie\", \"film\", \"time\", \"story\",\\nand so on. Interestingly, \"good\", \"great\", and \"bad\" are also among the most fre‐\\nquent and therefore “least relevant” words according to the tf–idf measure, even\\nthough we might expect these to be very important for our sentiment analysis task.\\nInvestigating Model \\nCoefficients\\nFinally, let’s look in a bit more detail into what our logistic regression model actually\\nlearned from the data. Because there are so many features—27,271 after removing the\\ninfrequent ones—we clearly cannot look at all of the coefficients at the same time.\\nHowever, we can look at the largest coefficients, and see which words these corre‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 351, 'page_label': '338', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='infrequent ones—we clearly cannot look at all of the coefficients at the same time.\\nHowever, we can look at the largest coefficients, and see which words these corre‐\\nspond to. We will use the last model that we trained, based on the tf–idf features.\\nThe following bar chart ( Figure 7-2) shows the 25 largest and 25 smallest coefficients\\nof the logistic regression model, with the bars showing the size of each coefficient:\\n338 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 352, 'page_label': '339', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[26]:\\nmglearn.tools.visualize_coefficients(\\n    grid.best_estimator_.named_steps[\"logisticregression\"].coef_,\\n    feature_names, n_top_features=40)\\nFigure 7-2. Largest and smallest coefficients of logistic regression trained on tf-idf fea‐\\ntures\\nThe negative coefficients on the left belong to words that according to the model are\\nindicative of negative reviews, while the positive coefficients on the right belong to\\nwords that according to the model indicate positive reviews. Most of the terms are\\nquite intuitive, like \"worst\", \"waste\", \"disappointment\", and \"laughable\" indicat‐\\ning bad movie reviews, while \"excellent\", \"wonderful\", \"enjoyable\", and\\n\"refreshing\" indicate positive movie reviews. Some words are slightly less clear, like\\n\"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “best\\ntoday. ”\\nBag-of-Words with More Than One Word (n-Grams)\\nOne of the main disadvantages of using a bag-of-words representation is that word'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 352, 'page_label': '339', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='today. ”\\nBag-of-Words with More Than One Word (n-Grams)\\nOne of the main disadvantages of using a bag-of-words representation is that word\\norder is completely discarded. Therefore, the two strings “it’s bad, not good at all” and\\n“it’s good, not bad at all” have exactly the same representation, even though the mean‐\\nings are inverted. Putting “not” in front of a word is only one example (if an extreme\\none) of how context matters. Fortunately, there is a way of capturing context when\\nusing a bag-of-words representation, by not only considering the counts of single\\ntokens, but also the counts of pairs or triplets of tokens that appear next to each other.\\nPairs of tokens are known as bigrams, triplets of tokens are known as trigrams, and\\nmore generally sequences of tokens are known as n-grams. We can change the range\\nof tokens that are considered as features by changing the ngram_range parameter of\\nCountVectorizer or TfidfVectorizer. The ngram_range parameter is a tuple, con‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 352, 'page_label': '339', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='of tokens that are considered as features by changing the ngram_range parameter of\\nCountVectorizer or TfidfVectorizer. The ngram_range parameter is a tuple, con‐\\nBag-of-Words with More Than One Word (n-Grams) | 339'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 353, 'page_label': '340', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='sisting of the minimum length and the maximum length of the sequences of tokens\\nthat are considered. Here is an example on the toy data we used earlier:\\nIn[27]:\\nprint(\"bards_words:\\\\n{}\".format(bards_words))\\nOut[27]:\\nbards_words:\\n[\\'The fool doth think he is wise,\\',\\n \\'but the wise man knows himself to be a fool\\']\\nThe default is to create one feature per sequence of tokens that is at least one token\\nlong and at most one token long, or in other words exactly one token long (single\\ntokens are also called unigrams):\\nIn[28]:\\ncv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\\nprint(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\\nprint(\"Vocabulary:\\\\n{}\".format(cv.get_feature_names()))\\nOut[28]:\\nVocabulary size: 13\\nVocabulary:\\n[\\'be\\', \\'but\\', \\'doth\\', \\'fool\\', \\'he\\', \\'himself\\', \\'is\\', \\'knows\\', \\'man\\', \\'the\\',\\n \\'think\\', \\'to\\', \\'wise\\']\\nTo look only at bigrams—that is, only at sequences of two tokens following each\\nother—we can set ngram_range to (2, 2):\\nIn[29]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 353, 'page_label': '340', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='\\'think\\', \\'to\\', \\'wise\\']\\nTo look only at bigrams—that is, only at sequences of two tokens following each\\nother—we can set ngram_range to (2, 2):\\nIn[29]:\\ncv = CountVectorizer(ngram_range=(2, 2)).fit(bards_words)\\nprint(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\\nprint(\"Vocabulary:\\\\n{}\".format(cv.get_feature_names()))\\nOut[29]:\\nVocabulary size: 14\\nVocabulary:\\n[\\'be fool\\', \\'but the\\', \\'doth think\\', \\'fool doth\\', \\'he is\\', \\'himself to\\',\\n \\'is wise\\', \\'knows himself\\', \\'man knows\\', \\'the fool\\', \\'the wise\\',\\n \\'think he\\', \\'to be\\', \\'wise man\\']\\nUsing longer sequences of tokens usually results in many more features, and in more\\nspecific features. There is no common bigram between the two phrases in\\nbard_words:\\n340 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 354, 'page_label': '341', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[30]:\\nprint(\"Transformed data (dense):\\\\n{}\".format(cv.transform(bards_words).toarray()))\\nOut[30]:\\nTransformed data (dense):\\n[[0 0 1 1 1 0 1 0 0 1 0 1 0 0]\\n [1 1 0 0 0 1 0 1 1 0 1 0 1 1]]\\nFor most applications, the minimum number of tokens should be one, as single\\nwords often capture a lot of meaning. Adding bigrams helps in most cases. Adding\\nlonger sequences—up to 5-grams—might help too, but this will lead to an explosion\\nof the number of features and might lead to overfitting, as there will be many very\\nspecific features. In principle, the number of bigrams could be the number of\\nunigrams squared and the number of trigrams could be the number of unigrams to\\nthe power of three, leading to very large feature spaces. In practice, the number of\\nhigher n-grams that actually appear in the data is much smaller, because of the struc‐\\nture of the (English) language, though it is still large.\\nHere is what using unigrams, bigrams, and trigrams on bards_words looks like:\\nIn[31]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 354, 'page_label': '341', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='ture of the (English) language, though it is still large.\\nHere is what using unigrams, bigrams, and trigrams on bards_words looks like:\\nIn[31]:\\ncv = CountVectorizer(ngram_range=(1, 3)).fit(bards_words)\\nprint(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\\nprint(\"Vocabulary:\\\\n{}\".format(cv.get_feature_names()))\\nOut[31]:\\nVocabulary size: 39\\nVocabulary:\\n[\\'be\\', \\'be fool\\', \\'but\\', \\'but the\\', \\'but the wise\\', \\'doth\\', \\'doth think\\',\\n \\'doth think he\\', \\'fool\\', \\'fool doth\\', \\'fool doth think\\', \\'he\\', \\'he is\\',\\n \\'he is wise\\', \\'himself\\', \\'himself to\\', \\'himself to be\\', \\'is\\', \\'is wise\\',\\n \\'knows\\', \\'knows himself\\', \\'knows himself to\\', \\'man\\', \\'man knows\\',\\n \\'man knows himself\\', \\'the\\', \\'the fool\\', \\'the fool doth\\', \\'the wise\\',\\n \\'the wise man\\', \\'think\\', \\'think he\\', \\'think he is\\', \\'to\\', \\'to be\\',\\n \\'to be fool\\', \\'wise\\', \\'wise man\\', \\'wise man knows\\']\\nLet’s try out the TfidfVectorizer on the IMDb movie review data and find the best\\nsetting of n-gram range using a grid search:\\nIn[32]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 354, 'page_label': '341', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='\\'to be fool\\', \\'wise\\', \\'wise man\\', \\'wise man knows\\']\\nLet’s try out the TfidfVectorizer on the IMDb movie review data and find the best\\nsetting of n-gram range using a grid search:\\nIn[32]:\\npipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())\\n# running the grid search takes a long time because of the\\n# relatively large grid and the inclusion of trigrams\\nparam_grid = {\"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10, 100],\\n              \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3)]}\\ngrid = GridSearchCV(pipe, param_grid, cv=5)\\ngrid.fit(text_train, y_train)\\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\\nprint(\"Best parameters:\\\\n{}\".format(grid.best_params_))\\nBag-of-Words with More Than One Word (n-Grams) | 341'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 355, 'page_label': '342', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Out[32]:\\nBest cross-validation score: 0.91\\nBest parameters:\\n{\\'tfidfvectorizer__ngram_range\\': (1, 3), \\'logisticregression__C\\': 100}\\nAs you can see from the results, we improved performance by a bit more than a per‐\\ncent by adding bigram and trigram features. We can visualize the cross-validation\\naccuracy as a function of the ngram_range and C parameter as a heat map, as we did\\nin Chapter 5 (see Figure 7-3):\\nIn[33]:\\n# extract scores from grid_search\\nscores = grid.cv_results_[\\'mean_test_score\\'].reshape(-1, 3).T\\n# visualize heat map\\nheatmap = mglearn.tools.heatmap(\\n    scores, xlabel=\"C\", ylabel=\"ngram_range\", cmap=\"viridis\", fmt=\"%.3f\",\\n    xticklabels=param_grid[\\'logisticregression__C\\'],\\n    yticklabels=param_grid[\\'tfidfvectorizer__ngram_range\\'])\\nplt.colorbar(heatmap)\\nFigure 7-3. Heat map visualization of mean cross-validation accuracy as a function of\\nthe parameters ngram_range and C\\nFrom the heat map we can see that using bigrams increases performance quite a bit,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 355, 'page_label': '342', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='the parameters ngram_range and C\\nFrom the heat map we can see that using bigrams increases performance quite a bit,\\nwhile adding trigrams only provides a very small benefit in terms of accuracy. To\\nunderstand better how the model improved, we can visualize the important coeffi‐\\n342 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 356, 'page_label': '343', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='cient for the best model, which includes unigrams, bigrams, and trigrams (see\\nFigure 7-4):\\nIn[34]:\\n# extract feature names and coefficients\\nvect = grid.best_estimator_.named_steps[\\'tfidfvectorizer\\']\\nfeature_names = np.array(vect.get_feature_names())\\ncoef = grid.best_estimator_.named_steps[\\'logisticregression\\'].coef_\\nmglearn.tools.visualize_coefficients(coef, feature_names, n_top_features=40)\\nFigure 7-4. Most important features when using unigrams, bigrams, and trigrams with\\ntf-idf rescaling\\nThere are particularly interesting features containing the word “worth” that were not\\npresent in the unigram model: \"not worth\" is indicative of a negative review, while\\n\"definitely worth\" and \"well worth\" are indicative of a positive review. This is a\\nprime example of context influencing the meaning of the word “worth. ”\\nNext, we’ll visualize only trigrams, to provide further insight into why these features\\nare helpful. Many of the useful bigrams and trigrams consist of common words that'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 356, 'page_label': '343', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Next, we’ll visualize only trigrams, to provide further insight into why these features\\nare helpful. Many of the useful bigrams and trigrams consist of common words that\\nwould not be informative on their own, as in the phrases \"none of the\", \"the only\\ngood\", \"on and on\" , \"this is one\" , \"of the most\" , and so on. However, the\\nimpact of these features is quite limited compared to the importance of the unigram\\nfeatures, as you can see in Figure 7-5:\\nIn[35]:\\n# find 3-gram features\\nmask = np.array([len(feature.split(\" \")) for feature in feature_names]) == 3\\n# visualize only 3-gram features\\nmglearn.tools.visualize_coefficients(coef.ravel()[mask],\\n                                     feature_names[mask], n_top_features=40)Bag-of-Words with More Than One Word (n-Grams) | 343'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 357, 'page_label': '344', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 7-5. Visualization of only the important trigram features of the model\\nAdvanced Tokenization, Stemming, and Lemmatization\\nAs mentioned previously, the feature extraction in the CountVectorizer and Tfidf\\nVectorizer is relatively simple, and much more elaborate methods are possible. One\\nparticular step that is often improved in more sophisticated text-processing applica‐\\ntions is the first step in the bag-of-words model: tokenization. This step defines what\\nconstitutes a word for the purpose of feature extraction.\\nWe saw earlier that the vocabulary often contains singular and plural versions of\\nsome words, as in \"drawback\" and \"drawbacks\", \"drawer\" and \"drawers\", and\\n\"drawing\" and \"drawings\". For the purposes of a bag-of-words model, the semantics\\nof \"drawback\" and \"drawbacks\" are so close that distinguishing them will only\\nincrease overfitting, and not allow the model to fully exploit the training data. Simi‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 357, 'page_label': '344', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='of \"drawback\" and \"drawbacks\" are so close that distinguishing them will only\\nincrease overfitting, and not allow the model to fully exploit the training data. Simi‐\\nlarly, we found the vocabulary includes words like \"replace\", \"replaced\", \"replace\\nment\", \"replaces\", and \"replacing\", which are different verb forms and a noun\\nrelating to the verb “to replace. ” Similarly to having singular and plural forms of a\\nnoun, treating different verb forms and related words as distinct tokens is disadvanta‐\\ngeous for building a model that generalizes well.\\nThis problem can be overcome by representing each word using its word stem, which\\ninvolves identifying (or \\nconflating) all the words that have the same word stem. If this\\nis done by using a rule-based heuristic, like dropping common suffixes, it is usually\\nreferred to as stemming. If instead a dictionary of known word forms is used (an\\nexplicit and human-verified system), and the role of the word in the sentence is taken'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 357, 'page_label': '344', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='referred to as stemming. If instead a dictionary of known word forms is used (an\\nexplicit and human-verified system), and the role of the word in the sentence is taken\\ninto account, the process is referred to as lemmatization and the standardized form of\\nthe word is referred to as the lemma. Both processing methods, lemmatization and\\nstemming, are forms of normalization that try to extract some normal form of a\\nword. Another interesting case of normalization is spelling correction, which can be\\nhelpful in practice but is outside of the scope of this book.\\n344 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 358, 'page_label': '345', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='8 For details of the interface, consult the nltk and spacy documentation. We are more interested in the general\\nprinciples here.\\nTo get a better understanding of normalization, let’s compare a method for stemming\\n—the Porter stemmer, a widely used collection of heuristics (here imported from the\\nnltk package)—to lemmatization as implemented in the spacy package:8\\nIn[36]:\\nimport spacy\\nimport nltk\\n# load spacy\\'s English-language models\\nen_nlp = spacy.load(\\'en\\')\\n# instantiate nltk\\'s Porter stemmer\\nstemmer = nltk.stem.PorterStemmer()\\n# define function to compare lemmatization in spacy with stemming in nltk\\ndef compare_normalization(doc):\\n    # tokenize document in spacy\\n    doc_spacy = en_nlp(doc)\\n    # print lemmas found by spacy\\n    print(\"Lemmatization:\")\\n    print([token.lemma_ for token in doc_spacy])\\n    # print tokens found by Porter stemmer\\n    print(\"Stemming:\")\\n    print([stemmer.stem(token.norm_.lower()) for token in doc_spacy])'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 358, 'page_label': '345', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='print([token.lemma_ for token in doc_spacy])\\n    # print tokens found by Porter stemmer\\n    print(\"Stemming:\")\\n    print([stemmer.stem(token.norm_.lower()) for token in doc_spacy])\\nWe will compare lemmatization and the Porter stemmer on a sentence designed to\\nshow some of the differences:\\nIn[37]:\\ncompare_normalization(u\"Our meeting today was worse than yesterday, \"\\n                       \"I\\'m scared of meeting the clients tomorrow.\")\\nOut[37]:\\nLemmatization:\\n[\\'our\\', \\'meeting\\', \\'today\\', \\'be\\', \\'bad\\', \\'than\\', \\'yesterday\\', \\',\\', \\'i\\', \\'be\\',\\n \\'scared\\', \\'of\\', \\'meet\\', \\'the\\', \\'client\\', \\'tomorrow\\', \\'.\\']\\nStemming:\\n[\\'our\\', \\'meet\\', \\'today\\', \\'wa\\', \\'wors\\', \\'than\\', \\'yesterday\\', \\',\\', \\'i\\', \"\\'m\",\\n \\'scare\\', \\'of\\', \\'meet\\', \\'the\\', \\'client\\', \\'tomorrow\\', \\'.\\']\\nStemming is always restricted to trimming the word to a stem, so \"was\" becomes\\n\"wa\", while lemmatization can retrieve the correct base verb form, \"be\". Similarly,\\nlemmatization can normalize \"worse\" to \"bad\", while stemming produces \"wors\".'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 358, 'page_label': '345', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='\"wa\", while lemmatization can retrieve the correct base verb form, \"be\". Similarly,\\nlemmatization can normalize \"worse\" to \"bad\", while stemming produces \"wors\".\\nAnother major difference is that stemming reduces both occurrences of \"meeting\" to\\n\"meet\". Using lemmatization, the first occurrence of \"meeting\" is recognized as a\\nAdvanced Tokenization, Stemming, and Lemmatization | 345'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 359, 'page_label': '346', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='noun and left as is, while the second occurrence is recognized as a verb and reduced\\nto \"meet\". In general, lemmatization is a much more involved process than stem‐\\nming, but it usually produces better results than stemming when used for normaliz‐\\ning tokens for machine learning.\\nWhile scikit-learn implements neither form of normalization, CountVectorizer\\nallows specifying your own tokenizer to convert each document into a list of tokens\\nusing the tokenizer parameter. We can use the lemmatization from spacy to create a\\ncallable that will take a string and produce a list of lemmas:\\nIn[38]:\\n# Technicality: we want to use the regexp-based tokenizer\\n# that is used by CountVectorizer and only use the lemmatization\\n# from spacy. To this end, we replace en_nlp.tokenizer (the spacy tokenizer)\\n# with the regexp-based tokenization.\\nimport re\\n# regexp used in CountVectorizer\\nregexp = re.compile(\\'(?u)\\\\\\\\b\\\\\\\\w\\\\\\\\w+\\\\\\\\b\\')\\n# load spacy language model and save old tokenizer\\nen_nlp = spacy.load(\\'en\\')'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 359, 'page_label': '346', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='# with the regexp-based tokenization.\\nimport re\\n# regexp used in CountVectorizer\\nregexp = re.compile(\\'(?u)\\\\\\\\b\\\\\\\\w\\\\\\\\w+\\\\\\\\b\\')\\n# load spacy language model and save old tokenizer\\nen_nlp = spacy.load(\\'en\\')\\nold_tokenizer = en_nlp.tokenizer\\n# replace the tokenizer with the preceding regexp\\nen_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(\\n    regexp.findall(string))\\n# create a custom tokenizer using the spacy document processing pipeline\\n# (now using our own tokenizer)\\ndef custom_tokenizer(document):\\n    doc_spacy = en_nlp(document, entity=False, parse=False)\\n    return [token.lemma_ for token in doc_spacy]\\n# define a count vectorizer with the custom tokenizer\\nlemma_vect = CountVectorizer(tokenizer=custom_tokenizer, min_df=5)\\nLet’s transform the data and inspect the vocabulary size:\\nIn[39]:\\n# transform text_train using CountVectorizer with lemmatization\\nX_train_lemma = lemma_vect.fit_transform(text_train)\\nprint(\"X_train_lemma.shape: {}\".format(X_train_lemma.shape))'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 359, 'page_label': '346', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[39]:\\n# transform text_train using CountVectorizer with lemmatization\\nX_train_lemma = lemma_vect.fit_transform(text_train)\\nprint(\"X_train_lemma.shape: {}\".format(X_train_lemma.shape))\\n# standard CountVectorizer for reference\\nvect = CountVectorizer(min_df=5).fit(text_train)\\nX_train = vect.transform(text_train)\\nprint(\"X_train.shape: {}\".format(X_train.shape))\\n346 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 360, 'page_label': '347', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content=\"Out[39]:\\nX_train_lemma.shape:  (25000, 21596)\\nX_train.shape:  (25000, 27271)\\nAs you can see from the output, lemmatization reduced the number of features from\\n27,271 (with the standard CountVectorizer processing) to 21,596. Lemmatization\\ncan be seen as a kind of regularization, as it conflates certain features. Therefore, we\\nexpect lemmatization to improve performance most when the dataset is small. To\\nillustrate how lemmatization can help, we will use StratifiedShuffleSplit for\\ncross-validation, using only 1% of the data as training data and the rest as test data:\\nIn[40]:\\n# build a grid search using only 1% of the data as the training set\\nfrom sklearn.model_selection import StratifiedShuffleSplit\\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\\ncv = StratifiedShuffleSplit(n_iter=5, test_size=0.99,\\n                            train_size=0.01, random_state=0)\\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv=cv)\\n# perform grid search with standard CountVectorizer\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 360, 'page_label': '347', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='train_size=0.01, random_state=0)\\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv=cv)\\n# perform grid search with standard CountVectorizer\\ngrid.fit(X_train, y_train)\\nprint(\"Best cross-validation score \"\\n      \"(standard CountVectorizer): {:.3f}\".format(grid.best_score_))\\n# perform grid search with lemmatization\\ngrid.fit(X_train_lemma, y_train)\\nprint(\"Best cross-validation score \"\\n      \"(lemmatization): {:.3f}\".format(grid.best_score_))\\nOut[40]:\\nBest cross-validation score (standard CountVectorizer): 0.721\\nBest cross-validation score (lemmatization): 0.731\\nIn this case, lemmatization provided a modest improvement in performance. As with\\nmany of the different feature extraction techniques, the result varies depending on\\nthe dataset. Lemmatization and stemming can sometimes help in building better (or\\nat least more compact) models, so we suggest you give these techniques a try when\\ntrying to squeeze out the last bit of performance on a particular task.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 360, 'page_label': '347', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='at least more compact) models, so we suggest you give these techniques a try when\\ntrying to squeeze out the last bit of performance on a particular task.\\nTopic Modeling and Document Clustering\\nOne particular technique that is often applied to text data is topic modeling, which is\\nan umbrella term describing the task of assigning each document to one or multiple\\ntopics, usually without supervision. A good example for this is news data, which\\nmight be categorized into topics like “politics, ” “sports, ” “finance, ” and so on. If each\\ndocument is assigned a single topic, this is the task of clustering the documents, as\\ndiscussed in Chapter 3 . If each document can have more than one topic, the task\\nTopic Modeling and Document Clustering | 347'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 361, 'page_label': '348', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='9 There is another machine learning model that is also often abbreviated LDA: Linear Discriminant Analysis, a\\nlinear classification model. This leads to quite some confusion. In this book, LDA refers to Latent Dirichlet\\nAllocation.\\nrelates to the decomposition methods from Chapter 3. Each of the components we\\nlearn then corresponds to one topic, and the coefficients of the components in the\\nrepresentation of a document tell us how strongly related that document is to a par‐\\nticular topic. Often, when people talk about topic modeling, they refer to one particu‐\\nlar decomposition method called Latent Dirichlet Allocation (often LDA for short).9\\nLatent Dirichlet Allocation\\nIntuitively, the LDA model tries to find groups of words (the topics) that appear\\ntogether frequently. LDA also requires that each document can be understood as a\\n“mixture” of a subset of the topics. It is important to understand that for the machine'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 361, 'page_label': '348', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='together frequently. LDA also requires that each document can be understood as a\\n“mixture” of a subset of the topics. It is important to understand that for the machine\\nlearning model a “topic” might not be what we would normally call a topic in every‐\\nday speech, but that it resembles more the components extracted by PCA or NMF\\n(which we discussed in Chapter 3), which might or might not have a semantic mean‐\\ning. Even if there is a semantic meaning for an LDA “topic” , it might not be some‐\\nthing we’ d usually call a topic. Going back to the example of news articles, we might\\nhave a collection of articles about sports, politics, and finance, written by two specific\\nauthors. In a politics article, we might expect to see words like “governor, ” “vote, ”\\n“party, ” etc., while in a sports article we might expect words like “team, ” “score, ” and\\n“season. ” Words in each of these groups will likely appear together, while it’s less likely'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 361, 'page_label': '348', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='“party, ” etc., while in a sports article we might expect words like “team, ” “score, ” and\\n“season. ” Words in each of these groups will likely appear together, while it’s less likely\\nthat, for example, “team” and “governor” will appear together. However, these are not\\nthe only groups of words we might expect to appear together. The two reporters\\nmight prefer different phrases or different choices of words. Maybe one of them likes\\nto use the word “demarcate” and one likes the word “polarize. ” Other “topics” would\\nthen be “words often used by reporter A ” and “words often used by reporter B, ”\\nthough these are not topics in the usual sense of the word.\\nLet’s apply LDA to our movie review dataset to see how it works in practice. For\\nunsupervised text document models, it is often good to remove very common words,\\nas they might otherwise dominate the analysis. We’ll remove words that appear in at\\nleast 20 percent of the documents, and we’ll limit the bag-of-words model to the'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 361, 'page_label': '348', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='as they might otherwise dominate the analysis. We’ll remove words that appear in at\\nleast 20 percent of the documents, and we’ll limit the bag-of-words model to the\\n10,000 words that are most common after removing the top 20 percent:\\nIn[41]:\\nvect = CountVectorizer(max_features=10000, max_df=.15)\\nX = vect.fit_transform(text_train)\\n348 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 362, 'page_label': '349', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='10 In fact, NMF and LDA solve quite related problems, and we could also use NMF to extract topics.\\nWe will learn a topic model with 10 topics, which is few enough that we can look at all\\nof them. Similarly to the components in NMF , topics don’t have an inherent ordering,\\nand changing the number of topics will change all of the topics. 10 We’ll use the\\n\"batch\" learning method, which is somewhat slower than the default ( \"online\") but\\nusually provides better results, and increase \"max_iter\", which can also lead to better\\nmodels:\\nIn[42]:\\nfrom sklearn.decomposition import LatentDirichletAllocation\\nlda = LatentDirichletAllocation(n_topics=10, learning_method=\"batch\",\\n                                max_iter=25, random_state=0)\\n# We build the model and transform the data in one step\\n# Computing transform takes some time,\\n# and we can save time by doing both at once\\ndocument_topics = lda.fit_transform(X)\\nLike the decomposition methods we saw in Chapter 3, LatentDirichletAllocation'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 362, 'page_label': '349', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='# and we can save time by doing both at once\\ndocument_topics = lda.fit_transform(X)\\nLike the decomposition methods we saw in Chapter 3, LatentDirichletAllocation\\nhas a components_ attribute that stores how important each word is for each topic.\\nThe size of components_ is (n_topics, n_words):\\nIn[43]:\\nlda.components_.shape\\nOut[43]:\\n(10, 10000)\\nTo understand better what the different topics mean, we will look at the most impor‐\\ntant words for each of the topics. The print_topics function provides a nice format‐\\nting for these features:\\nIn[44]:\\n# For each topic (a row in the components_), sort the features (ascending)\\n# Invert rows with [:, ::-1] to make sorting descending\\nsorting = np.argsort(lda.components_, axis=1)[:, ::-1]\\n# Get the feature names from the vectorizer\\nfeature_names = np.array(vect.get_feature_names())\\nIn[45]:\\n# Print out the 10 topics:\\nmglearn.tools.print_topics(topics=range(10), feature_names=feature_names,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 362, 'page_label': '349', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='feature_names = np.array(vect.get_feature_names())\\nIn[45]:\\n# Print out the 10 topics:\\nmglearn.tools.print_topics(topics=range(10), feature_names=feature_names,\\n                           sorting=sorting, topics_per_chunk=5, n_words=10)\\nTopic Modeling and Document Clustering | 349'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 363, 'page_label': '350', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Out[45]:\\ntopic 0       topic 1       topic 2       topic 3       topic 4\\n--------      --------      --------      --------      --------\\nbetween       war           funny         show          didn\\nyoung         world         worst         series        saw\\nfamily        us            comedy        episode       am\\nreal          our           thing         tv            thought\\nperformance   american      guy           episodes      years\\nbeautiful     documentary   re            shows         book\\nwork          history       stupid        season        watched\\neach          new           actually      new           now\\nboth          own           nothing       television    dvd\\ndirector      point         want          years         got\\ntopic 5       topic 6       topic 7       topic 8       topic 9\\n--------      --------      --------      --------      --------\\nhorror        kids          cast          performance   house'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 363, 'page_label': '350', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='topic 5       topic 6       topic 7       topic 8       topic 9\\n--------      --------      --------      --------      --------\\nhorror        kids          cast          performance   house\\naction        action        role          role          woman\\neffects       animation     john          john          gets\\nbudget        game          version       actor         killer\\nnothing       fun           novel         oscar         girl\\noriginal      disney        both          cast          wife\\ndirector      children      director      plays         horror\\nminutes       10            played        jack          young\\npretty        kid           performance   joe           goes\\ndoesn         old           mr            performances  around\\nJudging from the important words, topic 1 seems to be about historical and war mov‐\\nies, topic 2 might be about bad comedies, topic 3 might be about TV series. Topic 4\\nseems to capture some very common words, while topic 6 appears to be about child‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 363, 'page_label': '350', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='ies, topic 2 might be about bad comedies, topic 3 might be about TV series. Topic 4\\nseems to capture some very common words, while topic 6 appears to be about child‐\\nren’s movies and topic 8 seems to capture award-related reviews. Using only 10 topics,\\neach of the topics needs to be very broad, so that they can together cover all the dif‐\\nferent kinds of reviews in our dataset.\\nNext, we will learn another model, this time with 100 topics. Using more topics\\nmakes the analysis much harder, but makes it more likely that topics can specialize to\\ninteresting subsets of the data:\\nIn[46]:\\nlda100 = LatentDirichletAllocation(n_topics=100, learning_method=\"batch\",\\n                                   max_iter=25, random_state=0)\\ndocument_topics100 = lda100.fit_transform(X)\\nLooking at all 100 topics would be a bit overwhelming, so we selected some interest‐\\ning and representative topics:\\n350 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 364, 'page_label': '351', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[47]:\\ntopics = np.array([7, 16, 24, 25, 28, 36, 37, 45, 51, 53, 54, 63, 89, 97])\\nsorting = np.argsort(lda100.components_, axis=1)[:, ::-1]\\nfeature_names = np.array(vect.get_feature_names())\\nmglearn.tools.print_topics(topics=topics, feature_names=feature_names,\\n                           sorting=sorting, topics_per_chunk=7, n_words=20)\\nOut[48]:\\ntopic 7       topic 16      topic 24      topic 25      topic 28\\n--------      --------      --------      --------      --------\\nthriller      worst         german        car           beautiful\\nsuspense      awful         hitler        gets          young\\nhorror        boring        nazi          guy           old\\natmosphere    horrible      midnight      around        romantic\\nmystery       stupid        joe           down          between\\nhouse         thing         germany       kill          romance\\ndirector      terrible      years         goes          wonderful\\nquite         script        history       killed        heart'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 364, 'page_label': '351', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='house         thing         germany       kill          romance\\ndirector      terrible      years         goes          wonderful\\nquite         script        history       killed        heart\\nbit           nothing       new           going         feel\\nde            worse         modesty       house         year\\nperformances  waste         cowboy        away          each\\ndark          pretty        jewish        head          french\\ntwist         minutes       past          take          sweet\\nhitchcock     didn          kirk          another       boy\\ntension       actors        young         getting       loved\\ninteresting   actually      spanish       doesn         girl\\nmysterious    re            enterprise    now           relationship\\nmurder        supposed      von           night         saw\\nending        mean          nazis         right         both\\ncreepy        want          spock         woman         simple'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 364, 'page_label': '351', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='murder        supposed      von           night         saw\\nending        mean          nazis         right         both\\ncreepy        want          spock         woman         simple\\ntopic 36      topic 37      topic 41      topic 45      topic 51\\n--------      --------      --------      --------      --------\\nperformance   excellent     war           music         earth\\nrole          highly        american      song          space\\nactor         amazing       world         songs         planet\\ncast          wonderful     soldiers      rock          superman\\nplay          truly         military      band          alien\\nactors        superb        army          soundtrack    world\\nperformances  actors        tarzan        singing       evil\\nplayed        brilliant     soldier       voice         humans\\nsupporting    recommend     america       singer        aliens\\ndirector      quite         country       sing          human'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 364, 'page_label': '351', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='played        brilliant     soldier       voice         humans\\nsupporting    recommend     america       singer        aliens\\ndirector      quite         country       sing          human\\noscar         performance   americans     musical       creatures\\nroles         performances  during        roll          miike\\nactress       perfect       men           fan           monsters\\nexcellent     drama         us            metal         apes\\nscreen        without       government    concert       clark\\nplays         beautiful     jungle        playing       burton\\naward         human         vietnam       hear          tim\\nwork          moving        ii            fans          outer\\nplaying       world         political     prince        men\\ngives         recommended   against       especially    moon\\nTopic Modeling and Document Clustering | 351'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 365, 'page_label': '352', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='topic 53      topic 54      topic 63      topic 89      topic 97\\n--------      --------      --------      --------      --------\\nscott         money         funny         dead          didn\\ngary          budget        comedy        zombie        thought\\nstreisand     actors        laugh         gore          wasn\\nstar          low           jokes         zombies       ending\\nhart          worst         humor         blood         minutes\\nlundgren      waste         hilarious     horror        got\\ndolph         10            laughs        flesh         felt\\ncareer        give          fun           minutes       part\\nsabrina       want          re            body          going\\nrole          nothing       funniest      living        seemed\\ntemple        terrible      laughing      eating        bit\\nphantom       crap          joke          flick         found\\njudy          must          few           budget        though\\nmelissa       reviews       moments       head          nothing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 365, 'page_label': '352', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='phantom       crap          joke          flick         found\\njudy          must          few           budget        though\\nmelissa       reviews       moments       head          nothing\\nzorro         imdb          guy           gory          lot\\ngets          director      unfunny       evil          saw\\nbarbra        thing         times         shot          long\\ncast          believe       laughed       low           interesting\\nshort         am            comedies      fulci         few\\nserial        actually      isn           re            half\\nThe topics we extracted this time seem to be more specific, though many are hard to\\ninterpret. Topic 7 seems to be about horror movies and thrillers; topics 16 and 54\\nseem to capture bad reviews, while topic 63 mostly seems to be capturing positive\\nreviews of comedies. If we want to make further inferences using the topics that were\\ndiscovered, we should confirm the intuition we gained from looking at the highest-'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 365, 'page_label': '352', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='reviews of comedies. If we want to make further inferences using the topics that were\\ndiscovered, we should confirm the intuition we gained from looking at the highest-\\nranking words for each topic by looking at the documents that are assigned to these\\ntopics. For example, topic 45 seems to be about music. Let’s check which kinds of\\nreviews are assigned to this topic:\\nIn[49]:\\n# sort by weight of \"music\" topic 45\\nmusic = np.argsort(document_topics100[:, 45])[::-1]\\n# print the five documents where the topic is most important\\nfor i in music[:10]:\\n    # pshow first two sentences\\n    print(b\".\".join(text_train[i].split(b\".\")[:2]) + b\".\\\\n\")\\nOut[49]:\\nb\\'I love this movie and never get tired of watching. The music in it is great.\\\\n\\'\\nb\"I enjoyed Still Crazy more than any film I have seen in years. A successful\\n  band from the 70\\'s decide to give it another try.\\\\n\"\\nb\\'Hollywood Hotel was the last movie musical that Busby Berkeley directed for'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 365, 'page_label': '352', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='band from the 70\\'s decide to give it another try.\\\\n\"\\nb\\'Hollywood Hotel was the last movie musical that Busby Berkeley directed for\\n  Warner Bros. His directing style had changed or evolved to the point that\\n  this film does not contain his signature overhead shots or huge production\\n  numbers with thousands of extras.\\\\n\\'\\nb\"What happens to washed up rock-n-roll stars in the late 1990\\'s?\\n  They launch a comeback / reunion tour. At least, that\\'s what the members of\\n  Strange Fruit, a (fictional) 70\\'s stadium rock group do.\\\\n\"\\n352 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 366, 'page_label': '353', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='b\\'As a big-time Prince fan of the last three to four years, I really can\\\\\\'t\\n  believe I\\\\\\'ve only just got round to watching \"Purple Rain\". The brand new\\n  2-disc anniversary Special Edition led me to buy it.\\\\n\\'\\nb\"This film is worth seeing alone for Jared Harris\\' outstanding portrayal\\n  of John Lennon. It doesn\\'t matter that Harris doesn\\'t exactly resemble\\n  Lennon; his mannerisms, expressions, posture, accent and attitude are\\n  pure Lennon.\\\\n\"\\nb\"The funky, yet strictly second-tier British glam-rock band Strange Fruit\\n  breaks up at the end of the wild\\'n\\'wacky excess-ridden 70\\'s. The individual\\n  band members go their separate ways and uncomfortably settle into lackluster\\n  middle age in the dull and uneventful 90\\'s: morose keyboardist Stephen Rea\\n  winds up penniless and down on his luck, vain, neurotic, pretentious lead\\n  singer Bill Nighy tries (and fails) to pursue a floundering solo career,\\n  paranoid drummer Timothy Spall resides in obscurity on a remote farm so he'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 366, 'page_label': '353', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='singer Bill Nighy tries (and fails) to pursue a floundering solo career,\\n  paranoid drummer Timothy Spall resides in obscurity on a remote farm so he\\n  can avoid paying a hefty back taxes debt, and surly bass player Jimmy Nail\\n  installs roofs for a living.\\\\n\"\\nb\"I just finished reading a book on Anita Loos\\' work and the photo in TCM\\n  Magazine of MacDonald in her angel costume looked great (impressive wings),\\n  so I thought I\\'d watch this movie. I\\'d never heard of the film before, so I\\n  had no preconceived notions about it whatsoever.\\\\n\"\\nb\\'I love this movie!!! Purple Rain came out the year I was born and it has had\\n  my heart since I can remember. Prince is so tight in this movie.\\\\n\\'\\nb\"This movie is sort of a Carrie meets Heavy Metal. It\\'s about a highschool\\n  guy who gets picked on alot and he totally gets revenge with the help of a\\n  Heavy Metal ghost.\\\\n\"\\nAs we can see, this topic covers a wide variety of music-centered reviews, from musi‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 366, 'page_label': '353', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='guy who gets picked on alot and he totally gets revenge with the help of a\\n  Heavy Metal ghost.\\\\n\"\\nAs we can see, this topic covers a wide variety of music-centered reviews, from musi‐\\ncals, to biographical movies, to some hard-to-specify genre in the last review. Another\\ninteresting way to inspect the topics is to see how much weight each topic gets over‐\\nall, by summing the document_topics over all reviews. We name each topic by the\\ntwo most common words. Figure 7-6 shows the topic weights learned:\\nIn[50]:\\nfig, ax = plt.subplots(1, 2, figsize=(10, 10))\\ntopic_names = [\"{:>2} \".format(i) + \" \".join(words)\\n               for i, words in enumerate(feature_names[sorting[:, :2]])]\\n# two column bar chart:\\nfor col in [0, 1]:\\n    start = col * 50\\n    end = (col + 1) * 50\\n    ax[col].barh(np.arange(50), np.sum(document_topics100, axis=0)[start:end])\\n    ax[col].set_yticks(np.arange(50))\\n    ax[col].set_yticklabels(topic_names[start:end], ha=\"left\", va=\"top\")\\n    ax[col].invert_yaxis()'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 366, 'page_label': '353', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='ax[col].set_yticks(np.arange(50))\\n    ax[col].set_yticklabels(topic_names[start:end], ha=\"left\", va=\"top\")\\n    ax[col].invert_yaxis()\\n    ax[col].set_xlim(0, 2000)\\n    yax = ax[col].get_yaxis()\\n    yax.set_tick_params(pad=130)\\nplt.tight_layout()\\nTopic Modeling and Document Clustering | 353'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 367, 'page_label': '354', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Figure 7-6. Topic weights learned by LDA\\nThe most important topics are 97, which seems to consist mostly of stopwords, possi‐\\nbly with a slight negative direction; topic 16, which is clearly about bad reviews; fol‐\\nlowed by some genre-specific topics and 36 and 37, both of which seem to contain\\nlaudatory words.\\nIt seems like LDA mostly discovered two kind of topics, genre-specific and rating-\\nspecific, in addition to several more unspecific topics. This is an interesting discovery,\\nas most reviews are made up of some movie-specific comments and some comments\\nthat justify or emphasize the rating.\\nTopic models like LDA are interesting methods to understand large text corpora in\\nthe absence of labels—or, as here, even if labels are available. The LDA algorithm is\\nrandomized, though, and changing the random_state parameter can lead to quite\\n354 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 368, 'page_label': '355', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='different outcomes. While identifying topics can be helpful, any conclusions you\\ndraw from an unsupervised model should be taken with a grain of salt, and we rec‐\\nommend verifying your intuition by looking at the documents in a specific topic. The\\ntopics produced by the LDA.transform method can also sometimes be used as a com‐\\npact representation for supervised learning. This is particularly helpful when few\\ntraining examples are available.\\nSummary and Outlook\\nIn this chapter we talked about the basics of processing text, also known as natural\\nlanguage processing (NLP), with an example application classifying movie reviews.\\nThe tools discussed here should serve as a great starting point when trying to process\\ntext data. In particular for text classification tasks such as spam and fraud detection\\nor sentiment analysis, bag-of-words representations provide a simple and powerful\\nsolution. As is often the case in machine learning, the representation of the data is key'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 368, 'page_label': '355', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='or sentiment analysis, bag-of-words representations provide a simple and powerful\\nsolution. As is often the case in machine learning, the representation of the data is key\\nin NLP applications, and inspecting the tokens and n-grams that are extracted can\\ngive powerful insights into the modeling process. In text-processing applications, it is\\noften possible to introspect models in a meaningful way, as we saw in this chapter, for\\nboth supervised and unsupervised tasks. Y ou should take full advantage of this ability\\nwhen using NLP-based methods in practice.\\nNatural language and text processing is a large research field, and discussing the\\ndetails of advanced methods is far beyond the scope of this book. If you want to learn\\nmore, we recommend the O’Reilly book Natural Language Processing with Python by\\nSteven Bird, Ewan Klein, and Edward Loper, which provides an overview of NLP\\ntogether with an introduction to the nltk Python package for NLP . Another great and'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 368, 'page_label': '355', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Steven Bird, Ewan Klein, and Edward Loper, which provides an overview of NLP\\ntogether with an introduction to the nltk Python package for NLP . Another great and\\nmore conceptual book is the standard reference Introduction to Information Retrieval\\nby Christopher Manning, Prabhakar Raghavan, and Hinrich Schütze, which describes\\nfundamental algorithms in information retrieval, NLP , and machine learning. Both\\nbooks have online versions that can be accessed free of charge. As we discussed ear‐\\nlier, the classes CountVectorizer and TfidfVectorizer only implement relatively\\nsimple text-processing methods. For more advanced text-processing methods, we\\nrecommend the Python packages spacy (a relatively new but very efficient and well-\\ndesigned package), nltk (a very well-established and complete but somewhat dated\\nlibrary), and gensim (an NLP package with an emphasis on topic modeling).\\nThere have been several very exciting new developments in text processing in recent'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 368, 'page_label': '355', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='library), and gensim (an NLP package with an emphasis on topic modeling).\\nThere have been several very exciting new developments in text processing in recent\\nyears, which are outside of the scope of this book and relate to neural networks. The\\nfirst is the use of continuous vector representations, also known as word vectors or\\ndistributed word representations, as implemented in the word2vec library. The origi‐\\nnal paper “Distributed Representations of Words and Phrases and Their Composi‐\\ntionality” by Thomas Mikolov et al. is a great introduction to the subject. Both spacy\\nSummary and Outlook | 355'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 369, 'page_label': '356', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='and gensim provide functionality for the techniques discussed in this paper and its\\nfollow-ups.\\nAnother direction in NLP that has picked up momentum in recent years is the use of\\nrecurrent neural networks (RNNs) for text processing. RNNs are a particularly power‐\\nful type of neural network that can produce output that is again text, in contrast to\\nclassification models that can only assign class labels. The ability to produce text as\\noutput makes RNNs well suited for automatic translation and summarization. An\\nintroduction to the topic can be found in the relatively technical paper “Sequence to\\nSequence Learning with Neural Networks” by Ilya Suskever, Oriol Vinyals, and Quoc\\nLe. A more practical tutorial using the tensorflow framework can be found on the\\nTensorFlow website.\\n356 | Chapter 7: Working with Text Data'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 370, 'page_label': '357', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 8\\nWrapping Up\\nY ou now know how to apply the important machine learning algorithms for super‐\\nvised and unsupervised learning, which allow you to solve a wide variety of machine\\nlearning problems. Before we leave you to explore all the possibilities that machine\\nlearning offers, we want to give you some final words of advice, point you toward\\nsome additional resources, and give you suggestions on how you can further improve\\nyour machine learning and data science skills.\\nApproaching a Machine Learning Problem\\nWith all the great methods that we introduced in this book now at your fingertips, it\\nmay be tempting to jump in and start solving your data-related problem by just run‐\\nning your favorite algorithm. However, this is not usually a good way to begin your\\nanalysis. The machine learning algorithm is usually only a small part of a larger data\\nanalysis and decision-making process. To make effective use of machine learning, we'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 370, 'page_label': '357', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='analysis. The machine learning algorithm is usually only a small part of a larger data\\nanalysis and decision-making process. To make effective use of machine learning, we\\nneed to take a step back and consider the problem at large. First, you should think\\nabout what kind of question you want to answer. Do you want to do exploratory anal‐\\nysis and just see if you find something interesting in the data? Or do you already have\\na particular goal in mind? Often you will start with a goal, like detecting fraudulent\\nuser transactions, making movie recommendations, or finding unknown planets. If\\nyou have such a goal, before building a system to achieve it, you should first think\\nabout how to define and measure success, and what the impact of a successful solu‐\\ntion would be to your overall business or research goals. Let’s say your goal is fraud\\ndetection.\\n357'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 371, 'page_label': '358', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Then the following questions open up:\\n• How do I measure if my fraud prediction is actually working?\\n• Do I have the right data to evaluate an algorithm?\\n• If I am successful, what will be the business impact of my solution?\\nAs we discussed in Chapter 5, it is best if you can measure the performance of your\\nalgorithm directly using a business metric, like increased profit or decreased losses.\\nThis is often hard to do, though. A question that can be easier to answer is “What if I\\nbuilt the perfect model?” If perfectly detecting any fraud will save your company $100\\na month, these possible savings will probably not be enough to warrant the effort of\\nyou even starting to develop an algorithm. On the other hand, if the model might\\nsave your company tens of thousands of dollars every month, the problem might be\\nworth exploring.\\nSay you’ve defined the problem to solve, you know a solution might have a significant'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 371, 'page_label': '358', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='save your company tens of thousands of dollars every month, the problem might be\\nworth exploring.\\nSay you’ve defined the problem to solve, you know a solution might have a significant\\nimpact for your project, and you’ve ensured that you have the right information to\\nevaluate success. The next steps are usually acquiring the data and building a working\\nprototype. In this book we have talked about many models you can employ, and how\\nto properly evaluate and tune these models. While trying out models, though, keep in\\nmind that this is only a small part of a larger data science workflow, and model build‐\\ning is often part of a feedback circle of collecting new data, cleaning data, building\\nmodels, and analyzing the models. Analyzing the mistakes a model makes can often\\nbe informative about what is missing in the data, what additional data could be col‐\\nlected, or how the task could be reformulated to make machine learning more effec‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 371, 'page_label': '358', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='be informative about what is missing in the data, what additional data could be col‐\\nlected, or how the task could be reformulated to make machine learning more effec‐\\ntive. Collecting more or different data or changing the task formulation slightly might\\nprovide a much higher payoff than running endless grid searches to tune parameters.\\nHumans in the Loop\\nY ou should also consider if and how you should have humans in the loop. Some pro‐\\ncesses (like pedestrian detection in a self-driving car) need to make immediate deci‐\\nsions. Others might not need immediate responses, and so it can be possible to have\\nhumans confirm uncertain decisions. Medical applications, for example, might need\\nvery high levels of precision that possibly cannot be achieved by a machine learning\\nalgorithm alone. But if an algorithm can make 90 percent, 50 percent, or maybe even\\njust 10 percent of decisions automatically, that might already increase response time'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 371, 'page_label': '358', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='algorithm alone. But if an algorithm can make 90 percent, 50 percent, or maybe even\\njust 10 percent of decisions automatically, that might already increase response time\\nor reduce cost. Many applications are dominated by “simple cases, ” for which an algo‐\\nrithm can make a decision, with relatively few “complicated cases, ” which can be\\nrerouted to a human.\\n358 | Chapter 8: Wrapping Up'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 372, 'page_label': '359', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='From Prototype to Production\\nThe tools we’ve discussed in this book are great for many machine learning applica‐\\ntions, and allow very quick analysis and prototyping. Python and scikit-learn are\\nalso used in production systems in many organizations—even very large ones like\\ninternational banks and global social media companies. However, many companies\\nhave complex infrastructure, and it is not always easy to include Python in these sys‐\\ntems. That is not necessarily a problem. In many companies, the data analytics teams\\nwork with languages like Python and R that allow the quick testing of ideas, while\\nproduction teams work with languages like Go, Scala, C++, and Java to build robust,\\nscalable systems. Data analysis has different requirements from building live services,\\nand so using different languages for these tasks makes sense. A relatively common\\nsolution is to reimplement the solution that was found by the analytics team inside'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 372, 'page_label': '359', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='and so using different languages for these tasks makes sense. A relatively common\\nsolution is to reimplement the solution that was found by the analytics team inside\\nthe larger framework, using a high-performance language. This can be easier than\\nembedding a whole library or programming language and converting from and to the\\ndifferent data formats.\\nRegardless of whether you can use scikit-learn in a production system or not, it is\\nimportant to keep in mind that production systems have different requirements from\\none-off analysis scripts. If an algorithm is deployed into a larger system, software\\nengineering aspects like reliability, predictability, runtime, and memory requirements\\ngain relevance. Simplicity is key in providing machine learning systems that perform\\nwell in these areas. Critically inspect each part of your data processing and prediction\\npipeline and ask yourself how much complexity each step creates, how robust each'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 372, 'page_label': '359', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='well in these areas. Critically inspect each part of your data processing and prediction\\npipeline and ask yourself how much complexity each step creates, how robust each\\ncomponent is to changes in the data or compute infrastructure, and if the benefit of\\neach component warrants the complexity. If you are building involved machine learn‐\\ning systems, we highly recommend reading the paper “Machine Learning: The High\\nInterest Credit Card of Technical Debt” , published by researchers in Google’s\\nmachine learning team. The paper highlights the trade-off in creating and maintain‐\\ning machine learning software in production at a large scale. While the issue of tech‐\\nnical debt is particularly pressing in large-scale and long-term projects, the lessons\\nlearned can help us build better software even for short-lived and smaller systems.\\nTesting Production Systems\\nIn this book, we covered how to evaluate algorithmic predictions based on a test set'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 372, 'page_label': '359', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='learned can help us build better software even for short-lived and smaller systems.\\nTesting Production Systems\\nIn this book, we covered how to evaluate algorithmic predictions based on a test set\\nthat we collected beforehand. This is known as offline evaluation. If your machine\\nlearning system is user-facing, this is only the first step in evaluating an algorithm,\\nthough. The next step is usually online testing or live testing, where the consequences\\nof employing the algorithm in the overall system are evaluated. Changing the recom‐\\nmendations or search results users are shown by a website can drastically change\\ntheir behavior and lead to unexpected consequences. To protect against these sur‐\\nprises, most user-facing services employ A/B testing, a form of blind user study. In\\nFrom Prototype to Production | 359'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 373, 'page_label': '360', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='A/B testing, without their knowledge a selected portion of users will be provided with\\na website or service using algorithm A, while the rest of the users will be provided\\nwith algorithm B. For both groups, relevant success metrics will be recorded for a set\\nperiod of time. Then, the metrics of algorithm A and algorithm B will be compared,\\nand a selection between the two approaches will be made according to these metrics.\\nUsing A/B testing enables us to evaluate the algorithms “in the wild, ” which might\\nhelp us to discover unexpected consequences when users are interacting with our\\nmodel. Often A is a new model, while B is the established system. There are more\\nelaborate mechanisms for online testing that go beyond A/B testing, such as bandit\\nalgorithms. A great introduction to this subject can be found in the book Bandit Algo‐\\nrithms for Website Optimization by John Myles White (O’Reilly). \\nBuilding Your Own Estimator'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 373, 'page_label': '360', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='algorithms. A great introduction to this subject can be found in the book Bandit Algo‐\\nrithms for Website Optimization by John Myles White (O’Reilly). \\nBuilding Your Own Estimator\\nThis book has covered a variety of tools and algorithms implemented in scikit-\\nlearn that can be used on a wide range of tasks. However, often there will be some\\nparticular processing you need to do for your data that is not implemented in\\nscikit-learn. It may be enough to just preprocess your data before passing it to your\\nscikit-learn model or pipeline. However, if your preprocessing is data dependent,\\nand you want to apply a grid search or cross-validation, things become trickier.\\nIn Chapter 6 we discussed the importance of putting all data-dependent processing\\ninside the cross-validation loop. So how can you use your own processing together\\nwith the scikit-learn tools? There is a simple solution: build your own estimator!\\nImplementing an estimator that is compatible with the scikit-learn interface, so'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 373, 'page_label': '360', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='with the scikit-learn tools? There is a simple solution: build your own estimator!\\nImplementing an estimator that is compatible with the scikit-learn interface, so\\nthat it can be used with Pipeline, GridSearchCV, and cross_val_score, is quite easy.\\nY ou can find detailed instructions in the scikit-learn documentation, but here is\\nthe gist. The simplest way to implement a transformer class is by inheriting from\\nBaseEstimator and TransformerMixin, and then implementing the __init__, fit,\\nand predict functions like this:\\n360 | Chapter 8: Wrapping Up'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 374, 'page_label': '361', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='In[1]:\\nfrom sklearn.base import BaseEstimator, TransformerMixin\\nclass MyTransformer(BaseEstimator, TransformerMixin):\\n    def __init__(self, first_parameter=1, second_parameter=2):\\n        # All parameters must be specified in the __init__ function\\n        self.first_parameter = 1\\n        self.second_parameter = 2\\n    def fit(self, X, y=None):\\n        # fit should only take X and y as parameters\\n        # Even if your model is unsupervised, you need to accept a y argument!\\n        # Model fitting code goes here\\n        print(\"fitting the model right here\")\\n        # fit returns self\\n        return self\\n    def transform(self, X):\\n        # transform takes as parameter only X\\n        # Apply some transformation to X\\n        X_transformed = X + 1\\n        return X_transformed\\nImplementing a classifier or regressor works similarly, only instead of Transformer\\nMixin you need to inherit from ClassifierMixin or RegressorMixin. Also, instead'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 374, 'page_label': '361', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='return X_transformed\\nImplementing a classifier or regressor works similarly, only instead of Transformer\\nMixin you need to inherit from ClassifierMixin or RegressorMixin. Also, instead\\nof implementing transform, you would implement predict.\\nAs you can see from the example given here, implementing your own estimator\\nrequires very little code, and most scikit-learn users build up a collection of cus‐\\ntom models over time.\\nWhere to Go from Here\\nThis book provides an introduction to machine learning and will make you an effec‐\\ntive practitioner. However, if you want to further your machine learning skills, here\\nare some suggestions of books and more specialized resources to investigate to dive\\ndeeper.\\nTheory\\nIn this book, we tried to provide an intuition of how the most common machine\\nlearning algorithms work, without requiring a strong foundation in mathematics or\\ncomputer science. However, many of the models we discussed use principles from'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 374, 'page_label': '361', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='learning algorithms work, without requiring a strong foundation in mathematics or\\ncomputer science. However, many of the models we discussed use principles from\\nprobability theory, linear algebra, and optimization. While it is not necessary to\\nunderstand all the details of how these algorithms are implemented, we think that\\nWhere to Go from Here | 361'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 375, 'page_label': '362', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='1 Andreas might not be entirely objective in this matter.\\nknowing some of the theory behind the algorithms will make you a better data scien‐\\ntist. There have been many good books written about the theory of machine learning,\\nand if we were able to excite you about the possibilities that machine learning opens\\nup, we suggest you pick up at least one of them and dig deeper. We already men‐\\ntioned Hastie, Tibshirani, and Friedman’s book The Elements of Statistical Learning in\\nthe Preface, but it is worth repeating this recommendation here. Another quite acces‐\\nsible book, with accompanying Python code, is Machine Learning: An Algorithmic\\nPerspective by Stephen Marsland (Chapman and Hall/CRC). Two other highly recom‐\\nmended classics are Pattern Recognition and Machine Learning by Christopher Bishop\\n(Springer), a book that emphasizes a probabilistic framework, and Machine Learning:\\nA Probabilistic Perspective  by Kevin Murphy (MIT Press), a comprehensive (read:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 375, 'page_label': '362', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='(Springer), a book that emphasizes a probabilistic framework, and Machine Learning:\\nA Probabilistic Perspective  by Kevin Murphy (MIT Press), a comprehensive (read:\\n1,000+ pages) dissertation on machine learning methods featuring in-depth discus‐\\nsions of state-of-the-art approaches, far beyond what we could cover in this book.\\nOther Machine Learning Frameworks and Packages\\nWhile scikit-learn is our favorite package for machine learning 1 and Python is our\\nfavorite language for machine learning, there are many other options out there.\\nDepending on your needs, Python and scikit-learn might not be the best fit for\\nyour particular situation. Often using Python is great for trying out and evaluating\\nmodels, but larger web services and applications are more commonly written in Java\\nor C++, and integrating into these systems might be necessary for your model to be\\ndeployed. Another reason you might want to look beyond scikit-learn is if you are'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 375, 'page_label': '362', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='or C++, and integrating into these systems might be necessary for your model to be\\ndeployed. Another reason you might want to look beyond scikit-learn is if you are\\nmore interested in statistical modeling and inference than prediction. In this case,\\nyou should consider the statsmodel package for Python, which implements several\\nlinear models with a more statistically minded interface. If you are not married to\\nPython, you might also consider using R, another lingua franca of data scientists. R is\\na language designed specifically for statistical analysis and is famous for its excellent\\nvisualization capabilities and the availability of many (often highly specialized) statis‐\\ntical modeling packages.\\nAnother popular machine learning package is vowpal wabbit  (often called vw to\\navoid possible tongue twisting), a highly optimized machine learning package written\\nin C++ with a command-line interface. vw is particularly useful for large datasets and'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 375, 'page_label': '362', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='avoid possible tongue twisting), a highly optimized machine learning package written\\nin C++ with a command-line interface. vw is particularly useful for large datasets and\\nfor streaming data. For running machine learning algorithms distributed on a cluster,\\none of the most popular solutions at the time of writing is mllib, a Scala library built\\non top of the spark distributed computing environment.\\n362 | Chapter 8: Wrapping Up'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 376, 'page_label': '363', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Ranking, Recommender Systems, and Other Kinds of Learning\\nBecause this is an introductory book, we focused on the most common machine\\nlearning tasks: classification and regression in supervised learning, and clustering and\\nsignal decomposition in unsupervised learning. There are many more kinds of\\nmachine learning out there, with many important applications. There are two partic‐\\nularly important topics that we did not cover in this book. The first is ranking, in\\nwhich we want to retrieve answers to a particular query, ordered by their relevance.\\nY ou’ve probably already used a ranking system today; this is how search engines\\noperate. Y ou input a search query and obtain a sorted list of answers, ranked by how\\nrelevant they are. A great introduction to ranking is provided in Manning, Raghavan,\\nand Schütze’s book Introduction to Information Retrieval. The second topic is recom‐\\nmender systems , which provide suggestions to users based on their preferences.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 376, 'page_label': '363', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='and Schütze’s book Introduction to Information Retrieval. The second topic is recom‐\\nmender systems , which provide suggestions to users based on their preferences.\\nY ou’ve probably encountered recommender systems under headings like “People Y ou\\nMay Know, ” “Customers Who Bought This Item Also Bought, ” or “Top Picks for\\nY ou. ” There is plenty of literature on the topic, and if you want to dive right in you\\nmight be interested in the now classic “Netflix prize challenge”, in which the Netflix\\nvideo streaming site released a large dataset of movie preferences and offered a prize\\nof $1 million to the team that could provide the best recommendations. Another\\ncommon application is prediction of time series (like stock prices), which also has a\\nwhole body of literature devoted to it. There are many more machine learning tasks\\nout there—much more than we can list here—and we encourage you to seek out\\ninformation from books, research papers, and online communities to find the para‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 376, 'page_label': '363', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='out there—much more than we can list here—and we encourage you to seek out\\ninformation from books, research papers, and online communities to find the para‐\\ndigms that best apply to your situation.\\nProbabilistic Modeling, Inference, and Probabilistic Programming\\nMost machine learning packages provide predefined machine learning models that\\napply one particular algorithm. However, many real-world problems have a particular\\nstructure that, when properly incorporated into the model, can yield much better-\\nperforming predictions. Often, the structure of a particular problem can be expressed\\nusing the language of probability theory. Such structure commonly arises from hav‐\\ning a mathematical model of the situation for which you want to predict. To under‐\\nstand what we mean by a structured problem, consider the following example.\\nLet’s say you want to build a mobile application that provides a very detailed position'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 376, 'page_label': '363', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='stand what we mean by a structured problem, consider the following example.\\nLet’s say you want to build a mobile application that provides a very detailed position\\nestimate in an outdoor space, to help users navigate a historical site. A mobile phone\\nprovides many sensors to help you get precise location measurements, like the GPS,\\naccelerometer, and compass. Y ou also have an exact map of the area. This problem is\\nhighly structured. Y ou know where the paths and points of interest are from your\\nmap. Y ou also have rough positions from the GPS, and the accelerometer and com‐\\npass in the user’s device provide you with very precise relative measurements. But\\nthrowing these all together into a black-box machine learning system to predict posi‐\\ntions might not be the best idea. This would throw away all the information you\\nWhere to Go from Here | 363'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 377, 'page_label': '364', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='2 A preprint of Deep Learning can be viewed at http://www.deeplearningbook.org/.\\nalready know about how the real world works. If the compass and accelerometer tell\\nyou a user is going north, and the GPS is telling you the user is going south, you\\nprobably can’t trust the GPS. If your position estimate tells you the user just walked\\nthrough a wall, you should also be highly skeptical. It’s possible to express this situa‐\\ntion using a probabilistic model, and then use machine learning or probabilistic\\ninference to find out how much you should trust each measurement, and to reason\\nabout what the best guess for the location of a user is.\\nOnce you’ve expressed the situation and your model of how the different factors work\\ntogether in the right way, there are methods to compute the predictions using these\\ncustom models directly. The most general of these methods are called probabilistic\\nprogramming languages, and they provide a very elegant and compact way to express'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 377, 'page_label': '364', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='custom models directly. The most general of these methods are called probabilistic\\nprogramming languages, and they provide a very elegant and compact way to express\\na learning problem. Examples of popular probabilistic programming languages are\\nPyMC (which can be used in Python) and Stan (a framework that can be used from\\nseveral languages, including Python). While these packages require some under‐\\nstanding of probability theory, they simplify the creation of new models significantly.\\nNeural Networks\\nWhile we touched on the subject of neural networks briefly in Chapters 2 and 7, this\\nis a rapidly evolving area of machine learning, with innovations and new applications\\nbeing announced on a weekly basis. Recent breakthroughs in machine learning and\\nartificial intelligence, such as the victory of the Alpha Go program against human\\nchampions in the game of Go, the constantly improving performance of speech'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 377, 'page_label': '364', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='artificial intelligence, such as the victory of the Alpha Go program against human\\nchampions in the game of Go, the constantly improving performance of speech\\nunderstanding, and the availability of near-instantaneous speech translation, have all\\nbeen driven by these advances. While the progress in this field is so fast-paced that\\nany current reference to the state of the art will soon be outdated, the recent book\\nDeep Learning by Ian Goodfellow, Y oshua Bengio, and Aaron Courville (MIT Press)\\nis a comprehensive introduction into the subject.2\\nScaling to Larger Datasets\\nIn this book, we always assumed that the data we were working with could be stored\\nin a NumPy array or SciPy sparse matrix in memory (RAM). Even though modern\\nservers often have hundreds of gigabytes (GB) of RAM, this is a fundamental restric‐\\ntion on the size of data you can work with. Not everybody can afford to buy such a\\nlarge machine, or even to rent one from a cloud provider. In most applications, the'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 377, 'page_label': '364', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='tion on the size of data you can work with. Not everybody can afford to buy such a\\nlarge machine, or even to rent one from a cloud provider. In most applications, the\\ndata that is used to build a machine learning system is relatively small, though, and\\nfew machine learning datasets consist of hundreds of gigabites of data or more. This\\nmakes expanding your RAM or renting a machine from a cloud provider a viable sol‐\\nution in many cases. If you need to work with terabytes of data, however, or you need\\n364 | Chapter 8: Wrapping Up'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 378, 'page_label': '365', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='to process large amounts of data on a budget, there are two basic strategies: out-of-\\ncore learning and parallelization over a cluster.\\nOut-of-core learning describes learning from data that cannot be stored in main\\nmemory, but where the learning takes place on a single computer (or even a single\\nprocessor within a computer). The data is read from a source like the hard disk or the\\nnetwork either one sample at a time or in chunks of multiple samples, so that each\\nchunk fits into RAM. This subset of the data is then processed and the model is upda‐\\nted to reflect what was learned from the data. Then, this chunk of the data is dis‐\\ncarded and the next bit of data is read. Out-of-core learning is implemented for some\\nof the models in scikit-learn, and you can find details on it in the online user\\nguide. Because out-of-core learning requires all of the data to be processed by a single\\ncomputer, this can lead to long runtimes on very large datasets. Also, not all machine'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 378, 'page_label': '365', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='guide. Because out-of-core learning requires all of the data to be processed by a single\\ncomputer, this can lead to long runtimes on very large datasets. Also, not all machine\\nlearning algorithms can be implemented in this way.\\nThe other strategy for scaling is distributing the data over multiple machines in a\\ncompute cluster, and letting each computer process part of the data. This can be\\nmuch faster for some models, and the size of the data that can be processed is only\\nlimited by the size of the cluster. However, such computations often require relatively\\ncomplex infrastructure. One of the most popular distributed computing platforms at\\nthe moment is the spark platform built on top of Hadoop. spark includes some\\nmachine learning functionality within the MLLib package. If your data is already on a\\nHadoop filesystem, or you are already using spark to preprocess your data, this might\\nbe the easiest option. If you don’t already have such infrastructure in place, establish‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 378, 'page_label': '365', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Hadoop filesystem, or you are already using spark to preprocess your data, this might\\nbe the easiest option. If you don’t already have such infrastructure in place, establish‐\\ning and integrating a spark cluster might be too large an effort, however. The vw\\npackage mentioned earlier provides some distributed features and might be a better\\nsolution in this case.\\nHoning Your Skills\\nAs with many things in life, only practice will allow you to become an expert in the\\ntopics we covered in this book. Feature extraction, preprocessing, visualization, and\\nmodel building can vary widely between different tasks and different datasets. Maybe\\nyou are lucky enough to already have access to a variety of datasets and tasks. If you\\ndon’t already have a task in mind, a good place to start is machine learning competi‐\\ntions, in which a dataset with a given task is published, and teams compete in creating\\nthe best possible predictions. Many companies, nonprofit organizations, and univer‐'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 378, 'page_label': '365', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='tions, in which a dataset with a given task is published, and teams compete in creating\\nthe best possible predictions. Many companies, nonprofit organizations, and univer‐\\nsities host these competitions. One of the most popular places to find them is Kaggle,\\na website that regularly holds data science competitions, some of which have substan‐\\ntial prize money attached.\\nThe Kaggle forums are also a good source of information about the latest tools and\\ntricks in machine learning, and a wide range of datasets are available on the site. Even\\nmore datasets with associated tasks can be found on the OpenML platform , which\\nWhere to Go from Here | 365'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 379, 'page_label': '366', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='hosts over 20,000 datasets with over 50,000 associated machine learning tasks. Work‐\\ning with these datasets can provide a great opportunity to practice your machine\\nlearning skills. A disadvantage of competitions is that they already provide a particu‐\\nlar metric to optimize, and usually a fixed, preprocessed dataset. Keep in mind that\\ndefining the problem and collecting the data are also important aspects of real-world\\nproblems, and that representing the problem in the right way might be much more\\nimportant than squeezing the last percent of accuracy out of a classifier.\\nConclusion\\nWe hope we have convinced you of the usefulness of machine learning in a wide vari‐\\nety of applications, and how easily machine learning can be implemented in practice.\\nKeep digging into the data, and don’t lose sight of the larger picture.\\n366 | Chapter 8: Wrapping Up'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 380, 'page_label': '367', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Index\\nA\\nA/B testing, 359\\naccuracy, 22, 282\\nacknowledgments, xi\\nadjusted rand index (ARI), 191\\nagglomerative clustering\\nevaluating and comparing, 191\\nexample of, 183\\nhierarchical clustering, 184\\nlinkage choices, 182\\nprinciple of, 182\\nalgorithm chains and pipelines, 305-321\\nbuilding pipelines, 308\\nbuilding pipelines with make_pipeline,\\n313-316\\ngrid search preprocessing steps, 317\\ngrid-searching for model selection, 319\\nimportance of, 305\\noverview of, 320\\nparameter selection with preprocessing, 306\\npipeline interface, 312\\nusing pipelines in grid searches, 309-311\\nalgorithm parameter, 118\\nalgorithms (see also models; problem solving)\\nevaluating, 28\\nminimal code to apply to algorithm, 24\\nsample datasets, 30-34\\nscaling\\nMinMaxScaler, 102, 135-139, 190, 230,\\n308, 319\\nNormalizer, 134\\nRobustScaler, 133\\nStandardScaler, 114, 133, 138, 144, 150,\\n190-195, 314-320\\nsupervised, classification\\ndecision trees, 70-83\\ngradient boosting, 88-91, 119, 124\\nk-nearest neighbors, 35-44'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 380, 'page_label': '367', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='RobustScaler, 133\\nStandardScaler, 114, 133, 138, 144, 150,\\n190-195, 314-320\\nsupervised, classification\\ndecision trees, 70-83\\ngradient boosting, 88-91, 119, 124\\nk-nearest neighbors, 35-44\\nkernelized support vector machines,\\n92-104\\nlinear SVMs, 56\\nlogistic regression, 56\\nnaive Bayes, 68-70\\nneural networks, 104-119\\nrandom forests, 84-88\\nsupervised, regression\\ndecision trees, 70-83\\ngradient boosting, 88-91\\nk-nearest neighbors, 40\\nLasso, 53-55\\nlinear regression (OLS), 47, 220-229\\nneural networks, 104-119\\nrandom forests, 84-88\\nRidge, 49-55, 67, 112, 231, 234, 310,\\n317-319\\nunsupervised, clustering\\nagglomerative clustering, 182-187,\\n191-195, 203-207\\nDBSCAN, 187-190\\nk-means, 168-181\\nunsupervised, manifold learning\\nt-SNE, 163-168\\nunsupervised, signal decomposition\\nnon-negative matrix factorization,\\n156-163\\nprincipal component analysis, 140-155\\nalpha parameter in linear models, 50\\nAnaconda, 6\\n367'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 381, 'page_label': '368', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='analysis of variance (ANOV A), 236\\narea under the curve (AUC), 294-296\\nattributions, x\\naverage precision, 292\\nB\\nbag-of-words representation\\napplying to movie reviews, 330-334\\napplying to toy dataset, 329\\nmore than one word (n-grams), 339-344\\nsteps in computing, 327\\nBernoulliNB, 68\\nbigrams, 339\\nbinary classification, 25, 56, 276-296\\nbinning, 144, 220-224\\nbootstrap samples, 84\\nBoston Housing dataset, 34\\nboundary points, 188\\nBunch objects, 33\\nbusiness metric, 275, 358\\nC\\nC parameter in SVC, 99\\ncalibration, 288\\ncancer dataset, 32\\ncategorical features\\ncategorical data, defined, 324\\ndefined, 211\\nencoded as numbers, 218\\nexample of, 212\\nrepresentation in training and test sets, 217\\nrepresenting using one-hot-encoding, 213\\ncategorical variables (see categorical features)\\nchaining (see algorithm chains and pipelines)\\nclass labels, 25\\nclassification problems\\nbinary vs. multiclass, 25\\nexamples of, 26\\ngoals for, 25\\niris classification example, 14\\nk-nearest neighbors, 35\\nlinear models, 56'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 381, 'page_label': '368', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='class labels, 25\\nclassification problems\\nbinary vs. multiclass, 25\\nexamples of, 26\\ngoals for, 25\\niris classification example, 14\\nk-nearest neighbors, 35\\nlinear models, 56\\nnaive Bayes classifiers, 68\\nvs. regression problems, 26\\nclassifiers\\nDecisionTreeClassifier, 75, 278\\nDecisionTreeRegressor, 75, 80\\nKNeighborsClassifier, 21-24, 37-43\\nKNeighborsRegressor, 42-47\\nLinearSVC, 56-59, 65, 67, 68\\nLogisticRegression, 56-62, 67, 209, 253, 279,\\n315, 332-347\\nMLPClassifier, 107-119\\nnaive Bayes, 68-70\\nSVC, 56, 100, 134, 139, 260, 269-272, 273,\\n305-309, 313-320\\nuncertainty estimates from, 119-127\\ncluster centers, 168\\nclustering algorithms\\nagglomerative clustering, 182-187\\napplications for, 131\\ncomparing on faces dataset, 195-207\\nDBSCAN, 187-190\\nevaluating with ground truth, 191-193\\nevaluating without ground truth, 193-195\\ngoals of, 168\\nk-means clustering, 168-181\\nsummary of, 207\\ncode examples\\ndownloading, x\\npermission for use, x\\ncoef_ attribute, 47, 50\\ncomments and questions, xi\\ncompetitions, 365'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 381, 'page_label': '368', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='goals of, 168\\nk-means clustering, 168-181\\nsummary of, 207\\ncode examples\\ndownloading, x\\npermission for use, x\\ncoef_ attribute, 47, 50\\ncomments and questions, xi\\ncompetitions, 365\\nconflation, 344\\nconfusion matrices, 279-286\\ncontext, 343\\ncontinuous features, 211, 218\\ncore samples/core points, 187\\ncorpus, 325\\ncos function, 232\\nCountVectorizer, 334\\ncross-validation\\nanalyzing results of, 267-271\\nbenefits of, 254\\ncross-validation splitters, 256\\ngrid search and, 263-275\\nin scikit-learn, 253\\nleave-one-out cross-validation, 257\\nnested, 272\\nparallelizing with grid search, 274\\nprinciple of, 252\\npurpose of, 254\\nshuffle-split cross-validation, 258\\nstratified k-fold, 254-256\\nwith groups, 259\\ncross_val_score function, 254, 307\\n368 | Index'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 382, 'page_label': '369', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='D\\ndata points, defined, 4\\ndata representation, 211-250 (see also feature\\nextraction/feature engineering; text data)\\nautomatic feature selection, 236-241\\nbinning and, 220-224\\ncategorical features, 212-220\\neffect on model performance, 211\\ninteger features, 218\\nmodel complexity vs. dataset size, 29\\noverview of, 250\\ntable analogy, 4\\nin training vs. test sets, 217\\nunderstanding your data, 4\\nunivariate nonlinear transformations,\\n232-236\\ndata transformations, 134\\n(see also preprocessing)\\ndata-driven research, 1\\nDBSCAN\\nevaluating and comparing, 191-207\\nparameters, 189\\nprinciple of, 187\\nreturned cluster assignments, 190\\nstrengths and weaknesses, 187\\ndecision boundaries, 37, 56\\ndecision function, 120\\ndecision trees\\nanalyzing, 76\\nbuilding, 71\\ncontrolling complexity of, 74\\ndata representation and, 220-224\\nfeature importance in, 77\\nif/else structure of, 70\\nparameters, 82\\nvs. random forests, 83\\nstrengths and weaknesses, 83\\ndecision_function, 286\\ndeep learning (see neural networks)\\ndendrograms, 184'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 382, 'page_label': '369', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='feature importance in, 77\\nif/else structure of, 70\\nparameters, 82\\nvs. random forests, 83\\nstrengths and weaknesses, 83\\ndecision_function, 286\\ndeep learning (see neural networks)\\ndendrograms, 184\\ndense regions, 187\\ndimensionality reduction, 141, 156\\ndiscrete features, 211\\ndiscretization, 220-224\\ndistributed computing, 362\\ndocument clustering, 347\\ndocuments, defined, 325\\ndual_coef_ attribute, 98\\nE\\neigenfaces, 147\\nembarrassingly parallel, 274\\nencoding, 328\\nensembles\\ndefined, 83\\ngradient boosted regression trees, 88-92\\nrandom forests, 83-88\\nEnthought Canopy, 6\\nestimators, 21, 360\\nestimator_ attribute of RFECV, 85\\nevaluation metrics and scoring\\nfor binary classification, 276-296\\nfor multiclass classification, 296-299\\nmetric selection, 275\\nmodel selection and, 300\\nregression metrics, 299\\ntesting production systems, 359\\nexp function, 232\\nexpert knowledge, 242-250\\nF\\nf(x)=y formula, 18\\nfacial recognition, 147, 157\\nfactor analysis (FA), 163\\nfalse positive rate (FPR), 292'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 382, 'page_label': '369', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='testing production systems, 359\\nexp function, 232\\nexpert knowledge, 242-250\\nF\\nf(x)=y formula, 18\\nfacial recognition, 147, 157\\nfactor analysis (FA), 163\\nfalse positive rate (FPR), 292\\nfalse positive/false negative errors, 277\\nfeature extraction/feature engineering, 211-250\\n(see also data representation; text data)\\naugmenting data with, 211\\nautomatic feature selection, 236-241\\ncategorical features, 212-220\\ncontinuous vs. discrete features, 211\\ndefined, 4, 34, 211\\ninteraction features, 224-232\\nwith non-negative matrix factorization, 156\\noverview of, 250\\npolynomial features, 224-232\\nwith principal component analysis, 147\\nunivariate nonlinear transformations,\\n232-236\\nusing expert knowledge, 242-250\\nfeature importance, 77\\nfeatures, defined, 4\\nfeature_names attribute, 33\\nfeed-forward neural networks, 104\\nfit method, 21, 68, 119, 135\\nfit_transform method, 138\\nfloating-point numbers, 26\\nIndex | 369'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 383, 'page_label': '370', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='folds, 252\\nforge dataset, 30\\nframeworks, 362\\nfree string data, 324\\nfreeform text data, 325\\nG\\ngamma parameter, 100\\nGaussian kernels of SVC, 97, 100\\nGaussianNB, 68\\ngeneralization\\nbuilding models for, 26\\ndefined, 17\\nexamples of, 27\\nget_dummies function, 218\\nget_support method of feature selection, 237\\ngradient boosted regression trees\\nfor feature selection, 220-224\\nlearning_rate parameter, 89\\nparameters, 91\\nvs. random forests, 88\\nstrengths and weaknesses, 91\\ntraining set accuracy, 90\\ngraphviz module, 76\\ngrid search\\naccessing pipeline attributes, 315\\nalternate strategies for, 272\\navoiding overfitting, 261\\nmodel selection with, 319\\nnested cross-validation, 272\\nparallelizing with cross-validation, 274\\npipeline preprocessing, 317\\nsearching non-grid spaces, 271\\nsimple example of, 261\\ntuning parameters with, 260\\nusing pipelines in, 309-311\\nwith cross-validation, 263-275\\nGridSearchCV\\nbest_estimator_ attribute, 267\\nbest_params_ attribute, 266\\nbest_score_ attribute, 266\\nH'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 383, 'page_label': '370', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='tuning parameters with, 260\\nusing pipelines in, 309-311\\nwith cross-validation, 263-275\\nGridSearchCV\\nbest_estimator_ attribute, 267\\nbest_params_ attribute, 266\\nbest_score_ attribute, 266\\nH\\nhandcoded rules, disadvantages of, 1\\nheat maps, 146\\nhidden layers, 106\\nhidden units, 105\\nhierarchical clustering, 184\\nhigh recall, 293\\nhigh-dimensional datasets, 32\\nhistograms, 144\\nhit rate, 283\\nhold-out sets, 17\\nhuman involvement/oversight, 358\\nI\\nimbalanced datasets, 277\\nindependent component analysis (ICA), 163\\ninference, 363\\ninformation leakage, 310\\ninformation retrieval (IR), 325\\ninteger features, 218\\n\"intelligent\" applications, 1\\ninteractions, 34, 224-232\\nintercept_ attribute, 47\\niris classification application\\ndata inspection, 19\\ndataset for, 14\\ngoals for, 13\\nk-nearest neighbors, 20\\nmaking predictions, 22\\nmodel evaluation, 22\\nmulticlass problem, 26\\noverview of, 23\\ntraining and testing data, 17\\niterative feature selection, 240\\nJ\\nJupyter Notebook, 7\\nK\\nk-fold cross-validation, 252'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 383, 'page_label': '370', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='making predictions, 22\\nmodel evaluation, 22\\nmulticlass problem, 26\\noverview of, 23\\ntraining and testing data, 17\\niterative feature selection, 240\\nJ\\nJupyter Notebook, 7\\nK\\nk-fold cross-validation, 252\\nk-means clustering\\napplying with scikit-learn, 170\\nvs. classification, 171\\ncluster centers, 169\\ncomplex datasets, 179\\nevaluating and comparing, 191\\nexample of, 168\\nfailures of, 173\\nstrengths and weaknesses, 181\\nvector quantization with, 176\\nk-nearest neighbors (k-NN)\\nanalyzing KNeighborsClassifier, 37\\nanalyzing KNeighborsRegressor, 43\\nbuilding, 20\\nclassification, 35-37\\n370 | Index'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 384, 'page_label': '371', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='vs. linear models, 46\\nparameters, 44\\npredictions with, 35\\nregression, 40\\nstrengths and weaknesses, 44\\nKaggle, 365\\nkernelized support vector machines (SVMs)\\nkernel trick, 97\\nlinear models and nonlinear features, 92\\nvs. linear support vector machines, 92\\nmathematics of, 92\\nparameters, 104\\npredictions with, 98\\npreprocessing data for, 102\\nstrengths and weaknesses, 104\\ntuning SVM parameters, 99\\nunderstanding, 98\\nknn object, 21\\nL\\nL1 regularization, 53\\nL2 regularization, 49, 60, 67\\nLasso model, 53\\nLatent Dirichlet Allocation (LDA), 348-355\\nleafs, 71\\nleakage, 310\\nlearn from the past approach, 243\\nlearning_rate parameter, 89\\nleave-one-out cross-validation, 257\\nlemmatization, 344-347\\nlinear functions, 56\\nlinear models\\nclassification, 56\\ndata representation and, 220-224\\nvs. k-nearest neighbors, 46\\nLasso, 53\\nlinear SVMs, 56\\nlogistic regression, 56\\nmulticlass classification, 63\\nordinary least squares, 47\\nparameters, 67\\npredictions with, 45\\nregression, 45\\nridge regression, 49'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 384, 'page_label': '371', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Lasso, 53\\nlinear SVMs, 56\\nlogistic regression, 56\\nmulticlass classification, 63\\nordinary least squares, 47\\nparameters, 67\\npredictions with, 45\\nregression, 45\\nridge regression, 49\\nstrengths and weaknesses, 67\\nlinear regression, 47, 224-232\\nlinear support vector machines (SVMs), 56\\nlinkage arrays, 185\\nlive testing, 359\\nlog function, 232\\nloss functions, 56\\nlow-dimensional datasets, 32\\nM\\nmachine learning\\nalgorithm chains and pipelines, 305-321\\napplications for, 1-5\\napproach to problem solving, 357-366\\nbenefits of Python for, 5\\nbuilding your own systems, vii\\ndata representation, 211-250\\nexamples of, 1, 13-23\\nmathematics of, vii\\nmodel evaluation and improvement,\\n251-303\\npreprocessing and scaling, 132-140\\nprerequisites to learning, vii\\nresources, ix, 361-366\\nscikit-learn and, 5-13\\nsupervised learning, 25-129\\nunderstanding your data, 4\\nunsupervised learning, 131-209\\nworking with text data, 323-356\\nmake_pipeline function\\naccessing step attributes, 314\\ndisplaying steps attribute, 314'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 384, 'page_label': '371', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='understanding your data, 4\\nunsupervised learning, 131-209\\nworking with text data, 323-356\\nmake_pipeline function\\naccessing step attributes, 314\\ndisplaying steps attribute, 314\\ngrid-searched pipelines and, 315\\nsyntax for, 313\\nmanifold learning algorithms\\napplications for, 164\\nexample of, 164\\nresults of, 168\\nvisualizations with, 163\\nmathematical functions for feature transforma‐\\ntions, 232\\nmatplotlib, 9\\nmax_features parameter, 84\\nmeta-estimators for trees and forests, 266\\nmethod chaining, 68\\nmetrics (see evaluation metrics and scoring)\\nmglearn, 11\\nmllib, 362\\nmodel-based feature selection, 238\\nmodels (see also algorithms)\\ncalibrated, 288\\ncapable of generalization, 26\\ncoefficients with text data, 338-347\\ncomplexity vs. dataset size, 29\\nIndex | 371'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 385, 'page_label': '372', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='cross-validation of, 252-260\\neffect of data representation choices on, 211\\nevaluation and improvement, 251-252\\nevaluation metrics and scoring, 275-302\\niris classification application, 13-23\\noverfitting vs. underfitting, 28\\npipeline preprocessing and, 317\\nselecting, 300\\nselecting with grid search, 319\\ntheory behind, 361\\ntuning parameters with grid search, 260-275\\nmovie reviews, 325\\nmulticlass classification\\nvs. binary classification, 25\\nevaluation metrics and scoring for, 296-299\\nlinear models for, 63\\nuncertainty estimates, 124\\nmultilayer perceptrons (MLPs), 104\\nMultinomialNB, 68\\nN\\nn-grams, 339\\nnaive Bayes classifiers\\nkinds in scikit-learn, 68\\nparameters, 70\\nstrengths and weaknesses, 70\\nnatural language processing (NLP), 325, 355\\nnegative class, 26\\nnested cross-validation, 272\\nNetflix prize challenge, 363\\nneural networks (deep learning)\\naccuracy of, 114\\nestimating complexity in, 118\\npredictions with, 104\\nrandomization in, 113\\nrecent breakthroughs in, 364\\nstrengths and weaknesses, 117'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 385, 'page_label': '372', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='neural networks (deep learning)\\naccuracy of, 114\\nestimating complexity in, 118\\npredictions with, 104\\nrandomization in, 113\\nrecent breakthroughs in, 364\\nstrengths and weaknesses, 117\\ntuning, 108\\nnon-negative matrix factorization (NMF)\\napplications for, 156\\napplying to face images, 157\\napplying to synthetic data, 156\\nnormalization, 344\\nnormalized mutual information (NMI), 191\\nNumPy (Numeric Python) library, 7\\nO\\noffline evaluation, 359\\none-hot-encoding, 213-217\\none-out-of-N encoding, 213-217\\none-vs.-rest approach, 63\\nonline resources, ix\\nonline testing, 359\\nOpenML platform, 365\\noperating points, 289\\nordinary least squares (OLS), 47\\nout-of-core learning, 364\\noutlier detection, 197\\noverfitting, 28, 261\\nP\\npair plots, 19\\npandas\\nbenefits of, 10\\nchecking string-encoded data, 214\\ncolumn indexing in, 216\\nconverting data to one-hot-encoding, 214\\nget_dummies function, 218\\nparallelization over a cluster, 364\\npermissions, x\\npipelines (see algorithm chains and pipelines)\\npolynomial features, 224-232'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 385, 'page_label': '372', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='converting data to one-hot-encoding, 214\\nget_dummies function, 218\\nparallelization over a cluster, 364\\npermissions, x\\npipelines (see algorithm chains and pipelines)\\npolynomial features, 224-232\\npolynomial kernels, 97\\npolynomial regression, 228\\npositive class, 26\\nPOSIX time, 244\\npre- and post-pruning, 74\\nprecision, 282, 358\\nprecision-recall curves, 289-292\\npredict for the future approach, 243\\npredict method, 22, 37, 68, 267\\npredict_proba function, 122, 286\\npreprocessing, 132-140\\ndata transformation application, 134\\neffect on supervised learning, 138\\nkinds of, 133\\nparameter selection with, 306\\npipelines and, 317\\npurpose of, 132\\nscaling training and test data, 136\\nprincipal component analysis (PCA)\\ndrawbacks of, 146\\nexample of, 140\\nfeature extraction with, 147\\nunsupervised nature of, 145\\nvisualizations with, 142\\nwhitening option, 150\\nprobabilistic modeling, 363\\n372 | Index'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 386, 'page_label': '373', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='probabilistic programming, 363\\nproblem solving\\nbuilding your own estimators, 360\\nbusiness metrics and, 358\\ninitial approach to, 357\\nresources, 361-366\\nsimple vs. complicated cases, 358\\nsteps of, 358\\ntesting your system, 359\\ntool choice, 359\\nproduction systems\\ntesting, 359\\ntool choice, 359\\npruning for decision trees, 74\\npseudorandom number generators, 18\\npure leafs, 73\\nPyMC language, 364\\nPython\\nbenefits of, 5\\nprepackaged distributions, 6\\nPython 2 vs. Python 3, 12\\nPython(x,y), 6\\nstatsmodel package, 362\\nR\\nR language, 362\\nradial basis function (RBF) kernel, 97\\nrandom forests\\nanalyzing, 85\\nbuilding, 84\\ndata representation and, 220-224\\nvs. decision trees, 83\\nvs. gradient boosted regression trees, 88\\nparameters, 88\\npredictions with, 84\\nrandomization in, 83\\nstrengths and weaknesses, 87\\nrandom_state parameter, 18\\nranking, 363\\nreal numbers, 26\\nrecall, 282\\nreceiver operating characteristics (ROC)\\ncurves, 292-296\\nrecommender systems, 363\\nrectified linear unit (relu), 106'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 386, 'page_label': '373', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='random_state parameter, 18\\nranking, 363\\nreal numbers, 26\\nrecall, 282\\nreceiver operating characteristics (ROC)\\ncurves, 292-296\\nrecommender systems, 363\\nrectified linear unit (relu), 106\\nrectifying nonlinearity, 106\\nrecurrent neural networks (RNNs), 356\\nrecursive feature elimination (RFE), 240\\nregression\\nf_regression, 236, 310\\nLinearRegression, 47-56, 81, 247\\nregression problems\\nBoston Housing dataset, 34\\nvs. classification problems, 26\\nevaluation metrics and scoring, 299\\nexamples of, 26\\ngoals for, 26\\nk-nearest neighbors, 40\\nLasso, 53\\nlinear models, 45\\nridge regression, 49\\nwave dataset illustration, 31\\nregularization\\nL1 regularization, 53\\nL2 regularization, 49, 60\\nrescaling\\nexample of, 132-140\\nkernel SVMs, 102\\nresources, ix\\nridge regression, 49\\nrobustness-based clustering, 194\\nroots, 72\\nS\\nSafari Books Online, x\\nsamples, defined, 4\\nscaling, 132-140\\ndata transformation application, 134\\neffect on supervised learning, 138\\ninto larger datasets, 364\\nkinds of, 133\\npurpose of, 132'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 386, 'page_label': '373', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='roots, 72\\nS\\nSafari Books Online, x\\nsamples, defined, 4\\nscaling, 132-140\\ndata transformation application, 134\\neffect on supervised learning, 138\\ninto larger datasets, 364\\nkinds of, 133\\npurpose of, 132\\ntraining and test data, 136\\nscatter plots, 19\\nscikit-learn\\nalternate frameworks, 362\\nbenefits of, 5\\nBunch objects, 33\\ncancer dataset, 32\\ncore code for, 24\\ndata and labels in, 18\\ndocumentation, 6\\nfeature_names attribute, 33\\nfit method, 21, 68, 119, 135\\nfit_transform method, 138\\ninstalling, 6\\nknn object, 21\\nlibraries and tools, 7-11\\nIndex | 373'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 387, 'page_label': '374', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='predict method, 22, 37, 68\\nPython 2 vs. Python 3, 12\\nrandom_state parameter, 18\\nscaling mechanisms in, 139\\nscore method, 23, 37, 43\\ntransform method, 135\\nuser guide, 6\\nversions used, 12\\nscikit-learn classes and functions\\naccuracy_score, 193\\nadjusted_rand_score, 191\\nAgglomerativeClustering, 182, 191, 203-207\\naverage_precision_score, 292\\nBaseEstimator, 360\\nclassification_report, 284-288, 298\\nconfusion_matrix, 279-299\\nCountVectorizer, 329-355\\ncross_val_score, 253, 256, 300, 307, 360\\nDBSCAN, 187-190\\nDecisionTreeClassifier, 75, 278\\nDecisionTreeRegressor, 75, 80\\nDummyClassifier, 278\\nElasticNet class, 55\\nENGLISH_STOP_WORDS, 334\\nEstimator, 21\\nexport_graphviz, 76\\nf1_score, 284, 291\\nfetch_lfw_people, 147\\nf_regression, 236, 310\\nGradientBoostingClassifier, 88-91, 119, 124\\nGridSearchCV, 263-275, 300-301, 305-309,\\n315-320, 360\\nGroupKFold, 259\\nKFold, 256, 260\\nKMeans, 174-181\\nKNeighborsClassifier, 21-24, 37-43\\nKNeighborsRegressor, 42-47\\nLasso, 53-55\\nLatentDirichletAllocation, 348\\nLeaveOneOut, 257'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 387, 'page_label': '374', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='315-320, 360\\nGroupKFold, 259\\nKFold, 256, 260\\nKMeans, 174-181\\nKNeighborsClassifier, 21-24, 37-43\\nKNeighborsRegressor, 42-47\\nLasso, 53-55\\nLatentDirichletAllocation, 348\\nLeaveOneOut, 257\\nLinearRegression, 47-56, 81, 247\\nLinearSVC, 56-59, 65, 67, 68\\nload_boston, 34, 230, 317\\nload_breast_cancer, 32, 38, 59, 75, 134, 144,\\n236, 305\\nload_digits, 164, 278\\nload_files, 326\\nload_iris, 14, 124, 253\\nLogisticRegression, 56-62, 67, 209, 253, 279,\\n315, 332-347\\nmake_blobs, 92, 119, 136, 173-183, 188, 286\\nmake_circles, 119\\nmake_moons, 85, 108, 175, 190-195\\nmake_pipeline, 313-319\\nMinMaxScaler, 102, 133, 135-139, 190, 230,\\n308, 309, 319\\nMLPClassifier, 107-119\\nNMF, 140, 159-163, 179-182, 348\\nNormalizer, 134\\nOneHotEncoder, 218, 247\\nParameterGrid, 274\\nPCA, 140-166, 179, 195-206, 313-314, 348\\nPipeline, 305-319, 320\\nPolynomialFeatures, 227-230, 248, 317\\nprecision_recall_curve, 289-292\\nRandomForestClassifier, 84-86, 238, 290,\\n319\\nRandomForestRegressor, 84, 231, 240\\nRFE, 240-241'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 387, 'page_label': '374', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='Pipeline, 305-319, 320\\nPolynomialFeatures, 227-230, 248, 317\\nprecision_recall_curve, 289-292\\nRandomForestClassifier, 84-86, 238, 290,\\n319\\nRandomForestRegressor, 84, 231, 240\\nRFE, 240-241\\nRidge, 49, 67, 112, 231, 234, 310, 317-319\\nRobustScaler, 133\\nroc_auc_score, 294-301\\nroc_curve, 293-296\\nSCORERS, 301\\nSelectFromModel, 238\\nSelectPercentile, 236, 310\\nShuffleSplit, 258, 258\\nsilhouette_score, 193\\nStandardScaler, 114, 133, 138, 144, 150,\\n190-195, 314-320\\nStratifiedKFold, 260, 274\\nStratifiedShuffleSplit, 258, 347\\nSVC, 56, 100, 134, 139, 260-267, 269-272,\\n305-309, 313-320\\nSVR, 92, 229\\nTfidfVectorizer, 336-356\\ntrain_test_split, 17-19, 251, 286, 289\\nTransformerMixin, 360\\nTSNE, 166\\nSciPy, 8\\nscore method, 23, 37, 43, 267, 308\\nsensitivity, 283\\nsentiment analysis example, 325\\nshapes, defined, 16\\nshuffle-split cross-validation, 258\\nsin function, 232\\nsoft voting strategy, 84\\n374 | Index'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 388, 'page_label': '375', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='spark computing environment, 362\\nsparse coding (dictionary learning), 163\\nsparse datasets, 44\\nsplits, 252\\nStan language, 364\\nstatsmodel package, 362\\nstemming, 344-347\\nstopwords, 334\\nstratified k-fold cross-validation, 254-256\\nstring-encoded categorical data, 214\\nsupervised learning, 25-129 (see also classifica‐\\ntion problems; regression problems)\\nalgorithms for\\ndecision trees, 70-83\\nensembles of decision trees, 83-92\\nk-nearest neighbors, 35-44\\nkernelized support vector machines,\\n92-104\\nlinear models, 45-68\\nnaive Bayes classifiers, 68\\nneural networks (deep learning),\\n104-119\\noverview of, 2\\ndata representation, 4\\nexamples of, 3\\ngeneralization, 26\\ngoals for, 25\\nmodel complexity vs. dataset size, 29\\noverfitting vs. underfitting, 28\\noverview of, 127\\nsample datasets, 30-34\\nuncertainty estimates, 119-127\\nsupport vectors, 98\\nsynthetic datasets, 30\\nT\\nt-SNE algorithm (see manifold learning algo‐\\nrithms)\\ntangens hyperbolicus (tanh), 106\\nterm frequency–inverse document frequency\\n(tf–idf), 336-347'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 388, 'page_label': '375', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='support vectors, 98\\nsynthetic datasets, 30\\nT\\nt-SNE algorithm (see manifold learning algo‐\\nrithms)\\ntangens hyperbolicus (tanh), 106\\nterm frequency–inverse document frequency\\n(tf–idf), 336-347\\nterminal nodes, 71\\ntest data/test sets\\nBoston Housing dataset, 34\\ndefined, 17\\nforge dataset, 30\\nwave dataset, 31\\nWisconsin Breast Cancer dataset, 32\\ntext data, 323-356\\nbag-of-words representation, 327-334\\nexamples of, 323\\nmodel coefficients, 338\\noverview of, 355\\nrescaling data with tf-idf, 336-338\\nsentiment analysis example, 325\\nstopwords, 334\\ntopic modeling and document clustering,\\n347-355\\ntypes of, 323-325\\ntime series predictions, 363\\ntokenization, 328, 344-347\\ntop nodes, 72\\ntopic modeling, with LDA, 347-355\\ntraining data, 17\\ntrain_test_split function, 254\\ntransform method, 135, 312, 334\\ntransformations\\nselecting, 235\\nunivariate nonlinear, 232-236\\nunsupervised, 131\\ntree module, 76\\ntrigrams, 339\\ntrue positive rate (TPR), 283, 292\\ntrue positives/true negatives, 281\\ntypographical conventions, ix\\nU'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 388, 'page_label': '375', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='selecting, 235\\nunivariate nonlinear, 232-236\\nunsupervised, 131\\ntree module, 76\\ntrigrams, 339\\ntrue positive rate (TPR), 283, 292\\ntrue positives/true negatives, 281\\ntypographical conventions, ix\\nU\\nuncertainty estimates\\napplications for, 119\\ndecision function, 120\\nin binary classification evaluation, 286-288\\nmulticlass classification, 124\\npredicting probabilities, 122\\nunderfitting, 28\\nunigrams, 340\\nunivariate nonlinear transformations, 232-236\\nunivariate statistics, 236\\nunsupervised learning, 131-209\\nalgorithms for\\nagglomerative clustering, 182-187\\nclustering, 168-207\\nDBSCAN, 187-190\\nk-means clustering, 168-181\\nmanifold learning with t-SNE, 163-168\\nnon-negative matrix factorization,\\n156-163\\noverview of, 3\\nprincipal component analysis, 140-155\\nIndex | 375'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 389, 'page_label': '376', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='challenges of, 132\\ndata representation, 4\\nexamples of, 3\\noverview of, 208\\nscaling and preprocessing for, 132-140\\ntypes of, 131\\nunsupervised transformations, 131\\nV\\nvalue_counts function, 214\\nvector quantization, 176\\nvocabulary building, 328\\nvoting, 36\\nvowpal wabbit, 362\\nW\\nwave dataset, 31\\nweak learners, 88\\nweights, 47, 106\\nwhitening option, 150\\nWisconsin Breast Cancer dataset, 32\\nword stems, 344\\nX\\nxgboost package, 91\\nxkcd Color Survey, 324\\n376 | Index'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 390, 'page_label': '377', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='About the Authors\\nAndreas Müller  received his PhD in machine learning from the University of Bonn.\\nAfter working as a machine learning researcher on computer vision applications at\\nAmazon for a year, he joined the Center for Data Science at New Y ork University. For\\nthe last four years, he has been a maintainer of and one of the core contributors to\\nscikit-learn, a machine learning toolkit widely used in industry and academia, and\\nhas authored and contributed to several other widely used machine learning pack‐\\nages. His mission is to create open tools to lower the barrier of entry for machine\\nlearning applications, promote reproducible science, and democratize the access to\\nhigh-quality machine learning algorithms.\\nSarah Guido is a data scientist who has spent a lot of time working in start-ups. She\\nloves Python, machine learning, large quantities of data, and the tech world. An\\naccomplished conference speaker, Sarah attended the University of Michigan for grad'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 390, 'page_label': '377', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='loves Python, machine learning, large quantities of data, and the tech world. An\\naccomplished conference speaker, Sarah attended the University of Michigan for grad\\nschool and currently resides in New Y ork City.\\nColophon\\nThe animal on the cover of Introduction to Machine Learning with Python  is a hell‐\\nbender salamander (Cryptobranchus alleganiensis), an amphibian native to the eastern\\nUnited States (ranging from New Y ork to Georgia). It has many colorful nicknames,\\nincluding “ Allegheny alligator, ” “snot otter, ” and “mud-devil. ” The origin of the name\\n“hellbender” is unclear: one theory is that early settlers found the salamander’s\\nappearance unsettling and supposed it to be a demonic creature trying to return to\\nhell.\\nThe hellbender salamander is a member of the giant salamander family, and can grow\\nas large as 29 inches long. This is the third-largest aquatic salamander species in the\\nworld. Their bodies are rather flat, with thick folds of skin along their sides. While'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 390, 'page_label': '377', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='as large as 29 inches long. This is the third-largest aquatic salamander species in the\\nworld. Their bodies are rather flat, with thick folds of skin along their sides. While\\nthey do have a single gill on each side of the neck, hellbenders largely rely on their\\nskin folds to breathe: gas flows in and out through capillaries near the surface of the\\nskin.\\nBecause of this, their ideal habitat is in clear, fast-moving, shallow streams, which\\nprovide plenty of oxygen. The hellbender shelters under rocks and hunts primarily by\\nsense of smell, though it is also able to detect vibrations in the water. Its diet is made\\nup of crayfish, small fish, and occasionally the eggs of its own species. The hellbender\\nis also a key member of its ecosystem as prey: predators include various fish, snakes,\\nand turtles.\\nHellbender salamander populations have decreased significantly in the last few deca‐\\ndes. Water quality is the largest issue, as their respiratory system makes them very'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 390, 'page_label': '377', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='and turtles.\\nHellbender salamander populations have decreased significantly in the last few deca‐\\ndes. Water quality is the largest issue, as their respiratory system makes them very\\nsensitive to polluted or murky water. An increase in agriculture and other human'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'author': 'Andreas C. Müller and Sarah Guido', 'title': 'Introduction to Machine Learning with Python', 'trapped': '/False', 'moddate': '2020-08-19T07:09:16+02:00', 'source': '..\\\\data\\\\pdf\\\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'total_pages': 392, 'page': 391, 'page_label': '378', 'source_file': 'Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf', 'file_type': 'pdf'}, page_content='activity near their habitat means greater amounts of sediment and chemicals in the\\nwater. In an effort to save this endangered species, biologists have begun to raise the\\namphibians in captivity and release them when they reach a less vulnerable age.\\nMany of the animals on O’Reilly covers are endangered; all of them are important to\\nthe world. To learn more about how you can help, go to animals.oreilly.com.\\nThe cover image is from Wood’s Animate Creation. The cover fonts are URW Type‐\\nwriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font is\\nAdobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 1, 'page_label': '2', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='About Pearson\\nPearson is the world’s learning company, with presence across\\n70 countries worldwide. Our unique insights and world-class\\nexpertise comes from a long history of working closely with\\nrenowned teachers, authors and thought leaders, as a result of\\nwhich, we have emerged as the preferred choice for millions of\\nteachers and learners across the world.\\nWe believe learning opens up opportunities, creates fulfilling\\ncareers and hence better lives. We hence collaborate with the\\nbest of minds to deliver you class-leading products, spread\\nacross the Higher Education and K12 spectrum.\\nSuperior learning experience and improved outcomes are at\\nthe heart of everything we do. This product is the result of one\\nsuch effort.\\nYour feedback plays a critical role in the evolution of our\\nproducts and you can contact us at - reachus@pearson.com.\\nWe look forward to it.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 2, 'page_label': '3', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='Machine Learning'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 3, 'page_label': '4', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='Machine Learning\\n \\n \\nSaikat Dutt \\nDirector \\nCognizant Technology Solutions\\nSubramanian Chandramouli \\nAssociate Director \\nCognizant Technology Solutions\\nAmit Kumar Das \\nAssistant Professor \\nInstitute of Engineering & Management'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 4, 'page_label': '5', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='This book is dedicated to the people without whom the dream\\ncould not have come true – my parents, Tarun Kumar Dutt and\\nSrilekha Dutt; constant inspiration and support from my wife,\\nAdity and unconditional love of my sons Deepro and Devarko.\\nSaikat Dutt\\nMy sincerest thanks and appreciation go to several people…\\nMy parents Subramanian and Lalitha\\nMy wife Ramya\\nMy son Shri Krishna\\nMy daughter Shri Siva Ranjani\\nAnd my colleagues and friends\\nS. Chandramouli\\nMy humble gratitude goes to -'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 5, 'page_label': '6', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='My parents Mrs Juthika Das and Dr Ranajit Kumar Das\\nMy wife Arpana and two lovely daughters Ashmita and Ankita\\nMy academic collaborator, mentor and brother — Dr Saptarsi\\nGoswami and Mr  Mrityunjoy Panday\\nAmit Kumar Das'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 6, 'page_label': '7', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='Contents\\nPreface\\nAcknowledgements\\nAbout the Authors\\nModel Syllabus for Machine Learning\\nLesson plan\\n1 Introduction to Machine Learning\\n1.1 Introduction\\n1.2 What is Human Learning?\\n1.3 Types of Human Learning\\n1.3.1 Learning under expert guidance\\n1.3.2 Learning guided by knowledge gained from experts\\n1.3.3 Learning by self\\n1.4 What is Machine Learning?\\n1.4.1 How do machines learn?\\n1.4.2 Well-posed learning problem\\n1.5 Types of Machine Learning\\n1.5.1 Supervised learning\\n1.5.2 Unsupervised learning\\n1.5.3 Reinforcement learning\\n1.5.4 Comparison – supervised, unsupervised, and reinforcement learning'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 7, 'page_label': '8', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='1.6 Problems Not To Be Solved Using Machine Learning\\n1.7 Applications of Machine Learning\\n1.7.1 Banking and finance\\n1.7.2 Insurance\\n1.7.3 Healthcare\\n1.8 State-of-The-Art Languages/Tools In Machine Learning\\n1.8.1 Python\\n1.8.2 R\\n1.8.3 Matlab\\n1.8.4 SAS\\n1.8.5 Other languages/tools\\n1.9 Issues in Machine Learning\\n1.10 Summary\\n2 Preparing to Model\\n2.1 Introduction\\n2.2 Machine Learning Activities\\n2.3 Basic Types of Data in Machine Learning\\n2.4 Exploring Structure of Data\\n2.4.1 Exploring numerical data\\n2.4.2 Plotting and exploring numerical data\\n2.4.3 Exploring categorical data\\n2.4.4 Exploring relationship between variables\\n2.5 Data Quality and Remediation\\n2.5.1 Data quality\\n2.5.2 Data remediation'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 8, 'page_label': '9', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='2.6 Data Pre-Processing\\n2.6.1 Dimensionality reduction\\n2.6.2 Feature subset selection\\n2.7 Summary\\n3 Modelling and Evaluation\\n3.1 Introduction\\n3.2 Selecting a Model\\n3.2.1 Predictive models\\n3.2.2 Descriptive models\\n3.3 Training a Model (for Supervised Learning)\\n3.3.1 Holdout method\\n3.3.2 K-fold Cross-validation method\\n3.3.3 Bootstrap sampling\\n3.3.4 Lazy vs. Eager learner\\n3.4 Model Representation and Interpretability\\n3.4.1 Underfitting\\n3.4.2 Overfitting\\n3.4.3 Bias – variance trade-off\\n3.5 Evaluating Performance of a Model\\n3.5.1 Supervised learning – classification\\n3.5.2 Supervised learning – regression\\n3.5.3 Unsupervised learning – clustering\\n3.6 Improving Performance of a Model\\n3.7 Summary\\n4 Basics of Feature Engineering'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 9, 'page_label': '10', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='4.1 Introduction\\n4.1.1 What is a feature?\\n4.1.2 What is feature engineering?\\n4.2 Feature Transformation\\n4.2.1 Feature construction\\n4.2.2 Feature extraction\\n4.3 Feature Subset Selection\\n4.3.1 Issues in high-dimensional data\\n4.3.2 Key drivers of feature selection – feature relevance and redundancy\\n4.3.3 Measures of feature relevance and redundancy\\n4.3.4 Overall feature selection process\\n4.3.5 Feature selection approaches\\n4.4 Summary\\n5 Brief Overview of Probability\\n5.1 Introduction\\n5.2 Importance of Statistical Tools in Machine Learning\\n5.3 Concept of Probability – Frequentist and Bayesian Interpretation\\n5.3.1 A brief review of probability theory\\n5.4 Random Variables\\n5.4.1 Discrete random variables\\n5.4.2 Continuous random variables\\n5.5 Some Common Discrete Distributions\\n5.5.1 Bernoulli distributions\\n5.5.2 Binomial distribution\\n5.5.3 The multinomial and multinoulli distributions'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 10, 'page_label': '11', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='5.5.4 Poisson distribution\\n5.6 Some Common Continuous Distributions\\n5.6.1 Uniform distribution\\n5.6.2 Gaussian (normal) distribution\\n5.6.3 The laplace distribution\\n5.7 Multiple Random Variables\\n5.7.1 Bivariate random variables\\n5.7.2 Joint distribution functions\\n5.7.3 Joint probability mass functions\\n5.7.4 Joint probability density functions\\n5.7.5 Conditional distributions\\n5.7.6 Covariance and correlation\\n5.8 Central Limit Theorem\\n5.9 Sampling Distributions\\n5.9.1 Sampling with replacement\\n5.9.2 Sampling without replacement\\n5.9.3 Mean and variance of sample\\n5.10 Hypothesis Testing\\n5.11 Monte Carlo Approximation\\n5.12 Summary\\n6 Bayesian Concept Learning\\n6.1 Introduction\\n6.2 Why Bayesian Methods are Important?\\n6.3 Bayes’ Theorem\\n6.3.1 Prior'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 11, 'page_label': '12', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='6.3.2 Posterior\\n6.3.3 Likelihood\\n6.4 Bayes’ Theorem and Concept Learning\\n6.4.1 Brute-force Bayesian algorithm\\n6.4.2 Concept of consistent learners\\n6.4.3 Bayes optimal classifier\\n6.4.4 Naïve Bayes classifier\\n6.4.5 Applications of Naïve Bayes classifier\\n6.4.6 Handling Continuous Numeric Features in Naïve Bayes Classifier\\n6.5 Bayesian Belief Network\\n6.5.1 Independence and conditional independence\\n6.5.2 Use of the Bayesian Belief network in machine learning\\n6.6 Summary\\n7 Supervised Learning : Classification\\n7.1 Introduction\\n7.2 Example of Supervised Learning\\n7.3 Classification Model\\n7.4 Classification Learning Steps\\n7.5 Common Classification Algorithms\\n7.5.1 k-Nearest Neighbour (kNN)\\n7.5.2 Decision tree\\n7.5.3 Random forest model\\n7.5.4 Support vector machines\\n7.6 Summary\\n8 Super vised Learning : Regression'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 12, 'page_label': '13', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='8.1 Introduction\\n8.2 Example of Regression\\n8.3 Common Regression Algorithms\\n8.3.1 Simple linear regression\\n8.3.2 Multiple linear regression\\n8.3.3 Assumptions in Regression Analysis\\n8.3.4 Main Problems in Regression Analysis\\n8.3.5 Improving Accuracy of the Linear Regression Model\\n8.3.6 Polynomial Regression Model\\n8.3.7 Logistic Regression\\n8.3.8 Maximum Likelihood Estimation\\n8.4 Summary\\n9 Unsupervised Learning\\n9.1 Introduction\\n9.2 Unsupervised vs Supervised Learning\\n9.3 Application of Unsupervised Learning\\n9.4 Clustering\\n9.4.1 Clustering as a machine learning task\\n9.4.2 Different types of clustering techniques\\n9.4.3 Partitioning methods\\n9.4.4 K-Medoids: a representative object-based technique\\n9.4.5 Hierarchical clustering\\n9.4.6 Density-based methods - DBSCAN\\n9.5 Finding Pattern using Association Rule\\n9.5.1 Definition of common terms'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 13, 'page_label': '14', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='9.5.2 Association rule\\n9.5.3 The apriori algorithm for association rule learning\\n9.5.4 Build the apriori principle rules\\n9.6 Summary\\n10 Basics of Neural Network\\n10.1 Introduction\\n10.2 Understanding the Biological Neuron\\n10.3 Exploring the Artificial Neuron\\n10.4 Types of Activation Functions\\n10.4.1 Identity function\\n10.4.2 Threshold/step function\\n10.4.3 ReLU (Rectified Linear Unit) function\\n10.4.4 Sigmoid function\\n10.4.5 Hyperbolic tangent function\\n10.5 Early Implementations of ANN\\n10.5.1 McCulloch–Pitts model of neuron\\n10.5.2 Rosenblatt’s perceptron\\n10.5.3 ADALINE network model\\n10.6 Architectures of Neural Network\\n10.6.1 Single-layer feed forward network\\n10.6.2 Multi-layer feed forward ANNs\\n10.6.3 Competitive network\\n10.6.4 Recurrent network\\n10.7 Learning Process in ANN\\n10.7.1 Number of layers'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 14, 'page_label': '15', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='10.7.2 Direction of signal flow\\n10.7.3 Number of nodes in layers\\n10.7.4 Weight of interconnection between neurons\\n10.8 Backpropagation\\n10.9 Deep Learning\\n10.10 Summary\\n11 Other Types of Learning\\n11.1 Introduction\\n11.2 Representation Learning\\n11.2.1 Supervised neural networks and multilayer perceptron\\n11.2.2 Independent component analysis (Unsupervised)\\n11.2.3 Autoencoders\\n11.2.4 Various forms of clustering\\n11.3 Active Learning\\n11.3.1 Heuristics for active learning\\n11.3.2 Active learning query strategies\\n11.4 Instance-Based Learning (Memory-based Learning)\\n11.4.1 Radial basis function\\n11.4.2 Pros and cons of instance-based learning method\\n11.5 Association Rule Learning Algorithm\\n11.5.1 Apriori algorithm\\n11.5.2 Eclat algorithm\\n11.6 Ensemble Learning Algorithm\\n11.6.1 Bootstrap aggregation (Bagging)\\n11.6.2 Boosting'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 15, 'page_label': '16', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='11.6.3 Gradient boosting machines (GBM)\\n11.7 Regularization Algorithm\\n11.8 Summary\\nAppendix A: Programming Machine Learning in R\\nAppendix B: Programming Machine Learning in Python\\nAppendix C: A Case Study on Machine Learning Application: Grouping\\nSimilar Service Requests and Classifying a New One\\nModel Question Paper-1\\nModel Question Paper-2\\nModel Question Paper-3\\nIndex'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 16, 'page_label': '17', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='Preface\\nRepeated requests from Computer Science and IT engineering\\nstudents who are the readers of our previous books encouraged\\nus to write this book on machine learning. The concept of\\nmachine learning and the huge potential of its application is\\nstill niche knowledge and not so well-spread among the\\nstudent community. So, we thought of writing this book\\nspecifically for techies, college students, and junior managers\\nso that they understood the machine learning concepts easily.\\nThey should not only use the machine learning software\\npackages, but understand the concepts behind those packages.\\nThe application of machine learning is getting boosted day by\\nday. From recommending products to the buyers, to predicting\\nthe future real estate market, to helping medical practitioners\\nin diagnosis, to optimizing energy consumption, thus helping\\nthe cause of Green Earth, machine learning is finding its utility\\nin every sphere of life.\\nDue care was taken to write this book in simple English and'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 16, 'page_label': '17', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='the cause of Green Earth, machine learning is finding its utility\\nin every sphere of life.\\nDue care was taken to write this book in simple English and\\nto present the machine learning concepts in an easily\\nunderstandable way which can be used as a textbook for both\\ngraduate and advanced undergraduate classes in machine\\nlearning or as a reference text.\\nWhom Is This Book For?\\nReaders of this book will gain a thorough understanding of\\nmachine learning concepts. Not only students but also'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 17, 'page_label': '18', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='software professionals will find a variety of techniques with\\nsufficient discussions in this book that cater to the needs of the\\nprofessional environments. Technical managers will get an\\ninsight into weaving machine learning into the overall\\nsoftware engineering process. Students, developers, and\\ntechnical managers with a basic background in computer\\nscience will find the material in this book easily readable.\\nHow This Book Is Organized?\\nEach chapter starts with an introductory paragraph, which\\ngives overview of the chapter along with a chapter-coverage\\ntable listing out the topics covered in the chapter. Sample\\nquestions at the end of the chapter helps the students to\\nprepare for the examination. Discussion points and Points to\\nPonder given inside the chapters help to clarify and understand\\nthe chapters easily and also explore the thinking ability of the\\nstudents and professionals.\\nA recap summary is given at the end of each chapter for'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 17, 'page_label': '18', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='the chapters easily and also explore the thinking ability of the\\nstudents and professionals.\\nA recap summary is given at the end of each chapter for\\nquick review of the topics. Throughout this book, you will see\\nmany exercises and discussion questions. Please don’t skip\\nthese – they are important in order to understand the machine\\nlearning concepts fully.\\nThis book starts with an introduction to Machine Learning\\nwhich lays the theoretical foundation for the remaining\\nchapters. Modelling, Feature engineering, and basic\\nprobability are discussed as chapters before entering into the\\nworld of machine learning which helps to grip the machine\\nlearning concepts easily at a later point of time.\\nBonus topics of machine learning exercise with multiple\\nexamples are discussed in Machine learning language of R &'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 18, 'page_label': '19', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='Python. Appendix A discusses Programming Machine\\nLearning in R and Appendix B discusses Programming\\nMachine Learning in Python.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 19, 'page_label': '20', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='Acknowledgements\\nWe are grateful to Pearson Education, who came forward to\\npublish this book. Ms. Neha Goomer and Mr. Purushothaman\\nChandrasekaran of Pearson Education were always kind and\\nunderstanding. Mr. Purushothaman reviewed this book with\\nabundant patience and helped us say what we had wanted to,\\nimprovising each and every page of this book with care. Their\\npatience and guidance were invaluable. Thank you very much\\nNeha and Puru.\\n–All authors\\nThe journey through traditional Project Management, Agile\\nProject Management, Program and Portfolio Management\\nalong with use of artificial intelligence and machine learning\\nin the field, has been very rewarding, as it has given us the\\nopportunity to work for some of the best organizations in the\\nworld and learn from some of the best minds. Along the way,\\nseveral individuals have been instrumental in providing us\\nwith the guidance, opportunities and insights we needed to\\nexcel in this field. We wish to personally thank Mr. Rajesh'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 19, 'page_label': '20', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='several individuals have been instrumental in providing us\\nwith the guidance, opportunities and insights we needed to\\nexcel in this field. We wish to personally thank Mr. Rajesh\\nBalaji Ramachandran, Senior Vice-president, Cognizant\\nTechnology Solutions and Mr. Pradeep Shilige, Executive Vice\\nPresident, Cognizant Technology Solutions; Mr. Alexis\\nSamuel, Senior Vice-president, Cognizant Technology\\nSolutions and Mr.Hariharan Mathrubutham,Vice-president,\\nCognizant Technology Solutions and Mr. Krishna Prasad\\nYerramilli, Assistant Vice-president, Cognizant Technology'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 20, 'page_label': '21', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='Solutions for their inspiration and help in creating this book.\\nThey have immensely contributed to improve our skills.\\n–Saikat and Chandramouli\\nThis book wouldn’t have been possible without the constant\\ninspiration and support of my lovely wife Adity. My parents\\nhave always been enthusiastic about this project and provided\\nme continuous guidance at every necessary step. The\\nunconditional love and affection my sons – Deepro and\\nDevarko ushered on me constantly provided me the motivation\\nto work hard on this crucial project.\\nThis book is the culmination of all the learning that I gained\\nfrom many highly reputed professionals in the industry. I was\\nfortunate to work with them and gained knowledge from them\\nwhich helped me molding my professional career. Prof.\\nIndranil Bose and Prof. Bodhibrata Nag from IIM Calcutta\\nguided me enormously in different aspects of life and career.\\nMy heartfelt thanks go to all the wonderful people who\\ncontributed in many ways in conceptualizing this book and'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 20, 'page_label': '21', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='guided me enormously in different aspects of life and career.\\nMy heartfelt thanks go to all the wonderful people who\\ncontributed in many ways in conceptualizing this book and\\nwished success of this project.\\n–Saikat Dutt\\nThis book is the result of all the learning I have gained from\\nmany highly reputed professionals in the industry. I was\\nfortunate to work with them and in the process, acquire\\nknowledge that helped me in molding my professional career. I\\nthank Mr. Chandra Sekaran, Group Chief Executive,Tech and\\nOps, Cognizant, for his continuous encouragement and\\nunabated passion for strategic value creation, whose advice\\nwas invaluable for working on this book. I am obliged to Mr.\\nChandrasekar, Ex CIO, Standard Chartered Bank, for'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 21, 'page_label': '22', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='demonstrating how the lives, thoughts and feelings of others in\\nprofessional life are to be valued. He is a wonderful and\\ncheerful man who inspired me and gave me a lot of\\nencouragement when he launched my first book, Virtual\\nProject Management Office.\\nMs. Meena Karthikeyan, Vice-president, Cognizant\\nTechnology Solutions; Ms. Kalyani Sekhar, Assistant Vice-\\npresident, Cognizant Technology Solutions and Mr.\\nBalasubramanian Narayanan, Senior Director, Cognizant\\nTechnology Solutions guided me enormously in different\\naspects of professional life.\\nMy parents (Mr. Subramanian and Ms. Lalitha) have always\\nbeen enthusiastic about this project. Their unconditional love\\nand affection provided the much-needed moral support. My\\nson, Shri Krishna, and daughter, Shri Siva Ranjani, constantly\\nadded impetus to my motivation to work hard.\\nThis book would not have been possible without the\\nconstant inspiration and support of my wife, Ramya. She was\\nunfailingly supportive and encouraging during the long'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 21, 'page_label': '22', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='This book would not have been possible without the\\nconstant inspiration and support of my wife, Ramya. She was\\nunfailingly supportive and encouraging during the long\\nmonths that I had spent glued to my laptop while writing this\\nbook. Last and not the least, I beg forgiveness of all those who\\nhave been with me over the course of the years and whose\\nnames I have failed to mention here.\\n–S. Chandramouli\\nFirst of all, I would like to thank the Almighty for everything.\\nIt is the constant support and encouragement from my family\\nwhich made it possible for me to put my heart and soul in my\\nfirst authoring venture. My parents have always been my role\\nmodel, my wife a source of constant strength and my'),\n",
       " Document(metadata={'producer': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creator': 'calibre (4.99.4) [http://calibre-ebook.com]', 'creationdate': '2021-05-28T18:47:48+00:00', 'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli', 'moddate': '2021-05-28T18:47:48+00:00', 'title': 'Machine Learning', 'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf', 'total_pages': 741, 'page': 22, 'page_label': '23', 'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf', 'file_type': 'pdf'}, page_content='daughters are my happiness. Without my family, I wouldn’t\\nhave got the luxury of time to spend on hours in writing the\\nbook. Any amount of thanks will fell short to express my\\ngratitude and pleasure of being part of the family.\\nI would also thank the duo who have been my academic\\ncollaborator, mentor and brother - Dr. Saptarsi Goswami and\\nMr. Mrityunjoy Panday - without them I would be so\\nincomplete. My deep respects for my research guides and\\nmentors Amlan sir and Basabi madam for the knowledge and\\nsupport that I’ve been privileged to receive from them.\\nMy sincere gratitude to my mentors in my past organization,\\nCognizant Technology Solutions, Mr. Rajarshi Chatterjee, Mr.\\nManoj Paul, Mr. Debapriya Dasgupta and Mr. Balaji\\nVenkatesan for the valuable learning that I received from them.\\nI would thank all my colleagues at Institute of Engineering &\\nManagement, who have given me relentless support and\\nencouragement.\\nLast, but not the least, my students who are always a source'),\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NEW\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "### Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs\n",
    "\n",
    "\n",
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f36314a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding and vector DB\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List,Any,Tuple,Dict\n",
    "import uuid\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a0f1553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x2f03fc81a60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd3f111",
   "metadata": {},
   "source": [
    "## Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aae22b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 6282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x2f03e1307d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c20caada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 2094 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 66/66 [00:04<00:00, 14.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (2094, 384)\n",
      "Adding 2094 documents to vector store...\n",
      "Successfully added 2094 documents to vector store\n",
      "Total documents in collection: 8376\n"
     ]
    }
   ],
   "source": [
    "embedings=embedding_manager.generate_embeddings([doc.page_content for doc in chunks])\n",
    "vectorstore.add_documents(chunks,embedings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f79dd11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d977b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'The choice of the model used to solve a specific learning problem is a human task'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 70.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_dca8b02e_2019',\n",
       "  'content': 'Model Question Paper-1\\n<< NAME OF THE INSTITUTE >>\\nDEPARTMENT OF << XXX >>\\n \\nFinal Semester Examination, August 2018\\nSUBJECT NAME: BASICS OF MACHINE\\nLEARNING\\nSUBJECT CODE: <XXX>\\nTIME: 3 HOUR\\nFULL MARKS: 80\\nPART A (Multiple Choice Type Questions)\\n10 X 1 = 10\\n1. Answer any ten questions.\\n1. A computer program is said to learn from __________ with\\nrespect to some class of tasks T and performance measure P, if its\\nperformance at tasks in T, as measured by P, improves with it.\\n1. Training\\n2. Experience\\n3. Database\\n4. Algorithm\\n2. This type of learning to be used when there is no idea about the\\nclass or label of a particular data\\n1. Supervised learning algorithm',\n",
       "  'metadata': {'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf',\n",
       "   'content_length': 664,\n",
       "   'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf',\n",
       "   'doc_index': 2019,\n",
       "   'page': 699,\n",
       "   'file_type': 'pdf'},\n",
       "  'similarity_score': 0.1644531488418579,\n",
       "  'distance': 0.8355468511581421,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_e650bbb6_2019',\n",
       "  'content': 'Model Question Paper-1\\n<< NAME OF THE INSTITUTE >>\\nDEPARTMENT OF << XXX >>\\n \\nFinal Semester Examination, August 2018\\nSUBJECT NAME: BASICS OF MACHINE\\nLEARNING\\nSUBJECT CODE: <XXX>\\nTIME: 3 HOUR\\nFULL MARKS: 80\\nPART A (Multiple Choice Type Questions)\\n10 X 1 = 10\\n1. Answer any ten questions.\\n1. A computer program is said to learn from __________ with\\nrespect to some class of tasks T and performance measure P, if its\\nperformance at tasks in T, as measured by P, improves with it.\\n1. Training\\n2. Experience\\n3. Database\\n4. Algorithm\\n2. This type of learning to be used when there is no idea about the\\nclass or label of a particular data\\n1. Supervised learning algorithm',\n",
       "  'metadata': {'page': 699,\n",
       "   'doc_index': 2019,\n",
       "   'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf',\n",
       "   'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 664},\n",
       "  'similarity_score': 0.1644531488418579,\n",
       "  'distance': 0.8355468511581421,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_735a0f30_2019',\n",
       "  'content': 'Model Question Paper-1\\n<< NAME OF THE INSTITUTE >>\\nDEPARTMENT OF << XXX >>\\n \\nFinal Semester Examination, August 2018\\nSUBJECT NAME: BASICS OF MACHINE\\nLEARNING\\nSUBJECT CODE: <XXX>\\nTIME: 3 HOUR\\nFULL MARKS: 80\\nPART A (Multiple Choice Type Questions)\\n10 X 1 = 10\\n1. Answer any ten questions.\\n1. A computer program is said to learn from __________ with\\nrespect to some class of tasks T and performance measure P, if its\\nperformance at tasks in T, as measured by P, improves with it.\\n1. Training\\n2. Experience\\n3. Database\\n4. Algorithm\\n2. This type of learning to be used when there is no idea about the\\nclass or label of a particular data\\n1. Supervised learning algorithm',\n",
       "  'metadata': {'doc_index': 2019,\n",
       "   'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf',\n",
       "   'content_length': 664,\n",
       "   'page': 699},\n",
       "  'similarity_score': 0.1644531488418579,\n",
       "  'distance': 0.8355468511581421,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_943c7c0d_2019',\n",
       "  'content': 'Model Question Paper-1\\n<< NAME OF THE INSTITUTE >>\\nDEPARTMENT OF << XXX >>\\n \\nFinal Semester Examination, August 2018\\nSUBJECT NAME: BASICS OF MACHINE\\nLEARNING\\nSUBJECT CODE: <XXX>\\nTIME: 3 HOUR\\nFULL MARKS: 80\\nPART A (Multiple Choice Type Questions)\\n10 X 1 = 10\\n1. Answer any ten questions.\\n1. A computer program is said to learn from __________ with\\nrespect to some class of tasks T and performance measure P, if its\\nperformance at tasks in T, as measured by P, improves with it.\\n1. Training\\n2. Experience\\n3. Database\\n4. Algorithm\\n2. This type of learning to be used when there is no idea about the\\nclass or label of a particular data\\n1. Supervised learning algorithm',\n",
       "  'metadata': {'author': 'Amit Kumar Das & Saikat Dutt & Subramanian Chandramouli',\n",
       "   'page': 699,\n",
       "   'file_type': 'pdf',\n",
       "   'creator': 'calibre (4.99.4) [http://calibre-ebook.com]',\n",
       "   'moddate': '2021-05-28T18:47:48+00:00',\n",
       "   'total_pages': 741,\n",
       "   'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf',\n",
       "   'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf',\n",
       "   'producer': 'calibre (4.99.4) [http://calibre-ebook.com]',\n",
       "   'doc_index': 2019,\n",
       "   'creationdate': '2021-05-28T18:47:48+00:00',\n",
       "   'page_label': '700',\n",
       "   'content_length': 664,\n",
       "   'title': 'Machine Learning'},\n",
       "  'similarity_score': 0.1644531488418579,\n",
       "  'distance': 0.8355468511581421,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_ebf38422_1188',\n",
       "  'content': 'to abstract feature-based concept map from the input data and\\ngeneralize the same in the form of a classification algorithm to\\ndecide whether a face in the gathering is potentially criminal\\nor not.\\nWhen we talk about the learning process, abstraction is a\\nsignificant step as it represents raw input data in a summarized\\nand structured format, such that a meaningful insight is\\nobtained from the data. This structured representation of raw\\ninput data to the meaningful pattern is called a model. The\\nmodel might have different forms. It might be a mathematical\\nequation, it might be a graph or tree structure, it might be a\\ncomputational block, etc. The decision regarding which model\\nis to be selected for a specific data set is taken by the learning\\ntask, based on the problem to be solved and the type of data.\\nFor example, when the problem is related to prediction and the\\ntarget field is numeric and continuous, the regression model is\\nassigned. The process of assigning a model, and fitting a',\n",
       "  'metadata': {'file_type': 'pdf',\n",
       "   'source': '..\\\\data\\\\pdf\\\\Machine Learning ( etc.) (z-lib.org).pdf',\n",
       "   'page': 133,\n",
       "   'source_file': 'Machine Learning ( etc.) (z-lib.org).pdf',\n",
       "   'doc_index': 1188,\n",
       "   'content_length': 998},\n",
       "  'similarity_score': 0.15465795993804932,\n",
       "  'distance': 0.8453420400619507,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"The choice of the model used to solve a specific learning problem is a human task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7972802b",
   "metadata": {},
   "source": [
    "## integration vectordb context pipeline with LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "275754c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "groq_api_key=os.getenv('groq_api_key')\n",
    "\n",
    "chatLLM = ChatGroq(groq_api_key=groq_api_key,model_name=\"openai/gpt-oss-120b\",temperature=0.1,max_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d54bd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_simple(query :str,retriever,llm,top_k=3):\n",
    "    results=retriever.retrieve(query,top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context :\n",
    "        return (\"no relevant context found\")\n",
    "\n",
    "    prompt = f\"\"\"Based on this context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response=llm.invoke(prompt.format(context,query))\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef401860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'what is meant by The choice of the model used to solve a specific learning problem is a human task'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 70.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**What the sentence means**\n",
      "\n",
      "> *“The choice of the model used to solve a specific learning problem is a human task.”*\n",
      "\n",
      "In machine‑learning projects the **model** is the mathematical or computational structure that will turn raw data into predictions (e.g., a linear regression equation, a decision‑tree, a convolutional neural network, etc.). Selecting which of those possible structures to use is **not done automatically by the data itself**; it is a decision that must be made by a person (or a team of people) who understands:\n",
      "\n",
      "| Aspect | Why a human must decide |\n",
      "|--------|--------------------------|\n",
      "| **Problem definition** | Different problems require different kinds of outputs (binary class, multi‑class, real‑valued score, ranking, etc.). A human translates the business or scientific goal into a learning task (classification, regression, clustering, …). |\n",
      "| **Data characteristics** | The size, dimensionality, noise level, missing‑value pattern, and type of features (images, text, tabular) heavily influence which models are feasible and likely to work well. |\n",
      "| **Domain knowledge** | Prior knowledge about the domain (e.g., facial‑recognition for security, medical diagnosis, fraud detection) can suggest useful inductive biases (e.g., spatial invariance → CNNs). |\n",
      "| **Computational constraints** | Available hardware, latency requirements, memory limits, and budget dictate whether a heavy model (deep net) or a lightweight one (logistic regression) is appropriate. |\n",
      "| **Interpretability needs** | In some applications (legal, medical) the model must be explainable; a human will favor transparent models (decision trees, rule‑based systems) over black‑box ones. |\n",
      "| **Regulatory / ethical considerations** | Laws or ethical guidelines may prohibit certain data uses or model types; a human must ensure compliance. |\n",
      "| **Performance trade‑offs** | Humans evaluate validation results, over‑/under‑fitting, bias‑variance balance, and decide whether to try a more complex model, add regularisation, or collect more data. |\n",
      "| **Risk tolerance** | For high‑stakes decisions (e.g., “potentially criminal” face detection) the cost of false positives vs. false negatives is a policy decision that influences model choice. |\n",
      "\n",
      "Because all of these factors involve judgment, experience, and often conflicting objectives, **model selection is a human‑driven activity**.\n",
      "\n",
      "---\n",
      "\n",
      "### How the human‑driven model‑selection process typically works\n",
      "\n",
      "1. **Define the learning task**  \n",
      "   - Is it binary classification (criminal / not‑criminal), multi‑class, regression, etc.?\n",
      "\n",
      "2. **Explore the data**  \n",
      "   - Visualise distributions, check class imbalance, assess feature types, detect outliers.\n",
      "\n",
      "3. **List candidate model families**  \n",
      "   - E.g., logistic regression, support‑vector machines, random forests, gradient‑boosted trees, convolutional neural networks, etc.\n",
      "\n",
      "4. **Apply domain constraints**  \n",
      "   - If real‑time inference on a low‑power device is required → prefer lightweight models.  \n",
      "   - If legal transparency is mandatory → prefer interpretable models.\n",
      "\n",
      "5. **Prototype & benchmark**  \n",
      "   - Train a few representative models on a validation split, record metrics (accuracy, precision, recall, ROC‑AUC, inference time, memory footprint).\n",
      "\n",
      "6. **Analyse trade‑offs**  \n",
      "   - Compare performance vs. interpretability vs. cost.  \n",
      "   - Use tools like confusion matrices to see how each model behaves on the “criminal” class.\n",
      "\n",
      "7. **Select the final model**  \n",
      "   - Choose the model that best satisfies the overall objective (e.g., highest recall while keeping false‑positive rate below a threshold).\n",
      "\n",
      "8. **Iterate**  \n",
      "   - If no model meets requirements, go back: collect more data, engineer new features, or try a different model family.\n",
      "\n",
      "---\n",
      "\n",
      "### Example: “Potentially criminal” face detection\n",
      "\n",
      "| Requirement | Human decision impact |\n",
      "|-------------|-----------------------|\n",
      "| **Output** | Binary label → classification model. |\n",
      "| **Data type** | Images of faces → models that handle spatial patterns (CNNs) are natural candidates. |\n",
      "| **Interpretability** | Law‑enforcement may need to justify a flag → consider adding a shallow decision‑\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"what is meant by The choice of the model used to solve a specific learning problem is a human task\",rag_retriever,chatLLM)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a4244ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'what is meant by The choice of the model used to solve a specific learning problem is a human task'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 62.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: It means that deciding **which kind of model (e.g., regression, decision‑tree, neural network, etc.) to apply to a given data set and problem is not done automatically by the learning algorithm itself; it is a judgment made by a human practitioner. The human examines the task (prediction vs. classification), the nature of the target variable (numeric, categorical), the characteristics of the data, performance requirements, interpretability needs, computational constraints, etc., and then selects and configures the appropriate model. In other words, model selection is a design decision that requires human expertise, not a built‑in step of the learning process.\n",
      "Sources: [{'source': 'Machine Learning ( etc.) (z-lib.org).pdf', 'page': 133, 'score': 0.18903273344039917, 'preview': 'to abstract feature-based concept map from the input data and\\ngeneralize the same in the form of a classification algorithm to\\ndecide whether a face in the gathering is potentially criminal\\nor not.\\nWhen we talk about the learning process, abstraction is a\\nsignificant step as it represents raw input ...'}, {'source': 'Machine Learning ( etc.) (z-lib.org).pdf', 'page': 133, 'score': 0.18903273344039917, 'preview': 'to abstract feature-based concept map from the input data and\\ngeneralize the same in the form of a classification algorithm to\\ndecide whether a face in the gathering is potentially criminal\\nor not.\\nWhen we talk about the learning process, abstraction is a\\nsignificant step as it represents raw input ...'}, {'source': 'Machine Learning ( etc.) (z-lib.org).pdf', 'page': 133, 'score': 0.18903273344039917, 'preview': 'to abstract feature-based concept map from the input data and\\ngeneralize the same in the form of a classification algorithm to\\ndecide whether a face in the gathering is potentially criminal\\nor not.\\nWhen we talk about the learning process, abstraction is a\\nsignificant step as it represents raw input ...'}]\n",
      "Confidence: 0.18903273344039917\n",
      "Context Preview: to abstract feature-based concept map from the input data and\n",
      "generalize the same in the form of a classification algorithm to\n",
      "decide whether a face in the gathering is potentially criminal\n",
      "or not.\n",
      "When we talk about the learning process, abstraction is a\n",
      "significant step as it represents raw input \n"
     ]
    }
   ],
   "source": [
    "# --- Enhanced RAG Pipeline Features ---\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag_advanced(\"what is meant by The choice of the model used to solve a specific learning problem is a human task\", rag_retriever,chatLLM, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359e800c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
